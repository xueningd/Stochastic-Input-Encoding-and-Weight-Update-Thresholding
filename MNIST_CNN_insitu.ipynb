{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4bbb5f0d4b64394b2892484ce9a402a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_def9c63f3e6448bfb47d3bdc6d3e3c6e",
              "IPY_MODEL_fa812defd57b4de1866da66f91dfa592",
              "IPY_MODEL_afdf6f76c5564f8f9197c4be8fc50f6b"
            ],
            "layout": "IPY_MODEL_0f86802ef648423da655a31de04e4881"
          }
        },
        "def9c63f3e6448bfb47d3bdc6d3e3c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45296200417f43ffaad29860013b6b67",
            "placeholder": "​",
            "style": "IPY_MODEL_b88a1ebc48da48ae833a81f19586984e",
            "value": "100%"
          }
        },
        "fa812defd57b4de1866da66f91dfa592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d53a24ac8cb43f0a27f28739594ea85",
            "max": 9912422,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f5c9222836c468fa369752050aa94ed",
            "value": 9912422
          }
        },
        "afdf6f76c5564f8f9197c4be8fc50f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a61acc4bdbc40ef91b6ad7391215085",
            "placeholder": "​",
            "style": "IPY_MODEL_69d412bf61fb44a79687c6491c33089c",
            "value": " 9912422/9912422 [00:00&lt;00:00, 31683820.84it/s]"
          }
        },
        "0f86802ef648423da655a31de04e4881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45296200417f43ffaad29860013b6b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b88a1ebc48da48ae833a81f19586984e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d53a24ac8cb43f0a27f28739594ea85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5c9222836c468fa369752050aa94ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a61acc4bdbc40ef91b6ad7391215085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d412bf61fb44a79687c6491c33089c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "749307ed87f247768e0eb3a00024a21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6f0a62c28e44b44a01723f1055f5b5d",
              "IPY_MODEL_e5cec915a9cb4ebd8279ec02b2678268",
              "IPY_MODEL_a0349ecadcd94f1ab8254654b9971200"
            ],
            "layout": "IPY_MODEL_68c221dae9214708a64f6571f50cb3a8"
          }
        },
        "d6f0a62c28e44b44a01723f1055f5b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a601569bb542f3b4f026a1bb857921",
            "placeholder": "​",
            "style": "IPY_MODEL_cc6a0891d5844b13854a369b27a9bbd7",
            "value": "100%"
          }
        },
        "e5cec915a9cb4ebd8279ec02b2678268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5660662e95db4606a4c2771d2f479801",
            "max": 28881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4d613c74b5e4d78b7d094d8d62cfc2e",
            "value": 28881
          }
        },
        "a0349ecadcd94f1ab8254654b9971200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838b7779a6b04f7494c71a87220a4ce9",
            "placeholder": "​",
            "style": "IPY_MODEL_ce6663366828466bae40bc875c3f7bdf",
            "value": " 28881/28881 [00:00&lt;00:00, 141546.24it/s]"
          }
        },
        "68c221dae9214708a64f6571f50cb3a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a601569bb542f3b4f026a1bb857921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6a0891d5844b13854a369b27a9bbd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5660662e95db4606a4c2771d2f479801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d613c74b5e4d78b7d094d8d62cfc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "838b7779a6b04f7494c71a87220a4ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce6663366828466bae40bc875c3f7bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10cf2fbe98d8496eb24c85c178ff4678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a23c7d5c65684609b7cfb6b79ec71a20",
              "IPY_MODEL_5dd8da40f40e483f8eead93d7851dc7a",
              "IPY_MODEL_23caa1b373d5498b8b7891d880a68a26"
            ],
            "layout": "IPY_MODEL_89429efde3ed423d9c61eddbdbf0f9b6"
          }
        },
        "a23c7d5c65684609b7cfb6b79ec71a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cee94d2971844d99fa2c9ea4d1ec9f6",
            "placeholder": "​",
            "style": "IPY_MODEL_5da3c6fd86b84cfb9698d3aad097f316",
            "value": "100%"
          }
        },
        "5dd8da40f40e483f8eead93d7851dc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b0269607935412c9280c20fd536f6d1",
            "max": 1648877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb8b2b62ae36400f97a6f5dd270aa1eb",
            "value": 1648877
          }
        },
        "23caa1b373d5498b8b7891d880a68a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35db9c3f4b31431bbfeee09714f66111",
            "placeholder": "​",
            "style": "IPY_MODEL_3edabe6d35d64014ab044ba975a01839",
            "value": " 1648877/1648877 [00:00&lt;00:00, 165841.86it/s]"
          }
        },
        "89429efde3ed423d9c61eddbdbf0f9b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cee94d2971844d99fa2c9ea4d1ec9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da3c6fd86b84cfb9698d3aad097f316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b0269607935412c9280c20fd536f6d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8b2b62ae36400f97a6f5dd270aa1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35db9c3f4b31431bbfeee09714f66111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3edabe6d35d64014ab044ba975a01839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed1d003e9bff401ab672e885bdaa2d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_477c3136d962455cbcefa9348c26afa2",
              "IPY_MODEL_034d71ec7b7140cb9315f521d2a20b85",
              "IPY_MODEL_5f7435797bd94338b61c65417051740e"
            ],
            "layout": "IPY_MODEL_31103c83794a4dda8342c70300503f40"
          }
        },
        "477c3136d962455cbcefa9348c26afa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81da6e9593b74090b8d1a97d11572dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_3e22e075db024b6395ad6bd0a17a50fe",
            "value": "100%"
          }
        },
        "034d71ec7b7140cb9315f521d2a20b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_790e08e83d794663ac6c3e10096ea2f3",
            "max": 4542,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_878a5dcc66b54f2f87dab1e8648ed24b",
            "value": 4542
          }
        },
        "5f7435797bd94338b61c65417051740e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_361dc7d323fa42aa81f77cb6e34a8a45",
            "placeholder": "​",
            "style": "IPY_MODEL_f9a13d13c96e458aa9428e9b8923b79e",
            "value": " 4542/4542 [00:00&lt;00:00, 23436.77it/s]"
          }
        },
        "31103c83794a4dda8342c70300503f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81da6e9593b74090b8d1a97d11572dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e22e075db024b6395ad6bd0a17a50fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "790e08e83d794663ac6c3e10096ea2f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878a5dcc66b54f2f87dab1e8648ed24b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "361dc7d323fa42aa81f77cb6e34a8a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a13d13c96e458aa9428e9b8923b79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST dataset with SGD/Thresholded update/Manhattan Learning\n",
        "\n",
        "All models have stochastic input encoding added directly to the crossbar model."
      ],
      "metadata": {
        "id": "JyVtXWe1L7Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Manhattan Optimizer"
      ],
      "metadata": {
        "id": "YO4l3Yp7ZAZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from typing import List, Optional\n",
        "import numpy as np\n",
        "\n",
        "count_list = []\n",
        "last_layer = []\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class ManhattanSGD(Optimizer):\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, *, maximize=False, foreach: Optional[bool] = None):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        maximize=maximize, foreach=foreach)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(ManhattanSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "            group.setdefault('maximize', False)\n",
        "            group.setdefault('foreach', None)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs the Manhattan Learning rule such that\n",
        "        \\Delta W(i,j) = sgn(\\Delta w(i,j))\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            d_p_list = []\n",
        "            momentum_buffer_list = []\n",
        "            has_sparse_grad = False\n",
        "\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        # buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                        #buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()#.mul_(group['lr'])\n",
        "                        buf = param_state['momentum_buffer'] =  torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device)\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer'].float()\n",
        "                        #buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                        #buf.add_(1 - dampening, torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device))\n",
        "                        #buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()#.mul_(group['lr'])\n",
        "                        buf = param_state['momentum_buffer'] =  torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                else:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "\n",
        "        return loss\n",
        ""
      ],
      "metadata": {
        "id": "aghI32fnZCfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST example in Pytorch\n"
      ],
      "metadata": {
        "id": "-rmbJSpAL-JQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff8nNDWPL4_w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net_ex(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net_ex, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 3)\n",
        "        self.conv2 = nn.Conv2d(16, 64, 3, 3)\n",
        "        self.fc1 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "e5IZ4sFZMUST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        #print(optimizer.loss_and_gradients(model_objective))\n",
        "        optimizer.step()\n",
        "        if batch_idx % 128 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "Ge34ryOiMend"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, prints = True):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    if prints:\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "dxPEpjdzMquC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNISTtransform = transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../datasets/', train=False, download=True,\n",
        "                               transform=MNISTtransform),\n",
        "                               batch_size=32, shuffle=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../datasets/', train=True, download=True,\n",
        "                               transform=MNISTtransform),\n",
        "                               batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423,
          "referenced_widgets": [
            "d4bbb5f0d4b64394b2892484ce9a402a",
            "def9c63f3e6448bfb47d3bdc6d3e3c6e",
            "fa812defd57b4de1866da66f91dfa592",
            "afdf6f76c5564f8f9197c4be8fc50f6b",
            "0f86802ef648423da655a31de04e4881",
            "45296200417f43ffaad29860013b6b67",
            "b88a1ebc48da48ae833a81f19586984e",
            "7d53a24ac8cb43f0a27f28739594ea85",
            "2f5c9222836c468fa369752050aa94ed",
            "4a61acc4bdbc40ef91b6ad7391215085",
            "69d412bf61fb44a79687c6491c33089c",
            "749307ed87f247768e0eb3a00024a21c",
            "d6f0a62c28e44b44a01723f1055f5b5d",
            "e5cec915a9cb4ebd8279ec02b2678268",
            "a0349ecadcd94f1ab8254654b9971200",
            "68c221dae9214708a64f6571f50cb3a8",
            "68a601569bb542f3b4f026a1bb857921",
            "cc6a0891d5844b13854a369b27a9bbd7",
            "5660662e95db4606a4c2771d2f479801",
            "a4d613c74b5e4d78b7d094d8d62cfc2e",
            "838b7779a6b04f7494c71a87220a4ce9",
            "ce6663366828466bae40bc875c3f7bdf",
            "10cf2fbe98d8496eb24c85c178ff4678",
            "a23c7d5c65684609b7cfb6b79ec71a20",
            "5dd8da40f40e483f8eead93d7851dc7a",
            "23caa1b373d5498b8b7891d880a68a26",
            "89429efde3ed423d9c61eddbdbf0f9b6",
            "7cee94d2971844d99fa2c9ea4d1ec9f6",
            "5da3c6fd86b84cfb9698d3aad097f316",
            "7b0269607935412c9280c20fd536f6d1",
            "eb8b2b62ae36400f97a6f5dd270aa1eb",
            "35db9c3f4b31431bbfeee09714f66111",
            "3edabe6d35d64014ab044ba975a01839",
            "ed1d003e9bff401ab672e885bdaa2d5f",
            "477c3136d962455cbcefa9348c26afa2",
            "034d71ec7b7140cb9315f521d2a20b85",
            "5f7435797bd94338b61c65417051740e",
            "31103c83794a4dda8342c70300503f40",
            "81da6e9593b74090b8d1a97d11572dd1",
            "3e22e075db024b6395ad6bd0a17a50fe",
            "790e08e83d794663ac6c3e10096ea2f3",
            "878a5dcc66b54f2f87dab1e8648ed24b",
            "361dc7d323fa42aa81f77cb6e34a8a45",
            "f9a13d13c96e458aa9428e9b8923b79e"
          ]
        },
        "id": "_h2Dp1gANIOb",
        "outputId": "12425e76-1796-4ec2-8cb0-41cbdd49e9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4bbb5f0d4b64394b2892484ce9a402a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ../datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "749307ed87f247768e0eb3a00024a21c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ../datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10cf2fbe98d8496eb24c85c178ff4678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ../datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed1d003e9bff401ab672e885bdaa2d5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "torch.manual_seed(88)\n",
        "\n",
        "model = Net_ex().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "for epoch in range(1, 6):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo0E_iJxMyRe",
        "outputId": "4ea893dd-c843-428e-b0b1-4c4a1f67e831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308532\n",
            "Train Epoch: 1 [4096/60000 (7%)]\tLoss: 0.985617\n",
            "Train Epoch: 1 [8192/60000 (14%)]\tLoss: 0.412349\n",
            "Train Epoch: 1 [12288/60000 (20%)]\tLoss: 0.458174\n",
            "Train Epoch: 1 [16384/60000 (27%)]\tLoss: 0.279242\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.242156\n",
            "Train Epoch: 1 [24576/60000 (41%)]\tLoss: 0.214007\n",
            "Train Epoch: 1 [28672/60000 (48%)]\tLoss: 0.453503\n",
            "Train Epoch: 1 [32768/60000 (55%)]\tLoss: 0.324504\n",
            "Train Epoch: 1 [36864/60000 (61%)]\tLoss: 0.476829\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.802377\n",
            "Train Epoch: 1 [45056/60000 (75%)]\tLoss: 0.330615\n",
            "Train Epoch: 1 [49152/60000 (82%)]\tLoss: 0.173653\n",
            "Train Epoch: 1 [53248/60000 (89%)]\tLoss: 0.706552\n",
            "Train Epoch: 1 [57344/60000 (96%)]\tLoss: 0.341181\n",
            "\n",
            "Test set: Average loss: 0.2920, Accuracy: 9122/10000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.224121\n",
            "Train Epoch: 2 [4096/60000 (7%)]\tLoss: 0.407726\n",
            "Train Epoch: 2 [8192/60000 (14%)]\tLoss: 0.134190\n",
            "Train Epoch: 2 [12288/60000 (20%)]\tLoss: 0.218614\n",
            "Train Epoch: 2 [16384/60000 (27%)]\tLoss: 0.272133\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.484504\n",
            "Train Epoch: 2 [24576/60000 (41%)]\tLoss: 0.167690\n",
            "Train Epoch: 2 [28672/60000 (48%)]\tLoss: 0.056726\n",
            "Train Epoch: 2 [32768/60000 (55%)]\tLoss: 0.435958\n",
            "Train Epoch: 2 [36864/60000 (61%)]\tLoss: 0.291452\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.213017\n",
            "Train Epoch: 2 [45056/60000 (75%)]\tLoss: 0.229365\n",
            "Train Epoch: 2 [49152/60000 (82%)]\tLoss: 0.052578\n",
            "Train Epoch: 2 [53248/60000 (89%)]\tLoss: 0.064964\n",
            "Train Epoch: 2 [57344/60000 (96%)]\tLoss: 0.211970\n",
            "\n",
            "Test set: Average loss: 0.2400, Accuracy: 9259/10000 (93%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.214356\n",
            "Train Epoch: 3 [4096/60000 (7%)]\tLoss: 0.251316\n",
            "Train Epoch: 3 [8192/60000 (14%)]\tLoss: 0.221953\n",
            "Train Epoch: 3 [12288/60000 (20%)]\tLoss: 0.358003\n",
            "Train Epoch: 3 [16384/60000 (27%)]\tLoss: 0.116170\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.295101\n",
            "Train Epoch: 3 [24576/60000 (41%)]\tLoss: 0.245369\n",
            "Train Epoch: 3 [28672/60000 (48%)]\tLoss: 0.305786\n",
            "Train Epoch: 3 [32768/60000 (55%)]\tLoss: 0.204084\n",
            "Train Epoch: 3 [36864/60000 (61%)]\tLoss: 0.160635\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.180677\n",
            "Train Epoch: 3 [45056/60000 (75%)]\tLoss: 0.122689\n",
            "Train Epoch: 3 [49152/60000 (82%)]\tLoss: 0.103573\n",
            "Train Epoch: 3 [53248/60000 (89%)]\tLoss: 0.335761\n",
            "Train Epoch: 3 [57344/60000 (96%)]\tLoss: 0.404240\n",
            "\n",
            "Test set: Average loss: 0.2196, Accuracy: 9330/10000 (93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.169789\n",
            "Train Epoch: 4 [4096/60000 (7%)]\tLoss: 0.310259\n",
            "Train Epoch: 4 [8192/60000 (14%)]\tLoss: 0.092416\n",
            "Train Epoch: 4 [12288/60000 (20%)]\tLoss: 0.127011\n",
            "Train Epoch: 4 [16384/60000 (27%)]\tLoss: 0.210762\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.228584\n",
            "Train Epoch: 4 [24576/60000 (41%)]\tLoss: 0.078376\n",
            "Train Epoch: 4 [28672/60000 (48%)]\tLoss: 0.169593\n",
            "Train Epoch: 4 [32768/60000 (55%)]\tLoss: 0.637858\n",
            "Train Epoch: 4 [36864/60000 (61%)]\tLoss: 0.091392\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.874114\n",
            "Train Epoch: 4 [45056/60000 (75%)]\tLoss: 0.120673\n",
            "Train Epoch: 4 [49152/60000 (82%)]\tLoss: 0.252579\n",
            "Train Epoch: 4 [53248/60000 (89%)]\tLoss: 0.443696\n",
            "Train Epoch: 4 [57344/60000 (96%)]\tLoss: 0.077646\n",
            "\n",
            "Test set: Average loss: 0.2040, Accuracy: 9361/10000 (94%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.068494\n",
            "Train Epoch: 5 [4096/60000 (7%)]\tLoss: 0.063863\n",
            "Train Epoch: 5 [8192/60000 (14%)]\tLoss: 0.260646\n",
            "Train Epoch: 5 [12288/60000 (20%)]\tLoss: 0.217854\n",
            "Train Epoch: 5 [16384/60000 (27%)]\tLoss: 0.246574\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.096586\n",
            "Train Epoch: 5 [24576/60000 (41%)]\tLoss: 0.389024\n",
            "Train Epoch: 5 [28672/60000 (48%)]\tLoss: 0.194575\n",
            "Train Epoch: 5 [32768/60000 (55%)]\tLoss: 0.066100\n",
            "Train Epoch: 5 [36864/60000 (61%)]\tLoss: 0.102815\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.028883\n",
            "Train Epoch: 5 [45056/60000 (75%)]\tLoss: 0.199668\n",
            "Train Epoch: 5 [49152/60000 (82%)]\tLoss: 0.234112\n",
            "Train Epoch: 5 [53248/60000 (89%)]\tLoss: 0.197432\n",
            "Train Epoch: 5 [57344/60000 (96%)]\tLoss: 0.382447\n",
            "\n",
            "Test set: Average loss: 0.1844, Accuracy: 9451/10000 (95%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test(model, device, test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEUd9kWL_TLk",
        "outputId": "74525b0a-b3f5-48e3-930f-89a621e46479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1844, Accuracy: 9451/10000 (95%)\n",
            "\n",
            "94.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.fc1.weight.data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPWFgsk6_ffs",
        "outputId": "13299980-d608-4172-a036-0d35004d9013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "org_weight = copy.deepcopy(model.fc1.weight.data)\n",
        "org_acc = test(model, device, test_loader)\n",
        "\n",
        "results = []\n",
        "\n",
        "for i in range(model.fc1.weight.data.shape[0]):\n",
        "    res = []\n",
        "    for j in range(model.fc1.weight.data.shape[1]):\n",
        "        model.fc1.weight.data = copy.deepcopy(org_weight)\n",
        "        model.fc1.weight.data[i][j] = model.fc1.weight.data[i][j] + 1\n",
        "\n",
        "        acc_1 = test(model, device, test_loader, prints = False)\n",
        "\n",
        "        model.fc1.weight.data = copy.deepcopy(org_weight)\n",
        "        model.fc1.weight.data[i][j] = model.fc1.weight.data[i][j] - 1\n",
        "\n",
        "        acc_2 = test(model, device, test_loader, prints = False)\n",
        "        res.append(((acc_1 + acc_2)/2)/org_acc)\n",
        "    results.append(res)\n",
        "    print(res)\n",
        "print(results)\n",
        "\n",
        "ax = sns.heatmap(results, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "oaQfB7Lw_wyL",
        "outputId": "243682a8-c1fb-4328-feff-985c79ef319e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1844, Accuracy: 9451/10000 (95%)\n",
            "\n",
            "[0.9993651465453391, 0.9982541529996825, 0.9994180509998942, 0.9982012485451275, 0.9995767643635594, 0.9970373505449157, 0.9911649560893027, 0.9977780129086868, 0.9952915035445984, 0.9915881917257433, 0.9993651465453391, 0.9967728282721404, 0.9976722039995767, 0.9963495926356999, 0.9964554015448102, 0.9952385990900433, 0.9916940006348534, 0.994286318908052, 0.9956618347264838, 0.9989948153634535, 0.997883821817797, 0.9929108030896201, 0.9991535287271188, 0.9988361019997883, 0.9939688921807215, 0.9944979367262724, 0.997989630726907, 0.9941276055443867, 0.9851338482700241, 0.9971960639085811, 0.997989630726907, 0.9965612104539202, 0.9998412866363348, 0.9957147391810389, 0.9943921278171622, 0.9934398476351708, 0.9977780129086868, 0.9939159877261665, 0.998518675272458, 0.9869855041794519, 0.9997354777272246, 0.9974076817268014, 0.998677388636123, 0.9998412866363348, 0.9928578986350651, 0.9927520897259547, 0.9993651465453391, 0.9979367262723521, 0.9990477198180087, 0.9960321659083693, 0.9977251084541318, 0.9908475293619723, 0.9949211723627129, 0.9992593376362289, 0.9941805099989419, 0.9977251084541318, 0.9927520897259547, 0.9972489683631361, 0.9971960639085811, 0.9845518992699186, 0.993598560998836, 0.9997883821817796, 0.9961379748174796, 0.9991006242725636]\n",
            "[0.9999999999999999, 0.9994180509998941, 0.9995238599090043, 0.9979367262723521, 0.998677388636123, 0.9995238599090043, 0.9947095545444925, 0.997989630726907, 0.9994180509998941, 0.9994180509998941, 0.9996296688181143, 0.9989948153634535, 0.9996825732726695, 0.9964024970902549, 0.9995238599090043, 0.9961908792720348, 0.9975663950904666, 0.998518675272458, 0.9983070574542375, 0.997883821817797, 0.9994709554544493, 0.9987302930906782, 0.9997354777272246, 0.9985715797270129, 0.9981483440905724, 0.9968257327266956, 0.9998412866363348, 0.9990477198180087, 0.9969844460903607, 0.9994709554544493, 0.9993651465453391, 0.999312242090784, 0.9994180509998942, 0.9960321659083693, 0.998624484181568, 0.9987831975452334, 0.9991535287271188, 0.9991006242725639, 0.999312242090784, 0.9968257327266956, 0.9990477198180087, 0.997989630726907, 0.9974076817268014, 0.9999470955454448, 0.999312242090784, 0.9983599619087925, 0.9997883821817798, 0.9966670193630303, 0.9960321659083693, 0.9959263569992592, 0.9966670193630303, 0.9802137339964024, 0.998624484181568, 1.0002116178182203, 0.9985715797270129, 0.9990477198180087, 0.9989948153634536, 0.999312242090784, 0.999312242090784, 0.997883821817797, 0.9965612104539202, 0.9999470955454448, 0.9989948153634535, 1.000052904454555]\n",
            "[0.9991535287271188, 0.9982012485451275, 0.9969844460903607, 0.9896836313617605, 0.9992064331816739, 0.9976192995450216, 0.9803195429055126, 0.9933340387260606, 0.9989948153634536, 0.999312242090784, 0.9988361019997883, 0.9876732620886678, 0.9967728282721404, 0.9905301026346419, 0.9966141149084752, 0.9928578986350651, 0.9801079250872923, 0.9967199238175852, 0.9960321659083693, 0.9994709554544493, 0.9964554015448102, 0.9775685112686487, 0.9981483440905724, 0.9972489683631361, 0.9975134906359115, 0.9833880012697068, 0.9967199238175855, 0.9968786371812507, 0.9738651994497936, 0.991482382816633, 0.9958734525447043, 0.9963495926356998, 0.9975134906359115, 0.9907946249074172, 0.9956089302719289, 0.9966141149084752, 0.9954502169082636, 0.997989630726907, 0.9977780129086868, 0.9915881917257433, 0.9982012485451275, 0.9925933763622897, 0.9911120516347476, 0.9998412866363348, 0.9988890064543434, 0.9975134906359115, 0.9982012485451275, 0.9952915035445984, 0.9972489683631361, 0.9826473389059359, 0.9928578986350651, 0.9917469050894083, 0.9963495926356998, 0.9998412866363348, 0.9862977462702359, 0.9886784467252142, 0.9968786371812507, 0.9948682679081579, 0.9934398476351708, 0.9967199238175852, 0.9769336578139879, 0.9993651465453389, 0.9971431594540261, 0.9991535287271188]\n",
            "[0.9981483440905724, 0.9989948153634535, 0.9968786371812507, 0.994286318908052, 1.000052904454555, 0.9992593376362289, 0.98206538990583, 0.9939159877261665, 0.9996825732726694, 0.9998941910908898, 0.9984657708179029, 0.9865622685430112, 0.9974076817268012, 0.9852925616336895, 0.9948682679081579, 0.9944450322717172, 0.9794201671780763, 0.9970902549994709, 0.995767643635594, 0.9970373505449158, 0.9953973124537084, 0.9815363453602793, 0.9988361019997883, 0.9968786371812507, 0.993598560998836, 0.9911649560893027, 0.9985715797270129, 0.9968786371812507, 0.9814834409057243, 0.9913765739075229, 0.9969844460903607, 0.9989419109088984, 0.9937572743625013, 0.9948682679081579, 0.9982012485451275, 0.9987302930906782, 0.9971431594540259, 0.997830917363242, 0.9932282298169506, 0.9936514654533911, 0.9994180509998941, 0.9959263569992592, 0.9930695164532853, 0.9999999999999999, 0.9994709554544493, 0.9976722039995767, 0.9991006242725636, 0.9983070574542375, 0.9989419109088984, 0.9764575177229922, 0.9952385990900433, 0.9903184848164215, 0.983493810178817, 0.9997883821817798, 0.9887313511797692, 0.9894191090889851, 0.9981483440905724, 0.9974605861813564, 0.9946566500899374, 0.9973018728176911, 0.9778859379959792, 0.9997883821817796, 0.9971960639085811, 0.9997354777272246]\n",
            "[0.9992064331816739, 0.9983070574542375, 0.9989419109088984, 0.9877261665432229, 0.9995767643635594, 0.9989948153634535, 0.9932811342715057, 0.9984128663633477, 0.9952915035445984, 0.9981483440905724, 0.9994709554544493, 0.9880435932705532, 0.9974605861813566, 0.9897894402708708, 0.9902126759073114, 0.9940747010898315, 0.9944979367262724, 0.9963495926356999, 0.9967199238175852, 0.9973018728176911, 0.9989419109088984, 0.9834938101788169, 0.9996296688181143, 0.9967199238175855, 0.9967728282721406, 0.9960321659083693, 0.9898423447254258, 0.9967199238175855, 0.9680986139032907, 0.9971431594540261, 0.9976722039995767, 0.9970373505449157, 0.9925933763622897, 0.9959263569992592, 0.9909004338165274, 0.9965083059993651, 0.9979367262723521, 0.9932282298169506, 0.9954502169082636, 0.9925404719077346, 1.0001587133636651, 0.9977780129086868, 0.9985715797270129, 0.9999999999999999, 0.9955560258173738, 0.9953973124537084, 0.9996825732726694, 0.9979367262723521, 0.9980954396360173, 0.9813776319966141, 0.9976722039995767, 0.9759284731774415, 0.9929637075441752, 0.9998412866363348, 0.9955560258173738, 0.9972489683631361, 0.9962437837265897, 0.997883821817797, 0.9965612104539202, 0.9991006242725636, 0.9961379748174796, 0.9997883821817798, 0.9969844460903607, 0.9987302930906782]\n",
            "[0.9968786371812507, 0.997830917363242, 0.9970373505449158, 0.9961379748174797, 0.9996296688181143, 0.9979367262723521, 0.9867209819066765, 0.9984128663633477, 0.9965083059993651, 0.9984657708179028, 0.9974076817268012, 0.987884879906888, 0.9961379748174797, 0.981271823087504, 0.9877261665432229, 0.9929637075441752, 0.9871971219976723, 0.9969315416358057, 0.9947624589990477, 0.995026981271823, 0.9949740768172679, 0.9865093640884562, 0.997989630726907, 0.9989419109088984, 0.986350650724791, 0.9980425351814622, 0.9985715797270129, 0.9949211723627129, 0.9828589567241561, 0.9941276055443867, 0.9972489683631361, 0.9983599619087927, 0.9920643318167388, 0.9905301026346417, 0.9968786371812507, 0.9963495926356998, 0.9979367262723521, 0.9939159877261665, 0.9930695164532852, 0.986244841815681, 0.9995238599090043, 0.9990477198180087, 0.9951856946354883, 0.9997883821817796, 0.9967728282721404, 0.9980425351814622, 0.9987302930906782, 0.997989630726907, 0.9990477198180087, 0.9859274150883505, 0.9953444079991534, 0.9888900645434345, 0.9811131097238386, 0.9994709554544493, 0.9912178605438579, 0.9955031213628186, 0.9923817585440693, 0.9953973124537084, 0.9951327901809331, 0.9938101788170564, 0.9883610199978837, 0.9995238599090043, 0.9984657708179029, 0.9981483440905723]\n",
            "[0.9996825732726695, 0.9965612104539202, 0.9995238599090043, 0.9967199238175855, 0.9992593376362289, 0.9977251084541318, 0.9936514654533911, 0.9984128663633477, 0.9926462808168447, 0.994233414453497, 0.9995238599090043, 0.9980425351814621, 0.9999470955454448, 0.9971431594540261, 0.9946037456353825, 0.9965083059993651, 0.9991006242725636, 0.9963495926356999, 0.9983599619087927, 0.9996825732726695, 0.9991535287271188, 0.9949211723627129, 0.9993651465453391, 0.9984128663633477, 0.9956089302719289, 0.9974605861813564, 0.997989630726907, 0.997883821817797, 0.9797375939054068, 1.0001058089091102, 0.999312242090784, 0.9970373505449158, 0.9997354777272246, 0.991429478362078, 0.9949211723627129, 0.9932282298169506, 0.997778012908687, 0.9968257327266956, 0.9992593376362289, 0.9837054279970373, 0.9998941910908898, 0.9982541529996825, 0.9994180509998941, 0.9997883821817796, 0.9952385990900433, 0.9966670193630303, 0.9997883821817798, 0.9969315416358057, 0.9990477198180084, 0.9951327901809331, 0.9973018728176911, 0.9928578986350651, 0.9977251084541316, 0.9997883821817798, 0.9965612104539202, 0.9998412866363348, 0.9959792614538143, 0.9992593376362289, 0.9997354777272247, 0.9929637075441752, 0.9980425351814622, 1.0001058089091102, 0.9984128663633477, 0.9987302930906782]\n",
            "[0.9984128663633477, 0.9988890064543434, 0.9980425351814622, 0.9913236694529679, 0.9996825732726694, 0.9974076817268014, 0.9875145487250026, 0.9948153634536027, 0.9980425351814621, 0.9971431594540259, 0.9995767643635594, 0.994233414453497, 0.9901597714527562, 0.9886255422706591, 0.9958205480901491, 0.994339223362607, 0.9854512749973546, 0.9958205480901491, 0.9933340387260606, 0.9976722039995767, 0.9951856946354883, 0.9854512749973546, 0.9991535287271188, 0.9975134906359115, 0.9940747010898318, 0.9828060522696009, 0.9960850703629245, 0.9949211723627129, 0.9837583324515923, 0.9897365358163156, 0.9984128663633477, 0.9958205480901492, 0.9956089302719289, 0.9974076817268014, 0.9937572743625013, 0.9976722039995767, 0.9984657708179028, 0.9903713892709765, 0.9952915035445984, 0.9962437837265897, 0.9980425351814621, 0.9940217966352766, 0.9911120516347476, 0.9999470955454448, 0.9966670193630303, 0.9949211723627129, 0.9987302930906782, 0.998677388636123, 0.9976722039995767, 0.9835996190879273, 0.9931753253623956, 0.9791027404507459, 0.9891545868162098, 0.9994709554544493, 0.9947095545444926, 0.990635911543752, 0.9955031213628188, 0.993492752089726, 0.9890487779070997, 0.9894191090889851, 0.992117236271294, 0.9996296688181143, 0.997883821817797, 0.9996825732726695]\n",
            "[0.998677388636123, 0.9971431594540261, 0.9992593376362289, 0.9877261665432229, 0.9999999999999999, 0.997830917363242, 0.9826473389059359, 0.9980954396360174, 0.9964554015448099, 0.9989419109088984, 0.998624484181568, 0.9838641413607025, 0.9968786371812507, 0.9823828166331604, 0.9869855041794519, 0.9938101788170564, 0.983493810178817, 0.9973547772722462, 0.995714739181039, 0.9987831975452333, 0.997989630726907, 0.982118294360385, 0.9980954396360173, 0.9979367262723521, 0.9967728282721406, 0.993598560998836, 0.9960850703629245, 0.9948682679081579, 0.9707967410855993, 0.993492752089726, 0.9952915035445984, 0.9961379748174796, 0.9976192995450216, 0.9824357210877155, 0.9958205480901492, 0.9962437837265897, 0.9987302930906782, 0.9961379748174796, 0.997883821817797, 0.9921172362712939, 0.999312242090784, 0.9964554015448099, 0.9924875674531795, 0.9997354777272246, 0.9976192995450216, 0.9984128663633477, 0.9991006242725636, 0.9950798857263782, 0.9991535287271188, 0.97487038408634, 0.9940217966352766, 0.9913236694529679, 0.9917469050894085, 1.0001587133636651, 0.9891016823616547, 0.9937572743625012, 0.9958205480901491, 0.9959792614538143, 0.9951856946354883, 0.9985715797270129, 0.9873558353613373, 0.9995238599090043, 0.9980425351814622, 0.9977251084541318]\n",
            "[0.998677388636123, 0.9982541529996825, 0.9984657708179029, 0.9823828166331604, 0.9996296688181143, 0.9974076817268014, 0.9849751349063591, 0.9976722039995767, 0.9967199238175855, 0.9986773886361232, 0.9983070574542375, 0.98206538990583, 0.9910591471801924, 0.9815363453602793, 0.9846577081790286, 0.9939159877261665, 0.9851867527245793, 0.9967199238175852, 0.9941805099989419, 0.9967199238175852, 0.9985715797270129, 0.9802666384509576, 0.9990477198180087, 0.9968257327266956, 0.994392127817162, 0.9904771981800867, 0.9883610199978837, 0.9939688921807215, 0.965559200084647, 0.9895778224526505, 0.9953973124537084, 0.9958205480901492, 0.9889429689979896, 0.9938101788170564, 0.9904242937255315, 0.9969844460903607, 0.9994180509998942, 0.9883081155433288, 0.9924875674531797, 0.9973547772722462, 0.9989948153634536, 0.9953444079991534, 0.992170140725849, 0.9996825732726695, 0.9965083059993651, 0.995079885726378, 0.9992593376362289, 0.9971960639085811, 0.9996296688181143, 0.9708496455401544, 0.994339223362607, 0.9831234789969314, 0.9859803195429054, 0.9994709554544493, 0.993492752089726, 0.9934927520897259, 0.9958205480901491, 0.9930695164532852, 0.9892603957253201, 0.9973018728176911, 0.9935456565442811, 0.9992593376362289, 0.9959792614538143, 0.9982012485451275]\n",
            "[[0.9993651465453391, 0.9982541529996825, 0.9994180509998942, 0.9982012485451275, 0.9995767643635594, 0.9970373505449157, 0.9911649560893027, 0.9977780129086868, 0.9952915035445984, 0.9915881917257433, 0.9993651465453391, 0.9967728282721404, 0.9976722039995767, 0.9963495926356999, 0.9964554015448102, 0.9952385990900433, 0.9916940006348534, 0.994286318908052, 0.9956618347264838, 0.9989948153634535, 0.997883821817797, 0.9929108030896201, 0.9991535287271188, 0.9988361019997883, 0.9939688921807215, 0.9944979367262724, 0.997989630726907, 0.9941276055443867, 0.9851338482700241, 0.9971960639085811, 0.997989630726907, 0.9965612104539202, 0.9998412866363348, 0.9957147391810389, 0.9943921278171622, 0.9934398476351708, 0.9977780129086868, 0.9939159877261665, 0.998518675272458, 0.9869855041794519, 0.9997354777272246, 0.9974076817268014, 0.998677388636123, 0.9998412866363348, 0.9928578986350651, 0.9927520897259547, 0.9993651465453391, 0.9979367262723521, 0.9990477198180087, 0.9960321659083693, 0.9977251084541318, 0.9908475293619723, 0.9949211723627129, 0.9992593376362289, 0.9941805099989419, 0.9977251084541318, 0.9927520897259547, 0.9972489683631361, 0.9971960639085811, 0.9845518992699186, 0.993598560998836, 0.9997883821817796, 0.9961379748174796, 0.9991006242725636], [0.9999999999999999, 0.9994180509998941, 0.9995238599090043, 0.9979367262723521, 0.998677388636123, 0.9995238599090043, 0.9947095545444925, 0.997989630726907, 0.9994180509998941, 0.9994180509998941, 0.9996296688181143, 0.9989948153634535, 0.9996825732726695, 0.9964024970902549, 0.9995238599090043, 0.9961908792720348, 0.9975663950904666, 0.998518675272458, 0.9983070574542375, 0.997883821817797, 0.9994709554544493, 0.9987302930906782, 0.9997354777272246, 0.9985715797270129, 0.9981483440905724, 0.9968257327266956, 0.9998412866363348, 0.9990477198180087, 0.9969844460903607, 0.9994709554544493, 0.9993651465453391, 0.999312242090784, 0.9994180509998942, 0.9960321659083693, 0.998624484181568, 0.9987831975452334, 0.9991535287271188, 0.9991006242725639, 0.999312242090784, 0.9968257327266956, 0.9990477198180087, 0.997989630726907, 0.9974076817268014, 0.9999470955454448, 0.999312242090784, 0.9983599619087925, 0.9997883821817798, 0.9966670193630303, 0.9960321659083693, 0.9959263569992592, 0.9966670193630303, 0.9802137339964024, 0.998624484181568, 1.0002116178182203, 0.9985715797270129, 0.9990477198180087, 0.9989948153634536, 0.999312242090784, 0.999312242090784, 0.997883821817797, 0.9965612104539202, 0.9999470955454448, 0.9989948153634535, 1.000052904454555], [0.9991535287271188, 0.9982012485451275, 0.9969844460903607, 0.9896836313617605, 0.9992064331816739, 0.9976192995450216, 0.9803195429055126, 0.9933340387260606, 0.9989948153634536, 0.999312242090784, 0.9988361019997883, 0.9876732620886678, 0.9967728282721404, 0.9905301026346419, 0.9966141149084752, 0.9928578986350651, 0.9801079250872923, 0.9967199238175852, 0.9960321659083693, 0.9994709554544493, 0.9964554015448102, 0.9775685112686487, 0.9981483440905724, 0.9972489683631361, 0.9975134906359115, 0.9833880012697068, 0.9967199238175855, 0.9968786371812507, 0.9738651994497936, 0.991482382816633, 0.9958734525447043, 0.9963495926356998, 0.9975134906359115, 0.9907946249074172, 0.9956089302719289, 0.9966141149084752, 0.9954502169082636, 0.997989630726907, 0.9977780129086868, 0.9915881917257433, 0.9982012485451275, 0.9925933763622897, 0.9911120516347476, 0.9998412866363348, 0.9988890064543434, 0.9975134906359115, 0.9982012485451275, 0.9952915035445984, 0.9972489683631361, 0.9826473389059359, 0.9928578986350651, 0.9917469050894083, 0.9963495926356998, 0.9998412866363348, 0.9862977462702359, 0.9886784467252142, 0.9968786371812507, 0.9948682679081579, 0.9934398476351708, 0.9967199238175852, 0.9769336578139879, 0.9993651465453389, 0.9971431594540261, 0.9991535287271188], [0.9981483440905724, 0.9989948153634535, 0.9968786371812507, 0.994286318908052, 1.000052904454555, 0.9992593376362289, 0.98206538990583, 0.9939159877261665, 0.9996825732726694, 0.9998941910908898, 0.9984657708179029, 0.9865622685430112, 0.9974076817268012, 0.9852925616336895, 0.9948682679081579, 0.9944450322717172, 0.9794201671780763, 0.9970902549994709, 0.995767643635594, 0.9970373505449158, 0.9953973124537084, 0.9815363453602793, 0.9988361019997883, 0.9968786371812507, 0.993598560998836, 0.9911649560893027, 0.9985715797270129, 0.9968786371812507, 0.9814834409057243, 0.9913765739075229, 0.9969844460903607, 0.9989419109088984, 0.9937572743625013, 0.9948682679081579, 0.9982012485451275, 0.9987302930906782, 0.9971431594540259, 0.997830917363242, 0.9932282298169506, 0.9936514654533911, 0.9994180509998941, 0.9959263569992592, 0.9930695164532853, 0.9999999999999999, 0.9994709554544493, 0.9976722039995767, 0.9991006242725636, 0.9983070574542375, 0.9989419109088984, 0.9764575177229922, 0.9952385990900433, 0.9903184848164215, 0.983493810178817, 0.9997883821817798, 0.9887313511797692, 0.9894191090889851, 0.9981483440905724, 0.9974605861813564, 0.9946566500899374, 0.9973018728176911, 0.9778859379959792, 0.9997883821817796, 0.9971960639085811, 0.9997354777272246], [0.9992064331816739, 0.9983070574542375, 0.9989419109088984, 0.9877261665432229, 0.9995767643635594, 0.9989948153634535, 0.9932811342715057, 0.9984128663633477, 0.9952915035445984, 0.9981483440905724, 0.9994709554544493, 0.9880435932705532, 0.9974605861813566, 0.9897894402708708, 0.9902126759073114, 0.9940747010898315, 0.9944979367262724, 0.9963495926356999, 0.9967199238175852, 0.9973018728176911, 0.9989419109088984, 0.9834938101788169, 0.9996296688181143, 0.9967199238175855, 0.9967728282721406, 0.9960321659083693, 0.9898423447254258, 0.9967199238175855, 0.9680986139032907, 0.9971431594540261, 0.9976722039995767, 0.9970373505449157, 0.9925933763622897, 0.9959263569992592, 0.9909004338165274, 0.9965083059993651, 0.9979367262723521, 0.9932282298169506, 0.9954502169082636, 0.9925404719077346, 1.0001587133636651, 0.9977780129086868, 0.9985715797270129, 0.9999999999999999, 0.9955560258173738, 0.9953973124537084, 0.9996825732726694, 0.9979367262723521, 0.9980954396360173, 0.9813776319966141, 0.9976722039995767, 0.9759284731774415, 0.9929637075441752, 0.9998412866363348, 0.9955560258173738, 0.9972489683631361, 0.9962437837265897, 0.997883821817797, 0.9965612104539202, 0.9991006242725636, 0.9961379748174796, 0.9997883821817798, 0.9969844460903607, 0.9987302930906782], [0.9968786371812507, 0.997830917363242, 0.9970373505449158, 0.9961379748174797, 0.9996296688181143, 0.9979367262723521, 0.9867209819066765, 0.9984128663633477, 0.9965083059993651, 0.9984657708179028, 0.9974076817268012, 0.987884879906888, 0.9961379748174797, 0.981271823087504, 0.9877261665432229, 0.9929637075441752, 0.9871971219976723, 0.9969315416358057, 0.9947624589990477, 0.995026981271823, 0.9949740768172679, 0.9865093640884562, 0.997989630726907, 0.9989419109088984, 0.986350650724791, 0.9980425351814622, 0.9985715797270129, 0.9949211723627129, 0.9828589567241561, 0.9941276055443867, 0.9972489683631361, 0.9983599619087927, 0.9920643318167388, 0.9905301026346417, 0.9968786371812507, 0.9963495926356998, 0.9979367262723521, 0.9939159877261665, 0.9930695164532852, 0.986244841815681, 0.9995238599090043, 0.9990477198180087, 0.9951856946354883, 0.9997883821817796, 0.9967728282721404, 0.9980425351814622, 0.9987302930906782, 0.997989630726907, 0.9990477198180087, 0.9859274150883505, 0.9953444079991534, 0.9888900645434345, 0.9811131097238386, 0.9994709554544493, 0.9912178605438579, 0.9955031213628186, 0.9923817585440693, 0.9953973124537084, 0.9951327901809331, 0.9938101788170564, 0.9883610199978837, 0.9995238599090043, 0.9984657708179029, 0.9981483440905723], [0.9996825732726695, 0.9965612104539202, 0.9995238599090043, 0.9967199238175855, 0.9992593376362289, 0.9977251084541318, 0.9936514654533911, 0.9984128663633477, 0.9926462808168447, 0.994233414453497, 0.9995238599090043, 0.9980425351814621, 0.9999470955454448, 0.9971431594540261, 0.9946037456353825, 0.9965083059993651, 0.9991006242725636, 0.9963495926356999, 0.9983599619087927, 0.9996825732726695, 0.9991535287271188, 0.9949211723627129, 0.9993651465453391, 0.9984128663633477, 0.9956089302719289, 0.9974605861813564, 0.997989630726907, 0.997883821817797, 0.9797375939054068, 1.0001058089091102, 0.999312242090784, 0.9970373505449158, 0.9997354777272246, 0.991429478362078, 0.9949211723627129, 0.9932282298169506, 0.997778012908687, 0.9968257327266956, 0.9992593376362289, 0.9837054279970373, 0.9998941910908898, 0.9982541529996825, 0.9994180509998941, 0.9997883821817796, 0.9952385990900433, 0.9966670193630303, 0.9997883821817798, 0.9969315416358057, 0.9990477198180084, 0.9951327901809331, 0.9973018728176911, 0.9928578986350651, 0.9977251084541316, 0.9997883821817798, 0.9965612104539202, 0.9998412866363348, 0.9959792614538143, 0.9992593376362289, 0.9997354777272247, 0.9929637075441752, 0.9980425351814622, 1.0001058089091102, 0.9984128663633477, 0.9987302930906782], [0.9984128663633477, 0.9988890064543434, 0.9980425351814622, 0.9913236694529679, 0.9996825732726694, 0.9974076817268014, 0.9875145487250026, 0.9948153634536027, 0.9980425351814621, 0.9971431594540259, 0.9995767643635594, 0.994233414453497, 0.9901597714527562, 0.9886255422706591, 0.9958205480901491, 0.994339223362607, 0.9854512749973546, 0.9958205480901491, 0.9933340387260606, 0.9976722039995767, 0.9951856946354883, 0.9854512749973546, 0.9991535287271188, 0.9975134906359115, 0.9940747010898318, 0.9828060522696009, 0.9960850703629245, 0.9949211723627129, 0.9837583324515923, 0.9897365358163156, 0.9984128663633477, 0.9958205480901492, 0.9956089302719289, 0.9974076817268014, 0.9937572743625013, 0.9976722039995767, 0.9984657708179028, 0.9903713892709765, 0.9952915035445984, 0.9962437837265897, 0.9980425351814621, 0.9940217966352766, 0.9911120516347476, 0.9999470955454448, 0.9966670193630303, 0.9949211723627129, 0.9987302930906782, 0.998677388636123, 0.9976722039995767, 0.9835996190879273, 0.9931753253623956, 0.9791027404507459, 0.9891545868162098, 0.9994709554544493, 0.9947095545444926, 0.990635911543752, 0.9955031213628188, 0.993492752089726, 0.9890487779070997, 0.9894191090889851, 0.992117236271294, 0.9996296688181143, 0.997883821817797, 0.9996825732726695], [0.998677388636123, 0.9971431594540261, 0.9992593376362289, 0.9877261665432229, 0.9999999999999999, 0.997830917363242, 0.9826473389059359, 0.9980954396360174, 0.9964554015448099, 0.9989419109088984, 0.998624484181568, 0.9838641413607025, 0.9968786371812507, 0.9823828166331604, 0.9869855041794519, 0.9938101788170564, 0.983493810178817, 0.9973547772722462, 0.995714739181039, 0.9987831975452333, 0.997989630726907, 0.982118294360385, 0.9980954396360173, 0.9979367262723521, 0.9967728282721406, 0.993598560998836, 0.9960850703629245, 0.9948682679081579, 0.9707967410855993, 0.993492752089726, 0.9952915035445984, 0.9961379748174796, 0.9976192995450216, 0.9824357210877155, 0.9958205480901492, 0.9962437837265897, 0.9987302930906782, 0.9961379748174796, 0.997883821817797, 0.9921172362712939, 0.999312242090784, 0.9964554015448099, 0.9924875674531795, 0.9997354777272246, 0.9976192995450216, 0.9984128663633477, 0.9991006242725636, 0.9950798857263782, 0.9991535287271188, 0.97487038408634, 0.9940217966352766, 0.9913236694529679, 0.9917469050894085, 1.0001587133636651, 0.9891016823616547, 0.9937572743625012, 0.9958205480901491, 0.9959792614538143, 0.9951856946354883, 0.9985715797270129, 0.9873558353613373, 0.9995238599090043, 0.9980425351814622, 0.9977251084541318], [0.998677388636123, 0.9982541529996825, 0.9984657708179029, 0.9823828166331604, 0.9996296688181143, 0.9974076817268014, 0.9849751349063591, 0.9976722039995767, 0.9967199238175855, 0.9986773886361232, 0.9983070574542375, 0.98206538990583, 0.9910591471801924, 0.9815363453602793, 0.9846577081790286, 0.9939159877261665, 0.9851867527245793, 0.9967199238175852, 0.9941805099989419, 0.9967199238175852, 0.9985715797270129, 0.9802666384509576, 0.9990477198180087, 0.9968257327266956, 0.994392127817162, 0.9904771981800867, 0.9883610199978837, 0.9939688921807215, 0.965559200084647, 0.9895778224526505, 0.9953973124537084, 0.9958205480901492, 0.9889429689979896, 0.9938101788170564, 0.9904242937255315, 0.9969844460903607, 0.9994180509998942, 0.9883081155433288, 0.9924875674531797, 0.9973547772722462, 0.9989948153634536, 0.9953444079991534, 0.992170140725849, 0.9996825732726695, 0.9965083059993651, 0.995079885726378, 0.9992593376362289, 0.9971960639085811, 0.9996296688181143, 0.9708496455401544, 0.994339223362607, 0.9831234789969314, 0.9859803195429054, 0.9994709554544493, 0.993492752089726, 0.9934927520897259, 0.9958205480901491, 0.9930695164532852, 0.9892603957253201, 0.9973018728176911, 0.9935456565442811, 0.9992593376362289, 0.9959792614538143, 0.9982012485451275]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-363adf959798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"YlGnBu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(results, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "XXZUKct0MwFQ",
        "outputId": "20863be4-73b7-404f-eac4-eb76b609623b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM3IuXjVJDsj",
        "outputId": "d52b116a-52c8-43da-99c9-bbaf61b62097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print([np.prod(p.size()) for p in model_parameters])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyozYq_LJNGy",
        "outputId": "5d0f23c9-e528-41ca-f918-6895a43a146c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[144, 16, 9216, 64, 640, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-situ Training"
      ],
      "metadata": {
        "id": "tWkpkExZ6xJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passive Crossbar"
      ],
      "metadata": {
        "id": "it34eQI44Bnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "crossbar.py\n",
        "Louis Primeau\n",
        "University of Toronto Department of Electrical and Computer Engineering\n",
        "louis.primeau@mail.utoronto.ca\n",
        "July 29th 2020\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import savemat\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from scipy.io import savemat\n",
        "\n",
        "# Implements scipy's minmax scaler except just between 0 and 1 for torch Tensors.\n",
        "# Taken from a ptrblck post on the PyTorch forums. Love that dude.\n",
        "class MinMaxScaler(object):\n",
        "    def __call__(self, tensor):\n",
        "        self.scale = 1.0 / (tensor.max(dim=1, keepdim=True)[0] - tensor.min(dim=1, keepdim=True)[0])\n",
        "        self.min = tensor.min(dim=1, keepdim=True)[0]\n",
        "        tensor.sub_(self.min).mul_(self.scale)\n",
        "        return tensor\n",
        "    def inverse_transform(self, tensor):\n",
        "        tensor.div_(self.scale).add_(self.min)\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "VfYD4Y2k6y-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ticket:\n",
        "    def __init__(self, row, col, m_rows, m_cols, matrix, mat_scale_factor, crossbar, uvect, decode, inputres, outputres):\n",
        "        self.row, self.col = row, col\n",
        "        self.m_rows, self.m_cols = m_rows, m_cols\n",
        "        self.crossbar = crossbar\n",
        "        self.mat_scale_factor = mat_scale_factor\n",
        "        self.matrix = matrix\n",
        "        self.uvect = uvect\n",
        "        self.inputres = inputres\n",
        "        self.adcres = outputres\n",
        "        self.decode = torch.matmul(self.uvect.t(),self.matrix)\n",
        "\n",
        "    def prep_vector(self, vector, v_bits):\n",
        "\n",
        "        # Scale vector to [0, 2^v_bits]\n",
        "        vect_min = torch.min(vector)\n",
        "        vector = vector - vect_min\n",
        "        vect_scale_factor = torch.max(vector) / (2**v_bits - 1)\n",
        "        vector = vector / vect_scale_factor if vect_scale_factor != 0.0 else vector\n",
        "\n",
        "        # decompose vector by bit\n",
        "        bit_vector = torch.zeros(vector.size(0),v_bits)\n",
        "        bin2s = lambda x : ''.join(reversed( [str((int(x) >> i) & 1) for i in range(v_bits)] ) )\n",
        "        for j in range(vector.size(0)):\n",
        "            bit_vector[j,:] = torch.Tensor([float(i) for i in list(bin2s(vector[j]))])\n",
        "        bit_vector *= self.crossbar.V\n",
        "\n",
        "        # Pad bit vector with unselected voltages\n",
        "        pad_vector = torch.zeros(self.crossbar.size[0], v_bits)\n",
        "\n",
        "        pad_vector[self.row:self.row + self.m_rows,:] = bit_vector\n",
        "\n",
        "        return pad_vector, vect_scale_factor, vect_min\n",
        "\n",
        "    def vmm(self, vector):\n",
        "        # Baseline VMM operation without CODEX\n",
        "        v_bits = self.inputres\n",
        "        assert vector.size(1) == 1, \"vector wrong shape\"\n",
        "\n",
        "        crossbar = self.crossbar\n",
        "        # Rescale vector and convert to bits.\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        # Perform crossbar VMM\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        # Add binary outputs\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "\n",
        "        # Rescale binary outputs\n",
        "        rout = (rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5131 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "        return rout.view(-1,1)\n",
        "\n",
        "    def CODEXvmm(self, xvector):\n",
        "        # CODEX VMM operation\n",
        "        assert xvector.size(1) == 1, \"vector wrong shape\"\n",
        "        v_bits=self.inputres\n",
        "        crossbar = self.crossbar\n",
        "\n",
        "        #Add encoding vector u to x\n",
        "        vector = xvector + self.uvect\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits+1)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        # The rout on the line below this comment contains\n",
        "        # the raw output currents that the ADC will receive.\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "        rout = 2*(rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5231 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "        rout = rout - self.decode\n",
        "        return rout.view(-1,1)\n",
        "\n",
        "    def modified_CODEXvmm(self, xvector):\n",
        "        # CODEX VMM operation\n",
        "        assert xvector.size(1) == 1, \"vector wrong shape\"\n",
        "        v_bits=self.inputres\n",
        "        crossbar = self.crossbar\n",
        "\n",
        "        #Add encoding vector u to x\n",
        "        vector = xvector + self.uvect\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits+1)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        # The rout on the line below this comment contains\n",
        "        # the raw output currents that the ADC will receive.\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "        rout = 2*(rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5231 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "\n",
        "        # do not decode except during inference\n",
        "        # rout = rout - self.decode\n",
        "        return rout.view(-1,1)"
      ],
      "metadata": {
        "id": "55guiXm660Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import random\n",
        "import copy\n",
        "\n",
        "class linear(torch.autograd.Function):\n",
        "    #From Louis: Custom pytorch autograd function for crossbar VMM operation\n",
        "    @staticmethod\n",
        "    def forward(ctx, ticket, x, W, b):\n",
        "        ctx.save_for_backward(x, W, b)\n",
        "        #return ticket.CODEXvmm(x) + b\n",
        "        return ticket.CODEXvmm(x) + b\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dx):\n",
        "        x, W, b = ctx.saved_tensors\n",
        "        grad_input = W.t().mm(dx)\n",
        "        grad_weight = dx.mm(x.t())\n",
        "        grad_bias = dx\n",
        "        return (None, grad_input, grad_weight, grad_bias)\n",
        "\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, cb,uvect):\n",
        "        super(Linear, self).__init__()\n",
        "        self.W = torch.nn.parameter.Parameter(torch.rand(output_size, input_size))\n",
        "        self.b = torch.nn.parameter.Parameter(torch.rand(output_size, 1))\n",
        "        self.cb = cb\n",
        "\n",
        "        #Instantiate Linear layer with pool of random encoding vectors to sample from\n",
        "        self.uvectlist = uvect\n",
        "        self.uvectidx = 0\n",
        "        # Decoding vector is calculated ideally here off-chip, but calculating decoding vector on-chip is also possible\n",
        "        self.decode = torch.matmul(self.uvectlist[self.uvectidx].t(),torch.transpose(self.W,0,1)).detach().clone()\n",
        "        self.ticket = cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "        self.f = linear()\n",
        "        self.cbon = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.f.apply(self.ticket, x, self.W, self.b) if self.cbon else self.W.matmul(x) + self.b\n",
        "\n",
        "    def remap(self):\n",
        "        #Should call the remap crossbar function after 1 or a couple update steps\n",
        "        self.cb.clear()\n",
        "        self.ticket = self.cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "\n",
        "    def update_decode(self):\n",
        "        #Update decoding vector by updating U*G.\n",
        "        self.decode = torch.matmul(self.uvectlist[self.uvectidx].t(),torch.transpose(self.W,0,1)).detach().clone()\n",
        "\n",
        "    def resample(self):\n",
        "        #Sample random new uvector from provided uvectlist\n",
        "        self.cb.clear()\n",
        "        self.uvectidx = random.randint(0, len(uvectlist)-1)\n",
        "        self.ticket = self.cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "\n",
        "    def use_cb(self, state):\n",
        "        self.cbon = state\n"
      ],
      "metadata": {
        "id": "-by13uiW64b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class crossbar:\n",
        "    def __init__(self, device_params):\n",
        "\n",
        "        # Power Supply Voltage\n",
        "        self.V = device_params[\"Vdd\"]\n",
        "\n",
        "        # DAC resolution\n",
        "        self.input_resolution = device_params[\"dac_resolution\"]\n",
        "        self.output_resolution = device_params[\"adc_resolution\"]\n",
        "\n",
        "        # Wordline Resistance\n",
        "        self.r_wl = torch.Tensor((device_params[\"r_wl\"],))\n",
        "        # Bitline Resistance\n",
        "        self.r_bl = torch.Tensor((device_params[\"r_bl\"],))\n",
        "\n",
        "        # Number of rows, columns\n",
        "        self.size = device_params[\"m\"], device_params[\"n\"]\n",
        "\n",
        "        # High resistance state\n",
        "        self.g_on = 1 / torch.normal(device_params[\"r_on_mean\"], device_params[\"r_on_stddev\"], size=self.size)\n",
        "        #self.g_on = (1 / device_params[\"r_on_mean\"]) * torch.ones(self.size)\n",
        "\n",
        "        # Low Resistance state\n",
        "        self.g_off = 1 / torch.normal(device_params[\"r_off_mean\"], device_params[\"r_off_stddev\"], size=self.size)\n",
        "        #self.g_off = (1 / device_params[\"r_off_mean\"]) * torch.ones(self.size)\n",
        "\n",
        "        self.g_wl = torch.Tensor((1 / device_params[\"r_wl\"],))\n",
        "        self.g_bl = torch.Tensor((1 / device_params[\"r_bl\"],))\n",
        "\n",
        "        # Resolution\n",
        "        self.resolution = device_params[\"device_resolution\"]\n",
        "        # Conductance tensor, m x n x 2**resolution\n",
        "\n",
        "        # 2**self.resolution - 1 so that there's a conductance state in the middle.\n",
        "        self.conductance_states = torch.cat([torch.cat([torch.linspace(self.g_off[i,j], self.g_on[i,j],2**self.resolution - 1).unsqueeze(0)\n",
        "                                                        for j in range(self.size[1])],dim=0).unsqueeze(0)\n",
        "                                             for i in range(self.size[0])],dim=0)\n",
        "\n",
        "        # Bias Scheme\n",
        "        self.bias_voltage = self.V * device_params[\"bias_scheme\"]\n",
        "\n",
        "        # Tile size (1x1 = 1T1R, nxm = passive, etc.)\n",
        "        self.tile_rows = device_params[\"tile_rows\"]\n",
        "        self.tile_cols = device_params[\"tile_cols\"]\n",
        "        assert self.size[0] % self.tile_rows == 0, \"tile size does not divide crossbar size in row direction\"\n",
        "        assert self.size[1] % self.tile_cols == 0, \"tile size does not divide crossbar size in col direction\"\n",
        "\n",
        "        # Resistance of CMOS lines\n",
        "        self.r_cmos_line = device_params[\"r_cmos_line\"]\n",
        "\n",
        "        # Conductance Matrix; initialize each memristor at the on resstance\n",
        "        self.W = torch.ones(self.size) * self.g_on\n",
        "\n",
        "        # Stuck-on & stuck-on device nonideality\n",
        "        self.p_stuck_on = device_params[\"p_stuck_on\"]\n",
        "        self.p_stuck_off = device_params[\"p_stuck_off\"]\n",
        "        self.devicefaults = False\n",
        "\n",
        "        self.mapped = []\n",
        "\n",
        "        self.saved_tiles = {}\n",
        "\n",
        "    def apply_stuck(self, p_stuck_on, p_stuck_off):\n",
        "\n",
        "        state_dist = torch.distributions.categorical.Categorical(probs=torch.Tensor([p_stuck_on, p_stuck_off, 1 - p_stuck_on - p_stuck_off]))\n",
        "        state_mask = state_dist.sample(self.size)\n",
        "\n",
        "        self.W[state_mask == 0] = self.g_off[state_mask==0]\n",
        "        self.W[state_mask == 1] = self.g_on[state_mask==1]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def map(self, matrix):\n",
        "        assert not(matrix.size(0) > self.size[0] or matrix.size(1)*2 > self.size[1]), \"input too large\"\n",
        "        midpoint = self.conductance_states.size(2) // 2\n",
        "\n",
        "        for i in range(matrix.size(0)):\n",
        "            for j in range(matrix.size(1)):\n",
        "\n",
        "                shifted = self.conductance_states[i,j] - self.conductance_states[i,j,midpoint]\n",
        "                idx = torch.min(torch.abs(shifted - matrix[i,j]), dim=0)[1]\n",
        "\n",
        "                self.W[i,2*j+1] = self.conductance_states[i,j,idx]\n",
        "                self.W[i,2*j] = self.conductance_states[i,j,midpoint-(idx-midpoint)]\n",
        "\n",
        "    def solve(self, voltage):\n",
        "        output = torch.zeros((voltage.size(1), self.size[1]))\n",
        "        for i in range(self.size[0] // self.tile_rows):\n",
        "            for j in range(self.size[1] // self.tile_cols):\n",
        "                for k in range(voltage.size(1)):\n",
        "                    coords = (i*self.tile_rows, (i+1)*self.tile_rows, j*self.tile_cols, (j+1)*self.tile_rows)\n",
        "                    vect = voltage[i*self.tile_rows:(i+1)*self.tile_rows,k]\n",
        "                    solution = self.circuit_solve(coords, vect, torch.zeros(self.size[1]), torch.ones(self.size[1]), torch.zeros(self.size[0]))\n",
        "                    output[k] += torch.cat((torch.zeros(j*self.tile_cols), solution, torch.zeros((self.size[1] // self.tile_cols - j - 1) * self.tile_cols)))\n",
        "        return output\n",
        "\n",
        "    \"\"\"\n",
        "    A Comprehensive Crossbar Array Model With Solutions for Line Resistance and Nonlinear Device Characteristics\n",
        "    An Chen\n",
        "    IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL. 60, NO. 4, APRIL 2013\n",
        "    \"\"\"\n",
        "\n",
        "    def hash_M(self, a, b, c, d):\n",
        "        return str(a) + \"_\" + str(b) + \"_\" + str(c) + \"_\" + str(d)\n",
        "\n",
        "    def make_M(self, a, b, c, d):\n",
        "\n",
        "        conductances = self.W[a:b,c:d]\n",
        "        g_wl, g_bl = self.g_wl, self.g_bl\n",
        "        g_s_wl_in, g_s_wl_out = torch.ones(self.tile_rows) * 1, torch.ones(self.tile_rows) * 1e-9\n",
        "        g_s_bl_in, g_s_bl_out = torch.ones(self.tile_rows) * 1e-9, torch.ones(self.tile_rows) * 1\n",
        "        m, n = self.tile_rows, self.tile_cols\n",
        "\n",
        "        A = torch.block_diag(*tuple(torch.diag(conductances[i,:])\n",
        "                          + torch.diag(torch.cat((g_wl, g_wl * 2 * torch.ones(n-2), g_wl)))\n",
        "                          + torch.diag(g_wl * -1 *torch.ones(n-1), diagonal = 1)\n",
        "                          + torch.diag(g_wl * -1 *torch.ones(n-1), diagonal = -1)\n",
        "                          + torch.diag(torch.cat((g_s_wl_in[i].view(1), torch.zeros(n - 2), g_s_wl_out[i].view(1))))\n",
        "                                   for i in range(m)))\n",
        "\n",
        "        B = torch.block_diag(*tuple(-torch.diag(conductances[i,:]) for i in range(m)))\n",
        "\n",
        "        def makec(j):\n",
        "            c = torch.zeros(m, m*n)\n",
        "            for i in range(m):\n",
        "                c[i,n*(i) + j] = conductances[i,j]\n",
        "            return c\n",
        "\n",
        "        C = torch.cat([makec(j) for j in range(n)],dim=0)\n",
        "\n",
        "        def maked(j):\n",
        "            d = torch.zeros(m, m*n)\n",
        "\n",
        "            def c(k):\n",
        "                return(k - 1)\n",
        "\n",
        "            i = 1\n",
        "            d[c(i),c(j)] = -g_s_bl_in[c(j)] - g_bl - conductances[c(i),c(j)]\n",
        "            d[c(i), n*i + c(j)] = g_bl\n",
        "\n",
        "            i = m\n",
        "            d[c(i), n*(i-2) + c(j)] = g_bl\n",
        "            d[c(i), n*(i-1) + c(j)] = -g_s_bl_out[c(j)] - conductances[c(i),c(j)] - g_bl\n",
        "\n",
        "            for i in range(2, m):\n",
        "                d[c(i), n*(i-2) + c(j)] = g_bl\n",
        "                d[c(i), n*(i-1) + c(j)] = -g_bl - conductances[c(i),c(j)] - g_bl\n",
        "                d[c(i), n*(i+1) + c(j)] = g_bl\n",
        "\n",
        "            return d\n",
        "\n",
        "        D = torch.cat([maked(j) for j in range(1,n+1)], dim=0)\n",
        "\n",
        "        M = torch.cat((torch.cat((A,B),dim=1), torch.cat((C,D),dim=1)), dim=0)\n",
        "\n",
        "        self.saved_tiles[self.hash_M(a,b,c,d)] = M\n",
        "\n",
        "        return torch.inverse(M)\n",
        "\n",
        "    def circuit_solve(self, coords,  v_wl_in, v_bl_in, v_bl_out, v_wl_out):\n",
        "\n",
        "        g_wl, g_bl = self.g_wl, self.g_bl\n",
        "        g_s_wl_in, g_s_wl_out = torch.ones(self.tile_rows) * 1, torch.ones(self.tile_rows) * 1e-9\n",
        "        g_s_bl_in, g_s_bl_out = torch.ones(self.tile_rows) * 1e-9, torch.ones(self.tile_rows) * 1\n",
        "        m, n = self.tile_rows, self.tile_cols\n",
        "\n",
        "\n",
        "        if self.hash_M(*coords) not in self.saved_tiles.keys():\n",
        "            #print(coords)\n",
        "            M = self.make_M(*coords)\n",
        "        else:\n",
        "            M = self.saved_tiles[self.hash_M(*coords)]\n",
        "\n",
        "        E = torch.cat([torch.cat(((v_wl_in[i]*g_s_wl_in[i]).view(1), #EW\n",
        "                                  torch.zeros(n-2),\n",
        "                                  (v_wl_out[i]*g_s_wl_out[i]).view(1)))\n",
        "                                 for i in range(m)] +\n",
        "                      [torch.cat(((-v_bl_in[i]*g_s_bl_in[i]).view(1), #EB\n",
        "                                  torch.zeros(m-2),\n",
        "                                  (-v_bl_in[i]*g_s_bl_out[i]).view(1)))\n",
        "                                 for i in range(n)]\n",
        "        ).view(-1, 1)\n",
        "\n",
        "        V = torch.matmul(M, E)\n",
        "\n",
        "        V = torch.chunk(torch.solve(E, M)[0], 2)\n",
        "\n",
        "        return torch.sum((V[1] - V[0]).view(m,n)*self.W[coords[0]:coords[1],coords[2]:coords[3]],dim=0)\n",
        "\n",
        "    def register_linear(self, matrix, uvectlist, decode, bias=None):\n",
        "\n",
        "        row, col = self.find_space(matrix.size(0), matrix.size(1))\n",
        "        # Need to add checks for bias size and col size\n",
        "\n",
        "        # Scale matrix\n",
        "        mat_scale_factor = torch.max(torch.abs(matrix)) / torch.max(self.g_on) * 2\n",
        "        scaled_matrix = matrix / mat_scale_factor\n",
        "\n",
        "        midpoint = self.conductance_states.size(2) // 2\n",
        "        for i in range(row, row + scaled_matrix.size(0)):\n",
        "            for j in range(col, col + scaled_matrix.size(1)):\n",
        "\n",
        "                shifted = self.conductance_states[i,j] - self.conductance_states[i,j,midpoint]\n",
        "                idx = torch.min(torch.abs(shifted - scaled_matrix[i-row,j-col]), dim=0)[1]\n",
        "                self.W[i,2*j+1] = self.conductance_states[i,j,idx]\n",
        "                self.W[i,2*j] = self.conductance_states[i,j,midpoint-(idx-midpoint)]\n",
        "\n",
        "        return ticket(row, col, matrix.size(0), matrix.size(1), matrix, mat_scale_factor, self, uvectlist, decode, self.input_resolution, self.output_resolution)\n",
        "\n",
        "    def which_tiles(self, row, col, m_row, m_col):\n",
        "        return itertools.product(range(row // self.tile_rows, (row + m_row) // self.tile_rows + 1),\n",
        "                                 range(col // self.tile_cols,(col + m_col) // self.tile_cols + 1),\n",
        "        )\n",
        "\n",
        "    def find_space(self, m_row, m_col):\n",
        "        if not self.mapped:\n",
        "            self.mapped.append((0,0,m_row,m_col))\n",
        "        else:\n",
        "            self.mapped.append((self.mapped[-1][0] + self.mapped[-1][2], self.mapped[-1][1] + self.mapped[-1][3], m_row, m_col))\n",
        "        return self.mapped[-1][0], self.mapped[-1][1]\n",
        "\n",
        "    def clear(self):\n",
        "        self.mapped = []\n",
        "        self.W = torch.ones(self.size) * self.g_on\n",
        "\n",
        "    def conductance_update(self):\n",
        "        self.conductance_states = torch.cat([torch.cat([torch.linspace(self.g_off[i,j], self.g_on[i,j],2**self.resolution - 1).unsqueeze(0)\n",
        "                                                        for j in range(self.size[1])],dim=0).unsqueeze(0)\n",
        "                                             for i in range(self.size[0])],dim=0)\n"
      ],
      "metadata": {
        "id": "Jjze-vio665V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Idea is that CODEX allows us to use higher ADC inpt resolution by\n",
        "# Reducing the ADC sensing range.\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 10,\n",
        "                 \"r_bl\": 10,\n",
        "                 \"m\": 600,\n",
        "                 \"n\": 600,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 8,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}"
      ],
      "metadata": {
        "id": "GNjN9Zjk6_WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_tester(model, test_loader, test_size, epoch, log = True):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            if batch_idx * len(data) > test_size:\n",
        "                break\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            total+=target.size(0)\n",
        "            loss = F.nll_loss(output, target)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            if batch_idx % test_size == 0 and log:\n",
        "              with open('log_baseline_test.csv', 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([batch_idx + test_size * 100, test_loss/(batch_idx+1), correct.item()/total])\n",
        "              print(\"Epoch\", epoch, 'iteration',batch_idx, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    return torch.div(correct, float(total))"
      ],
      "metadata": {
        "id": "H2MNUTDe7UDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD only training"
      ],
      "metadata": {
        "id": "7n0RejD438_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net_trainer(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        # output = model(data)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        # init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "        #                        cur_avg_abs_grad, j+1)\n",
        "        # thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 800, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}\")\n",
        "                with open('log_baseline_train.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward()\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "            # if avg_loss == -1.0:\n",
        "            #     avg_loss = loss\n",
        "            # else:\n",
        "            #     avg_loss += loss\n",
        "            # if avg_loss > config['naive_loss_thr']:\n",
        "            #     thr = init_thr\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                cur_epoch_updates += 1\n",
        "                # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                # thr = local_sched(config['learning_rate'],\n",
        "                #                                 config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                optimizer.step()\n",
        "                model.fc1.remap()\n",
        "                #netowrk.fc2.remap()\n",
        "                optimizer.zero_grad()\n",
        "            #     avg_loss = -1.0\n",
        "            # thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss"
      ],
      "metadata": {
        "id": "g6rqyECx7i8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thresholded update method"
      ],
      "metadata": {
        "id": "KYP99PNC4E2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net_trainer_thresh(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    threshold_update = False\n",
        "\n",
        "    def local_sched(learning_rate, gamma, cur_avg_abs_grad, epoch_n):\n",
        "        new_thr = cur_avg_abs_grad*(1+(math.exp(-learning_rate*gamma*epoch_n)))\n",
        "        upper_bound = cur_avg_abs_grad * config['max_thresh_multiplier']\n",
        "\n",
        "        return min(new_thr, upper_bound)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "                               cur_avg_abs_grad, j+1)\n",
        "        thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 800, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}, Threshold: {thr}\")\n",
        "                with open('log_baseline_train.csv', 'a') as f:\n",
        "                  writer = csv.writer(f)\n",
        "                  writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward()\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "            if avg_loss == -1.0:\n",
        "                avg_loss = loss\n",
        "            else:\n",
        "                avg_loss += loss\n",
        "            # if avg_loss > config['naive_loss_thr']:\n",
        "            #     thr = init_thr\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                if threshold_update:\n",
        "                    thr = local_sched(config['learning_rate'],\n",
        "                                                    config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                    threshold_update = False\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                if cur_avg_abs_grad > thr:\n",
        "                    cur_epoch_updates += 1\n",
        "                    # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                    # thr = local_sched(config['learning_rate'],\n",
        "                    #                                 config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                    optimizer.step()\n",
        "                    model.fc1.remap()\n",
        "                    #netowrk.fc2.remap()\n",
        "                    optimizer.zero_grad()\n",
        "                    threshold_update = True\n",
        "                avg_loss = -1.0\n",
        "                thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss, thr_per_batch"
      ],
      "metadata": {
        "id": "6mNUj6sjX0Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan learning rule"
      ],
      "metadata": {
        "id": "LENEmQRf4Io9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def net_trainer_manh(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], dampening=config['dampening'])\n",
        "    #optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        # output = model(data)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        # init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "        #                        cur_avg_abs_grad, j+1)\n",
        "        # thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 800, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}\")\n",
        "                with open('log_baseline_train.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 400, j, False).item()])\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward()\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                cur_epoch_updates += 1\n",
        "\n",
        "                optimizer.step()\n",
        "                model.fc1.remap()\n",
        "                #netowrk.fc2.remap()\n",
        "                optimizer.zero_grad()\n",
        "            #     avg_loss = -1.0\n",
        "            # thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss"
      ],
      "metadata": {
        "id": "etVgy7277xq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Code"
      ],
      "metadata": {
        "id": "b_63U4jF77ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, 3)\n",
        "        self.conv2 = nn.Conv2d(16, 64, 3, 3)\n",
        "       # self.conv3 = nn.Conv2d(16, 32, 2, 2)\n",
        "        #self.fc1 = nn.Linear(9216, 10)\n",
        "\n",
        "        # self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.uvect = [2*(torch.rand(64,1) - 0.5) for i in range(0,10)] #\n",
        "        crb1 = crossbar(device_params)\n",
        "        # Can test using more than 1 crossbar linear layers.\n",
        "        # Easiest implementation is to create a crossbar for each linear layer\n",
        "        self.fc1 = Linear(64, 10,crb1,self.uvect)\n",
        "        self.fc1.use_cb(True)\n",
        "        #self.fc2 = nn.Linear(64*2*2, 10)\n",
        "        self.traincount = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #x = F.relu(self.conv3(x))\n",
        "        out = F.max_pool2d(x, 2)\n",
        "\n",
        "        out = out.view(64, 1)\n",
        "        #print(out.shape)\n",
        "        out = self.fc1(out)\n",
        "        out = out.t()\n",
        "        out = F.log_softmax(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "g5hoaHpe72BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Idea is that CODEX allows us to use higher ADC inpt resolution by\n",
        "# Reducing the ADC sensing range.\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 10,\n",
        "                 \"r_bl\": 10,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 8,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}"
      ],
      "metadata": {
        "id": "_GFm0PGT8MVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNISTtransform = transforms.Compose([\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../datasets/', train=False, download=True,\n",
        "                               transform=MNISTtransform),\n",
        "                               batch_size=1, shuffle=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../datasets/', train=True, download=True,\n",
        "                               transform=MNISTtransform),\n",
        "                               batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "K2nJyrDh8f_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "net_base = Net()\n",
        "net_base = net_base.to(device)\n",
        "if device == 'cuda':\n",
        "    net_base = torch.nn.DataParallel(net_base)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 5,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.01, #0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 2,\n",
        "    \"test_interval\": 5000,\n",
        "    \"dampening\": 0.1\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "#optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss = net_trainer(net_base, train_loader, test_loader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0yLqfosYLcW",
        "outputId": "1e483556-b590-4699-873f-cca2444d3a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 3.784 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 3.646 | Acc: 8.614% (69/801)\n",
            "Test accuracy: 0.08614232391119003\n",
            "Epoch: 1, Loss: 0.0018906896002590656 Updates: 1/1, Avg Grad: 8.64160101627931e-05\n",
            "Epoch: 1, Loss: 0.010919912718236446 Updates: 6/1001, Avg Grad: 0.01361001469194889\n",
            "Epoch: 1, Loss: 0.022612689062952995 Updates: 11/2001, Avg Grad: 0.017039235681295395\n",
            "Epoch: 1, Loss: 0.010265294462442398 Updates: 16/3001, Avg Grad: 0.012855246663093567\n",
            "Epoch: 1, Loss: 0.009833801537752151 Updates: 21/4001, Avg Grad: 0.013128605671226978\n",
            "Epoch 0 iteration 0 Loss: 1.947 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 1.592 | Acc: 47.316% (379/801)\n",
            "Test accuracy: 0.4731585383415222\n",
            "Epoch: 1, Loss: 0.013807320967316628 Updates: 26/5001, Avg Grad: 0.01917598582804203\n",
            "Epoch: 1, Loss: 0.007337197195738554 Updates: 31/6001, Avg Grad: 0.01796833612024784\n",
            "Epoch: 1, Loss: 0.0021184468641877174 Updates: 36/7001, Avg Grad: 0.01700310781598091\n",
            "Epoch: 1, Loss: 0.010060545057058334 Updates: 41/8001, Avg Grad: 0.014759024605154991\n",
            "Epoch: 1, Loss: 0.007589193992316723 Updates: 46/9001, Avg Grad: 0.015902038663625717\n",
            "Epoch 0 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.921 | Acc: 71.411% (572/801)\n",
            "Test accuracy: 0.7141073942184448\n",
            "Epoch: 1, Loss: 0.0026551729533821344 Updates: 51/10001, Avg Grad: 0.012096690014004707\n",
            "Epoch: 1, Loss: 0.00019942123617511243 Updates: 56/11001, Avg Grad: 0.024481654167175293\n",
            "Epoch: 1, Loss: 0.00016530718130525202 Updates: 61/12001, Avg Grad: 0.01339614950120449\n",
            "Epoch: 1, Loss: 0.0001203701103804633 Updates: 66/13001, Avg Grad: 0.024262158200144768\n",
            "Epoch: 1, Loss: 4.9673704779706895e-05 Updates: 71/14001, Avg Grad: 0.014710141345858574\n",
            "Epoch 0 iteration 0 Loss: 0.157 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.713 | Acc: 78.277% (627/801)\n",
            "Test accuracy: 0.7827715277671814\n",
            "Epoch: 1, Loss: 0.0008206166094169021 Updates: 76/15001, Avg Grad: 0.022570112720131874\n",
            "Epoch: 1, Loss: 1.75596524059074e-05 Updates: 81/16001, Avg Grad: 0.023374268785119057\n",
            "Epoch: 1, Loss: 0.015439770184457302 Updates: 86/17001, Avg Grad: 0.022354576736688614\n",
            "Epoch: 1, Loss: 0.021466154605150223 Updates: 91/18001, Avg Grad: 0.019910279661417007\n",
            "Epoch: 1, Loss: 0.00018187318346463144 Updates: 96/19001, Avg Grad: 0.024977337568998337\n",
            "Epoch 0 iteration 0 Loss: 3.902 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 0.677 | Acc: 78.652% (630/801)\n",
            "Test accuracy: 0.7865168452262878\n",
            "Epoch: 1, Loss: 9.130597504736215e-07 Updates: 101/20001, Avg Grad: 0.014537903480231762\n",
            "Epoch: 1, Loss: 0.0036029156763106585 Updates: 106/21001, Avg Grad: 0.033767275512218475\n",
            "Epoch: 1, Loss: 0.013840330764651299 Updates: 111/22001, Avg Grad: 0.01933770813047886\n",
            "Epoch: 1, Loss: 0.00046595727326348424 Updates: 116/23001, Avg Grad: 0.021529166027903557\n",
            "Epoch: 1, Loss: 0.012556172907352448 Updates: 121/24001, Avg Grad: 0.0181110929697752\n",
            "Epoch 0 iteration 0 Loss: 0.007 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.567 | Acc: 81.898% (656/801)\n",
            "Test accuracy: 0.8189762830734253\n",
            "Epoch: 1, Loss: 0.00018890782666858286 Updates: 126/25001, Avg Grad: 0.02945873700082302\n",
            "Epoch: 1, Loss: 0.0006307978765107691 Updates: 131/26001, Avg Grad: 0.01971312239766121\n",
            "Epoch: 1, Loss: 0.00031971250427886844 Updates: 136/27001, Avg Grad: 0.02651713229715824\n",
            "Epoch: 1, Loss: 0.00017853504687082022 Updates: 141/28001, Avg Grad: 0.030623633414506912\n",
            "Epoch: 1, Loss: 2.4355244022444822e-05 Updates: 146/29001, Avg Grad: 0.03287505358457565\n",
            "Epoch 0 iteration 0 Loss: 1.053 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 0.566 | Acc: 81.773% (655/801)\n",
            "Test accuracy: 0.8177278637886047\n",
            "Epoch: 1, Loss: 0.011759010143578053 Updates: 151/30001, Avg Grad: 0.02322043851017952\n",
            "Epoch: 1, Loss: 0.0006140295299701393 Updates: 156/31001, Avg Grad: 0.012780867516994476\n",
            "Epoch: 1, Loss: 0.0012135329889133573 Updates: 161/32001, Avg Grad: 0.0224316269159317\n",
            "Epoch: 1, Loss: 0.0015353383496403694 Updates: 166/33001, Avg Grad: 0.024882536381483078\n",
            "Epoch: 1, Loss: 0.00026006155530922115 Updates: 171/34001, Avg Grad: 0.020944982767105103\n",
            "Epoch 0 iteration 0 Loss: 0.159 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.536 | Acc: 84.894% (680/801)\n",
            "Test accuracy: 0.8489388227462769\n",
            "Epoch: 1, Loss: 0.0012883220333606005 Updates: 176/35001, Avg Grad: 0.01717114821076393\n",
            "Epoch: 1, Loss: 0.00017535753431729972 Updates: 181/36001, Avg Grad: 0.015342304483056068\n",
            "Epoch: 1, Loss: 0.0013431436382234097 Updates: 186/37001, Avg Grad: 0.020710106939077377\n",
            "Epoch: 1, Loss: 1.9796589185716584e-05 Updates: 191/38001, Avg Grad: 0.015735343098640442\n",
            "Epoch: 1, Loss: 0.00010660978296073154 Updates: 196/39001, Avg Grad: 0.015805769711732864\n",
            "Epoch 0 iteration 0 Loss: 0.093 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.490 | Acc: 84.519% (677/801)\n",
            "Test accuracy: 0.8451935052871704\n",
            "Epoch: 1, Loss: 0.0076661319471895695 Updates: 201/40001, Avg Grad: 0.012948150746524334\n",
            "Epoch: 1, Loss: 5.75806825509062e-06 Updates: 206/41001, Avg Grad: 0.019436009228229523\n",
            "Epoch: 1, Loss: 0.00010388072405476123 Updates: 211/42001, Avg Grad: 0.015648800879716873\n",
            "Epoch: 1, Loss: 0.0007110323058441281 Updates: 216/43001, Avg Grad: 0.022907480597496033\n",
            "Epoch: 1, Loss: 0.00028676106012426317 Updates: 221/44001, Avg Grad: 0.012450079433619976\n",
            "Epoch 0 iteration 0 Loss: 0.018 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.509 | Acc: 84.644% (678/801)\n",
            "Test accuracy: 0.846441924571991\n",
            "Epoch: 1, Loss: 7.736951374681666e-05 Updates: 226/45001, Avg Grad: 0.015537519939243793\n",
            "Epoch: 1, Loss: 0.015852684155106544 Updates: 231/46001, Avg Grad: 0.018756520003080368\n",
            "Epoch: 1, Loss: 6.609842967009172e-05 Updates: 236/47001, Avg Grad: 0.022178057581186295\n",
            "Epoch: 1, Loss: 0.0003057977301068604 Updates: 241/48001, Avg Grad: 0.02198384329676628\n",
            "Epoch: 1, Loss: 8.238494046963751e-05 Updates: 246/49001, Avg Grad: 0.012367798946797848\n",
            "Epoch 0 iteration 0 Loss: 0.047 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.425 | Acc: 86.517% (693/801)\n",
            "Test accuracy: 0.8651685118675232\n",
            "Epoch: 1, Loss: 0.0002782647788990289 Updates: 251/50001, Avg Grad: 0.01474492996931076\n",
            "Epoch: 1, Loss: 0.00031693902565166354 Updates: 256/51001, Avg Grad: 0.0158135574311018\n",
            "Epoch: 1, Loss: 0.0009379290277138352 Updates: 261/52001, Avg Grad: 0.0208860095590353\n",
            "Epoch: 1, Loss: 0.00011303843348287046 Updates: 266/53001, Avg Grad: 0.010761377401649952\n",
            "Epoch: 1, Loss: 0.0005909297033213079 Updates: 271/54001, Avg Grad: 0.026838500052690506\n",
            "Epoch 0 iteration 0 Loss: 2.208 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 0.439 | Acc: 86.642% (694/801)\n",
            "Test accuracy: 0.8664169907569885\n",
            "Epoch: 1, Loss: 9.222753578796983e-05 Updates: 276/55001, Avg Grad: 0.01200371515005827\n",
            "Epoch: 1, Loss: 0.0003994257713202387 Updates: 281/56001, Avg Grad: 0.01624407060444355\n",
            "Epoch: 1, Loss: 0.00031762485741637647 Updates: 286/57001, Avg Grad: 0.013735269196331501\n",
            "Epoch: 1, Loss: 0.0020299165043979883 Updates: 291/58001, Avg Grad: 0.0161949023604393\n",
            "Epoch: 1, Loss: 0.0008261498878709972 Updates: 296/59001, Avg Grad: 0.01881990022957325\n",
            "Epoch 1 iteration 0 Loss: 0.042 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.402 | Acc: 87.640% (702/801)\n",
            "Test accuracy: 0.8764045238494873\n",
            "Epoch: 2, Loss: 0.0009296747157350183 Updates: 1/1, Avg Grad: 0.00022593882749788463\n",
            "Epoch: 2, Loss: 2.7235937523073517e-05 Updates: 6/1001, Avg Grad: 0.014302350580692291\n",
            "Epoch: 2, Loss: 1.887746839202009e-05 Updates: 11/2001, Avg Grad: 0.016186468303203583\n",
            "Epoch: 2, Loss: 3.3676209568511695e-05 Updates: 16/3001, Avg Grad: 0.024568961933255196\n",
            "Epoch: 2, Loss: 5.229507587500848e-05 Updates: 21/4001, Avg Grad: 0.011571620590984821\n",
            "Epoch 1 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.451 | Acc: 87.765% (703/801)\n",
            "Test accuracy: 0.8776529431343079\n",
            "Epoch: 2, Loss: 0.00013467464304994792 Updates: 26/5001, Avg Grad: 0.016165466979146004\n",
            "Epoch: 2, Loss: 5.879271338926628e-05 Updates: 31/6001, Avg Grad: 0.015578212216496468\n",
            "Epoch: 2, Loss: 4.510205326369032e-05 Updates: 36/7001, Avg Grad: 0.01862298510968685\n",
            "Epoch: 2, Loss: 6.499236860690871e-06 Updates: 41/8001, Avg Grad: 0.019195470958948135\n",
            "Epoch: 2, Loss: 0.010548665188252926 Updates: 46/9001, Avg Grad: 0.013196143321692944\n",
            "Epoch 1 iteration 0 Loss: 0.018 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.367 | Acc: 89.888% (720/801)\n",
            "Test accuracy: 0.898876428604126\n",
            "Epoch: 2, Loss: 2.4190345357055776e-05 Updates: 51/10001, Avg Grad: 0.02211856283247471\n",
            "Epoch: 2, Loss: 1.5887979316175915e-05 Updates: 56/11001, Avg Grad: 0.019023418426513672\n",
            "Epoch: 2, Loss: 0.01742268167436123 Updates: 61/12001, Avg Grad: 0.017012545838952065\n",
            "Epoch: 2, Loss: 0.015195570886135101 Updates: 66/13001, Avg Grad: 0.017308007925748825\n",
            "Epoch: 2, Loss: 0.0002673126000445336 Updates: 71/14001, Avg Grad: 0.015573414973914623\n",
            "Epoch 1 iteration 0 Loss: 0.295 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.354 | Acc: 88.514% (709/801)\n",
            "Test accuracy: 0.8851435780525208\n",
            "Epoch: 2, Loss: 0.002249851357191801 Updates: 76/15001, Avg Grad: 0.013926014304161072\n",
            "Epoch: 2, Loss: 0.014218142256140709 Updates: 81/16001, Avg Grad: 0.012072859331965446\n",
            "Epoch: 2, Loss: 0.0004370405222289264 Updates: 86/17001, Avg Grad: 0.01701214537024498\n",
            "Epoch: 2, Loss: 0.0010420122416689992 Updates: 91/18001, Avg Grad: 0.018559588119387627\n",
            "Epoch: 2, Loss: 2.0075614884262905e-05 Updates: 96/19001, Avg Grad: 0.011765052564442158\n",
            "Epoch 1 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.364 | Acc: 88.764% (711/801)\n",
            "Test accuracy: 0.8876404762268066\n",
            "Epoch: 2, Loss: 6.105720240157098e-05 Updates: 101/20001, Avg Grad: 0.009652569890022278\n",
            "Epoch: 2, Loss: 0.00028909285902045667 Updates: 106/21001, Avg Grad: 0.01739269122481346\n",
            "Epoch: 2, Loss: 3.6874600482406095e-05 Updates: 111/22001, Avg Grad: 0.01301660854369402\n",
            "Epoch: 2, Loss: 0.00017278263112530112 Updates: 116/23001, Avg Grad: 0.01653272844851017\n",
            "Epoch: 2, Loss: 4.347088179201819e-05 Updates: 121/24001, Avg Grad: 0.011080975644290447\n",
            "Epoch 1 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.354 | Acc: 88.639% (710/801)\n",
            "Test accuracy: 0.8863919973373413\n",
            "Epoch: 2, Loss: 0.0005759963532909751 Updates: 126/25001, Avg Grad: 0.015848759561777115\n",
            "Epoch: 2, Loss: 3.824589657597244e-05 Updates: 131/26001, Avg Grad: 0.012868125922977924\n",
            "Epoch: 2, Loss: 6.298402149695903e-05 Updates: 136/27001, Avg Grad: 0.012787586078047752\n",
            "Epoch: 2, Loss: 0.030973734334111214 Updates: 141/28001, Avg Grad: 0.01100566703826189\n",
            "Epoch: 2, Loss: 0.00017582654254511 Updates: 146/29001, Avg Grad: 0.01118573360145092\n",
            "Epoch 1 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.318 | Acc: 90.012% (721/801)\n",
            "Test accuracy: 0.9001248478889465\n",
            "Epoch: 2, Loss: 4.362348590802867e-06 Updates: 151/30001, Avg Grad: 0.010021833702921867\n",
            "Epoch: 2, Loss: 0.00011101888230768964 Updates: 156/31001, Avg Grad: 0.013585217297077179\n",
            "Epoch: 2, Loss: 6.446408951887861e-05 Updates: 161/32001, Avg Grad: 0.013736873865127563\n",
            "Epoch: 2, Loss: 1.0983929996655206e-06 Updates: 166/33001, Avg Grad: 0.01085155550390482\n",
            "Epoch: 2, Loss: 0.002493263455107808 Updates: 171/34001, Avg Grad: 0.016563251614570618\n",
            "Epoch 1 iteration 0 Loss: 0.027 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.285 | Acc: 91.386% (732/801)\n",
            "Test accuracy: 0.9138576984405518\n",
            "Epoch: 2, Loss: 0.013301759958267212 Updates: 176/35001, Avg Grad: 0.011729740537703037\n",
            "Epoch: 2, Loss: 2.5079427359742112e-05 Updates: 181/36001, Avg Grad: 0.01773933321237564\n",
            "Epoch: 2, Loss: 1.5483936294913292e-05 Updates: 186/37001, Avg Grad: 0.00854278914630413\n",
            "Epoch: 2, Loss: 0.001554769929498434 Updates: 191/38001, Avg Grad: 0.009877466596662998\n",
            "Epoch: 2, Loss: 0.00042740412754938006 Updates: 196/39001, Avg Grad: 0.008243209682404995\n",
            "Epoch 1 iteration 0 Loss: 0.055 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.323 | Acc: 91.885% (736/801)\n",
            "Test accuracy: 0.9188514351844788\n",
            "Epoch: 2, Loss: 0.00046053019468672574 Updates: 201/40001, Avg Grad: 0.011058309115469456\n",
            "Epoch: 2, Loss: 7.724752322246786e-06 Updates: 206/41001, Avg Grad: 0.010626648552715778\n",
            "Epoch: 2, Loss: 3.905558969563572e-06 Updates: 211/42001, Avg Grad: 0.01737658493220806\n",
            "Epoch: 2, Loss: 4.082751559053577e-07 Updates: 216/43001, Avg Grad: 0.017494695261120796\n",
            "Epoch: 2, Loss: 7.398081652354449e-05 Updates: 221/44001, Avg Grad: 0.0120004341006279\n",
            "Epoch 1 iteration 0 Loss: 0.377 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.316 | Acc: 91.386% (732/801)\n",
            "Test accuracy: 0.9138576984405518\n",
            "Epoch: 2, Loss: 4.1783023334573954e-05 Updates: 226/45001, Avg Grad: 0.01470249891281128\n",
            "Epoch: 2, Loss: 3.4338074328843504e-05 Updates: 231/46001, Avg Grad: 0.011819673702120781\n",
            "Epoch: 2, Loss: 0.003308543236926198 Updates: 236/47001, Avg Grad: 0.012964983470737934\n",
            "Epoch: 2, Loss: 5.319863703334704e-06 Updates: 241/48001, Avg Grad: 0.013014634139835835\n",
            "Epoch: 2, Loss: 0.001161973807029426 Updates: 246/49001, Avg Grad: 0.00865653995424509\n",
            "Epoch 1 iteration 0 Loss: 0.072 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.346 | Acc: 89.513% (717/801)\n",
            "Test accuracy: 0.8951311111450195\n",
            "Epoch: 2, Loss: 0.00028370003565214574 Updates: 251/50001, Avg Grad: 0.012113494798541069\n",
            "Epoch: 2, Loss: 0.005447119940072298 Updates: 256/51001, Avg Grad: 0.012449490837752819\n",
            "Epoch: 2, Loss: 0.006183444056659937 Updates: 261/52001, Avg Grad: 0.013951743952929974\n",
            "Epoch: 2, Loss: 0.0011981293791905046 Updates: 266/53001, Avg Grad: 0.010127313435077667\n",
            "Epoch: 2, Loss: 0.00025580861256457865 Updates: 271/54001, Avg Grad: 0.01646525040268898\n",
            "Epoch 1 iteration 0 Loss: 0.252 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.280 | Acc: 92.509% (741/801)\n",
            "Test accuracy: 0.9250936508178711\n",
            "Epoch: 2, Loss: 0.00015317968791350722 Updates: 276/55001, Avg Grad: 0.011123191565275192\n",
            "Epoch: 2, Loss: 0.00021538950386457145 Updates: 281/56001, Avg Grad: 0.012455468997359276\n",
            "Epoch: 2, Loss: 0.000413662230130285 Updates: 286/57001, Avg Grad: 0.009342805482447147\n",
            "Epoch: 2, Loss: 0.0001710820070002228 Updates: 291/58001, Avg Grad: 0.010411418043076992\n",
            "Epoch: 2, Loss: 0.00024005433078855276 Updates: 296/59001, Avg Grad: 0.009613970294594765\n",
            "Epoch 2 iteration 0 Loss: 0.449 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.319 | Acc: 90.262% (723/801)\n",
            "Test accuracy: 0.9026217460632324\n",
            "Epoch: 3, Loss: 2.577860141173005e-05 Updates: 1/1, Avg Grad: 7.6125384111946914e-06\n",
            "Epoch: 3, Loss: 0.0011883844854310155 Updates: 6/1001, Avg Grad: 0.009954830631613731\n",
            "Epoch: 3, Loss: 0.00023564446019008756 Updates: 11/2001, Avg Grad: 0.013095920905470848\n",
            "Epoch: 3, Loss: 1.4958527572161984e-06 Updates: 16/3001, Avg Grad: 0.011555813252925873\n",
            "Epoch: 3, Loss: 5.472523480420932e-05 Updates: 21/4001, Avg Grad: 0.012766587547957897\n",
            "Epoch 2 iteration 0 Loss: 1.304 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 800 Loss: 0.294 | Acc: 90.512% (725/801)\n",
            "Test accuracy: 0.9051185846328735\n",
            "Epoch: 3, Loss: 0.0027237804606556892 Updates: 26/5001, Avg Grad: 0.013356621377170086\n",
            "Epoch: 3, Loss: 0.019665120169520378 Updates: 31/6001, Avg Grad: 0.017211075872182846\n",
            "Epoch: 3, Loss: 0.000191431405255571 Updates: 36/7001, Avg Grad: 0.011781103909015656\n",
            "Epoch: 3, Loss: 0.0023411037400364876 Updates: 41/8001, Avg Grad: 0.011319110170006752\n",
            "Epoch: 3, Loss: 0.004996453877538443 Updates: 46/9001, Avg Grad: 0.019544852897524834\n",
            "Epoch 2 iteration 0 Loss: 0.233 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.297 | Acc: 92.010% (737/801)\n",
            "Test accuracy: 0.9200998544692993\n",
            "Epoch: 3, Loss: 8.410886039200705e-06 Updates: 51/10001, Avg Grad: 0.011038859374821186\n",
            "Epoch: 3, Loss: 0.007146239746361971 Updates: 56/11001, Avg Grad: 0.010914558544754982\n",
            "Epoch: 3, Loss: 0.009356978349387646 Updates: 61/12001, Avg Grad: 0.01960226334631443\n",
            "Epoch: 3, Loss: 7.060333155095577e-05 Updates: 66/13001, Avg Grad: 0.013514948077499866\n",
            "Epoch: 3, Loss: 1.1970365449087694e-05 Updates: 71/14001, Avg Grad: 0.011165703646838665\n",
            "Epoch 2 iteration 0 Loss: 0.012 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.289 | Acc: 91.136% (730/801)\n",
            "Test accuracy: 0.9113608002662659\n",
            "Epoch: 3, Loss: 0.00019563281966838986 Updates: 76/15001, Avg Grad: 0.013961966149508953\n",
            "Epoch: 3, Loss: 2.539704837545287e-06 Updates: 81/16001, Avg Grad: 0.008161607198417187\n",
            "Epoch: 3, Loss: 4.651951894629747e-05 Updates: 86/17001, Avg Grad: 0.01006352435797453\n",
            "Epoch: 3, Loss: 8.278784662252292e-06 Updates: 91/18001, Avg Grad: 0.011871623806655407\n",
            "Epoch: 3, Loss: 0.00514628179371357 Updates: 96/19001, Avg Grad: 0.01679682359099388\n",
            "Epoch 2 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.324 | Acc: 90.512% (725/801)\n",
            "Test accuracy: 0.9051185846328735\n",
            "Epoch: 3, Loss: 0.016920309513807297 Updates: 101/20001, Avg Grad: 0.012586236000061035\n",
            "Epoch: 3, Loss: 1.3616006071970332e-05 Updates: 106/21001, Avg Grad: 0.010829967446625233\n",
            "Epoch: 3, Loss: 0.005267174914479256 Updates: 111/22001, Avg Grad: 0.010241552256047726\n",
            "Epoch: 3, Loss: 0.0009497891296632588 Updates: 116/23001, Avg Grad: 0.010831953957676888\n",
            "Epoch: 3, Loss: 0.0005836624186486006 Updates: 121/24001, Avg Grad: 0.013969396241009235\n",
            "Epoch 2 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.276 | Acc: 91.136% (730/801)\n",
            "Test accuracy: 0.9113608002662659\n",
            "Epoch: 3, Loss: 0.0002048867754638195 Updates: 126/25001, Avg Grad: 0.011957356706261635\n",
            "Epoch: 3, Loss: 0.00200112396851182 Updates: 131/26001, Avg Grad: 0.013060586526989937\n",
            "Epoch: 3, Loss: 0.007458069361746311 Updates: 136/27001, Avg Grad: 0.007834444753825665\n",
            "Epoch: 3, Loss: 0.0005867858999408782 Updates: 141/28001, Avg Grad: 0.011540677398443222\n",
            "Epoch: 3, Loss: 0.00142352981492877 Updates: 146/29001, Avg Grad: 0.014969388954341412\n",
            "Epoch 2 iteration 0 Loss: 0.033 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.256 | Acc: 92.385% (740/801)\n",
            "Test accuracy: 0.9238451719284058\n",
            "Epoch: 3, Loss: 0.0004164434503763914 Updates: 151/30001, Avg Grad: 0.015534657053649426\n",
            "Epoch: 3, Loss: 1.3885953649150906e-06 Updates: 156/31001, Avg Grad: 0.012165931984782219\n",
            "Epoch: 3, Loss: 0.0019397572614252567 Updates: 161/32001, Avg Grad: 0.010560227558016777\n",
            "Epoch: 3, Loss: 4.41118163507781e-06 Updates: 166/33001, Avg Grad: 0.01535074133425951\n",
            "Epoch: 3, Loss: 0.0013741017319262028 Updates: 171/34001, Avg Grad: 0.020476141944527626\n",
            "Epoch 2 iteration 0 Loss: 0.120 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.228 | Acc: 92.884% (744/801)\n",
            "Test accuracy: 0.9288389682769775\n",
            "Epoch: 3, Loss: 0.002364435466006398 Updates: 176/35001, Avg Grad: 0.01976270042359829\n",
            "Epoch: 3, Loss: 4.4290472942520864e-06 Updates: 181/36001, Avg Grad: 0.016037022694945335\n",
            "Epoch: 3, Loss: 7.694400119362399e-05 Updates: 186/37001, Avg Grad: 0.011761852540075779\n",
            "Epoch: 3, Loss: 4.386617001728155e-05 Updates: 191/38001, Avg Grad: 0.014497777447104454\n",
            "Epoch: 3, Loss: 2.4835671865730546e-05 Updates: 196/39001, Avg Grad: 0.009486498311161995\n",
            "Epoch 2 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.294 | Acc: 91.885% (736/801)\n",
            "Test accuracy: 0.9188514351844788\n",
            "Epoch: 3, Loss: 0.010840450413525105 Updates: 201/40001, Avg Grad: 0.0093643544241786\n",
            "Epoch: 3, Loss: 0.0015241531655192375 Updates: 206/41001, Avg Grad: 0.0121134202927351\n",
            "Epoch: 3, Loss: 9.218296327162534e-06 Updates: 211/42001, Avg Grad: 0.010719150304794312\n",
            "Epoch: 3, Loss: 0.0015311386669054627 Updates: 216/43001, Avg Grad: 0.012192754074931145\n",
            "Epoch: 3, Loss: 5.455616246763384e-06 Updates: 221/44001, Avg Grad: 0.010095175355672836\n",
            "Epoch 2 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.285 | Acc: 91.760% (735/801)\n",
            "Test accuracy: 0.9176030158996582\n",
            "Epoch: 3, Loss: 0.0007429332472383976 Updates: 226/45001, Avg Grad: 0.009466652758419514\n",
            "Epoch: 3, Loss: 0.0002559955173637718 Updates: 231/46001, Avg Grad: 0.011880100704729557\n",
            "Epoch: 3, Loss: 0.0017417680937796831 Updates: 236/47001, Avg Grad: 0.009426409378647804\n",
            "Epoch: 3, Loss: 6.380443664966151e-05 Updates: 241/48001, Avg Grad: 0.012502322904765606\n",
            "Epoch: 3, Loss: 0.023162158206105232 Updates: 246/49001, Avg Grad: 0.009139860048890114\n",
            "Epoch 2 iteration 0 Loss: 0.042 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.257 | Acc: 91.885% (736/801)\n",
            "Test accuracy: 0.9188514351844788\n",
            "Epoch: 3, Loss: 8.67726921569556e-05 Updates: 251/50001, Avg Grad: 0.01426394097507\n",
            "Epoch: 3, Loss: 1.541381971037481e-05 Updates: 256/51001, Avg Grad: 0.009596777148544788\n",
            "Epoch: 3, Loss: 0.002691441448405385 Updates: 261/52001, Avg Grad: 0.010861585848033428\n",
            "Epoch: 3, Loss: 0.00011644386540865526 Updates: 266/53001, Avg Grad: 0.012154214084148407\n",
            "Epoch: 3, Loss: 0.0002365905384067446 Updates: 271/54001, Avg Grad: 0.012854780070483685\n",
            "Epoch 2 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.248 | Acc: 92.759% (743/801)\n",
            "Test accuracy: 0.9275904893875122\n",
            "Epoch: 3, Loss: 0.00014358390762936324 Updates: 276/55001, Avg Grad: 0.00823674164712429\n",
            "Epoch: 3, Loss: 1.4224066035239957e-05 Updates: 281/56001, Avg Grad: 0.013953988440334797\n",
            "Epoch: 3, Loss: 0.00011930285108974203 Updates: 286/57001, Avg Grad: 0.008287450298666954\n",
            "Epoch: 3, Loss: 8.751425048103556e-05 Updates: 291/58001, Avg Grad: 0.008352620527148247\n",
            "Epoch: 3, Loss: 0.0003968299424741417 Updates: 296/59001, Avg Grad: 0.008173364214599133\n",
            "Epoch 3 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.270 | Acc: 91.386% (732/801)\n",
            "Test accuracy: 0.9138576984405518\n",
            "Epoch: 4, Loss: 0.00029276672285050154 Updates: 1/1, Avg Grad: 8.602227899245918e-05\n",
            "Epoch: 4, Loss: 4.481319774640724e-05 Updates: 6/1001, Avg Grad: 0.011056327261030674\n",
            "Epoch: 4, Loss: 0.0064103808254003525 Updates: 11/2001, Avg Grad: 0.010500690899789333\n",
            "Epoch: 4, Loss: 0.0007388025405816734 Updates: 16/3001, Avg Grad: 0.011215602979063988\n",
            "Epoch: 4, Loss: 0.0008427078137174249 Updates: 21/4001, Avg Grad: 0.01032047439366579\n",
            "Epoch 3 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.234 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 4, Loss: 0.0001065187607309781 Updates: 26/5001, Avg Grad: 0.013004841282963753\n",
            "Epoch: 4, Loss: 1.2472794878704008e-05 Updates: 31/6001, Avg Grad: 0.008211689069867134\n",
            "Epoch: 4, Loss: 3.5599394323071465e-05 Updates: 36/7001, Avg Grad: 0.007971899583935738\n",
            "Epoch: 4, Loss: 0.00014515320071950555 Updates: 41/8001, Avg Grad: 0.011334965005517006\n",
            "Epoch: 4, Loss: 3.280168311903253e-05 Updates: 46/9001, Avg Grad: 0.01734200306236744\n",
            "Epoch 3 iteration 0 Loss: 0.069 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.257 | Acc: 91.635% (734/801)\n",
            "Test accuracy: 0.9163545370101929\n",
            "Epoch: 4, Loss: 8.427601278526708e-05 Updates: 51/10001, Avg Grad: 0.008609376847743988\n",
            "Epoch: 4, Loss: 7.540362275904045e-05 Updates: 56/11001, Avg Grad: 0.00937710702419281\n",
            "Epoch: 4, Loss: 6.164682417875156e-06 Updates: 61/12001, Avg Grad: 0.009375551715493202\n",
            "Epoch: 4, Loss: 3.277207360952161e-05 Updates: 66/13001, Avg Grad: 0.010580095462501049\n",
            "Epoch: 4, Loss: 8.353162411367521e-05 Updates: 71/14001, Avg Grad: 0.006981738843023777\n",
            "Epoch 3 iteration 0 Loss: 0.184 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.287 | Acc: 92.010% (737/801)\n",
            "Test accuracy: 0.9200998544692993\n",
            "Epoch: 4, Loss: 0.0017657914431765676 Updates: 76/15001, Avg Grad: 0.008189089596271515\n",
            "Epoch: 4, Loss: 0.0009587848326191306 Updates: 81/16001, Avg Grad: 0.016123047098517418\n",
            "Epoch: 4, Loss: 1.9938477635150775e-05 Updates: 86/17001, Avg Grad: 0.011100757867097855\n",
            "Epoch: 4, Loss: 6.947456768102711e-06 Updates: 91/18001, Avg Grad: 0.013000828213989735\n",
            "Epoch: 4, Loss: 4.982700261280115e-07 Updates: 96/19001, Avg Grad: 0.014901812188327312\n",
            "Epoch 3 iteration 0 Loss: 0.016 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.229 | Acc: 93.758% (751/801)\n",
            "Test accuracy: 0.937578022480011\n",
            "Epoch: 4, Loss: 1.264343154616654e-05 Updates: 101/20001, Avg Grad: 0.019277289509773254\n",
            "Epoch: 4, Loss: 0.00036773987812921405 Updates: 106/21001, Avg Grad: 0.00928597990423441\n",
            "Epoch: 4, Loss: 0.0009447064367122948 Updates: 111/22001, Avg Grad: 0.008719990029931068\n",
            "Epoch: 4, Loss: 9.714613042888232e-07 Updates: 116/23001, Avg Grad: 0.01283455640077591\n",
            "Epoch: 4, Loss: 1.33193789224606e-05 Updates: 121/24001, Avg Grad: 0.0113513870164752\n",
            "Epoch 3 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.257 | Acc: 91.760% (735/801)\n",
            "Test accuracy: 0.9176030158996582\n",
            "Epoch: 4, Loss: 0.0001396885927533731 Updates: 126/25001, Avg Grad: 0.008477873168885708\n",
            "Epoch: 4, Loss: 4.2173716792603955e-05 Updates: 131/26001, Avg Grad: 0.010759305208921432\n",
            "Epoch: 4, Loss: 1.875276939244941e-05 Updates: 136/27001, Avg Grad: 0.008209715597331524\n",
            "Epoch: 4, Loss: 2.1188599930610508e-05 Updates: 141/28001, Avg Grad: 0.009935121983289719\n",
            "Epoch: 4, Loss: 0.00013097214105073363 Updates: 146/29001, Avg Grad: 0.010342626832425594\n",
            "Epoch 3 iteration 0 Loss: 0.087 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.230 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 4, Loss: 0.0008882735855877399 Updates: 151/30001, Avg Grad: 0.010641920380294323\n",
            "Epoch: 4, Loss: 1.2640547311093542e-06 Updates: 156/31001, Avg Grad: 0.010848709382116795\n",
            "Epoch: 4, Loss: 0.00017042590479832143 Updates: 161/32001, Avg Grad: 0.017552558332681656\n",
            "Epoch: 4, Loss: 0.0004590051539707929 Updates: 166/33001, Avg Grad: 0.008931375108659267\n",
            "Epoch: 4, Loss: 0.002469290979206562 Updates: 171/34001, Avg Grad: 0.01056446973234415\n",
            "Epoch 3 iteration 0 Loss: 0.162 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.189 | Acc: 94.007% (753/801)\n",
            "Test accuracy: 0.9400749206542969\n",
            "Epoch: 4, Loss: 8.10204801382497e-06 Updates: 176/35001, Avg Grad: 0.010910743847489357\n",
            "Epoch: 4, Loss: 0.003042476484552026 Updates: 181/36001, Avg Grad: 0.019084369763731956\n",
            "Epoch: 4, Loss: 0.0008396137272939086 Updates: 186/37001, Avg Grad: 0.008429504930973053\n",
            "Epoch: 4, Loss: 0.00298200617544353 Updates: 191/38001, Avg Grad: 0.011575460433959961\n",
            "Epoch: 4, Loss: 7.748003554297611e-07 Updates: 196/39001, Avg Grad: 0.008781695738434792\n",
            "Epoch 3 iteration 0 Loss: 0.014 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.247 | Acc: 94.007% (753/801)\n",
            "Test accuracy: 0.9400749206542969\n",
            "Epoch: 4, Loss: 2.3788516045897268e-06 Updates: 201/40001, Avg Grad: 0.011192506179213524\n",
            "Epoch: 4, Loss: 0.0009523782646283507 Updates: 206/41001, Avg Grad: 0.0133102061226964\n",
            "Epoch: 4, Loss: 0.00028344435850158334 Updates: 211/42001, Avg Grad: 0.011209352873265743\n",
            "Epoch: 4, Loss: 3.7091143894940615e-05 Updates: 216/43001, Avg Grad: 0.012327970936894417\n",
            "Epoch: 4, Loss: 2.2804508262197487e-05 Updates: 221/44001, Avg Grad: 0.014848376624286175\n",
            "Epoch 3 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.270 | Acc: 92.759% (743/801)\n",
            "Test accuracy: 0.9275904893875122\n",
            "Epoch: 4, Loss: 1.0306338481314015e-05 Updates: 226/45001, Avg Grad: 0.011009706184267998\n",
            "Epoch: 4, Loss: 0.002381719183176756 Updates: 231/46001, Avg Grad: 0.010075435973703861\n",
            "Epoch: 4, Loss: 7.73431092966348e-05 Updates: 236/47001, Avg Grad: 0.01562136597931385\n",
            "Epoch: 4, Loss: 1.7187026060128119e-06 Updates: 241/48001, Avg Grad: 0.008884536102414131\n",
            "Epoch: 4, Loss: 1.9541294022928923e-05 Updates: 246/49001, Avg Grad: 0.01271800883114338\n",
            "Epoch 3 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.289 | Acc: 91.261% (731/801)\n",
            "Test accuracy: 0.9126092195510864\n",
            "Epoch: 4, Loss: 0.00043756538070738316 Updates: 251/50001, Avg Grad: 0.017579419538378716\n",
            "Epoch: 4, Loss: 0.0008437989745289087 Updates: 256/51001, Avg Grad: 0.011363635770976543\n",
            "Epoch: 4, Loss: 2.557638981670607e-05 Updates: 261/52001, Avg Grad: 0.011030448600649834\n",
            "Epoch: 4, Loss: 7.133756753319176e-06 Updates: 266/53001, Avg Grad: 0.011999749578535557\n",
            "Epoch: 4, Loss: 8.359711500816047e-06 Updates: 271/54001, Avg Grad: 0.009934253059327602\n",
            "Epoch 3 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.237 | Acc: 93.758% (751/801)\n",
            "Test accuracy: 0.937578022480011\n",
            "Epoch: 4, Loss: 6.353451453833259e-07 Updates: 276/55001, Avg Grad: 0.011861449107527733\n",
            "Epoch: 4, Loss: 0.0025892460253089666 Updates: 281/56001, Avg Grad: 0.007904564030468464\n",
            "Epoch: 4, Loss: 0.005736042279750109 Updates: 286/57001, Avg Grad: 0.009489377029240131\n",
            "Epoch: 4, Loss: 5.298572318679362e-07 Updates: 291/58001, Avg Grad: 0.014056029729545116\n",
            "Epoch: 4, Loss: 9.165345545625314e-06 Updates: 296/59001, Avg Grad: 0.008321046829223633\n",
            "Epoch 4 iteration 0 Loss: 0.108 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.231 | Acc: 92.884% (744/801)\n",
            "Test accuracy: 0.9288389682769775\n",
            "Epoch: 5, Loss: 9.807798051042482e-05 Updates: 1/1, Avg Grad: 1.6510539353475906e-05\n",
            "Epoch: 5, Loss: 0.0004100272781215608 Updates: 6/1001, Avg Grad: 0.007695015519857407\n",
            "Epoch: 5, Loss: 0.005447193514555693 Updates: 11/2001, Avg Grad: 0.009844573214650154\n",
            "Epoch: 5, Loss: 2.8357999326544814e-05 Updates: 16/3001, Avg Grad: 0.010908333584666252\n",
            "Epoch: 5, Loss: 6.3831575971562415e-06 Updates: 21/4001, Avg Grad: 0.008190632797777653\n",
            "Epoch 4 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.207 | Acc: 93.383% (748/801)\n",
            "Test accuracy: 0.9338327050209045\n",
            "Epoch: 5, Loss: 3.308155328340945e-06 Updates: 26/5001, Avg Grad: 0.014004508964717388\n",
            "Epoch: 5, Loss: 5.883092399017187e-06 Updates: 31/6001, Avg Grad: 0.017587635666131973\n",
            "Epoch: 5, Loss: 3.805246888077818e-05 Updates: 36/7001, Avg Grad: 0.007545041851699352\n",
            "Epoch: 5, Loss: 1.3853176824341062e-05 Updates: 41/8001, Avg Grad: 0.012199734337627888\n",
            "Epoch: 5, Loss: 0.00029297976288944483 Updates: 46/9001, Avg Grad: 0.00999924261122942\n",
            "Epoch 4 iteration 0 Loss: 3.644 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 800 Loss: 0.312 | Acc: 91.511% (733/801)\n",
            "Test accuracy: 0.9151061177253723\n",
            "Epoch: 5, Loss: 0.00032056678901426494 Updates: 51/10001, Avg Grad: 0.007250218186527491\n",
            "Epoch: 5, Loss: 0.004764792043715715 Updates: 56/11001, Avg Grad: 0.00916253961622715\n",
            "Epoch: 5, Loss: 5.257345037534833e-06 Updates: 61/12001, Avg Grad: 0.007120432797819376\n",
            "Epoch: 5, Loss: 1.691457327979151e-05 Updates: 66/13001, Avg Grad: 0.010101394727826118\n",
            "Epoch: 5, Loss: 0.000237447748077102 Updates: 71/14001, Avg Grad: 0.009983927011489868\n",
            "Epoch 4 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.236 | Acc: 92.884% (744/801)\n",
            "Test accuracy: 0.9288389682769775\n",
            "Epoch: 5, Loss: 2.658296693880402e-07 Updates: 76/15001, Avg Grad: 0.013487030752003193\n",
            "Epoch: 5, Loss: 2.2042900127416942e-06 Updates: 81/16001, Avg Grad: 0.008138291537761688\n",
            "Epoch: 5, Loss: 0.00027397030498832464 Updates: 86/17001, Avg Grad: 0.009294180199503899\n",
            "Epoch: 5, Loss: 1.737769480314455e-06 Updates: 91/18001, Avg Grad: 0.011614610441029072\n",
            "Epoch: 5, Loss: 1.9847952614782116e-07 Updates: 96/19001, Avg Grad: 0.01020610798150301\n",
            "Epoch 4 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.246 | Acc: 92.385% (740/801)\n",
            "Test accuracy: 0.9238451719284058\n",
            "Epoch: 5, Loss: 0.006653091870248318 Updates: 101/20001, Avg Grad: 0.011783845722675323\n",
            "Epoch: 5, Loss: 0.0002940926351584494 Updates: 106/21001, Avg Grad: 0.00867778342217207\n",
            "Epoch: 5, Loss: 0.0002459194220136851 Updates: 111/22001, Avg Grad: 0.009067188948392868\n",
            "Epoch: 5, Loss: 0.026462165638804436 Updates: 116/23001, Avg Grad: 0.01400630921125412\n",
            "Epoch: 5, Loss: 0.00010173307964578271 Updates: 121/24001, Avg Grad: 0.009220763109624386\n",
            "Epoch 4 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.224 | Acc: 93.633% (750/801)\n",
            "Test accuracy: 0.9363296031951904\n",
            "Epoch: 5, Loss: 1.1770531500587822e-06 Updates: 126/25001, Avg Grad: 0.007904307916760445\n",
            "Epoch: 5, Loss: 4.180253017693758e-05 Updates: 131/26001, Avg Grad: 0.010892079211771488\n",
            "Epoch: 5, Loss: 1.694189995760098e-05 Updates: 136/27001, Avg Grad: 0.007886240258812904\n",
            "Epoch: 5, Loss: 6.138901653685025e-07 Updates: 141/28001, Avg Grad: 0.01204262487590313\n",
            "Epoch: 5, Loss: 0.00017091208428610116 Updates: 146/29001, Avg Grad: 0.013440201990306377\n",
            "Epoch 4 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.210 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 5, Loss: 0.00220570364035666 Updates: 151/30001, Avg Grad: 0.01283476036041975\n",
            "Epoch: 5, Loss: 6.064669378247345e-06 Updates: 156/31001, Avg Grad: 0.008813364431262016\n",
            "Epoch: 5, Loss: 0.0009797230595722795 Updates: 161/32001, Avg Grad: 0.01097930409014225\n",
            "Epoch: 5, Loss: 0.014969723299145699 Updates: 166/33001, Avg Grad: 0.013047859072685242\n",
            "Epoch: 5, Loss: 3.6029941838933155e-06 Updates: 171/34001, Avg Grad: 0.014368203468620777\n",
            "Epoch 4 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.256 | Acc: 92.634% (742/801)\n",
            "Test accuracy: 0.9263420701026917\n",
            "Epoch: 5, Loss: 1.8074821355185122e-06 Updates: 176/35001, Avg Grad: 0.011849994771182537\n",
            "Epoch: 5, Loss: 3.359450784046203e-05 Updates: 181/36001, Avg Grad: 0.014006157405674458\n",
            "Epoch: 5, Loss: 2.127433162968373e-06 Updates: 186/37001, Avg Grad: 0.0072821034118533134\n",
            "Epoch: 5, Loss: 0.0010606464929878712 Updates: 191/38001, Avg Grad: 0.012857106514275074\n",
            "Epoch: 5, Loss: 4.709528639068594e-06 Updates: 196/39001, Avg Grad: 0.008145377971231937\n",
            "Epoch 4 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.242 | Acc: 92.135% (738/801)\n",
            "Test accuracy: 0.9213483333587646\n",
            "Epoch: 5, Loss: 1.5036481272545643e-05 Updates: 201/40001, Avg Grad: 0.013316755183041096\n",
            "Epoch: 5, Loss: 0.004447021521627903 Updates: 206/41001, Avg Grad: 0.01355818659067154\n",
            "Epoch: 5, Loss: 2.2138224267109763e-06 Updates: 211/42001, Avg Grad: 0.012646740302443504\n",
            "Epoch: 5, Loss: 0.0037957429885864258 Updates: 216/43001, Avg Grad: 0.011968553997576237\n",
            "Epoch: 5, Loss: 0.006199523340910673 Updates: 221/44001, Avg Grad: 0.011419350281357765\n",
            "Epoch 4 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.241 | Acc: 92.634% (742/801)\n",
            "Test accuracy: 0.9263420701026917\n",
            "Epoch: 5, Loss: 0.0001590759347891435 Updates: 226/45001, Avg Grad: 0.0075372206047177315\n",
            "Epoch: 5, Loss: 0.000143681769259274 Updates: 231/46001, Avg Grad: 0.009516336023807526\n",
            "Epoch: 5, Loss: 2.4463790396112017e-05 Updates: 236/47001, Avg Grad: 0.007479764521121979\n",
            "Epoch: 5, Loss: 3.421257497393526e-05 Updates: 241/48001, Avg Grad: 0.00957387313246727\n",
            "Epoch: 5, Loss: 2.2918693503015675e-06 Updates: 246/49001, Avg Grad: 0.010234779678285122\n",
            "Epoch 4 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.243 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 5, Loss: 1.5081644960446283e-05 Updates: 251/50001, Avg Grad: 0.006353397853672504\n",
            "Epoch: 5, Loss: 1.9469046037556836e-06 Updates: 256/51001, Avg Grad: 0.007994204759597778\n",
            "Epoch: 5, Loss: 3.991306584794074e-05 Updates: 261/52001, Avg Grad: 0.008725343272089958\n",
            "Epoch: 5, Loss: 0.0015577843878418207 Updates: 266/53001, Avg Grad: 0.010403631255030632\n",
            "Epoch: 5, Loss: 1.3291659684000479e-07 Updates: 271/54001, Avg Grad: 0.014935575425624847\n",
            "Epoch 4 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.243 | Acc: 91.635% (734/801)\n",
            "Test accuracy: 0.9163545370101929\n",
            "Epoch: 5, Loss: 9.901250450639054e-06 Updates: 276/55001, Avg Grad: 0.008629744872450829\n",
            "Epoch: 5, Loss: 1.4073098100197967e-05 Updates: 281/56001, Avg Grad: 0.006854604929685593\n",
            "Epoch: 5, Loss: 0.0068470328114926815 Updates: 286/57001, Avg Grad: 0.010199366137385368\n",
            "Epoch: 5, Loss: 2.8413116524461657e-05 Updates: 291/58001, Avg Grad: 0.009415299631655216\n",
            "Epoch: 5, Loss: 0.0012604448711499572 Updates: 296/59001, Avg Grad: 0.011407067999243736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 5,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.01, #0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 2,\n",
        "    \"test_interval\": 5000,\n",
        "    \"dampening\": 0.1\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "#optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss = net_trainer(net, train_loader, test_loader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VLbF5JWvlt-",
        "outputId": "da925f50-c9a7-4666-e9a6-878f2f682f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 2.568 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 2.985 | Acc: 10.861% (87/801)\n",
            "Test accuracy: 0.1086142286658287\n",
            "Epoch: 1, Loss: 0.030000319704413414 Updates: 1/1, Avg Grad: 0.0002007457660511136\n",
            "Epoch: 1, Loss: 0.008897189050912857 Updates: 6/1001, Avg Grad: 0.012411796487867832\n",
            "Epoch: 1, Loss: 0.020243948325514793 Updates: 11/2001, Avg Grad: 0.018086981028318405\n",
            "Epoch: 1, Loss: 0.006830353755503893 Updates: 16/3001, Avg Grad: 0.01565719209611416\n",
            "Epoch: 1, Loss: 0.011096806265413761 Updates: 21/4001, Avg Grad: 0.014740079641342163\n",
            "Epoch 0 iteration 0 Loss: 2.359 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 1.428 | Acc: 57.179% (458/801)\n",
            "Test accuracy: 0.5717852711677551\n",
            "Epoch: 1, Loss: 0.008951055817306042 Updates: 26/5001, Avg Grad: 0.014473247341811657\n",
            "Epoch: 1, Loss: 0.007621733006089926 Updates: 31/6001, Avg Grad: 0.016578882932662964\n",
            "Epoch: 1, Loss: 0.0008220408344641328 Updates: 36/7001, Avg Grad: 0.017875786870718002\n",
            "Epoch: 1, Loss: 0.007150381803512573 Updates: 41/8001, Avg Grad: 0.015220502391457558\n",
            "Epoch: 1, Loss: 0.0037018777802586555 Updates: 46/9001, Avg Grad: 0.018595349043607712\n",
            "Epoch 0 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.786 | Acc: 75.655% (606/801)\n",
            "Test accuracy: 0.7565543055534363\n",
            "Epoch: 1, Loss: 0.0013897313037887216 Updates: 51/10001, Avg Grad: 0.01766459085047245\n",
            "Epoch: 1, Loss: 8.519490074831992e-05 Updates: 56/11001, Avg Grad: 0.0208428967744112\n",
            "Epoch: 1, Loss: 1.3286683497426566e-05 Updates: 61/12001, Avg Grad: 0.014991560950875282\n",
            "Epoch: 1, Loss: 0.00018171971896663308 Updates: 66/13001, Avg Grad: 0.02569282054901123\n",
            "Epoch: 1, Loss: 2.4900913558667526e-05 Updates: 71/14001, Avg Grad: 0.014899107627570629\n",
            "Epoch 0 iteration 0 Loss: 0.528 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.655 | Acc: 80.275% (643/801)\n",
            "Test accuracy: 0.802746593952179\n",
            "Epoch: 1, Loss: 0.003066535573452711 Updates: 76/15001, Avg Grad: 0.024342594668269157\n",
            "Epoch: 1, Loss: 8.988168701762334e-05 Updates: 81/16001, Avg Grad: 0.030553210526704788\n",
            "Epoch: 1, Loss: 0.004960804712027311 Updates: 86/17001, Avg Grad: 0.021930718794465065\n",
            "Epoch: 1, Loss: 0.008517458103597164 Updates: 91/18001, Avg Grad: 0.019678285345435143\n",
            "Epoch: 1, Loss: 0.0004917022888548672 Updates: 96/19001, Avg Grad: 0.02399914339184761\n",
            "Epoch 0 iteration 0 Loss: 4.389 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 0.604 | Acc: 81.898% (656/801)\n",
            "Test accuracy: 0.8189762830734253\n",
            "Epoch: 1, Loss: 1.968353899428621e-06 Updates: 101/20001, Avg Grad: 0.022640585899353027\n",
            "Epoch: 1, Loss: 0.0006914889672771096 Updates: 106/21001, Avg Grad: 0.02166617289185524\n",
            "Epoch: 1, Loss: 0.011209819465875626 Updates: 111/22001, Avg Grad: 0.023464692756533623\n",
            "Epoch: 1, Loss: 0.00023782573407515883 Updates: 116/23001, Avg Grad: 0.014585884287953377\n",
            "Epoch: 1, Loss: 0.021914217621088028 Updates: 121/24001, Avg Grad: 0.022602641955018044\n",
            "Epoch 0 iteration 0 Loss: 0.014 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.465 | Acc: 84.644% (678/801)\n",
            "Test accuracy: 0.846441924571991\n",
            "Epoch: 1, Loss: 4.375095522846095e-05 Updates: 126/25001, Avg Grad: 0.01670990325510502\n",
            "Epoch: 1, Loss: 0.0005925547447986901 Updates: 131/26001, Avg Grad: 0.013467604294419289\n",
            "Epoch: 1, Loss: 0.0005352349835447967 Updates: 136/27001, Avg Grad: 0.024932624772191048\n",
            "Epoch: 1, Loss: 0.0006407506298273802 Updates: 141/28001, Avg Grad: 0.036924637854099274\n",
            "Epoch: 1, Loss: 2.3937054720590822e-05 Updates: 146/29001, Avg Grad: 0.04085548222064972\n",
            "Epoch 0 iteration 0 Loss: 0.461 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.534 | Acc: 83.396% (668/801)\n",
            "Test accuracy: 0.8339575529098511\n",
            "Epoch: 1, Loss: 0.011550080962479115 Updates: 151/30001, Avg Grad: 0.024810869246721268\n",
            "Epoch: 1, Loss: 0.00037790750502608716 Updates: 156/31001, Avg Grad: 0.016210336238145828\n",
            "Epoch: 1, Loss: 0.0017463539261370897 Updates: 161/32001, Avg Grad: 0.02507087029516697\n",
            "Epoch: 1, Loss: 0.0009718039655126631 Updates: 166/33001, Avg Grad: 0.024748604744672775\n",
            "Epoch: 1, Loss: 0.0005147677729837596 Updates: 171/34001, Avg Grad: 0.0314413383603096\n",
            "Epoch 0 iteration 0 Loss: 0.105 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.594 | Acc: 83.396% (668/801)\n",
            "Test accuracy: 0.8339575529098511\n",
            "Epoch: 1, Loss: 0.0007818391313776374 Updates: 176/35001, Avg Grad: 0.019956674426794052\n",
            "Epoch: 1, Loss: 0.0002576635451987386 Updates: 181/36001, Avg Grad: 0.0163566917181015\n",
            "Epoch: 1, Loss: 0.0006023598252795637 Updates: 186/37001, Avg Grad: 0.03368871286511421\n",
            "Epoch: 1, Loss: 7.855959120206535e-05 Updates: 191/38001, Avg Grad: 0.021802935749292374\n",
            "Epoch: 1, Loss: 3.5532520996639505e-05 Updates: 196/39001, Avg Grad: 0.018554802983999252\n",
            "Epoch 0 iteration 0 Loss: 0.035 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.455 | Acc: 85.019% (681/801)\n",
            "Test accuracy: 0.8501872420310974\n",
            "Epoch: 1, Loss: 0.008428860455751419 Updates: 201/40001, Avg Grad: 0.015873059630393982\n",
            "Epoch: 1, Loss: 1.6580121155129746e-05 Updates: 206/41001, Avg Grad: 0.027292903512716293\n",
            "Epoch: 1, Loss: 0.0007618628442287445 Updates: 211/42001, Avg Grad: 0.018719609826803207\n",
            "Epoch: 1, Loss: 0.001756669138558209 Updates: 216/43001, Avg Grad: 0.01762351021170616\n",
            "Epoch: 1, Loss: 0.0014910103054717183 Updates: 221/44001, Avg Grad: 0.009930437430739403\n",
            "Epoch 0 iteration 0 Loss: 0.112 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.496 | Acc: 86.517% (693/801)\n",
            "Test accuracy: 0.8651685118675232\n",
            "Epoch: 1, Loss: 0.00031585581018589437 Updates: 226/45001, Avg Grad: 0.0202709399163723\n",
            "Epoch: 1, Loss: 0.0077774240635335445 Updates: 231/46001, Avg Grad: 0.02305629663169384\n",
            "Epoch: 1, Loss: 1.6349615179933608e-05 Updates: 236/47001, Avg Grad: 0.0344802625477314\n",
            "Epoch: 1, Loss: 0.00012837021495215595 Updates: 241/48001, Avg Grad: 0.023602204397320747\n",
            "Epoch: 1, Loss: 3.348082827869803e-05 Updates: 246/49001, Avg Grad: 0.010706136003136635\n",
            "Epoch 0 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 800 Loss: 0.406 | Acc: 87.516% (701/801)\n",
            "Test accuracy: 0.875156044960022\n",
            "Epoch: 1, Loss: 0.0006829046178609133 Updates: 251/50001, Avg Grad: 0.01606312021613121\n",
            "Epoch: 1, Loss: 0.000851720105856657 Updates: 256/51001, Avg Grad: 0.0167833399027586\n",
            "Epoch: 1, Loss: 0.00036888106842525303 Updates: 261/52001, Avg Grad: 0.021960260346531868\n",
            "Epoch: 1, Loss: 0.0005098138353787363 Updates: 266/53001, Avg Grad: 0.017006229609251022\n",
            "Epoch: 1, Loss: 0.00039012450724840164 Updates: 271/54001, Avg Grad: 0.022753184661269188\n",
            "Epoch 0 iteration 0 Loss: 3.988 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 800 Loss: 0.418 | Acc: 87.765% (703/801)\n",
            "Test accuracy: 0.8776529431343079\n",
            "Epoch: 1, Loss: 0.00019008714298252016 Updates: 276/55001, Avg Grad: 0.013641739264130592\n",
            "Epoch: 1, Loss: 4.052326039527543e-05 Updates: 281/56001, Avg Grad: 0.011257748119533062\n",
            "Epoch: 1, Loss: 8.721846097614616e-05 Updates: 286/57001, Avg Grad: 0.015550504438579082\n",
            "Epoch: 1, Loss: 0.0020169392228126526 Updates: 291/58001, Avg Grad: 0.017638985067605972\n",
            "Epoch: 1, Loss: 0.0001736785052344203 Updates: 296/59001, Avg Grad: 0.015717143192887306\n",
            "Epoch 1 iteration 0 Loss: 0.056 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.362 | Acc: 89.513% (717/801)\n",
            "Test accuracy: 0.8951311111450195\n",
            "Epoch: 2, Loss: 0.00022920539777260274 Updates: 1/1, Avg Grad: 6.572866550413892e-05\n",
            "Epoch: 2, Loss: 1.268921096198028e-05 Updates: 6/1001, Avg Grad: 0.01289560180157423\n",
            "Epoch: 2, Loss: 4.554446422844194e-05 Updates: 11/2001, Avg Grad: 0.01603296771645546\n",
            "Epoch: 2, Loss: 1.9682596757775173e-05 Updates: 16/3001, Avg Grad: 0.016700584441423416\n",
            "Epoch: 2, Loss: 0.0004947390407323837 Updates: 21/4001, Avg Grad: 0.015764793381094933\n",
            "Epoch 1 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.456 | Acc: 87.141% (698/801)\n",
            "Test accuracy: 0.8714107275009155\n",
            "Epoch: 2, Loss: 0.0001449841511202976 Updates: 26/5001, Avg Grad: 0.017042024061083794\n",
            "Epoch: 2, Loss: 0.00010291094804415479 Updates: 31/6001, Avg Grad: 0.016047367826104164\n",
            "Epoch: 2, Loss: 6.270855374168605e-05 Updates: 36/7001, Avg Grad: 0.019797105342149734\n",
            "Epoch: 2, Loss: 1.987851646845229e-05 Updates: 41/8001, Avg Grad: 0.02289491519331932\n",
            "Epoch: 2, Loss: 0.014004851691424847 Updates: 46/9001, Avg Grad: 0.028101300820708275\n",
            "Epoch 1 iteration 0 Loss: 0.031 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.367 | Acc: 89.388% (716/801)\n",
            "Test accuracy: 0.8938826322555542\n",
            "Epoch: 2, Loss: 8.070958574535325e-05 Updates: 51/10001, Avg Grad: 0.02633127011358738\n",
            "Epoch: 2, Loss: 8.976695244200528e-05 Updates: 56/11001, Avg Grad: 0.02415027841925621\n",
            "Epoch: 2, Loss: 0.026606883853673935 Updates: 61/12001, Avg Grad: 0.018002713099122047\n",
            "Epoch: 2, Loss: 0.009543126448988914 Updates: 66/13001, Avg Grad: 0.021407509222626686\n",
            "Epoch: 2, Loss: 0.00012385720037855208 Updates: 71/14001, Avg Grad: 0.011288697831332684\n",
            "Epoch 1 iteration 0 Loss: 0.072 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.361 | Acc: 89.014% (713/801)\n",
            "Test accuracy: 0.8901373147964478\n",
            "Epoch: 2, Loss: 0.002379616489633918 Updates: 76/15001, Avg Grad: 0.012371895834803581\n",
            "Epoch: 2, Loss: 0.014679111540317535 Updates: 81/16001, Avg Grad: 0.018004149198532104\n",
            "Epoch: 2, Loss: 0.00011453347542556003 Updates: 86/17001, Avg Grad: 0.015483155846595764\n",
            "Epoch: 2, Loss: 0.0003538333694450557 Updates: 91/18001, Avg Grad: 0.02044311724603176\n",
            "Epoch: 2, Loss: 8.377563062822446e-06 Updates: 96/19001, Avg Grad: 0.011680729687213898\n",
            "Epoch 1 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.351 | Acc: 90.762% (727/801)\n",
            "Test accuracy: 0.9076154828071594\n",
            "Epoch: 2, Loss: 4.23533892899286e-05 Updates: 101/20001, Avg Grad: 0.010516715236008167\n",
            "Epoch: 2, Loss: 0.00012450607027858496 Updates: 106/21001, Avg Grad: 0.016486477106809616\n",
            "Epoch: 2, Loss: 0.0003980036417488009 Updates: 111/22001, Avg Grad: 0.01279527135193348\n",
            "Epoch: 2, Loss: 3.3811193134170026e-05 Updates: 116/23001, Avg Grad: 0.019017212092876434\n",
            "Epoch: 2, Loss: 1.8675573301152326e-05 Updates: 121/24001, Avg Grad: 0.016283204779028893\n",
            "Epoch 1 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.384 | Acc: 87.266% (699/801)\n",
            "Test accuracy: 0.8726591467857361\n",
            "Epoch: 2, Loss: 0.00020062440307810903 Updates: 126/25001, Avg Grad: 0.01746414229273796\n",
            "Epoch: 2, Loss: 0.00016378398868255317 Updates: 131/26001, Avg Grad: 0.01889309100806713\n",
            "Epoch: 2, Loss: 0.00012012979277642444 Updates: 136/27001, Avg Grad: 0.018920693546533585\n",
            "Epoch: 2, Loss: 0.024527402594685555 Updates: 141/28001, Avg Grad: 0.018796565011143684\n",
            "Epoch: 2, Loss: 0.00064707244746387 Updates: 146/29001, Avg Grad: 0.015708651393651962\n",
            "Epoch 1 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.292 | Acc: 91.261% (731/801)\n",
            "Test accuracy: 0.9126092195510864\n",
            "Epoch: 2, Loss: 1.9612041342043085e-06 Updates: 151/30001, Avg Grad: 0.01198822446167469\n",
            "Epoch: 2, Loss: 0.00013678151299245656 Updates: 156/31001, Avg Grad: 0.013844268396496773\n",
            "Epoch: 2, Loss: 2.4144079361576587e-05 Updates: 161/32001, Avg Grad: 0.016713818535208702\n",
            "Epoch: 2, Loss: 2.5528111109451856e-06 Updates: 166/33001, Avg Grad: 0.01153427455574274\n",
            "Epoch: 2, Loss: 0.004428371321409941 Updates: 171/34001, Avg Grad: 0.021673867478966713\n",
            "Epoch 1 iteration 0 Loss: 0.022 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.293 | Acc: 90.886% (728/801)\n",
            "Test accuracy: 0.90886390209198\n",
            "Epoch: 2, Loss: 0.01723954826593399 Updates: 176/35001, Avg Grad: 0.019097164273262024\n",
            "Epoch: 2, Loss: 2.2746955437469296e-05 Updates: 181/36001, Avg Grad: 0.02168900892138481\n",
            "Epoch: 2, Loss: 3.4593795135151595e-05 Updates: 186/37001, Avg Grad: 0.014648186042904854\n",
            "Epoch: 2, Loss: 0.0015519101871177554 Updates: 191/38001, Avg Grad: 0.010709451511502266\n",
            "Epoch: 2, Loss: 0.0005738982581533492 Updates: 196/39001, Avg Grad: 0.010635359212756157\n",
            "Epoch 1 iteration 0 Loss: 0.057 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.345 | Acc: 89.388% (716/801)\n",
            "Test accuracy: 0.8938826322555542\n",
            "Epoch: 2, Loss: 9.667564154369757e-05 Updates: 201/40001, Avg Grad: 0.012327447533607483\n",
            "Epoch: 2, Loss: 2.2332787921186537e-05 Updates: 206/41001, Avg Grad: 0.01356132049113512\n",
            "Epoch: 2, Loss: 2.452488297421951e-05 Updates: 211/42001, Avg Grad: 0.019653627648949623\n",
            "Epoch: 2, Loss: 3.991917310486315e-06 Updates: 216/43001, Avg Grad: 0.016676457598805428\n",
            "Epoch: 2, Loss: 0.00010454152652528137 Updates: 221/44001, Avg Grad: 0.012732429429888725\n",
            "Epoch 1 iteration 0 Loss: 1.412 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 800 Loss: 0.326 | Acc: 90.512% (725/801)\n",
            "Test accuracy: 0.9051185846328735\n",
            "Epoch: 2, Loss: 2.724245860008523e-05 Updates: 226/45001, Avg Grad: 0.013552291318774223\n",
            "Epoch: 2, Loss: 4.742943565361202e-05 Updates: 231/46001, Avg Grad: 0.015524846501648426\n",
            "Epoch: 2, Loss: 0.0059661464765667915 Updates: 236/47001, Avg Grad: 0.018630610778927803\n",
            "Epoch: 2, Loss: 9.831649549596477e-06 Updates: 241/48001, Avg Grad: 0.01608666591346264\n",
            "Epoch: 2, Loss: 0.0012321866815909743 Updates: 246/49001, Avg Grad: 0.01446310244500637\n",
            "Epoch 1 iteration 0 Loss: 0.180 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.349 | Acc: 89.014% (713/801)\n",
            "Test accuracy: 0.8901373147964478\n",
            "Epoch: 2, Loss: 4.3027106585213915e-05 Updates: 251/50001, Avg Grad: 0.015419160947203636\n",
            "Epoch: 2, Loss: 0.0011178797576576471 Updates: 256/51001, Avg Grad: 0.014326058328151703\n",
            "Epoch: 2, Loss: 0.001604685210622847 Updates: 261/52001, Avg Grad: 0.01959851384162903\n",
            "Epoch: 2, Loss: 0.0018051952356472611 Updates: 266/53001, Avg Grad: 0.018533166497945786\n",
            "Epoch: 2, Loss: 0.0001483406958868727 Updates: 271/54001, Avg Grad: 0.01971985585987568\n",
            "Epoch 1 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 800 Loss: 0.290 | Acc: 92.010% (737/801)\n",
            "Test accuracy: 0.9200998544692993\n",
            "Epoch: 2, Loss: 0.00018679352069739252 Updates: 276/55001, Avg Grad: 0.019710972905158997\n",
            "Epoch: 2, Loss: 0.0004368947120383382 Updates: 281/56001, Avg Grad: 0.015048757195472717\n",
            "Epoch: 2, Loss: 0.0006928835064172745 Updates: 286/57001, Avg Grad: 0.01478014886379242\n",
            "Epoch: 2, Loss: 0.00012562453048303723 Updates: 291/58001, Avg Grad: 0.01780690625309944\n",
            "Epoch: 2, Loss: 0.004438826814293861 Updates: 296/59001, Avg Grad: 0.010708856396377087\n",
            "Epoch 2 iteration 0 Loss: 0.137 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.339 | Acc: 89.638% (718/801)\n",
            "Test accuracy: 0.8963795304298401\n",
            "Epoch: 3, Loss: 0.0002860203094314784 Updates: 1/1, Avg Grad: 9.368087921757251e-05\n",
            "Epoch: 3, Loss: 0.00044526002602651715 Updates: 6/1001, Avg Grad: 0.013657769188284874\n",
            "Epoch: 3, Loss: 4.3636919144773856e-05 Updates: 11/2001, Avg Grad: 0.014830872416496277\n",
            "Epoch: 3, Loss: 9.565629852659185e-07 Updates: 16/3001, Avg Grad: 0.014816182665526867\n",
            "Epoch: 3, Loss: 0.00011830287985503674 Updates: 21/4001, Avg Grad: 0.014908825047314167\n",
            "Epoch 2 iteration 0 Loss: 0.263 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.312 | Acc: 91.635% (734/801)\n",
            "Test accuracy: 0.9163545370101929\n",
            "Epoch: 3, Loss: 0.002997362520545721 Updates: 26/5001, Avg Grad: 0.011992281302809715\n",
            "Epoch: 3, Loss: 0.010426853783428669 Updates: 31/6001, Avg Grad: 0.02138568088412285\n",
            "Epoch: 3, Loss: 6.265734555199742e-05 Updates: 36/7001, Avg Grad: 0.017936518415808678\n",
            "Epoch: 3, Loss: 0.0035730497911572456 Updates: 41/8001, Avg Grad: 0.01521209068596363\n",
            "Epoch: 3, Loss: 0.009464534930884838 Updates: 46/9001, Avg Grad: 0.020196493715047836\n",
            "Epoch 2 iteration 0 Loss: 0.171 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.316 | Acc: 89.638% (718/801)\n",
            "Test accuracy: 0.8963795304298401\n",
            "Epoch: 3, Loss: 9.392809374730859e-07 Updates: 51/10001, Avg Grad: 0.012722263112664223\n",
            "Epoch: 3, Loss: 0.008040787652134895 Updates: 56/11001, Avg Grad: 0.01352720893919468\n",
            "Epoch: 3, Loss: 0.006678566802293062 Updates: 61/12001, Avg Grad: 0.017500702291727066\n",
            "Epoch: 3, Loss: 6.543663766933605e-05 Updates: 66/13001, Avg Grad: 0.019844720140099525\n",
            "Epoch: 3, Loss: 1.0110044058819767e-05 Updates: 71/14001, Avg Grad: 0.017001640051603317\n",
            "Epoch 2 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.329 | Acc: 90.886% (728/801)\n",
            "Test accuracy: 0.90886390209198\n",
            "Epoch: 3, Loss: 1.5042423910927027e-05 Updates: 76/15001, Avg Grad: 0.017541779205203056\n",
            "Epoch: 3, Loss: 2.2692299808113603e-06 Updates: 81/16001, Avg Grad: 0.01362802553921938\n",
            "Epoch: 3, Loss: 3.7429624626383884e-06 Updates: 86/17001, Avg Grad: 0.014222079887986183\n",
            "Epoch: 3, Loss: 2.0940685772075085e-06 Updates: 91/18001, Avg Grad: 0.01348063349723816\n",
            "Epoch: 3, Loss: 0.002675025723874569 Updates: 96/19001, Avg Grad: 0.013231922872364521\n",
            "Epoch 2 iteration 0 Loss: 0.132 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.319 | Acc: 90.886% (728/801)\n",
            "Test accuracy: 0.90886390209198\n",
            "Epoch: 3, Loss: 0.01356531959027052 Updates: 101/20001, Avg Grad: 0.016032764688134193\n",
            "Epoch: 3, Loss: 5.633636192214908e-06 Updates: 106/21001, Avg Grad: 0.011151798069477081\n",
            "Epoch: 3, Loss: 0.008257407695055008 Updates: 111/22001, Avg Grad: 0.013706174679100513\n",
            "Epoch: 3, Loss: 0.00022956292377784848 Updates: 116/23001, Avg Grad: 0.01626517064869404\n",
            "Epoch: 3, Loss: 0.00012178035103715956 Updates: 121/24001, Avg Grad: 0.021691972389817238\n",
            "Epoch 2 iteration 0 Loss: 0.014 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.287 | Acc: 92.509% (741/801)\n",
            "Test accuracy: 0.9250936508178711\n",
            "Epoch: 3, Loss: 1.4388105228135828e-05 Updates: 126/25001, Avg Grad: 0.016443025320768356\n",
            "Epoch: 3, Loss: 0.0020675465930253267 Updates: 131/26001, Avg Grad: 0.015007011592388153\n",
            "Epoch: 3, Loss: 0.006129426881670952 Updates: 136/27001, Avg Grad: 0.010362798348069191\n",
            "Epoch: 3, Loss: 0.0004116233321838081 Updates: 141/28001, Avg Grad: 0.015820423141121864\n",
            "Epoch: 3, Loss: 9.896570554701611e-05 Updates: 146/29001, Avg Grad: 0.0182846300303936\n",
            "Epoch 2 iteration 0 Loss: 0.011 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.257 | Acc: 92.260% (739/801)\n",
            "Test accuracy: 0.9225967526435852\n",
            "Epoch: 3, Loss: 0.0004632181371562183 Updates: 151/30001, Avg Grad: 0.02050239033997059\n",
            "Epoch: 3, Loss: 2.174500650653499e-06 Updates: 156/31001, Avg Grad: 0.016891902312636375\n",
            "Epoch: 3, Loss: 0.0011876396602019668 Updates: 161/32001, Avg Grad: 0.011935194954276085\n",
            "Epoch: 3, Loss: 6.731983376084827e-06 Updates: 166/33001, Avg Grad: 0.018600091338157654\n",
            "Epoch: 3, Loss: 0.003732145531103015 Updates: 171/34001, Avg Grad: 0.02312004566192627\n",
            "Epoch 2 iteration 0 Loss: 2.606 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 800 Loss: 0.219 | Acc: 93.508% (749/801)\n",
            "Test accuracy: 0.9350811243057251\n",
            "Epoch: 3, Loss: 0.0021937380079180002 Updates: 176/35001, Avg Grad: 0.021816957741975784\n",
            "Epoch: 3, Loss: 2.2424761482398026e-05 Updates: 181/36001, Avg Grad: 0.015292209573090076\n",
            "Epoch: 3, Loss: 0.0001306237536482513 Updates: 186/37001, Avg Grad: 0.017910126596689224\n",
            "Epoch: 3, Loss: 3.7230180168990046e-05 Updates: 191/38001, Avg Grad: 0.021746348589658737\n",
            "Epoch: 3, Loss: 0.0001067340635927394 Updates: 196/39001, Avg Grad: 0.012712945230305195\n",
            "Epoch 2 iteration 0 Loss: 0.019 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.313 | Acc: 91.635% (734/801)\n",
            "Test accuracy: 0.9163545370101929\n",
            "Epoch: 3, Loss: 0.01841900311410427 Updates: 201/40001, Avg Grad: 0.010852860286831856\n",
            "Epoch: 3, Loss: 0.005575517192482948 Updates: 206/41001, Avg Grad: 0.019138840958476067\n",
            "Epoch: 3, Loss: 3.8614853110630065e-06 Updates: 211/42001, Avg Grad: 0.011592530645430088\n",
            "Epoch: 3, Loss: 0.005042944103479385 Updates: 216/43001, Avg Grad: 0.019732896238565445\n",
            "Epoch: 3, Loss: 3.665686017484404e-05 Updates: 221/44001, Avg Grad: 0.017867090180516243\n",
            "Epoch 2 iteration 0 Loss: 0.019 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.303 | Acc: 90.387% (724/801)\n",
            "Test accuracy: 0.903870165348053\n",
            "Epoch: 3, Loss: 0.00010451292473590001 Updates: 226/45001, Avg Grad: 0.01991085149347782\n",
            "Epoch: 3, Loss: 0.0001300105213886127 Updates: 231/46001, Avg Grad: 0.015086431987583637\n",
            "Epoch: 3, Loss: 0.0003389284829609096 Updates: 236/47001, Avg Grad: 0.011359399184584618\n",
            "Epoch: 3, Loss: 8.916273509385064e-05 Updates: 241/48001, Avg Grad: 0.015838272869586945\n",
            "Epoch: 3, Loss: 0.010197963565587997 Updates: 246/49001, Avg Grad: 0.015602451749145985\n",
            "Epoch 2 iteration 0 Loss: 0.012 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.244 | Acc: 91.511% (733/801)\n",
            "Test accuracy: 0.9151061177253723\n",
            "Epoch: 3, Loss: 0.00013763978495262563 Updates: 251/50001, Avg Grad: 0.010866661556065083\n",
            "Epoch: 3, Loss: 1.3806219612888526e-05 Updates: 256/51001, Avg Grad: 0.009570888243615627\n",
            "Epoch: 3, Loss: 0.0033595606219023466 Updates: 261/52001, Avg Grad: 0.010910396464169025\n",
            "Epoch: 3, Loss: 0.0004596136277541518 Updates: 266/53001, Avg Grad: 0.011763523332774639\n",
            "Epoch: 3, Loss: 4.4693868403555825e-05 Updates: 271/54001, Avg Grad: 0.013599773868918419\n",
            "Epoch 2 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 800 Loss: 0.263 | Acc: 93.134% (746/801)\n",
            "Test accuracy: 0.9313358068466187\n",
            "Epoch: 3, Loss: 0.0004178258532192558 Updates: 276/55001, Avg Grad: 0.009797461330890656\n",
            "Epoch: 3, Loss: 0.00022860924946144223 Updates: 281/56001, Avg Grad: 0.017330272123217583\n",
            "Epoch: 3, Loss: 0.0005283636273816228 Updates: 286/57001, Avg Grad: 0.012959366664290428\n",
            "Epoch: 3, Loss: 7.448010819643969e-06 Updates: 291/58001, Avg Grad: 0.012749969959259033\n",
            "Epoch: 3, Loss: 0.0006058248691260815 Updates: 296/59001, Avg Grad: 0.01638863980770111\n",
            "Epoch 3 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.256 | Acc: 92.260% (739/801)\n",
            "Test accuracy: 0.9225967526435852\n",
            "Epoch: 4, Loss: 1.9737812181119807e-05 Updates: 1/1, Avg Grad: 7.430586265400052e-06\n",
            "Epoch: 4, Loss: 0.0019221408292651176 Updates: 6/1001, Avg Grad: 0.016411298885941505\n",
            "Epoch: 4, Loss: 0.0031199187505990267 Updates: 11/2001, Avg Grad: 0.01172496099025011\n",
            "Epoch: 4, Loss: 0.00024169645621441305 Updates: 16/3001, Avg Grad: 0.012087523937225342\n",
            "Epoch: 4, Loss: 0.00010043459042208269 Updates: 21/4001, Avg Grad: 0.011233468540012836\n",
            "Epoch 3 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.232 | Acc: 93.508% (749/801)\n",
            "Test accuracy: 0.9350811243057251\n",
            "Epoch: 4, Loss: 0.00010601693793432787 Updates: 26/5001, Avg Grad: 0.010043282061815262\n",
            "Epoch: 4, Loss: 6.332596967695281e-05 Updates: 31/6001, Avg Grad: 0.013020585291087627\n",
            "Epoch: 4, Loss: 2.0057805159012787e-05 Updates: 36/7001, Avg Grad: 0.010153878480196\n",
            "Epoch: 4, Loss: 0.00017900201783049852 Updates: 41/8001, Avg Grad: 0.006288868375122547\n",
            "Epoch: 4, Loss: 1.5939076547510922e-05 Updates: 46/9001, Avg Grad: 0.019258756190538406\n",
            "Epoch 3 iteration 0 Loss: 0.244 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.280 | Acc: 91.261% (731/801)\n",
            "Test accuracy: 0.9126092195510864\n",
            "Epoch: 4, Loss: 0.00013069401029497385 Updates: 51/10001, Avg Grad: 0.007498993072658777\n",
            "Epoch: 4, Loss: 6.301874236669391e-05 Updates: 56/11001, Avg Grad: 0.01532166637480259\n",
            "Epoch: 4, Loss: 5.300810244079912e-06 Updates: 61/12001, Avg Grad: 0.009370986372232437\n",
            "Epoch: 4, Loss: 2.6982801500707865e-05 Updates: 66/13001, Avg Grad: 0.014411011710762978\n",
            "Epoch: 4, Loss: 0.00045246208901517093 Updates: 71/14001, Avg Grad: 0.013712458312511444\n",
            "Epoch 3 iteration 0 Loss: 0.023 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.281 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 4, Loss: 0.0008787339902482927 Updates: 76/15001, Avg Grad: 0.010519805364310741\n",
            "Epoch: 4, Loss: 0.0004828829551115632 Updates: 81/16001, Avg Grad: 0.018655283376574516\n",
            "Epoch: 4, Loss: 9.50132598518394e-05 Updates: 86/17001, Avg Grad: 0.01174783892929554\n",
            "Epoch: 4, Loss: 0.00010201282566413283 Updates: 91/18001, Avg Grad: 0.015473741106688976\n",
            "Epoch: 4, Loss: 1.970737230294617e-06 Updates: 96/19001, Avg Grad: 0.017065975815057755\n",
            "Epoch 3 iteration 0 Loss: 0.046 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.238 | Acc: 93.633% (750/801)\n",
            "Test accuracy: 0.9363296031951904\n",
            "Epoch: 4, Loss: 0.000425947189796716 Updates: 101/20001, Avg Grad: 0.01754974201321602\n",
            "Epoch: 4, Loss: 0.0002450517495162785 Updates: 106/21001, Avg Grad: 0.012529140338301659\n",
            "Epoch: 4, Loss: 0.0001283742894884199 Updates: 111/22001, Avg Grad: 0.012717530131340027\n",
            "Epoch: 4, Loss: 3.8912648960831575e-06 Updates: 116/23001, Avg Grad: 0.01689501665532589\n",
            "Epoch: 4, Loss: 7.96101176092634e-06 Updates: 121/24001, Avg Grad: 0.013896006159484386\n",
            "Epoch 3 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.242 | Acc: 91.261% (731/801)\n",
            "Test accuracy: 0.9126092195510864\n",
            "Epoch: 4, Loss: 7.402074697893113e-05 Updates: 126/25001, Avg Grad: 0.009613082744181156\n",
            "Epoch: 4, Loss: 8.367406553588808e-05 Updates: 131/26001, Avg Grad: 0.012650599703192711\n",
            "Epoch: 4, Loss: 8.54714926390443e-06 Updates: 136/27001, Avg Grad: 0.013014157302677631\n",
            "Epoch: 4, Loss: 1.33598023239756e-05 Updates: 141/28001, Avg Grad: 0.00841464102268219\n",
            "Epoch: 4, Loss: 0.008749296888709068 Updates: 146/29001, Avg Grad: 0.011534878984093666\n",
            "Epoch 3 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.226 | Acc: 92.884% (744/801)\n",
            "Test accuracy: 0.9288389682769775\n",
            "Epoch: 4, Loss: 0.0026134317740797997 Updates: 151/30001, Avg Grad: 0.014409367926418781\n",
            "Epoch: 4, Loss: 1.3605889535028837e-06 Updates: 156/31001, Avg Grad: 0.011050114408135414\n",
            "Epoch: 4, Loss: 0.0005694568390026689 Updates: 161/32001, Avg Grad: 0.010298575274646282\n",
            "Epoch: 4, Loss: 0.00014106328308116645 Updates: 166/33001, Avg Grad: 0.009842043742537498\n",
            "Epoch: 4, Loss: 0.0015697686467319727 Updates: 171/34001, Avg Grad: 0.007115840911865234\n",
            "Epoch 3 iteration 0 Loss: 0.046 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.201 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 4, Loss: 5.827724635310005e-06 Updates: 176/35001, Avg Grad: 0.010823815129697323\n",
            "Epoch: 4, Loss: 0.0034462767653167248 Updates: 181/36001, Avg Grad: 0.020775360986590385\n",
            "Epoch: 4, Loss: 0.0004656276432797313 Updates: 186/37001, Avg Grad: 0.011808346025645733\n",
            "Epoch: 4, Loss: 0.0007903434452600777 Updates: 191/38001, Avg Grad: 0.011118057183921337\n",
            "Epoch: 4, Loss: 3.3496689866296947e-07 Updates: 196/39001, Avg Grad: 0.007069387473165989\n",
            "Epoch 3 iteration 0 Loss: 0.012 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.238 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 4, Loss: 9.804881301533896e-06 Updates: 201/40001, Avg Grad: 0.009990775026381016\n",
            "Epoch: 4, Loss: 0.012120341882109642 Updates: 206/41001, Avg Grad: 0.013585099950432777\n",
            "Epoch: 4, Loss: 8.225537021644413e-05 Updates: 211/42001, Avg Grad: 0.011702293530106544\n",
            "Epoch: 4, Loss: 7.934827408462297e-06 Updates: 216/43001, Avg Grad: 0.012951048091053963\n",
            "Epoch: 4, Loss: 0.00025572028243914247 Updates: 221/44001, Avg Grad: 0.020607125014066696\n",
            "Epoch 3 iteration 0 Loss: 0.063 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.265 | Acc: 92.509% (741/801)\n",
            "Test accuracy: 0.9250936508178711\n",
            "Epoch: 4, Loss: 8.72148939379258e-06 Updates: 226/45001, Avg Grad: 0.008555740118026733\n",
            "Epoch: 4, Loss: 0.0005103187286294997 Updates: 231/46001, Avg Grad: 0.013593286275863647\n",
            "Epoch: 4, Loss: 0.00016954386956058443 Updates: 236/47001, Avg Grad: 0.016997840255498886\n",
            "Epoch: 4, Loss: 5.930310180701781e-07 Updates: 241/48001, Avg Grad: 0.011620583012700081\n",
            "Epoch: 4, Loss: 1.0315856343368068e-05 Updates: 246/49001, Avg Grad: 0.013311858288943768\n",
            "Epoch 3 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.255 | Acc: 92.010% (737/801)\n",
            "Test accuracy: 0.9200998544692993\n",
            "Epoch: 4, Loss: 0.0002016870421357453 Updates: 251/50001, Avg Grad: 0.015493182465434074\n",
            "Epoch: 4, Loss: 0.0012324345298111439 Updates: 256/51001, Avg Grad: 0.014753659255802631\n",
            "Epoch: 4, Loss: 7.664171425858513e-05 Updates: 261/52001, Avg Grad: 0.01127512939274311\n",
            "Epoch: 4, Loss: 1.2278964277356863e-05 Updates: 266/53001, Avg Grad: 0.017888221889734268\n",
            "Epoch: 4, Loss: 3.1109033443499357e-05 Updates: 271/54001, Avg Grad: 0.009721219539642334\n",
            "Epoch 3 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 800 Loss: 0.242 | Acc: 93.009% (745/801)\n",
            "Test accuracy: 0.9300873875617981\n",
            "Epoch: 4, Loss: 4.172323375684073e-09 Updates: 276/55001, Avg Grad: 0.0135734174400568\n",
            "Epoch: 4, Loss: 0.017085151746869087 Updates: 281/56001, Avg Grad: 0.010494334623217583\n",
            "Epoch: 4, Loss: 0.003225033637136221 Updates: 286/57001, Avg Grad: 0.010140961967408657\n",
            "Epoch: 4, Loss: 7.3170735959138256e-06 Updates: 291/58001, Avg Grad: 0.019777949899435043\n",
            "Epoch: 4, Loss: 5.637803496938432e-06 Updates: 296/59001, Avg Grad: 0.010429062880575657\n",
            "Epoch 4 iteration 0 Loss: 0.069 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.230 | Acc: 92.509% (741/801)\n",
            "Test accuracy: 0.9250936508178711\n",
            "Epoch: 5, Loss: 0.0002457213995512575 Updates: 1/1, Avg Grad: 4.391766196931712e-05\n",
            "Epoch: 5, Loss: 0.0005964245065115392 Updates: 6/1001, Avg Grad: 0.006807300262153149\n",
            "Epoch: 5, Loss: 0.0013556793564930558 Updates: 11/2001, Avg Grad: 0.012342337518930435\n",
            "Epoch: 5, Loss: 1.638347930565942e-05 Updates: 16/3001, Avg Grad: 0.010568143799901009\n",
            "Epoch: 5, Loss: 1.688071461103391e-05 Updates: 21/4001, Avg Grad: 0.013815653510391712\n",
            "Epoch 4 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.208 | Acc: 93.508% (749/801)\n",
            "Test accuracy: 0.9350811243057251\n",
            "Epoch: 5, Loss: 2.9197749427112285e-06 Updates: 26/5001, Avg Grad: 0.012483968399465084\n",
            "Epoch: 5, Loss: 8.413860996370204e-06 Updates: 31/6001, Avg Grad: 0.020214572548866272\n",
            "Epoch: 5, Loss: 5.748124749516137e-05 Updates: 36/7001, Avg Grad: 0.006839144043624401\n",
            "Epoch: 5, Loss: 4.3986293007947097e-07 Updates: 41/8001, Avg Grad: 0.011911039240658283\n",
            "Epoch: 5, Loss: 0.0002333905576961115 Updates: 46/9001, Avg Grad: 0.010523015633225441\n",
            "Epoch 4 iteration 0 Loss: 3.312 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 800 Loss: 0.313 | Acc: 90.637% (726/801)\n",
            "Test accuracy: 0.9063670635223389\n",
            "Epoch: 5, Loss: 0.0001619302638573572 Updates: 51/10001, Avg Grad: 0.007275699637830257\n",
            "Epoch: 5, Loss: 0.009330367669463158 Updates: 56/11001, Avg Grad: 0.007169353775680065\n",
            "Epoch: 5, Loss: 3.9635517623537453e-07 Updates: 61/12001, Avg Grad: 0.00812037568539381\n",
            "Epoch: 5, Loss: 6.596019375137985e-05 Updates: 66/13001, Avg Grad: 0.012161599472165108\n",
            "Epoch: 5, Loss: 0.0007762015447951853 Updates: 71/14001, Avg Grad: 0.011764406226575375\n",
            "Epoch 4 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.234 | Acc: 93.134% (746/801)\n",
            "Test accuracy: 0.9313358068466187\n",
            "Epoch: 5, Loss: 5.531005058401206e-07 Updates: 76/15001, Avg Grad: 0.015361549332737923\n",
            "Epoch: 5, Loss: 5.989908231640584e-07 Updates: 81/16001, Avg Grad: 0.010183624923229218\n",
            "Epoch: 5, Loss: 3.5765100619755685e-05 Updates: 86/17001, Avg Grad: 0.009481636807322502\n",
            "Epoch: 5, Loss: 1.6722266309443512e-06 Updates: 91/18001, Avg Grad: 0.015212622471153736\n",
            "Epoch: 5, Loss: 1.395745925947267e-06 Updates: 96/19001, Avg Grad: 0.011115249246358871\n",
            "Epoch 4 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.268 | Acc: 92.135% (738/801)\n",
            "Test accuracy: 0.9213483333587646\n",
            "Epoch: 5, Loss: 0.0011892393231391907 Updates: 101/20001, Avg Grad: 0.015347525477409363\n",
            "Epoch: 5, Loss: 0.00015079634613357484 Updates: 106/21001, Avg Grad: 0.010732091031968594\n",
            "Epoch: 5, Loss: 0.0001271373184863478 Updates: 111/22001, Avg Grad: 0.01390924584120512\n",
            "Epoch: 5, Loss: 0.02016916126012802 Updates: 116/23001, Avg Grad: 0.013630621135234833\n",
            "Epoch: 5, Loss: 0.0001616088266018778 Updates: 121/24001, Avg Grad: 0.009712593629956245\n",
            "Epoch 4 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.225 | Acc: 92.509% (741/801)\n",
            "Test accuracy: 0.9250936508178711\n",
            "Epoch: 5, Loss: 3.5660659705172293e-06 Updates: 126/25001, Avg Grad: 0.009334025904536247\n",
            "Epoch: 5, Loss: 8.241295290645212e-06 Updates: 131/26001, Avg Grad: 0.012705358676612377\n",
            "Epoch: 5, Loss: 1.3841882719134446e-05 Updates: 136/27001, Avg Grad: 0.00954199768602848\n",
            "Epoch: 5, Loss: 3.21864028762775e-08 Updates: 141/28001, Avg Grad: 0.01642334833741188\n",
            "Epoch: 5, Loss: 0.00010322158050257713 Updates: 146/29001, Avg Grad: 0.01524254959076643\n",
            "Epoch 4 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.202 | Acc: 94.257% (755/801)\n",
            "Test accuracy: 0.942571759223938\n",
            "Epoch: 5, Loss: 3.250559893785976e-05 Updates: 151/30001, Avg Grad: 0.01133537758141756\n",
            "Epoch: 5, Loss: 3.994299731857609e-06 Updates: 156/31001, Avg Grad: 0.00999218039214611\n",
            "Epoch: 5, Loss: 5.074708315078169e-05 Updates: 161/32001, Avg Grad: 0.012309540994465351\n",
            "Epoch: 5, Loss: 0.008989510126411915 Updates: 166/33001, Avg Grad: 0.012719020247459412\n",
            "Epoch: 5, Loss: 5.702699581888737e-06 Updates: 171/34001, Avg Grad: 0.015791671350598335\n",
            "Epoch 4 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.251 | Acc: 92.634% (742/801)\n",
            "Test accuracy: 0.9263420701026917\n",
            "Epoch: 5, Loss: 1.1627514595602406e-06 Updates: 176/35001, Avg Grad: 0.012354067526757717\n",
            "Epoch: 5, Loss: 1.6810616216389462e-05 Updates: 181/36001, Avg Grad: 0.015211885794997215\n",
            "Epoch: 5, Loss: 3.1485178624279797e-06 Updates: 186/37001, Avg Grad: 0.012768832966685295\n",
            "Epoch: 5, Loss: 0.007930275984108448 Updates: 191/38001, Avg Grad: 0.015347813256084919\n",
            "Epoch: 5, Loss: 2.3258282908500405e-06 Updates: 196/39001, Avg Grad: 0.013491332530975342\n",
            "Epoch 4 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.236 | Acc: 92.509% (741/801)\n",
            "Test accuracy: 0.9250936508178711\n",
            "Epoch: 5, Loss: 0.00014203842147253454 Updates: 201/40001, Avg Grad: 0.014528706669807434\n",
            "Epoch: 5, Loss: 0.0029435171745717525 Updates: 206/41001, Avg Grad: 0.014368568547070026\n",
            "Epoch: 5, Loss: 4.663053550757468e-05 Updates: 211/42001, Avg Grad: 0.00949658639729023\n",
            "Epoch: 5, Loss: 0.0006624844390898943 Updates: 216/43001, Avg Grad: 0.01225350797176361\n",
            "Epoch: 5, Loss: 0.004836362786591053 Updates: 221/44001, Avg Grad: 0.011732326820492744\n",
            "Epoch 4 iteration 0 Loss: 0.032 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.239 | Acc: 92.260% (739/801)\n",
            "Test accuracy: 0.9225967526435852\n",
            "Epoch: 5, Loss: 0.0010690096532925963 Updates: 226/45001, Avg Grad: 0.009663177654147148\n",
            "Epoch: 5, Loss: 9.929148063747562e-07 Updates: 231/46001, Avg Grad: 0.012324258685112\n",
            "Epoch: 5, Loss: 0.00015490780060645193 Updates: 236/47001, Avg Grad: 0.011236930266022682\n",
            "Epoch: 5, Loss: 4.805583375855349e-05 Updates: 241/48001, Avg Grad: 0.013378302566707134\n",
            "Epoch: 5, Loss: 3.6006117625220213e-06 Updates: 246/49001, Avg Grad: 0.014321066439151764\n",
            "Epoch 4 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.215 | Acc: 93.258% (747/801)\n",
            "Test accuracy: 0.932584285736084\n",
            "Epoch: 5, Loss: 5.5710879678372294e-05 Updates: 251/50001, Avg Grad: 0.007546159438788891\n",
            "Epoch: 5, Loss: 1.7271575416089036e-05 Updates: 256/51001, Avg Grad: 0.00976079422980547\n",
            "Epoch: 5, Loss: 0.00011328724212944508 Updates: 261/52001, Avg Grad: 0.009743062779307365\n",
            "Epoch: 5, Loss: 0.0013996097259223461 Updates: 266/53001, Avg Grad: 0.008146537467837334\n",
            "Epoch: 5, Loss: 2.103999747760099e-07 Updates: 271/54001, Avg Grad: 0.017187928780913353\n",
            "Epoch 4 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 800 Loss: 0.230 | Acc: 91.760% (735/801)\n",
            "Test accuracy: 0.9176030158996582\n",
            "Epoch: 5, Loss: 2.2138224267109763e-06 Updates: 276/55001, Avg Grad: 0.011692741885781288\n",
            "Epoch: 5, Loss: 3.9746319089317694e-05 Updates: 281/56001, Avg Grad: 0.010076053440570831\n",
            "Epoch: 5, Loss: 0.003619869239628315 Updates: 286/57001, Avg Grad: 0.01498347707092762\n",
            "Epoch: 5, Loss: 6.0037869843654335e-05 Updates: 291/58001, Avg Grad: 0.01025521568953991\n",
            "Epoch: 5, Loss: 0.0002597526181489229 Updates: 296/59001, Avg Grad: 0.01519683562219143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 10,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.0004, #0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 2,\n",
        "    \"test_interval\": 5000,\n",
        "    \"dampening\": 0.1\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "#optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss = net_trainer_manh(net, train_loader, test_loader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bzTdisu8O1C",
        "outputId": "fec21453-28c0-4340-a447-50ed102c2d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 2.688 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 2.931 | Acc: 12.219% (49/401)\n",
            "Test accuracy: 0.12219451367855072\n",
            "Epoch: 1, Loss: 0.030000319704413414 Updates: 1/1, Avg Grad: 0.0002007457660511136\n",
            "Epoch: 1, Loss: 0.01132808718830347 Updates: 6/1001, Avg Grad: 0.014153003692626953\n",
            "Epoch: 1, Loss: 0.022904105484485626 Updates: 11/2001, Avg Grad: 0.012635499238967896\n",
            "Epoch: 1, Loss: 0.00916460994631052 Updates: 16/3001, Avg Grad: 0.013610957190394402\n",
            "Epoch: 1, Loss: 0.010731346905231476 Updates: 21/4001, Avg Grad: 0.014077511616051197\n",
            "Epoch 0 iteration 0 Loss: 2.300 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 2.370 | Acc: 27.182% (109/401)\n",
            "Test accuracy: 0.27182045578956604\n",
            "Epoch: 1, Loss: 0.0108350133523345 Updates: 26/5001, Avg Grad: 0.013668161816895008\n",
            "Epoch: 1, Loss: 0.011185097508132458 Updates: 31/6001, Avg Grad: 0.012205793522298336\n",
            "Epoch: 1, Loss: 0.01580694690346718 Updates: 36/7001, Avg Grad: 0.01156819798052311\n",
            "Epoch: 1, Loss: 0.013468630611896515 Updates: 41/8001, Avg Grad: 0.010343043133616447\n",
            "Epoch: 1, Loss: 0.010515471920371056 Updates: 46/9001, Avg Grad: 0.011497057974338531\n",
            "Epoch 0 iteration 0 Loss: 1.726 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 1.800 | Acc: 43.890% (176/401)\n",
            "Test accuracy: 0.43890273571014404\n",
            "Epoch: 1, Loss: 0.014044847339391708 Updates: 51/10001, Avg Grad: 0.01207233127206564\n",
            "Epoch: 1, Loss: 0.00881289504468441 Updates: 56/11001, Avg Grad: 0.012825963087379932\n",
            "Epoch: 1, Loss: 0.004417616408318281 Updates: 61/12001, Avg Grad: 0.009904528968036175\n",
            "Epoch: 1, Loss: 0.004479392431676388 Updates: 66/13001, Avg Grad: 0.012780735269188881\n",
            "Epoch: 1, Loss: 0.0009536836878396571 Updates: 71/14001, Avg Grad: 0.013102047145366669\n",
            "Epoch 0 iteration 0 Loss: 1.764 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 1.505 | Acc: 56.858% (228/401)\n",
            "Test accuracy: 0.5685785412788391\n",
            "Epoch: 1, Loss: 0.009711835533380508 Updates: 76/15001, Avg Grad: 0.014193197712302208\n",
            "Epoch: 1, Loss: 0.0015755576314404607 Updates: 81/16001, Avg Grad: 0.013452636078000069\n",
            "Epoch: 1, Loss: 0.007402611430734396 Updates: 86/17001, Avg Grad: 0.015557515434920788\n",
            "Epoch: 1, Loss: 0.011312834918498993 Updates: 91/18001, Avg Grad: 0.010486729443073273\n",
            "Epoch: 1, Loss: 0.0029574076179414988 Updates: 96/19001, Avg Grad: 0.013166563585400581\n",
            "Epoch 0 iteration 0 Loss: 2.600 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 1.273 | Acc: 65.835% (264/401)\n",
            "Test accuracy: 0.6583541035652161\n",
            "Epoch: 1, Loss: 0.0013207319425418973 Updates: 101/20001, Avg Grad: 0.012417511083185673\n",
            "Epoch: 1, Loss: 0.004232498351484537 Updates: 106/21001, Avg Grad: 0.011818590573966503\n",
            "Epoch: 1, Loss: 0.008389899507164955 Updates: 111/22001, Avg Grad: 0.012277903966605663\n",
            "Epoch: 1, Loss: 0.004182921256870031 Updates: 116/23001, Avg Grad: 0.011046920903027058\n",
            "Epoch: 1, Loss: 0.01214242447167635 Updates: 121/24001, Avg Grad: 0.011576471850275993\n",
            "Epoch 0 iteration 0 Loss: 0.438 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.984 | Acc: 74.813% (300/401)\n",
            "Test accuracy: 0.748129665851593\n",
            "Epoch: 1, Loss: 0.0016394838457927108 Updates: 126/25001, Avg Grad: 0.012064273469150066\n",
            "Epoch: 1, Loss: 0.008635234087705612 Updates: 131/26001, Avg Grad: 0.012951540760695934\n",
            "Epoch: 1, Loss: 0.004538343753665686 Updates: 136/27001, Avg Grad: 0.011452906765043736\n",
            "Epoch: 1, Loss: 0.006193385925143957 Updates: 141/28001, Avg Grad: 0.01300085335969925\n",
            "Epoch: 1, Loss: 0.001572181237861514 Updates: 146/29001, Avg Grad: 0.01712178625166416\n",
            "Epoch 0 iteration 0 Loss: 1.109 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.968 | Acc: 70.574% (283/401)\n",
            "Test accuracy: 0.7057356834411621\n",
            "Epoch: 1, Loss: 0.0041345409117639065 Updates: 151/30001, Avg Grad: 0.012447508051991463\n",
            "Epoch: 1, Loss: 0.002574025420472026 Updates: 156/31001, Avg Grad: 0.011085077188909054\n",
            "Epoch: 1, Loss: 0.005846974439918995 Updates: 161/32001, Avg Grad: 0.013500319793820381\n",
            "Epoch: 1, Loss: 0.0045858570374548435 Updates: 166/33001, Avg Grad: 0.018283672630786896\n",
            "Epoch: 1, Loss: 0.0014212026726454496 Updates: 171/34001, Avg Grad: 0.015040621161460876\n",
            "Epoch 0 iteration 0 Loss: 0.354 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.861 | Acc: 76.808% (308/401)\n",
            "Test accuracy: 0.7680798172950745\n",
            "Epoch: 1, Loss: 0.0026546278968453407 Updates: 176/35001, Avg Grad: 0.016584957018494606\n",
            "Epoch: 1, Loss: 0.0003944696800317615 Updates: 181/36001, Avg Grad: 0.01190198678523302\n",
            "Epoch: 1, Loss: 0.002333379816263914 Updates: 186/37001, Avg Grad: 0.015871070325374603\n",
            "Epoch: 1, Loss: 0.00032471976010128856 Updates: 191/38001, Avg Grad: 0.015780368819832802\n",
            "Epoch: 1, Loss: 0.00038956597563810647 Updates: 196/39001, Avg Grad: 0.014382402412593365\n",
            "Epoch 0 iteration 0 Loss: 0.323 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.724 | Acc: 78.554% (315/401)\n",
            "Test accuracy: 0.7855361700057983\n",
            "Epoch: 1, Loss: 0.005855736788362265 Updates: 201/40001, Avg Grad: 0.012643307447433472\n",
            "Epoch: 1, Loss: 0.0004816159198526293 Updates: 206/41001, Avg Grad: 0.012522699311375618\n",
            "Epoch: 1, Loss: 0.0020064059644937515 Updates: 211/42001, Avg Grad: 0.012817802838981152\n",
            "Epoch: 1, Loss: 0.0024492524098604918 Updates: 216/43001, Avg Grad: 0.012027040123939514\n",
            "Epoch: 1, Loss: 0.0017345926025882363 Updates: 221/44001, Avg Grad: 0.008707316592335701\n",
            "Epoch 0 iteration 0 Loss: 0.520 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.705 | Acc: 81.546% (327/401)\n",
            "Test accuracy: 0.8154613375663757\n",
            "Epoch: 1, Loss: 0.0018365494906902313 Updates: 226/45001, Avg Grad: 0.013958299532532692\n",
            "Epoch: 1, Loss: 0.010082339867949486 Updates: 231/46001, Avg Grad: 0.01510466169565916\n",
            "Epoch: 1, Loss: 0.0001345319178653881 Updates: 236/47001, Avg Grad: 0.023330094292759895\n",
            "Epoch: 1, Loss: 0.001265070866793394 Updates: 241/48001, Avg Grad: 0.014662250876426697\n",
            "Epoch: 1, Loss: 0.0009441127767786384 Updates: 246/49001, Avg Grad: 0.014638458378612995\n",
            "Epoch 0 iteration 0 Loss: 0.148 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.639 | Acc: 80.549% (323/401)\n",
            "Test accuracy: 0.805486261844635\n",
            "Epoch: 1, Loss: 0.0004398169694468379 Updates: 251/50001, Avg Grad: 0.013283287174999714\n",
            "Epoch: 1, Loss: 0.002694085007533431 Updates: 256/51001, Avg Grad: 0.011867552995681763\n",
            "Epoch: 1, Loss: 0.0017110879998654127 Updates: 261/52001, Avg Grad: 0.022627247497439384\n",
            "Epoch: 1, Loss: 0.00148499954957515 Updates: 266/53001, Avg Grad: 0.013943043537437916\n",
            "Epoch: 1, Loss: 0.0037998475600034 Updates: 271/54001, Avg Grad: 0.022590816020965576\n",
            "Epoch 0 iteration 0 Loss: 0.989 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.609 | Acc: 81.546% (327/401)\n",
            "Test accuracy: 0.8154613375663757\n",
            "Epoch: 1, Loss: 0.002577301813289523 Updates: 276/55001, Avg Grad: 0.012685105204582214\n",
            "Epoch: 1, Loss: 0.003183171385899186 Updates: 281/56001, Avg Grad: 0.017431309446692467\n",
            "Epoch: 1, Loss: 0.00029459333745762706 Updates: 286/57001, Avg Grad: 0.012324352748692036\n",
            "Epoch: 1, Loss: 0.0020389778073877096 Updates: 291/58001, Avg Grad: 0.021012617275118828\n",
            "Epoch: 1, Loss: 0.0008785804966464639 Updates: 296/59001, Avg Grad: 0.023521889001131058\n",
            "Epoch 1 iteration 0 Loss: 0.207 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.529 | Acc: 84.539% (339/401)\n",
            "Test accuracy: 0.8453865051269531\n",
            "Epoch: 2, Loss: 0.000529040873516351 Updates: 1/1, Avg Grad: 0.00013829037197865546\n",
            "Epoch: 2, Loss: 0.0001600556424818933 Updates: 6/1001, Avg Grad: 0.015062983147799969\n",
            "Epoch: 2, Loss: 3.8727946957806125e-05 Updates: 11/2001, Avg Grad: 0.01458703726530075\n",
            "Epoch: 2, Loss: 9.869631321635097e-05 Updates: 16/3001, Avg Grad: 0.013729223981499672\n",
            "Epoch: 2, Loss: 0.0010664619039744139 Updates: 21/4001, Avg Grad: 0.017605839297175407\n",
            "Epoch 1 iteration 0 Loss: 0.011 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.658 | Acc: 80.299% (322/401)\n",
            "Test accuracy: 0.8029925227165222\n",
            "Epoch: 2, Loss: 0.0012563898926600814 Updates: 26/5001, Avg Grad: 0.015579687431454659\n",
            "Epoch: 2, Loss: 0.0006153045687824488 Updates: 31/6001, Avg Grad: 0.015125112608075142\n",
            "Epoch: 2, Loss: 9.753088670549914e-05 Updates: 36/7001, Avg Grad: 0.0206662118434906\n",
            "Epoch: 2, Loss: 4.840058318222873e-05 Updates: 41/8001, Avg Grad: 0.010248593986034393\n",
            "Epoch: 2, Loss: 0.016152553260326385 Updates: 46/9001, Avg Grad: 0.019284509122371674\n",
            "Epoch 1 iteration 0 Loss: 0.332 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.541 | Acc: 84.539% (339/401)\n",
            "Test accuracy: 0.8453865051269531\n",
            "Epoch: 2, Loss: 0.0001504620595369488 Updates: 51/10001, Avg Grad: 0.01630040444433689\n",
            "Epoch: 2, Loss: 0.00010488357656868175 Updates: 56/11001, Avg Grad: 0.018820026889443398\n",
            "Epoch: 2, Loss: 0.026705723255872726 Updates: 61/12001, Avg Grad: 0.019074220210313797\n",
            "Epoch: 2, Loss: 0.011026904918253422 Updates: 66/13001, Avg Grad: 0.020214740186929703\n",
            "Epoch: 2, Loss: 0.00012459850404411554 Updates: 71/14001, Avg Grad: 0.013591587543487549\n",
            "Epoch 1 iteration 0 Loss: 0.106 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.492 | Acc: 86.534% (347/401)\n",
            "Test accuracy: 0.8653366565704346\n",
            "Epoch: 2, Loss: 0.0039575775153934956 Updates: 76/15001, Avg Grad: 0.01564454473555088\n",
            "Epoch: 2, Loss: 0.01351247075945139 Updates: 81/16001, Avg Grad: 0.02236616052687168\n",
            "Epoch: 2, Loss: 0.0017645054031163454 Updates: 86/17001, Avg Grad: 0.01812778227031231\n",
            "Epoch: 2, Loss: 0.0010339039145037532 Updates: 91/18001, Avg Grad: 0.018312711268663406\n",
            "Epoch: 2, Loss: 7.831960829207674e-05 Updates: 96/19001, Avg Grad: 0.016625596210360527\n",
            "Epoch 1 iteration 0 Loss: 0.026 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.516 | Acc: 85.536% (343/401)\n",
            "Test accuracy: 0.8553615808486938\n",
            "Epoch: 2, Loss: 0.00020760702318511903 Updates: 101/20001, Avg Grad: 0.012280290946364403\n",
            "Epoch: 2, Loss: 0.0008749772096052766 Updates: 106/21001, Avg Grad: 0.024311218410730362\n",
            "Epoch: 2, Loss: 0.00642738351598382 Updates: 111/22001, Avg Grad: 0.011399412527680397\n",
            "Epoch: 2, Loss: 0.00020678984583355486 Updates: 116/23001, Avg Grad: 0.022341618314385414\n",
            "Epoch: 2, Loss: 3.912005922757089e-05 Updates: 121/24001, Avg Grad: 0.011670062318444252\n",
            "Epoch 1 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.451 | Acc: 86.783% (348/401)\n",
            "Test accuracy: 0.8678303956985474\n",
            "Epoch: 2, Loss: 0.002495876746252179 Updates: 126/25001, Avg Grad: 0.018596377223730087\n",
            "Epoch: 2, Loss: 0.0003154132282361388 Updates: 131/26001, Avg Grad: 0.02337120845913887\n",
            "Epoch: 2, Loss: 0.003435194492340088 Updates: 136/27001, Avg Grad: 0.013034654781222343\n",
            "Epoch: 2, Loss: 0.038587190210819244 Updates: 141/28001, Avg Grad: 0.017776546999812126\n",
            "Epoch: 2, Loss: 0.0017297661397606134 Updates: 146/29001, Avg Grad: 0.017539747059345245\n",
            "Epoch 1 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.462 | Acc: 85.786% (344/401)\n",
            "Test accuracy: 0.8578553795814514\n",
            "Epoch: 2, Loss: 4.08844935009256e-05 Updates: 151/30001, Avg Grad: 0.012418370693922043\n",
            "Epoch: 2, Loss: 0.002919756807386875 Updates: 156/31001, Avg Grad: 0.016994541510939598\n",
            "Epoch: 2, Loss: 0.00012336293002590537 Updates: 161/32001, Avg Grad: 0.01292149256914854\n",
            "Epoch: 2, Loss: 6.985397340031341e-05 Updates: 166/33001, Avg Grad: 0.015874125063419342\n",
            "Epoch: 2, Loss: 0.006358779035508633 Updates: 171/34001, Avg Grad: 0.015583263710141182\n",
            "Epoch 1 iteration 0 Loss: 0.023 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.425 | Acc: 87.531% (351/401)\n",
            "Test accuracy: 0.8753117322921753\n",
            "Epoch: 2, Loss: 0.015220476314425468 Updates: 176/35001, Avg Grad: 0.01817150227725506\n",
            "Epoch: 2, Loss: 0.00017132335051428527 Updates: 181/36001, Avg Grad: 0.015003134496510029\n",
            "Epoch: 2, Loss: 0.0004671387141570449 Updates: 186/37001, Avg Grad: 0.015943879261612892\n",
            "Epoch: 2, Loss: 0.008516117930412292 Updates: 191/38001, Avg Grad: 0.013782603666186333\n",
            "Epoch: 2, Loss: 0.0003963509516324848 Updates: 196/39001, Avg Grad: 0.01672823540866375\n",
            "Epoch 1 iteration 0 Loss: 0.183 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.501 | Acc: 85.536% (343/401)\n",
            "Test accuracy: 0.8553615808486938\n",
            "Epoch: 2, Loss: 0.002298169070854783 Updates: 201/40001, Avg Grad: 0.013964029960334301\n",
            "Epoch: 2, Loss: 0.00044660174171440303 Updates: 206/41001, Avg Grad: 0.015355083160102367\n",
            "Epoch: 2, Loss: 2.304480403836351e-05 Updates: 211/42001, Avg Grad: 0.018677666783332825\n",
            "Epoch: 2, Loss: 4.2957966797985137e-05 Updates: 216/43001, Avg Grad: 0.02188359946012497\n",
            "Epoch: 2, Loss: 7.650788029422984e-05 Updates: 221/44001, Avg Grad: 0.016717659309506416\n",
            "Epoch 1 iteration 0 Loss: 1.570 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 400 Loss: 0.416 | Acc: 86.783% (348/401)\n",
            "Test accuracy: 0.8678303956985474\n",
            "Epoch: 2, Loss: 0.0001891144347609952 Updates: 226/45001, Avg Grad: 0.014749905094504356\n",
            "Epoch: 2, Loss: 0.0001760728337103501 Updates: 231/46001, Avg Grad: 0.012468676082789898\n",
            "Epoch: 2, Loss: 0.004921423736959696 Updates: 236/47001, Avg Grad: 0.019895076751708984\n",
            "Epoch: 2, Loss: 1.3840694009559229e-05 Updates: 241/48001, Avg Grad: 0.015686405822634697\n",
            "Epoch: 2, Loss: 0.006340488791465759 Updates: 246/49001, Avg Grad: 0.018638579174876213\n",
            "Epoch 1 iteration 0 Loss: 0.240 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.398 | Acc: 87.032% (349/401)\n",
            "Test accuracy: 0.8703241944313049\n",
            "Epoch: 2, Loss: 7.162521796999499e-05 Updates: 251/50001, Avg Grad: 0.01557727437466383\n",
            "Epoch: 2, Loss: 0.027176953852176666 Updates: 256/51001, Avg Grad: 0.012075710110366344\n",
            "Epoch: 2, Loss: 0.003136993618682027 Updates: 261/52001, Avg Grad: 0.0234236977994442\n",
            "Epoch: 2, Loss: 0.002919903490692377 Updates: 266/53001, Avg Grad: 0.015833914279937744\n",
            "Epoch: 2, Loss: 0.0011042251717299223 Updates: 271/54001, Avg Grad: 0.01733420230448246\n",
            "Epoch 1 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.415 | Acc: 88.030% (353/401)\n",
            "Test accuracy: 0.8802992701530457\n",
            "Epoch: 2, Loss: 0.00021337605721782893 Updates: 276/55001, Avg Grad: 0.019106898456811905\n",
            "Epoch: 2, Loss: 0.0008897409425117075 Updates: 281/56001, Avg Grad: 0.015167389996349812\n",
            "Epoch: 2, Loss: 0.00018341786926612258 Updates: 286/57001, Avg Grad: 0.014904421754181385\n",
            "Epoch: 2, Loss: 0.00011545031156856567 Updates: 291/58001, Avg Grad: 0.016709033399820328\n",
            "Epoch: 2, Loss: 0.0007816668367013335 Updates: 296/59001, Avg Grad: 0.01562555506825447\n",
            "Epoch 2 iteration 0 Loss: 0.211 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.489 | Acc: 85.536% (343/401)\n",
            "Test accuracy: 0.8553615808486938\n",
            "Epoch: 3, Loss: 0.0006662540254183114 Updates: 1/1, Avg Grad: 0.00019703232101164758\n",
            "Epoch: 3, Loss: 0.0035420090425759554 Updates: 6/1001, Avg Grad: 0.023719849064946175\n",
            "Epoch: 3, Loss: 9.059938020072877e-05 Updates: 11/2001, Avg Grad: 0.01228966936469078\n",
            "Epoch: 3, Loss: 7.961154187796637e-05 Updates: 16/3001, Avg Grad: 0.015660200268030167\n",
            "Epoch: 3, Loss: 0.0005690749385394156 Updates: 21/4001, Avg Grad: 0.016792993992567062\n",
            "Epoch 2 iteration 0 Loss: 0.776 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.426 | Acc: 87.531% (351/401)\n",
            "Test accuracy: 0.8753117322921753\n",
            "Epoch: 3, Loss: 0.006006693467497826 Updates: 26/5001, Avg Grad: 0.018630260601639748\n",
            "Epoch: 3, Loss: 0.01430606935173273 Updates: 31/6001, Avg Grad: 0.02056455798447132\n",
            "Epoch: 3, Loss: 0.0014795538736507297 Updates: 36/7001, Avg Grad: 0.021442603319883347\n",
            "Epoch: 3, Loss: 0.0046290261670947075 Updates: 41/8001, Avg Grad: 0.014936553314328194\n",
            "Epoch: 3, Loss: 0.009882166050374508 Updates: 46/9001, Avg Grad: 0.023972969502210617\n",
            "Epoch 2 iteration 0 Loss: 0.423 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.416 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 3, Loss: 4.105475090909749e-05 Updates: 51/10001, Avg Grad: 0.017797108739614487\n",
            "Epoch: 3, Loss: 0.0037997516337782145 Updates: 56/11001, Avg Grad: 0.012170517817139626\n",
            "Epoch: 3, Loss: 0.004635707009583712 Updates: 61/12001, Avg Grad: 0.02716113068163395\n",
            "Epoch: 3, Loss: 0.0005903581622987986 Updates: 66/13001, Avg Grad: 0.022519009187817574\n",
            "Epoch: 3, Loss: 7.479239138774574e-05 Updates: 71/14001, Avg Grad: 0.01786620356142521\n",
            "Epoch 2 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.489 | Acc: 85.786% (344/401)\n",
            "Test accuracy: 0.8578553795814514\n",
            "Epoch: 3, Loss: 0.00017775162996258587 Updates: 76/15001, Avg Grad: 0.017279334366321564\n",
            "Epoch: 3, Loss: 0.00010619140812195837 Updates: 81/16001, Avg Grad: 0.019506197422742844\n",
            "Epoch: 3, Loss: 2.1959529476589523e-05 Updates: 86/17001, Avg Grad: 0.013683217577636242\n",
            "Epoch: 3, Loss: 1.704998112472822e-06 Updates: 91/18001, Avg Grad: 0.016500525176525116\n",
            "Epoch: 3, Loss: 0.0028403166215866804 Updates: 96/19001, Avg Grad: 0.019925128668546677\n",
            "Epoch 2 iteration 0 Loss: 0.064 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.463 | Acc: 84.788% (340/401)\n",
            "Test accuracy: 0.8478803038597107\n",
            "Epoch: 3, Loss: 0.012507323175668716 Updates: 101/20001, Avg Grad: 0.01825994998216629\n",
            "Epoch: 3, Loss: 6.033712452335749e-06 Updates: 106/21001, Avg Grad: 0.014100775122642517\n",
            "Epoch: 3, Loss: 0.011576239950954914 Updates: 111/22001, Avg Grad: 0.01906803995370865\n",
            "Epoch: 3, Loss: 0.0016870166873559356 Updates: 116/23001, Avg Grad: 0.01770198717713356\n",
            "Epoch: 3, Loss: 0.0001946182019310072 Updates: 121/24001, Avg Grad: 0.024384688585996628\n",
            "Epoch 2 iteration 0 Loss: 0.027 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.386 | Acc: 87.781% (352/401)\n",
            "Test accuracy: 0.8778054714202881\n",
            "Epoch: 3, Loss: 0.0009863353334367275 Updates: 126/25001, Avg Grad: 0.019405987113714218\n",
            "Epoch: 3, Loss: 0.008545727469027042 Updates: 131/26001, Avg Grad: 0.01641479693353176\n",
            "Epoch: 3, Loss: 0.0028573081362992525 Updates: 136/27001, Avg Grad: 0.01574990153312683\n",
            "Epoch: 3, Loss: 0.0014570907223969698 Updates: 141/28001, Avg Grad: 0.019540777429938316\n",
            "Epoch: 3, Loss: 0.00029749044915661216 Updates: 146/29001, Avg Grad: 0.01952643319964409\n",
            "Epoch 2 iteration 0 Loss: 0.028 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.370 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 3, Loss: 0.0055611515417695045 Updates: 151/30001, Avg Grad: 0.02278687059879303\n",
            "Epoch: 3, Loss: 1.112890549848089e-05 Updates: 156/31001, Avg Grad: 0.01889374852180481\n",
            "Epoch: 3, Loss: 0.00325344642624259 Updates: 161/32001, Avg Grad: 0.009618105366826057\n",
            "Epoch: 3, Loss: 8.963814616436139e-05 Updates: 166/33001, Avg Grad: 0.020774509757757187\n",
            "Epoch: 3, Loss: 0.006016760598868132 Updates: 171/34001, Avg Grad: 0.027628779411315918\n",
            "Epoch 2 iteration 0 Loss: 1.114 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 400 Loss: 0.320 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 3, Loss: 0.0008790424326434731 Updates: 176/35001, Avg Grad: 0.02615237794816494\n",
            "Epoch: 3, Loss: 4.1857500036712736e-05 Updates: 181/36001, Avg Grad: 0.020651455968618393\n",
            "Epoch: 3, Loss: 0.0003859355638269335 Updates: 186/37001, Avg Grad: 0.013494186103343964\n",
            "Epoch: 3, Loss: 0.00018353739869780838 Updates: 191/38001, Avg Grad: 0.020647145807743073\n",
            "Epoch: 3, Loss: 0.000198265872313641 Updates: 196/39001, Avg Grad: 0.015740418806672096\n",
            "Epoch 2 iteration 0 Loss: 0.200 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.428 | Acc: 89.277% (358/401)\n",
            "Test accuracy: 0.8927680850028992\n",
            "Epoch: 3, Loss: 0.019279222935438156 Updates: 201/40001, Avg Grad: 0.015696341171860695\n",
            "Epoch: 3, Loss: 0.0017435221234336495 Updates: 206/41001, Avg Grad: 0.02155967988073826\n",
            "Epoch: 3, Loss: 1.8795772120938636e-06 Updates: 211/42001, Avg Grad: 0.019708219915628433\n",
            "Epoch: 3, Loss: 0.009428462013602257 Updates: 216/43001, Avg Grad: 0.023731499910354614\n",
            "Epoch: 3, Loss: 0.00011812650336651132 Updates: 221/44001, Avg Grad: 0.01855958253145218\n",
            "Epoch 2 iteration 0 Loss: 0.021 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.409 | Acc: 88.030% (353/401)\n",
            "Test accuracy: 0.8802992701530457\n",
            "Epoch: 3, Loss: 0.00018228065164294094 Updates: 226/45001, Avg Grad: 0.018868422135710716\n",
            "Epoch: 3, Loss: 0.00016158113430719823 Updates: 231/46001, Avg Grad: 0.020388802513480186\n",
            "Epoch: 3, Loss: 0.0031393105164170265 Updates: 236/47001, Avg Grad: 0.023187121376395226\n",
            "Epoch: 3, Loss: 0.0007737441337667406 Updates: 241/48001, Avg Grad: 0.015366712585091591\n",
            "Epoch: 3, Loss: 0.008410000242292881 Updates: 246/49001, Avg Grad: 0.017902042716741562\n",
            "Epoch 2 iteration 0 Loss: 0.112 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.392 | Acc: 88.529% (355/401)\n",
            "Test accuracy: 0.885286808013916\n",
            "Epoch: 3, Loss: 0.0014380845241248608 Updates: 251/50001, Avg Grad: 0.015025502070784569\n",
            "Epoch: 3, Loss: 9.818377293413505e-05 Updates: 256/51001, Avg Grad: 0.01562705636024475\n",
            "Epoch: 3, Loss: 0.0031815802212804556 Updates: 261/52001, Avg Grad: 0.017895469442009926\n",
            "Epoch: 3, Loss: 0.0004343210021033883 Updates: 266/53001, Avg Grad: 0.019029948860406876\n",
            "Epoch: 3, Loss: 0.000469468766823411 Updates: 271/54001, Avg Grad: 0.01528023462742567\n",
            "Epoch 2 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.393 | Acc: 87.032% (349/401)\n",
            "Test accuracy: 0.8703241944313049\n",
            "Epoch: 3, Loss: 8.819303184282035e-05 Updates: 276/55001, Avg Grad: 0.017097223550081253\n",
            "Epoch: 3, Loss: 0.0010194869246333838 Updates: 281/56001, Avg Grad: 0.02110203541815281\n",
            "Epoch: 3, Loss: 0.0005772142321802676 Updates: 286/57001, Avg Grad: 0.012301933951675892\n",
            "Epoch: 3, Loss: 3.574912261683494e-05 Updates: 291/58001, Avg Grad: 0.014762510545551777\n",
            "Epoch: 3, Loss: 4.149929736740887e-05 Updates: 296/59001, Avg Grad: 0.014173543080687523\n",
            "Epoch 3 iteration 0 Loss: 0.046 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.389 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 4, Loss: 4.4949651055503637e-05 Updates: 1/1, Avg Grad: 1.9025339497602545e-05\n",
            "Epoch: 4, Loss: 0.0020804950036108494 Updates: 6/1001, Avg Grad: 0.022143539041280746\n",
            "Epoch: 4, Loss: 0.005969426594674587 Updates: 11/2001, Avg Grad: 0.0074112266302108765\n",
            "Epoch: 4, Loss: 0.0027861944399774075 Updates: 16/3001, Avg Grad: 0.022542525082826614\n",
            "Epoch: 4, Loss: 0.0003058644651900977 Updates: 21/4001, Avg Grad: 0.02402106299996376\n",
            "Epoch 3 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.341 | Acc: 89.277% (358/401)\n",
            "Test accuracy: 0.8927680850028992\n",
            "Epoch: 4, Loss: 0.00017119836411438882 Updates: 26/5001, Avg Grad: 0.01859542727470398\n",
            "Epoch: 4, Loss: 0.0002401168312644586 Updates: 31/6001, Avg Grad: 0.02112613618373871\n",
            "Epoch: 4, Loss: 0.000543949194252491 Updates: 36/7001, Avg Grad: 0.017213333398103714\n",
            "Epoch: 4, Loss: 9.437400149181485e-05 Updates: 41/8001, Avg Grad: 0.009944452904164791\n",
            "Epoch: 4, Loss: 8.112481737043709e-05 Updates: 46/9001, Avg Grad: 0.02096247673034668\n",
            "Epoch 3 iteration 0 Loss: 0.232 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.383 | Acc: 88.529% (355/401)\n",
            "Test accuracy: 0.885286808013916\n",
            "Epoch: 4, Loss: 0.008361035957932472 Updates: 51/10001, Avg Grad: 0.015892429277300835\n",
            "Epoch: 4, Loss: 0.00015496558626182377 Updates: 56/11001, Avg Grad: 0.016888776794075966\n",
            "Epoch: 4, Loss: 4.293862275517313e-06 Updates: 61/12001, Avg Grad: 0.015027329325675964\n",
            "Epoch: 4, Loss: 3.935425047529861e-05 Updates: 66/13001, Avg Grad: 0.014556676149368286\n",
            "Epoch: 4, Loss: 0.0022234488278627396 Updates: 71/14001, Avg Grad: 0.015492938458919525\n",
            "Epoch 3 iteration 0 Loss: 0.022 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.401 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 4, Loss: 0.001099687535315752 Updates: 76/15001, Avg Grad: 0.023603811860084534\n",
            "Epoch: 4, Loss: 0.00033393598278053105 Updates: 81/16001, Avg Grad: 0.026242980733513832\n",
            "Epoch: 4, Loss: 2.02643932425417e-05 Updates: 86/17001, Avg Grad: 0.02102506160736084\n",
            "Epoch: 4, Loss: 3.721361281350255e-05 Updates: 91/18001, Avg Grad: 0.022129837423563004\n",
            "Epoch: 4, Loss: 5.9169706219108775e-05 Updates: 96/19001, Avg Grad: 0.022438401356339455\n",
            "Epoch 3 iteration 0 Loss: 0.237 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.330 | Acc: 88.529% (355/401)\n",
            "Test accuracy: 0.885286808013916\n",
            "Epoch: 4, Loss: 4.783681288245134e-05 Updates: 101/20001, Avg Grad: 0.029391396790742874\n",
            "Epoch: 4, Loss: 0.00017197011038661003 Updates: 106/21001, Avg Grad: 0.024032913148403168\n",
            "Epoch: 4, Loss: 0.0004619971732608974 Updates: 111/22001, Avg Grad: 0.014175152406096458\n",
            "Epoch: 4, Loss: 6.492420652648434e-05 Updates: 116/23001, Avg Grad: 0.01920909620821476\n",
            "Epoch: 4, Loss: 1.0708420631999616e-05 Updates: 121/24001, Avg Grad: 0.028972959145903587\n",
            "Epoch 3 iteration 0 Loss: 0.018 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.337 | Acc: 88.778% (356/401)\n",
            "Test accuracy: 0.8877805471420288\n",
            "Epoch: 4, Loss: 0.00026459922082722187 Updates: 126/25001, Avg Grad: 0.012616331689059734\n",
            "Epoch: 4, Loss: 0.0008800837676972151 Updates: 131/26001, Avg Grad: 0.01536520104855299\n",
            "Epoch: 4, Loss: 7.251882925629616e-05 Updates: 136/27001, Avg Grad: 0.022259939461946487\n",
            "Epoch: 4, Loss: 1.104029161069775e-05 Updates: 141/28001, Avg Grad: 0.021280622109770775\n",
            "Epoch: 4, Loss: 0.0052709924057126045 Updates: 146/29001, Avg Grad: 0.013769738376140594\n",
            "Epoch 3 iteration 0 Loss: 0.045 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.336 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 4, Loss: 0.001198158017359674 Updates: 151/30001, Avg Grad: 0.017815668135881424\n",
            "Epoch: 4, Loss: 1.0254589142277837e-05 Updates: 156/31001, Avg Grad: 0.015392480418086052\n",
            "Epoch: 4, Loss: 0.00254803616553545 Updates: 161/32001, Avg Grad: 0.01751519739627838\n",
            "Epoch: 4, Loss: 8.505486039211974e-05 Updates: 166/33001, Avg Grad: 0.016508910804986954\n",
            "Epoch: 4, Loss: 0.0011279493337497115 Updates: 171/34001, Avg Grad: 0.015799392014741898\n",
            "Epoch 3 iteration 0 Loss: 0.023 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.310 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 4, Loss: 0.0004415912553668022 Updates: 176/35001, Avg Grad: 0.01308140903711319\n",
            "Epoch: 4, Loss: 0.025695182383060455 Updates: 181/36001, Avg Grad: 0.018022717908024788\n",
            "Epoch: 4, Loss: 0.0006101459730416536 Updates: 186/37001, Avg Grad: 0.013796767219901085\n",
            "Epoch: 4, Loss: 0.008220572024583817 Updates: 191/38001, Avg Grad: 0.01909482851624489\n",
            "Epoch: 4, Loss: 5.425250947155291e-06 Updates: 196/39001, Avg Grad: 0.013292113319039345\n",
            "Epoch 3 iteration 0 Loss: 0.075 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.334 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 4, Loss: 0.0014221240999177098 Updates: 201/40001, Avg Grad: 0.015573549084365368\n",
            "Epoch: 4, Loss: 0.003985920455306768 Updates: 206/41001, Avg Grad: 0.022508522495627403\n",
            "Epoch: 4, Loss: 0.000281717861071229 Updates: 211/42001, Avg Grad: 0.02051614783704281\n",
            "Epoch: 4, Loss: 8.231751417042688e-05 Updates: 216/43001, Avg Grad: 0.02563723921775818\n",
            "Epoch: 4, Loss: 0.0001734770048642531 Updates: 221/44001, Avg Grad: 0.01676384173333645\n",
            "Epoch 3 iteration 0 Loss: 0.019 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.390 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 4, Loss: 0.0003226381086278707 Updates: 226/45001, Avg Grad: 0.016942093148827553\n",
            "Epoch: 4, Loss: 0.0010114164324477315 Updates: 231/46001, Avg Grad: 0.015159050934016705\n",
            "Epoch: 4, Loss: 0.0003150902921333909 Updates: 236/47001, Avg Grad: 0.028084341436624527\n",
            "Epoch: 4, Loss: 3.933751884233061e-07 Updates: 241/48001, Avg Grad: 0.01536899246275425\n",
            "Epoch: 4, Loss: 0.00033224470098502934 Updates: 246/49001, Avg Grad: 0.016387324780225754\n",
            "Epoch 3 iteration 0 Loss: 0.011 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.390 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 4, Loss: 0.00016731357027310878 Updates: 251/50001, Avg Grad: 0.02111278846859932\n",
            "Epoch: 4, Loss: 0.0012454126263037324 Updates: 256/51001, Avg Grad: 0.021666334941983223\n",
            "Epoch: 4, Loss: 3.544551873346791e-05 Updates: 261/52001, Avg Grad: 0.014790108427405357\n",
            "Epoch: 4, Loss: 2.8292804927332327e-05 Updates: 266/53001, Avg Grad: 0.022820036858320236\n",
            "Epoch: 4, Loss: 2.8448084776755422e-05 Updates: 271/54001, Avg Grad: 0.018604662269353867\n",
            "Epoch 3 iteration 0 Loss: 0.055 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.343 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 4, Loss: 5.9604289504022745e-08 Updates: 276/55001, Avg Grad: 0.014896000735461712\n",
            "Epoch: 4, Loss: 0.011325499042868614 Updates: 281/56001, Avg Grad: 0.016489330679178238\n",
            "Epoch: 4, Loss: 0.0025023911148309708 Updates: 286/57001, Avg Grad: 0.01458771526813507\n",
            "Epoch: 4, Loss: 1.3726567885896657e-05 Updates: 291/58001, Avg Grad: 0.026748448610305786\n",
            "Epoch: 4, Loss: 9.430498175788671e-05 Updates: 296/59001, Avg Grad: 0.014345690608024597\n",
            "Epoch 4 iteration 0 Loss: 0.397 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.342 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 5, Loss: 0.0002895575016736984 Updates: 1/1, Avg Grad: 7.332131644943729e-05\n",
            "Epoch: 5, Loss: 0.001737384358420968 Updates: 6/1001, Avg Grad: 0.01889299973845482\n",
            "Epoch: 5, Loss: 0.00029173280927352607 Updates: 11/2001, Avg Grad: 0.02116752415895462\n",
            "Epoch: 5, Loss: 3.166285023326054e-05 Updates: 16/3001, Avg Grad: 0.01897379755973816\n",
            "Epoch: 5, Loss: 0.0008626808412373066 Updates: 21/4001, Avg Grad: 0.021550025790929794\n",
            "Epoch 4 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.257 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 5, Loss: 4.3048970837844536e-05 Updates: 26/5001, Avg Grad: 0.01714806631207466\n",
            "Epoch: 5, Loss: 0.00016918143955990672 Updates: 31/6001, Avg Grad: 0.019841331988573074\n",
            "Epoch: 5, Loss: 0.00027573443367145956 Updates: 36/7001, Avg Grad: 0.014075979590415955\n",
            "Epoch: 5, Loss: 9.326575309387408e-06 Updates: 41/8001, Avg Grad: 0.015159882605075836\n",
            "Epoch: 5, Loss: 0.00029246427584439516 Updates: 46/9001, Avg Grad: 0.021342838183045387\n",
            "Epoch 4 iteration 0 Loss: 3.559 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 400 Loss: 0.432 | Acc: 88.279% (354/401)\n",
            "Test accuracy: 0.8827930092811584\n",
            "Epoch: 5, Loss: 0.0006249136640690267 Updates: 51/10001, Avg Grad: 0.012876657769083977\n",
            "Epoch: 5, Loss: 0.014137238264083862 Updates: 56/11001, Avg Grad: 0.017129456624388695\n",
            "Epoch: 5, Loss: 3.0712737498106435e-05 Updates: 61/12001, Avg Grad: 0.013894280418753624\n",
            "Epoch: 5, Loss: 0.0001081750015146099 Updates: 66/13001, Avg Grad: 0.023549458011984825\n",
            "Epoch: 5, Loss: 0.0002443729026708752 Updates: 71/14001, Avg Grad: 0.012434402480721474\n",
            "Epoch 4 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.349 | Acc: 88.778% (356/401)\n",
            "Test accuracy: 0.8877805471420288\n",
            "Epoch: 5, Loss: 3.2956463655864354e-06 Updates: 76/15001, Avg Grad: 0.021534565836191177\n",
            "Epoch: 5, Loss: 3.4427710033924086e-06 Updates: 81/16001, Avg Grad: 0.019712265580892563\n",
            "Epoch: 5, Loss: 0.00042195693822577596 Updates: 86/17001, Avg Grad: 0.01567818410694599\n",
            "Epoch: 5, Loss: 4.280904249753803e-05 Updates: 91/18001, Avg Grad: 0.014308108016848564\n",
            "Epoch: 5, Loss: 2.7236530513619073e-05 Updates: 96/19001, Avg Grad: 0.017023369669914246\n",
            "Epoch 4 iteration 0 Loss: 0.049 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.349 | Acc: 88.279% (354/401)\n",
            "Test accuracy: 0.8827930092811584\n",
            "Epoch: 5, Loss: 0.007227094378322363 Updates: 101/20001, Avg Grad: 0.012845603749155998\n",
            "Epoch: 5, Loss: 0.0004840544715989381 Updates: 106/21001, Avg Grad: 0.015265854075551033\n",
            "Epoch: 5, Loss: 0.00010224292054772377 Updates: 111/22001, Avg Grad: 0.020203234627842903\n",
            "Epoch: 5, Loss: 0.023228030651807785 Updates: 116/23001, Avg Grad: 0.013929794542491436\n",
            "Epoch: 5, Loss: 0.00014375243335962296 Updates: 121/24001, Avg Grad: 0.015347907319664955\n",
            "Epoch 4 iteration 0 Loss: 0.071 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.343 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 5, Loss: 7.38849439585465e-06 Updates: 126/25001, Avg Grad: 0.013923866674304008\n",
            "Epoch: 5, Loss: 6.332891643978655e-05 Updates: 131/26001, Avg Grad: 0.02197333611547947\n",
            "Epoch: 5, Loss: 3.5559151001507416e-05 Updates: 136/27001, Avg Grad: 0.017710935324430466\n",
            "Epoch: 5, Loss: 3.51960738953494e-06 Updates: 141/28001, Avg Grad: 0.0234563946723938\n",
            "Epoch: 5, Loss: 0.005181887187063694 Updates: 146/29001, Avg Grad: 0.017047030851244926\n",
            "Epoch 4 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.301 | Acc: 89.277% (358/401)\n",
            "Test accuracy: 0.8927680850028992\n",
            "Epoch: 5, Loss: 0.0016778873978182673 Updates: 151/30001, Avg Grad: 0.022160092368721962\n",
            "Epoch: 5, Loss: 6.201511132530868e-05 Updates: 156/31001, Avg Grad: 0.011054309085011482\n",
            "Epoch: 5, Loss: 6.96382558089681e-05 Updates: 161/32001, Avg Grad: 0.021130738779902458\n",
            "Epoch: 5, Loss: 0.013841575011610985 Updates: 166/33001, Avg Grad: 0.017061088234186172\n",
            "Epoch: 5, Loss: 0.00019622774561867118 Updates: 171/34001, Avg Grad: 0.025279972702264786\n",
            "Epoch 4 iteration 0 Loss: 0.037 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.344 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 5, Loss: 1.690388126007747e-05 Updates: 176/35001, Avg Grad: 0.02240077778697014\n",
            "Epoch: 5, Loss: 4.39672039647121e-05 Updates: 181/36001, Avg Grad: 0.025774260982871056\n",
            "Epoch: 5, Loss: 6.048595423635561e-06 Updates: 186/37001, Avg Grad: 0.020983656868338585\n",
            "Epoch: 5, Loss: 0.011848699301481247 Updates: 191/38001, Avg Grad: 0.021159671247005463\n",
            "Epoch: 5, Loss: 9.760103421285748e-05 Updates: 196/39001, Avg Grad: 0.015320828184485435\n",
            "Epoch 4 iteration 0 Loss: 0.063 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.340 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 5, Loss: 0.0008615566184744239 Updates: 201/40001, Avg Grad: 0.026290446519851685\n",
            "Epoch: 5, Loss: 0.008342227898538113 Updates: 206/41001, Avg Grad: 0.016919748857617378\n",
            "Epoch: 5, Loss: 2.33669579756679e-05 Updates: 211/42001, Avg Grad: 0.02669088914990425\n",
            "Epoch: 5, Loss: 5.738579056924209e-05 Updates: 216/43001, Avg Grad: 0.01992282271385193\n",
            "Epoch: 5, Loss: 0.01542440615594387 Updates: 221/44001, Avg Grad: 0.019443614408373833\n",
            "Epoch 4 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.353 | Acc: 88.778% (356/401)\n",
            "Test accuracy: 0.8877805471420288\n",
            "Epoch: 5, Loss: 0.0011599059216678143 Updates: 226/45001, Avg Grad: 0.021128762513399124\n",
            "Epoch: 5, Loss: 0.00035263426252640784 Updates: 231/46001, Avg Grad: 0.019527817144989967\n",
            "Epoch: 5, Loss: 0.0008764219819568098 Updates: 236/47001, Avg Grad: 0.01918640360236168\n",
            "Epoch: 5, Loss: 0.0008118611294776201 Updates: 241/48001, Avg Grad: 0.012573127634823322\n",
            "Epoch: 5, Loss: 9.56930307438597e-06 Updates: 246/49001, Avg Grad: 0.021316993981599808\n",
            "Epoch 4 iteration 0 Loss: 0.007 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.332 | Acc: 88.778% (356/401)\n",
            "Test accuracy: 0.8877805471420288\n",
            "Epoch: 5, Loss: 1.595571120560635e-05 Updates: 251/50001, Avg Grad: 0.013454812578856945\n",
            "Epoch: 5, Loss: 6.556194421136752e-05 Updates: 256/51001, Avg Grad: 0.016001414507627487\n",
            "Epoch: 5, Loss: 0.00014892738545313478 Updates: 261/52001, Avg Grad: 0.0175644401460886\n",
            "Epoch: 5, Loss: 0.0015030007343739271 Updates: 266/53001, Avg Grad: 0.02464483678340912\n",
            "Epoch: 5, Loss: 1.5053867628012085e-06 Updates: 271/54001, Avg Grad: 0.03398383408784866\n",
            "Epoch 4 iteration 0 Loss: 0.043 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.374 | Acc: 87.781% (352/401)\n",
            "Test accuracy: 0.8778054714202881\n",
            "Epoch: 5, Loss: 4.8280285227519926e-06 Updates: 276/55001, Avg Grad: 0.014315200038254261\n",
            "Epoch: 5, Loss: 0.0008080284460447729 Updates: 281/56001, Avg Grad: 0.021817129105329514\n",
            "Epoch: 5, Loss: 0.004967702552676201 Updates: 286/57001, Avg Grad: 0.02983071841299534\n",
            "Epoch: 5, Loss: 7.777797873131931e-05 Updates: 291/58001, Avg Grad: 0.023914169520139694\n",
            "Epoch: 5, Loss: 0.0025477400049567223 Updates: 296/59001, Avg Grad: 0.01719292439520359\n",
            "Epoch 5 iteration 0 Loss: 0.105 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.345 | Acc: 88.529% (355/401)\n",
            "Test accuracy: 0.885286808013916\n",
            "Epoch: 6, Loss: 5.603211320703849e-05 Updates: 1/1, Avg Grad: 2.8144417228759266e-05\n",
            "Epoch: 6, Loss: 3.6718396586366e-05 Updates: 6/1001, Avg Grad: 0.021015789359807968\n",
            "Epoch: 6, Loss: 0.00013219132961239666 Updates: 11/2001, Avg Grad: 0.013532424345612526\n",
            "Epoch: 6, Loss: 4.6138022298691794e-05 Updates: 16/3001, Avg Grad: 0.016750704497098923\n",
            "Epoch: 6, Loss: 5.599699761660304e-06 Updates: 21/4001, Avg Grad: 0.019418353214859962\n",
            "Epoch 5 iteration 0 Loss: 0.022 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.329 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 6, Loss: 0.00024940172443166375 Updates: 26/5001, Avg Grad: 0.021052131429314613\n",
            "Epoch: 6, Loss: 2.538129047024995e-05 Updates: 31/6001, Avg Grad: 0.014416083693504333\n",
            "Epoch: 6, Loss: 0.00033094504033215344 Updates: 36/7001, Avg Grad: 0.017830807715654373\n",
            "Epoch: 6, Loss: 0.001493666903115809 Updates: 41/8001, Avg Grad: 0.011574401520192623\n",
            "Epoch: 6, Loss: 3.931462924811058e-05 Updates: 46/9001, Avg Grad: 0.01773197203874588\n",
            "Epoch 5 iteration 0 Loss: 0.037 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.400 | Acc: 88.778% (356/401)\n",
            "Test accuracy: 0.8877805471420288\n",
            "Epoch: 6, Loss: 8.844777767080814e-05 Updates: 51/10001, Avg Grad: 0.011388134211301804\n",
            "Epoch: 6, Loss: 0.0025006162468343973 Updates: 56/11001, Avg Grad: 0.013155830092728138\n",
            "Epoch: 6, Loss: 1.4864143849990796e-05 Updates: 61/12001, Avg Grad: 0.026592735201120377\n",
            "Epoch: 6, Loss: 0.00014581147115677595 Updates: 66/13001, Avg Grad: 0.023256603628396988\n",
            "Epoch: 6, Loss: 0.0028254357166588306 Updates: 71/14001, Avg Grad: 0.026154931634664536\n",
            "Epoch 5 iteration 0 Loss: 0.058 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.271 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 6, Loss: 0.011179535649716854 Updates: 76/15001, Avg Grad: 0.018512967973947525\n",
            "Epoch: 6, Loss: 7.008731336100027e-05 Updates: 81/16001, Avg Grad: 0.015014426782727242\n",
            "Epoch: 6, Loss: 1.808765227906406e-05 Updates: 86/17001, Avg Grad: 0.021630899980664253\n",
            "Epoch: 6, Loss: 7.992551218194421e-06 Updates: 91/18001, Avg Grad: 0.014278588816523552\n",
            "Epoch: 6, Loss: 0.0001139631203841418 Updates: 96/19001, Avg Grad: 0.016723917797207832\n",
            "Epoch 5 iteration 0 Loss: 0.111 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.307 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 6, Loss: 0.000678383163176477 Updates: 101/20001, Avg Grad: 0.017097601667046547\n",
            "Epoch: 6, Loss: 0.007671445142477751 Updates: 106/21001, Avg Grad: 0.014380025677382946\n",
            "Epoch: 6, Loss: 0.0016784644685685635 Updates: 111/22001, Avg Grad: 0.011347156018018723\n",
            "Epoch: 6, Loss: 0.00023501100076828152 Updates: 116/23001, Avg Grad: 0.014275727793574333\n",
            "Epoch: 6, Loss: 0.00042163862963207066 Updates: 121/24001, Avg Grad: 0.026082638651132584\n",
            "Epoch 5 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.340 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 6, Loss: 4.055642875755439e-06 Updates: 126/25001, Avg Grad: 0.016789773479104042\n",
            "Epoch: 6, Loss: 0.0007269821944646537 Updates: 131/26001, Avg Grad: 0.02021665871143341\n",
            "Epoch: 6, Loss: 0.000469512160634622 Updates: 136/27001, Avg Grad: 0.020524021238088608\n",
            "Epoch: 6, Loss: 0.0015640462515875697 Updates: 141/28001, Avg Grad: 0.016123328357934952\n",
            "Epoch: 6, Loss: 1.097724998544436e-05 Updates: 146/29001, Avg Grad: 0.028292495757341385\n",
            "Epoch 5 iteration 0 Loss: 0.581 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.311 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 6, Loss: 0.00011557145626284182 Updates: 151/30001, Avg Grad: 0.019475197419524193\n",
            "Epoch: 6, Loss: 0.0005182338645681739 Updates: 156/31001, Avg Grad: 0.019912753254175186\n",
            "Epoch: 6, Loss: 3.8841502828290686e-05 Updates: 161/32001, Avg Grad: 0.020536044612526894\n",
            "Epoch: 6, Loss: 0.006019002757966518 Updates: 166/33001, Avg Grad: 0.022459739819169044\n",
            "Epoch: 6, Loss: 0.0001624668948352337 Updates: 171/34001, Avg Grad: 0.030346933752298355\n",
            "Epoch 5 iteration 0 Loss: 0.090 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.329 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 6, Loss: 0.00011050178000004962 Updates: 176/35001, Avg Grad: 0.008701920509338379\n",
            "Epoch: 6, Loss: 2.594107536424417e-05 Updates: 181/36001, Avg Grad: 0.015675855800509453\n",
            "Epoch: 6, Loss: 8.569165402150247e-06 Updates: 186/37001, Avg Grad: 0.01801803521811962\n",
            "Epoch: 6, Loss: 0.00021877273684367537 Updates: 191/38001, Avg Grad: 0.021929370239377022\n",
            "Epoch: 6, Loss: 7.929826097097248e-05 Updates: 196/39001, Avg Grad: 0.017212091013789177\n",
            "Epoch 5 iteration 0 Loss: 0.040 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.288 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 6, Loss: 5.249009063845733e-06 Updates: 201/40001, Avg Grad: 0.018535707145929337\n",
            "Epoch: 6, Loss: 0.0011115780798718333 Updates: 206/41001, Avg Grad: 0.021836332976818085\n",
            "Epoch: 6, Loss: 6.529544771183282e-05 Updates: 211/42001, Avg Grad: 0.022129282355308533\n",
            "Epoch: 6, Loss: 3.490988092380576e-05 Updates: 216/43001, Avg Grad: 0.022272292524576187\n",
            "Epoch: 6, Loss: 5.541850987356156e-05 Updates: 221/44001, Avg Grad: 0.01903153583407402\n",
            "Epoch 5 iteration 0 Loss: 0.071 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.262 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 6, Loss: 1.6575369954807684e-05 Updates: 226/45001, Avg Grad: 0.016477197408676147\n",
            "Epoch: 6, Loss: 0.016884692013263702 Updates: 231/46001, Avg Grad: 0.011315857991576195\n",
            "Epoch: 6, Loss: 0.00021308075520209968 Updates: 236/47001, Avg Grad: 0.022298689931631088\n",
            "Epoch: 6, Loss: 1.7683194528217427e-05 Updates: 241/48001, Avg Grad: 0.024733053520321846\n",
            "Epoch: 6, Loss: 4.3873853428522125e-05 Updates: 246/49001, Avg Grad: 0.01688588783144951\n",
            "Epoch 5 iteration 0 Loss: 0.020 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.361 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 6, Loss: 0.0045666866935789585 Updates: 251/50001, Avg Grad: 0.020021352916955948\n",
            "Epoch: 6, Loss: 0.006771804299205542 Updates: 256/51001, Avg Grad: 0.01635938510298729\n",
            "Epoch: 6, Loss: 1.6275380403385498e-06 Updates: 261/52001, Avg Grad: 0.026914771646261215\n",
            "Epoch: 6, Loss: 0.00012494847760535777 Updates: 266/53001, Avg Grad: 0.014481747522950172\n",
            "Epoch: 6, Loss: 0.0006277018110267818 Updates: 271/54001, Avg Grad: 0.013815611600875854\n",
            "Epoch 5 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.328 | Acc: 90.524% (363/401)\n",
            "Test accuracy: 0.9052368998527527\n",
            "Epoch: 6, Loss: 2.106336432916578e-05 Updates: 276/55001, Avg Grad: 0.022081172093749046\n",
            "Epoch: 6, Loss: 0.00025376948178745806 Updates: 281/56001, Avg Grad: 0.01916695199906826\n",
            "Epoch: 6, Loss: 0.0020215059630572796 Updates: 286/57001, Avg Grad: 0.028059840202331543\n",
            "Epoch: 6, Loss: 3.564851431292482e-05 Updates: 291/58001, Avg Grad: 0.01793861575424671\n",
            "Epoch: 6, Loss: 3.089874371653423e-05 Updates: 296/59001, Avg Grad: 0.02097460813820362\n",
            "Epoch 6 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.338 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 7, Loss: 0.0002885212597902864 Updates: 1/1, Avg Grad: 0.00010836638102773577\n",
            "Epoch: 7, Loss: 4.283741145627573e-05 Updates: 6/1001, Avg Grad: 0.016544628888368607\n",
            "Epoch: 7, Loss: 0.00025643379194661975 Updates: 11/2001, Avg Grad: 0.01932566612958908\n",
            "Epoch: 7, Loss: 0.0024997214786708355 Updates: 16/3001, Avg Grad: 0.01880352944135666\n",
            "Epoch: 7, Loss: 0.0006371386116370559 Updates: 21/4001, Avg Grad: 0.016659805551171303\n",
            "Epoch 6 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.300 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 7, Loss: 0.00020248789223842323 Updates: 26/5001, Avg Grad: 0.018480341881513596\n",
            "Epoch: 7, Loss: 0.00022531984723173082 Updates: 31/6001, Avg Grad: 0.01732286438345909\n",
            "Epoch: 7, Loss: 0.00021565897623077035 Updates: 36/7001, Avg Grad: 0.02307659573853016\n",
            "Epoch: 7, Loss: 9.179030513450925e-08 Updates: 41/8001, Avg Grad: 0.02272210083901882\n",
            "Epoch: 7, Loss: 0.000329286209307611 Updates: 46/9001, Avg Grad: 0.022491302341222763\n",
            "Epoch 6 iteration 0 Loss: 1.137 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 400 Loss: 0.310 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 7, Loss: 0.0006088833324611187 Updates: 51/10001, Avg Grad: 0.021571870893239975\n",
            "Epoch: 7, Loss: 0.00041514632175676525 Updates: 56/11001, Avg Grad: 0.01630322076380253\n",
            "Epoch: 7, Loss: 9.427749319002032e-05 Updates: 61/12001, Avg Grad: 0.018265044316649437\n",
            "Epoch: 7, Loss: 0.0014370535500347614 Updates: 66/13001, Avg Grad: 0.010801282711327076\n",
            "Epoch: 7, Loss: 0.0012273669708520174 Updates: 71/14001, Avg Grad: 0.020254645496606827\n",
            "Epoch 6 iteration 0 Loss: 0.086 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.307 | Acc: 90.524% (363/401)\n",
            "Test accuracy: 0.9052368998527527\n",
            "Epoch: 7, Loss: 3.907156497007236e-05 Updates: 76/15001, Avg Grad: 0.012418295256793499\n",
            "Epoch: 7, Loss: 4.095070471521467e-05 Updates: 81/16001, Avg Grad: 0.014913124963641167\n",
            "Epoch: 7, Loss: 0.00013393540575634688 Updates: 86/17001, Avg Grad: 0.01311543770134449\n",
            "Epoch: 7, Loss: 7.932290463941172e-05 Updates: 91/18001, Avg Grad: 0.011585438624024391\n",
            "Epoch: 7, Loss: 4.98177032568492e-05 Updates: 96/19001, Avg Grad: 0.016540786251425743\n",
            "Epoch 6 iteration 0 Loss: 4.222 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 400 Loss: 0.309 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 7, Loss: 5.001362296752632e-05 Updates: 101/20001, Avg Grad: 0.013280455954372883\n",
            "Epoch: 7, Loss: 0.0002623781329020858 Updates: 106/21001, Avg Grad: 0.023606613278388977\n",
            "Epoch: 7, Loss: 0.0002759076014626771 Updates: 111/22001, Avg Grad: 0.014895850792527199\n",
            "Epoch: 7, Loss: 3.035195368283894e-05 Updates: 116/23001, Avg Grad: 0.02547958865761757\n",
            "Epoch: 7, Loss: 2.3025226255413145e-05 Updates: 121/24001, Avg Grad: 0.0176687054336071\n",
            "Epoch 6 iteration 0 Loss: 0.772 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.305 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 7, Loss: 2.6123110728804022e-05 Updates: 126/25001, Avg Grad: 0.012039517052471638\n",
            "Epoch: 7, Loss: 0.001299382303841412 Updates: 131/26001, Avg Grad: 0.01878151297569275\n",
            "Epoch: 7, Loss: 3.540705074556172e-05 Updates: 136/27001, Avg Grad: 0.02642962895333767\n",
            "Epoch: 7, Loss: 0.00010621592082316056 Updates: 141/28001, Avg Grad: 0.02997874841094017\n",
            "Epoch: 7, Loss: 8.386488843825646e-06 Updates: 146/29001, Avg Grad: 0.019184136763215065\n",
            "Epoch 6 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.313 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 7, Loss: 6.389977352228016e-05 Updates: 151/30001, Avg Grad: 0.024418655782938004\n",
            "Epoch: 7, Loss: 0.00011535071826074272 Updates: 156/31001, Avg Grad: 0.022161753848195076\n",
            "Epoch: 7, Loss: 0.0033360738307237625 Updates: 161/32001, Avg Grad: 0.022934531792998314\n",
            "Epoch: 7, Loss: 2.6707668894232484e-06 Updates: 166/33001, Avg Grad: 0.02490926906466484\n",
            "Epoch: 7, Loss: 3.6765733966603875e-05 Updates: 171/34001, Avg Grad: 0.01816249080002308\n",
            "Epoch 6 iteration 0 Loss: 0.546 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.255 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 7, Loss: 1.586837061040569e-05 Updates: 176/35001, Avg Grad: 0.02944173291325569\n",
            "Epoch: 7, Loss: 0.009911024942994118 Updates: 181/36001, Avg Grad: 0.028591850772500038\n",
            "Epoch: 7, Loss: 4.178657036391087e-05 Updates: 186/37001, Avg Grad: 0.010688571259379387\n",
            "Epoch: 7, Loss: 5.078057938590064e-07 Updates: 191/38001, Avg Grad: 0.017550865188241005\n",
            "Epoch: 7, Loss: 1.5443531083292328e-05 Updates: 196/39001, Avg Grad: 0.025380561128258705\n",
            "Epoch 6 iteration 0 Loss: 0.007 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.295 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 7, Loss: 0.001941628404892981 Updates: 201/40001, Avg Grad: 0.01683620736002922\n",
            "Epoch: 7, Loss: 0.0002100434503518045 Updates: 206/41001, Avg Grad: 0.023259472101926804\n",
            "Epoch: 7, Loss: 0.0006616276223212481 Updates: 211/42001, Avg Grad: 0.018288705497980118\n",
            "Epoch: 7, Loss: 0.019814373925328255 Updates: 216/43001, Avg Grad: 0.013446638360619545\n",
            "Epoch: 7, Loss: 0.0030272221192717552 Updates: 221/44001, Avg Grad: 0.017235253006219864\n",
            "Epoch 6 iteration 0 Loss: 0.614 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.318 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 7, Loss: 1.4360765817400534e-05 Updates: 226/45001, Avg Grad: 0.018587037920951843\n",
            "Epoch: 7, Loss: 4.65325138065964e-05 Updates: 231/46001, Avg Grad: 0.015596741810441017\n",
            "Epoch: 7, Loss: 6.323652428363857e-07 Updates: 236/47001, Avg Grad: 0.01771228201687336\n",
            "Epoch: 7, Loss: 0.0008384869433939457 Updates: 241/48001, Avg Grad: 0.021934589371085167\n",
            "Epoch: 7, Loss: 6.399863195838407e-05 Updates: 246/49001, Avg Grad: 0.016892896965146065\n",
            "Epoch 6 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.269 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 7, Loss: 5.52186684217304e-05 Updates: 251/50001, Avg Grad: 0.019240692257881165\n",
            "Epoch: 7, Loss: 0.004195588640868664 Updates: 256/51001, Avg Grad: 0.022562751546502113\n",
            "Epoch: 7, Loss: 6.695598131045699e-05 Updates: 261/52001, Avg Grad: 0.02319095842540264\n",
            "Epoch: 7, Loss: 3.796314922510646e-05 Updates: 266/53001, Avg Grad: 0.018593551591038704\n",
            "Epoch: 7, Loss: 6.860024086563499e-07 Updates: 271/54001, Avg Grad: 0.02536115050315857\n",
            "Epoch 6 iteration 0 Loss: 4.015 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 400 Loss: 0.240 | Acc: 94.015% (377/401)\n",
            "Test accuracy: 0.9401496052742004\n",
            "Epoch: 7, Loss: 0.0007219061371870339 Updates: 276/55001, Avg Grad: 0.023045498877763748\n",
            "Epoch: 7, Loss: 6.88293730490841e-05 Updates: 281/56001, Avg Grad: 0.013422222808003426\n",
            "Epoch: 7, Loss: 0.001978885382413864 Updates: 286/57001, Avg Grad: 0.018910283222794533\n",
            "Epoch: 7, Loss: 0.00011700868344632909 Updates: 291/58001, Avg Grad: 0.017356451600790024\n",
            "Epoch: 7, Loss: 0.0036248869728296995 Updates: 296/59001, Avg Grad: 0.027748744934797287\n",
            "Epoch 7 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.383 | Acc: 88.030% (353/401)\n",
            "Test accuracy: 0.8802992701530457\n",
            "Epoch: 8, Loss: 1.8871531210606918e-05 Updates: 1/1, Avg Grad: 8.46335751703009e-06\n",
            "Epoch: 8, Loss: 6.450174987548962e-05 Updates: 6/1001, Avg Grad: 0.02027164027094841\n",
            "Epoch: 8, Loss: 0.0018262920202687383 Updates: 11/2001, Avg Grad: 0.015796726569533348\n",
            "Epoch: 8, Loss: 0.001473393407650292 Updates: 16/3001, Avg Grad: 0.026912584900856018\n",
            "Epoch: 8, Loss: 0.0008764770464040339 Updates: 21/4001, Avg Grad: 0.014503908343613148\n",
            "Epoch 7 iteration 0 Loss: 0.188 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.295 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 8, Loss: 6.822439172537997e-05 Updates: 26/5001, Avg Grad: 0.013329878449440002\n",
            "Epoch: 8, Loss: 0.00024432127247564495 Updates: 31/6001, Avg Grad: 0.027377327904105186\n",
            "Epoch: 8, Loss: 2.7143460101797245e-05 Updates: 36/7001, Avg Grad: 0.03621694818139076\n",
            "Epoch: 8, Loss: 0.0006884925533086061 Updates: 41/8001, Avg Grad: 0.01678917184472084\n",
            "Epoch: 8, Loss: 0.00017005200788844377 Updates: 46/9001, Avg Grad: 0.02244962938129902\n",
            "Epoch 7 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.289 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 8, Loss: 7.016606832621619e-05 Updates: 51/10001, Avg Grad: 0.021205488592386246\n",
            "Epoch: 8, Loss: 0.0006095386925153434 Updates: 56/11001, Avg Grad: 0.015796378254890442\n",
            "Epoch: 8, Loss: 0.00020744519133586437 Updates: 61/12001, Avg Grad: 0.011881886050105095\n",
            "Epoch: 8, Loss: 9.73748101387173e-05 Updates: 66/13001, Avg Grad: 0.01683412492275238\n",
            "Epoch: 8, Loss: 0.0001647293393034488 Updates: 71/14001, Avg Grad: 0.014280593022704124\n",
            "Epoch 7 iteration 0 Loss: 0.316 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.267 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 8, Loss: 0.0006662362720817327 Updates: 76/15001, Avg Grad: 0.01578661799430847\n",
            "Epoch: 8, Loss: 3.421435030759312e-05 Updates: 81/16001, Avg Grad: 0.014910520985722542\n",
            "Epoch: 8, Loss: 6.5307858676533215e-06 Updates: 86/17001, Avg Grad: 0.016950996592640877\n",
            "Epoch: 8, Loss: 8.793769666226581e-05 Updates: 91/18001, Avg Grad: 0.02463497593998909\n",
            "Epoch: 8, Loss: 0.001281557371839881 Updates: 96/19001, Avg Grad: 0.014509150758385658\n",
            "Epoch 7 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.316 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 8, Loss: 0.001055406522937119 Updates: 101/20001, Avg Grad: 0.01685674488544464\n",
            "Epoch: 8, Loss: 0.006283496040850878 Updates: 106/21001, Avg Grad: 0.025114774703979492\n",
            "Epoch: 8, Loss: 3.897516580764204e-05 Updates: 111/22001, Avg Grad: 0.014213085174560547\n",
            "Epoch: 8, Loss: 0.012286372482776642 Updates: 116/23001, Avg Grad: 0.016654079779982567\n",
            "Epoch: 8, Loss: 0.00065725261811167 Updates: 121/24001, Avg Grad: 0.014277881011366844\n",
            "Epoch 7 iteration 0 Loss: 0.059 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.298 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 8, Loss: 9.740112000145018e-05 Updates: 126/25001, Avg Grad: 0.021676763892173767\n",
            "Epoch: 8, Loss: 0.0010430729016661644 Updates: 131/26001, Avg Grad: 0.022757431492209435\n",
            "Epoch: 8, Loss: 0.00012753010378219187 Updates: 136/27001, Avg Grad: 0.02526196837425232\n",
            "Epoch: 8, Loss: 0.0004278341948520392 Updates: 141/28001, Avg Grad: 0.013779232278466225\n",
            "Epoch: 8, Loss: 0.0001711885561235249 Updates: 146/29001, Avg Grad: 0.016477270051836967\n",
            "Epoch 7 iteration 0 Loss: 0.025 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.250 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 8, Loss: 5.187154602026567e-05 Updates: 151/30001, Avg Grad: 0.02139975130558014\n",
            "Epoch: 8, Loss: 4.346438072388992e-05 Updates: 156/31001, Avg Grad: 0.024239590391516685\n",
            "Epoch: 8, Loss: 9.626414794183802e-06 Updates: 161/32001, Avg Grad: 0.014453606680035591\n",
            "Epoch: 8, Loss: 0.001390994293615222 Updates: 166/33001, Avg Grad: 0.01465236209332943\n",
            "Epoch: 8, Loss: 0.00029520189855247736 Updates: 171/34001, Avg Grad: 0.011394414119422436\n",
            "Epoch 7 iteration 0 Loss: 0.035 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.293 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 8, Loss: 4.858397460338892e-06 Updates: 176/35001, Avg Grad: 0.014548048377037048\n",
            "Epoch: 8, Loss: 8.118405094137415e-05 Updates: 181/36001, Avg Grad: 0.010430511087179184\n",
            "Epoch: 8, Loss: 2.245620999019593e-05 Updates: 186/37001, Avg Grad: 0.022959496825933456\n",
            "Epoch: 8, Loss: 7.581136287626578e-07 Updates: 191/38001, Avg Grad: 0.01936044916510582\n",
            "Epoch: 8, Loss: 0.0012278727954253554 Updates: 196/39001, Avg Grad: 0.024953262880444527\n",
            "Epoch 7 iteration 0 Loss: 0.034 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.268 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 8, Loss: 0.0007272223592735827 Updates: 201/40001, Avg Grad: 0.01697944663465023\n",
            "Epoch: 8, Loss: 0.00012955577403772622 Updates: 206/41001, Avg Grad: 0.01670760288834572\n",
            "Epoch: 8, Loss: 0.00011131676728837192 Updates: 211/42001, Avg Grad: 0.014594930224120617\n",
            "Epoch: 8, Loss: 0.003829462453722954 Updates: 216/43001, Avg Grad: 0.019441032782197\n",
            "Epoch: 8, Loss: 0.0006328249583020806 Updates: 221/44001, Avg Grad: 0.014301760122179985\n",
            "Epoch 7 iteration 0 Loss: 0.497 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.319 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 8, Loss: 3.772120180656202e-05 Updates: 226/45001, Avg Grad: 0.012401146814227104\n",
            "Epoch: 8, Loss: 1.4246057617128827e-05 Updates: 231/46001, Avg Grad: 0.01716979220509529\n",
            "Epoch: 8, Loss: 4.6898032451281324e-05 Updates: 236/47001, Avg Grad: 0.012579645030200481\n",
            "Epoch: 8, Loss: 0.005419767927378416 Updates: 241/48001, Avg Grad: 0.03292538598179817\n",
            "Epoch: 8, Loss: 2.950342832264141e-07 Updates: 246/49001, Avg Grad: 0.024832259863615036\n",
            "Epoch 7 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.244 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 8, Loss: 3.243808896513656e-05 Updates: 251/50001, Avg Grad: 0.023363709449768066\n",
            "Epoch: 8, Loss: 7.645270670764148e-05 Updates: 256/51001, Avg Grad: 0.014840686693787575\n",
            "Epoch: 8, Loss: 0.014473852701485157 Updates: 261/52001, Avg Grad: 0.023694274947047234\n",
            "Epoch: 8, Loss: 8.808790425973712e-07 Updates: 266/53001, Avg Grad: 0.01669973134994507\n",
            "Epoch: 8, Loss: 0.002468476304784417 Updates: 271/54001, Avg Grad: 0.018149763345718384\n",
            "Epoch 7 iteration 0 Loss: 0.070 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.284 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 8, Loss: 0.00044083272223360837 Updates: 276/55001, Avg Grad: 0.01723671145737171\n",
            "Epoch: 8, Loss: 0.0004461448988877237 Updates: 281/56001, Avg Grad: 0.02334756776690483\n",
            "Epoch: 8, Loss: 0.00011433015606598929 Updates: 286/57001, Avg Grad: 0.015886612236499786\n",
            "Epoch: 8, Loss: 9.88312967820093e-05 Updates: 291/58001, Avg Grad: 0.0204011257737875\n",
            "Epoch: 8, Loss: 1.4649604963778984e-05 Updates: 296/59001, Avg Grad: 0.01803646795451641\n",
            "Epoch 8 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.351 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 9, Loss: 1.7218115317518823e-05 Updates: 1/1, Avg Grad: 8.215021807700396e-06\n",
            "Epoch: 9, Loss: 2.0708414012915455e-05 Updates: 6/1001, Avg Grad: 0.024950645864009857\n",
            "Epoch: 9, Loss: 2.102122198266443e-05 Updates: 11/2001, Avg Grad: 0.022030573338270187\n",
            "Epoch: 9, Loss: 0.0011823000386357307 Updates: 16/3001, Avg Grad: 0.013566091656684875\n",
            "Epoch: 9, Loss: 1.976512066903524e-05 Updates: 21/4001, Avg Grad: 0.010573972947895527\n",
            "Epoch 8 iteration 0 Loss: 0.012 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.234 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 9, Loss: 6.490714440587908e-05 Updates: 26/5001, Avg Grad: 0.02277606911957264\n",
            "Epoch: 9, Loss: 0.00018127595831174403 Updates: 31/6001, Avg Grad: 0.012795524671673775\n",
            "Epoch: 9, Loss: 0.0002902690030168742 Updates: 36/7001, Avg Grad: 0.024381253868341446\n",
            "Epoch: 9, Loss: 0.00022861665638629347 Updates: 41/8001, Avg Grad: 0.016909342259168625\n",
            "Epoch: 9, Loss: 0.0009529359522275627 Updates: 46/9001, Avg Grad: 0.016279902309179306\n",
            "Epoch 8 iteration 0 Loss: 0.018 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.333 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 9, Loss: 5.921194315305911e-06 Updates: 51/10001, Avg Grad: 0.02026182785630226\n",
            "Epoch: 9, Loss: 3.8797148590674624e-05 Updates: 56/11001, Avg Grad: 0.01589580811560154\n",
            "Epoch: 9, Loss: 0.0005315116140991449 Updates: 61/12001, Avg Grad: 0.019104216247797012\n",
            "Epoch: 9, Loss: 0.0003632417065091431 Updates: 66/13001, Avg Grad: 0.016723288223147392\n",
            "Epoch: 9, Loss: 0.0003925650962628424 Updates: 71/14001, Avg Grad: 0.02730417624115944\n",
            "Epoch 8 iteration 0 Loss: 0.198 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.311 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 9, Loss: 9.500078022028902e-07 Updates: 76/15001, Avg Grad: 0.030014347285032272\n",
            "Epoch: 9, Loss: 2.7148503249918576e-06 Updates: 81/16001, Avg Grad: 0.01053431537002325\n",
            "Epoch: 9, Loss: 0.030906397849321365 Updates: 86/17001, Avg Grad: 0.018637537956237793\n",
            "Epoch: 9, Loss: 0.0001090405203285627 Updates: 91/18001, Avg Grad: 0.019853929057717323\n",
            "Epoch: 9, Loss: 0.0006044243345968425 Updates: 96/19001, Avg Grad: 0.023766234517097473\n",
            "Epoch 8 iteration 0 Loss: 0.149 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.313 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 9, Loss: 0.001957907108590007 Updates: 101/20001, Avg Grad: 0.011935370974242687\n",
            "Epoch: 9, Loss: 0.0047571416944265366 Updates: 106/21001, Avg Grad: 0.01824382320046425\n",
            "Epoch: 9, Loss: 1.3072077308606822e-05 Updates: 111/22001, Avg Grad: 0.01467141229659319\n",
            "Epoch: 9, Loss: 3.058003858313896e-05 Updates: 116/23001, Avg Grad: 0.024615317583084106\n",
            "Epoch: 9, Loss: 3.561478297342546e-05 Updates: 121/24001, Avg Grad: 0.016798248514533043\n",
            "Epoch 8 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.265 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 9, Loss: 0.0011256461730226874 Updates: 126/25001, Avg Grad: 0.015585176646709442\n",
            "Epoch: 9, Loss: 1.8545524653745815e-05 Updates: 131/26001, Avg Grad: 0.013417561538517475\n",
            "Epoch: 9, Loss: 8.938698010751978e-05 Updates: 136/27001, Avg Grad: 0.015456539578735828\n",
            "Epoch: 9, Loss: 1.5053867628012085e-06 Updates: 141/28001, Avg Grad: 0.023335862904787064\n",
            "Epoch: 9, Loss: 2.3381790015264414e-05 Updates: 146/29001, Avg Grad: 0.01958034187555313\n",
            "Epoch 8 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.290 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 9, Loss: 9.077291906578466e-06 Updates: 151/30001, Avg Grad: 0.010574011132121086\n",
            "Epoch: 9, Loss: 7.084428216330707e-05 Updates: 156/31001, Avg Grad: 0.019330088049173355\n",
            "Epoch: 9, Loss: 0.00029273692052811384 Updates: 161/32001, Avg Grad: 0.018025893718004227\n",
            "Epoch: 9, Loss: 3.285142156528309e-05 Updates: 166/33001, Avg Grad: 0.02220892533659935\n",
            "Epoch: 9, Loss: 6.789922917960212e-05 Updates: 171/34001, Avg Grad: 0.013172370381653309\n",
            "Epoch 8 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.368 | Acc: 90.524% (363/401)\n",
            "Test accuracy: 0.9052368998527527\n",
            "Epoch: 9, Loss: 0.00016699018306098878 Updates: 176/35001, Avg Grad: 0.019244639202952385\n",
            "Epoch: 9, Loss: 5.9058376791654155e-05 Updates: 181/36001, Avg Grad: 0.024099484086036682\n",
            "Epoch: 9, Loss: 2.304480403836351e-05 Updates: 186/37001, Avg Grad: 0.013672140426933765\n",
            "Epoch: 9, Loss: 2.2107293261797167e-05 Updates: 191/38001, Avg Grad: 0.035057276487350464\n",
            "Epoch: 9, Loss: 2.423008845653385e-05 Updates: 196/39001, Avg Grad: 0.01854313537478447\n",
            "Epoch 8 iteration 0 Loss: 0.219 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.324 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 9, Loss: 1.3480474990501534e-05 Updates: 201/40001, Avg Grad: 0.020123006775975227\n",
            "Epoch: 9, Loss: 0.00042507078615017235 Updates: 206/41001, Avg Grad: 0.027869824320077896\n",
            "Epoch: 9, Loss: 3.086438664468005e-05 Updates: 211/42001, Avg Grad: 0.015741659328341484\n",
            "Epoch: 9, Loss: 0.00020524261344689876 Updates: 216/43001, Avg Grad: 0.014772266149520874\n",
            "Epoch: 9, Loss: 0.00017092244524974376 Updates: 221/44001, Avg Grad: 0.017532074823975563\n",
            "Epoch 8 iteration 0 Loss: 0.014 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.185 | Acc: 93.516% (375/401)\n",
            "Test accuracy: 0.9351620674133301\n",
            "Epoch: 9, Loss: 4.614845238393173e-06 Updates: 226/45001, Avg Grad: 0.019167786464095116\n",
            "Epoch: 9, Loss: 0.0015533629339188337 Updates: 231/46001, Avg Grad: 0.01987018808722496\n",
            "Epoch: 9, Loss: 2.1187412130529992e-05 Updates: 236/47001, Avg Grad: 0.012968743219971657\n",
            "Epoch: 9, Loss: 0.0002580388681963086 Updates: 241/48001, Avg Grad: 0.021170932799577713\n",
            "Epoch: 9, Loss: 2.2178501239977777e-05 Updates: 246/49001, Avg Grad: 0.020780790597200394\n",
            "Epoch 8 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.333 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 9, Loss: 0.0035323407500982285 Updates: 251/50001, Avg Grad: 0.017889615148305893\n",
            "Epoch: 9, Loss: 0.0006112696137279272 Updates: 256/51001, Avg Grad: 0.010972707532346249\n",
            "Epoch: 9, Loss: 5.368687652662629e-06 Updates: 261/52001, Avg Grad: 0.015672408044338226\n",
            "Epoch: 9, Loss: 4.4159455683256965e-06 Updates: 266/53001, Avg Grad: 0.02278788760304451\n",
            "Epoch: 9, Loss: 4.2052553908433765e-05 Updates: 271/54001, Avg Grad: 0.011558497324585915\n",
            "Epoch 8 iteration 0 Loss: 0.429 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.294 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 9, Loss: 7.251823990372941e-05 Updates: 276/55001, Avg Grad: 0.01566343940794468\n",
            "Epoch: 9, Loss: 0.00032603053841739893 Updates: 281/56001, Avg Grad: 0.01754598692059517\n",
            "Epoch: 9, Loss: 5.974179202894447e-06 Updates: 286/57001, Avg Grad: 0.016434695571660995\n",
            "Epoch: 9, Loss: 4.424074722919613e-05 Updates: 291/58001, Avg Grad: 0.016130687668919563\n",
            "Epoch: 9, Loss: 2.6419575078762136e-05 Updates: 296/59001, Avg Grad: 0.018218718469142914\n",
            "Epoch 9 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.257 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 10, Loss: 8.573329978389665e-06 Updates: 1/1, Avg Grad: 3.445968104642816e-06\n",
            "Epoch: 10, Loss: 7.018605538178235e-05 Updates: 6/1001, Avg Grad: 0.02288188971579075\n",
            "Epoch: 10, Loss: 0.007993722334504128 Updates: 11/2001, Avg Grad: 0.017024388536810875\n",
            "Epoch: 10, Loss: 0.00015853141667321324 Updates: 16/3001, Avg Grad: 0.017896125093102455\n",
            "Epoch: 10, Loss: 0.00011765786621253937 Updates: 21/4001, Avg Grad: 0.015691356733441353\n",
            "Epoch 9 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.228 | Acc: 93.766% (376/401)\n",
            "Test accuracy: 0.9376558661460876\n",
            "Epoch: 10, Loss: 0.010131302289664745 Updates: 26/5001, Avg Grad: 0.02120169624686241\n",
            "Epoch: 10, Loss: 7.170063781813951e-06 Updates: 31/6001, Avg Grad: 0.01968318596482277\n",
            "Epoch: 10, Loss: 7.613747584400699e-05 Updates: 36/7001, Avg Grad: 0.019580073654651642\n",
            "Epoch: 10, Loss: 0.0001359132438665256 Updates: 41/8001, Avg Grad: 0.016491184011101723\n",
            "Epoch: 10, Loss: 0.00023471868189517409 Updates: 46/9001, Avg Grad: 0.02534053847193718\n",
            "Epoch 9 iteration 0 Loss: 0.132 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.257 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 10, Loss: 0.00016039449837990105 Updates: 51/10001, Avg Grad: 0.02118893899023533\n",
            "Epoch: 10, Loss: 0.003936964552849531 Updates: 56/11001, Avg Grad: 0.013721306808292866\n",
            "Epoch: 10, Loss: 4.8244150093523785e-05 Updates: 61/12001, Avg Grad: 0.01726212538778782\n",
            "Epoch: 10, Loss: 1.4904554518579971e-05 Updates: 66/13001, Avg Grad: 0.01848810538649559\n",
            "Epoch: 10, Loss: 0.0003771204501390457 Updates: 71/14001, Avg Grad: 0.016325339674949646\n",
            "Epoch 9 iteration 0 Loss: 2.243 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 400 Loss: 0.257 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 10, Loss: 8.941042324295267e-06 Updates: 76/15001, Avg Grad: 0.0162348635494709\n",
            "Epoch: 10, Loss: 7.555138381576398e-06 Updates: 81/16001, Avg Grad: 0.018493251875042915\n",
            "Epoch: 10, Loss: 0.004763880744576454 Updates: 86/17001, Avg Grad: 0.017664611339569092\n",
            "Epoch: 10, Loss: 2.643973311933223e-05 Updates: 91/18001, Avg Grad: 0.016598161309957504\n",
            "Epoch: 10, Loss: 3.815657692030072e-05 Updates: 96/19001, Avg Grad: 0.020820092409849167\n",
            "Epoch 9 iteration 0 Loss: 2.084 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 400 Loss: 0.250 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 10, Loss: 0.0002381627564318478 Updates: 101/20001, Avg Grad: 0.019929392263293266\n",
            "Epoch: 10, Loss: 0.0001696804101811722 Updates: 106/21001, Avg Grad: 0.018088916316628456\n",
            "Epoch: 10, Loss: 9.741340181790292e-05 Updates: 111/22001, Avg Grad: 0.01911998726427555\n",
            "Epoch: 10, Loss: 8.564315794501454e-05 Updates: 116/23001, Avg Grad: 0.021926399320364\n",
            "Epoch: 10, Loss: 1.8676607851375593e-06 Updates: 121/24001, Avg Grad: 0.018805325031280518\n",
            "Epoch 9 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.221 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 10, Loss: 0.000850008218549192 Updates: 126/25001, Avg Grad: 0.013300260528922081\n",
            "Epoch: 10, Loss: 0.00036539905704557896 Updates: 131/26001, Avg Grad: 0.019565844908356667\n",
            "Epoch: 10, Loss: 0.003503351239487529 Updates: 136/27001, Avg Grad: 0.01643316261470318\n",
            "Epoch: 10, Loss: 4.136156348977238e-05 Updates: 141/28001, Avg Grad: 0.016839418560266495\n",
            "Epoch: 10, Loss: 0.00012073318794136867 Updates: 146/29001, Avg Grad: 0.031618572771549225\n",
            "Epoch 9 iteration 0 Loss: 1.901 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 400 Loss: 0.311 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 10, Loss: 0.00014231359818950295 Updates: 151/30001, Avg Grad: 0.023493126034736633\n",
            "Epoch: 10, Loss: 0.0005856186035089195 Updates: 156/31001, Avg Grad: 0.020784661173820496\n",
            "Epoch: 10, Loss: 0.0005696727894246578 Updates: 161/32001, Avg Grad: 0.02052752487361431\n",
            "Epoch: 10, Loss: 8.907548908609897e-05 Updates: 166/33001, Avg Grad: 0.015301043167710304\n",
            "Epoch: 10, Loss: 0.0028191632591187954 Updates: 171/34001, Avg Grad: 0.012193243950605392\n",
            "Epoch 9 iteration 0 Loss: 0.019 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.326 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 10, Loss: 0.00293955416418612 Updates: 176/35001, Avg Grad: 0.020583054050803185\n",
            "Epoch: 10, Loss: 0.0011345887323841453 Updates: 181/36001, Avg Grad: 0.013104814104735851\n",
            "Epoch: 10, Loss: 9.997022971219849e-06 Updates: 186/37001, Avg Grad: 0.0161175187677145\n",
            "Epoch: 10, Loss: 0.008166754618287086 Updates: 191/38001, Avg Grad: 0.0168580524623394\n",
            "Epoch: 10, Loss: 0.00016103748930618167 Updates: 196/39001, Avg Grad: 0.017640113830566406\n",
            "Epoch 9 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.277 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 10, Loss: 0.0016398063162341714 Updates: 201/40001, Avg Grad: 0.01996013894677162\n",
            "Epoch: 10, Loss: 0.011465701274573803 Updates: 206/41001, Avg Grad: 0.026700058951973915\n",
            "Epoch: 10, Loss: 0.004951335955411196 Updates: 211/42001, Avg Grad: 0.017011256888508797\n",
            "Epoch: 10, Loss: 0.0002014002384385094 Updates: 216/43001, Avg Grad: 0.013405060395598412\n",
            "Epoch: 10, Loss: 4.408203949424205e-06 Updates: 221/44001, Avg Grad: 0.013155805878341198\n",
            "Epoch 9 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.246 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 10, Loss: 9.420145215699449e-05 Updates: 226/45001, Avg Grad: 0.01759224757552147\n",
            "Epoch: 10, Loss: 0.00016103227972052991 Updates: 231/46001, Avg Grad: 0.014568920247256756\n",
            "Epoch: 10, Loss: 0.00015568960225209594 Updates: 236/47001, Avg Grad: 0.01618753932416439\n",
            "Epoch: 10, Loss: 8.683408850629348e-06 Updates: 241/48001, Avg Grad: 0.01984000951051712\n",
            "Epoch: 10, Loss: 5.183512257644907e-06 Updates: 246/49001, Avg Grad: 0.024517597630620003\n",
            "Epoch 9 iteration 0 Loss: 0.040 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.264 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 10, Loss: 0.00016124930698424578 Updates: 251/50001, Avg Grad: 0.01374698244035244\n",
            "Epoch: 10, Loss: 0.003368628676980734 Updates: 256/51001, Avg Grad: 0.018394630402326584\n",
            "Epoch: 10, Loss: 8.215707566705532e-06 Updates: 261/52001, Avg Grad: 0.018056228756904602\n",
            "Epoch: 10, Loss: 0.0001727072085486725 Updates: 266/53001, Avg Grad: 0.014369741082191467\n",
            "Epoch: 10, Loss: 9.380890446664125e-07 Updates: 271/54001, Avg Grad: 0.01705719158053398\n",
            "Epoch 9 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.201 | Acc: 93.516% (375/401)\n",
            "Test accuracy: 0.9351620674133301\n",
            "Epoch: 10, Loss: 6.14444206803455e-06 Updates: 276/55001, Avg Grad: 0.019558068364858627\n",
            "Epoch: 10, Loss: 0.00010769841901492327 Updates: 281/56001, Avg Grad: 0.02018149010837078\n",
            "Epoch: 10, Loss: 0.0005557985859923065 Updates: 286/57001, Avg Grad: 0.014806780032813549\n",
            "Epoch: 10, Loss: 0.0003153650905005634 Updates: 291/58001, Avg Grad: 0.016667457297444344\n",
            "Epoch: 10, Loss: 1.2174315997981466e-05 Updates: 296/59001, Avg Grad: 0.014033010229468346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "net = Net()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 10,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.0008, #0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 2,\n",
        "    \"test_interval\": 5000,\n",
        "    \"dampening\": 0.1\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "#optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss = net_trainer_manh(net, train_loader, test_loader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx5X2BycNucW",
        "outputId": "c9c6b6b3-bc12-49d0-b562-b05bcbe60d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 2.717 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 2.980 | Acc: 9.975% (40/401)\n",
            "Test accuracy: 0.09975062310695648\n",
            "Epoch: 1, Loss: 0.010539717972278595 Updates: 1/1, Avg Grad: 0.00018276847549714148\n",
            "Epoch: 1, Loss: 0.015597233548760414 Updates: 6/1001, Avg Grad: 0.013224968686699867\n",
            "Epoch: 1, Loss: 0.005111351143568754 Updates: 11/2001, Avg Grad: 0.011806713417172432\n",
            "Epoch: 1, Loss: 0.019504191353917122 Updates: 16/3001, Avg Grad: 0.015482621267437935\n",
            "Epoch: 1, Loss: 0.009258979931473732 Updates: 21/4001, Avg Grad: 0.011901721358299255\n",
            "Epoch 0 iteration 0 Loss: 1.505 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 1.721 | Acc: 51.870% (208/401)\n",
            "Test accuracy: 0.5187032222747803\n",
            "Epoch: 1, Loss: 0.012585039250552654 Updates: 26/5001, Avg Grad: 0.010591374710202217\n",
            "Epoch: 1, Loss: 0.01211595069617033 Updates: 31/6001, Avg Grad: 0.01125903520733118\n",
            "Epoch: 1, Loss: 0.0016215718351304531 Updates: 36/7001, Avg Grad: 0.010116511024534702\n",
            "Epoch: 1, Loss: 0.009288745932281017 Updates: 41/8001, Avg Grad: 0.009752977639436722\n",
            "Epoch: 1, Loss: 0.0074162608943879604 Updates: 46/9001, Avg Grad: 0.010853635147213936\n",
            "Epoch 0 iteration 0 Loss: 0.118 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 1.177 | Acc: 68.329% (274/401)\n",
            "Test accuracy: 0.6832917928695679\n",
            "Epoch: 1, Loss: 0.008616061881184578 Updates: 51/10001, Avg Grad: 0.01171886920928955\n",
            "Epoch: 1, Loss: 0.0012316121719777584 Updates: 56/11001, Avg Grad: 0.016060085967183113\n",
            "Epoch: 1, Loss: 0.0009828221518546343 Updates: 61/12001, Avg Grad: 0.0109796691685915\n",
            "Epoch: 1, Loss: 0.0004586158029269427 Updates: 66/13001, Avg Grad: 0.011842390522360802\n",
            "Epoch: 1, Loss: 0.00034024391788989305 Updates: 71/14001, Avg Grad: 0.012216063216328621\n",
            "Epoch 0 iteration 0 Loss: 0.451 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.916 | Acc: 71.571% (287/401)\n",
            "Test accuracy: 0.7157106995582581\n",
            "Epoch: 1, Loss: 0.0018553697736933827 Updates: 76/15001, Avg Grad: 0.013620448298752308\n",
            "Epoch: 1, Loss: 0.00022314564557746053 Updates: 81/16001, Avg Grad: 0.014853510074317455\n",
            "Epoch: 1, Loss: 0.0014402036322280765 Updates: 86/17001, Avg Grad: 0.013148872181773186\n",
            "Epoch: 1, Loss: 0.007430153898894787 Updates: 91/18001, Avg Grad: 0.010531647130846977\n",
            "Epoch: 1, Loss: 0.0009934301488101482 Updates: 96/19001, Avg Grad: 0.013003307394683361\n",
            "Epoch 0 iteration 0 Loss: 2.426 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 400 Loss: 0.875 | Acc: 71.322% (286/401)\n",
            "Test accuracy: 0.7132169604301453\n",
            "Epoch: 1, Loss: 1.9952132788603194e-05 Updates: 101/20001, Avg Grad: 0.010502210818231106\n",
            "Epoch: 1, Loss: 0.012094277888536453 Updates: 106/21001, Avg Grad: 0.016101250424981117\n",
            "Epoch: 1, Loss: 0.012694972567260265 Updates: 111/22001, Avg Grad: 0.016100075095891953\n",
            "Epoch: 1, Loss: 0.0010785679332911968 Updates: 116/23001, Avg Grad: 0.009629497304558754\n",
            "Epoch: 1, Loss: 0.011983557604253292 Updates: 121/24001, Avg Grad: 0.008661692962050438\n",
            "Epoch 0 iteration 0 Loss: 0.034 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.605 | Acc: 82.045% (329/401)\n",
            "Test accuracy: 0.8204488754272461\n",
            "Epoch: 1, Loss: 8.598708518547937e-05 Updates: 126/25001, Avg Grad: 0.012896682135760784\n",
            "Epoch: 1, Loss: 0.0024106926284730434 Updates: 131/26001, Avg Grad: 0.010229450650513172\n",
            "Epoch: 1, Loss: 0.0009992464911192656 Updates: 136/27001, Avg Grad: 0.015103129670023918\n",
            "Epoch: 1, Loss: 0.0010762395104393363 Updates: 141/28001, Avg Grad: 0.015535754151642323\n",
            "Epoch: 1, Loss: 8.597829582868144e-05 Updates: 146/29001, Avg Grad: 0.021362561732530594\n",
            "Epoch 0 iteration 0 Loss: 0.363 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.669 | Acc: 80.050% (321/401)\n",
            "Test accuracy: 0.8004987239837646\n",
            "Epoch: 1, Loss: 0.013388042338192463 Updates: 151/30001, Avg Grad: 0.009406937286257744\n",
            "Epoch: 1, Loss: 0.001719548599794507 Updates: 156/31001, Avg Grad: 0.011368471197783947\n",
            "Epoch: 1, Loss: 0.0013938361080363393 Updates: 161/32001, Avg Grad: 0.01214144192636013\n",
            "Epoch: 1, Loss: 0.0015722003299742937 Updates: 166/33001, Avg Grad: 0.01495671458542347\n",
            "Epoch: 1, Loss: 0.0008406481938436627 Updates: 171/34001, Avg Grad: 0.015792492777109146\n",
            "Epoch 0 iteration 0 Loss: 0.417 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.593 | Acc: 80.798% (324/401)\n",
            "Test accuracy: 0.8079800605773926\n",
            "Epoch: 1, Loss: 0.0028140759095549583 Updates: 176/35001, Avg Grad: 0.012738652527332306\n",
            "Epoch: 1, Loss: 9.876293188426644e-05 Updates: 181/36001, Avg Grad: 0.009867793880403042\n",
            "Epoch: 1, Loss: 0.0011549119371920824 Updates: 186/37001, Avg Grad: 0.02044517919421196\n",
            "Epoch: 1, Loss: 0.00013469437544699758 Updates: 191/38001, Avg Grad: 0.021713126450777054\n",
            "Epoch: 1, Loss: 0.00023024034453555942 Updates: 196/39001, Avg Grad: 0.012565690092742443\n",
            "Epoch 0 iteration 0 Loss: 0.227 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.558 | Acc: 82.544% (331/401)\n",
            "Test accuracy: 0.8254364132881165\n",
            "Epoch: 1, Loss: 0.004153015092015266 Updates: 201/40001, Avg Grad: 0.01432616077363491\n",
            "Epoch: 1, Loss: 7.020662451395765e-05 Updates: 206/41001, Avg Grad: 0.010728693567216396\n",
            "Epoch: 1, Loss: 0.0005578024429269135 Updates: 211/42001, Avg Grad: 0.01226248312741518\n",
            "Epoch: 1, Loss: 0.0011421788949519396 Updates: 216/43001, Avg Grad: 0.016312647610902786\n",
            "Epoch: 1, Loss: 0.002217147732153535 Updates: 221/44001, Avg Grad: 0.008942650631070137\n",
            "Epoch 0 iteration 0 Loss: 0.265 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.553 | Acc: 84.539% (339/401)\n",
            "Test accuracy: 0.8453865051269531\n",
            "Epoch: 1, Loss: 7.547231507487595e-05 Updates: 226/45001, Avg Grad: 0.01517488993704319\n",
            "Epoch: 1, Loss: 0.009313191287219524 Updates: 231/46001, Avg Grad: 0.013754348270595074\n",
            "Epoch: 1, Loss: 3.028322862519417e-05 Updates: 236/47001, Avg Grad: 0.021025260910391808\n",
            "Epoch: 1, Loss: 0.0007844346691854298 Updates: 241/48001, Avg Grad: 0.015906918793916702\n",
            "Epoch: 1, Loss: 0.0008679552120156586 Updates: 246/49001, Avg Grad: 0.020641552284359932\n",
            "Epoch 0 iteration 0 Loss: 0.043 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.521 | Acc: 83.541% (335/401)\n",
            "Test accuracy: 0.8354114890098572\n",
            "Epoch: 1, Loss: 0.0004163913836237043 Updates: 251/50001, Avg Grad: 0.010890569537878036\n",
            "Epoch: 1, Loss: 0.0004061688086949289 Updates: 256/51001, Avg Grad: 0.015205937437713146\n",
            "Epoch: 1, Loss: 0.0008345660753548145 Updates: 261/52001, Avg Grad: 0.01717240735888481\n",
            "Epoch: 1, Loss: 0.0003736142534762621 Updates: 266/53001, Avg Grad: 0.011727211065590382\n",
            "Epoch: 1, Loss: 0.0008644370827823877 Updates: 271/54001, Avg Grad: 0.023699836805462837\n",
            "Epoch 0 iteration 0 Loss: 0.674 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 400 Loss: 0.504 | Acc: 86.534% (347/401)\n",
            "Test accuracy: 0.8653366565704346\n",
            "Epoch: 1, Loss: 0.00043560448102653027 Updates: 276/55001, Avg Grad: 0.016163799911737442\n",
            "Epoch: 1, Loss: 0.0004463962104637176 Updates: 281/56001, Avg Grad: 0.013486268930137157\n",
            "Epoch: 1, Loss: 0.0004855800070799887 Updates: 286/57001, Avg Grad: 0.016455311328172684\n",
            "Epoch: 1, Loss: 0.000999441253952682 Updates: 291/58001, Avg Grad: 0.01764758676290512\n",
            "Epoch: 1, Loss: 0.0006528867525048554 Updates: 296/59001, Avg Grad: 0.015613461844623089\n",
            "Epoch 1 iteration 0 Loss: 0.062 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.460 | Acc: 85.536% (343/401)\n",
            "Test accuracy: 0.8553615808486938\n",
            "Epoch: 2, Loss: 0.0011167380725964904 Updates: 1/1, Avg Grad: 0.0002589785144664347\n",
            "Epoch: 2, Loss: 3.364542135386728e-05 Updates: 6/1001, Avg Grad: 0.013057385571300983\n",
            "Epoch: 2, Loss: 4.5838005462428555e-05 Updates: 11/2001, Avg Grad: 0.017397824674844742\n",
            "Epoch: 2, Loss: 5.80032829020638e-05 Updates: 16/3001, Avg Grad: 0.013462340459227562\n",
            "Epoch: 2, Loss: 0.00017137978284154087 Updates: 21/4001, Avg Grad: 0.01871916651725769\n",
            "Epoch 1 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.587 | Acc: 80.798% (324/401)\n",
            "Test accuracy: 0.8079800605773926\n",
            "Epoch: 2, Loss: 0.0002693479473236948 Updates: 26/5001, Avg Grad: 0.01482400856912136\n",
            "Epoch: 2, Loss: 0.00029950568568892777 Updates: 31/6001, Avg Grad: 0.015023067593574524\n",
            "Epoch: 2, Loss: 5.445992428576574e-05 Updates: 36/7001, Avg Grad: 0.019556116312742233\n",
            "Epoch: 2, Loss: 4.8401765525341034e-05 Updates: 41/8001, Avg Grad: 0.012355776503682137\n",
            "Epoch: 2, Loss: 0.006735100876539946 Updates: 46/9001, Avg Grad: 0.015420600771903992\n",
            "Epoch 1 iteration 0 Loss: 0.221 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.436 | Acc: 87.781% (352/401)\n",
            "Test accuracy: 0.8778054714202881\n",
            "Epoch: 2, Loss: 0.00016141723608598113 Updates: 51/10001, Avg Grad: 0.021492820233106613\n",
            "Epoch: 2, Loss: 3.632905281847343e-05 Updates: 56/11001, Avg Grad: 0.02054629847407341\n",
            "Epoch: 2, Loss: 0.03699519485235214 Updates: 61/12001, Avg Grad: 0.024111416190862656\n",
            "Epoch: 2, Loss: 0.010254871100187302 Updates: 66/13001, Avg Grad: 0.021672453731298447\n",
            "Epoch: 2, Loss: 2.6867195629165508e-05 Updates: 71/14001, Avg Grad: 0.014181491918861866\n",
            "Epoch 1 iteration 0 Loss: 0.032 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.412 | Acc: 88.030% (353/401)\n",
            "Test accuracy: 0.8802992701530457\n",
            "Epoch: 2, Loss: 0.001098114880733192 Updates: 76/15001, Avg Grad: 0.020108558237552643\n",
            "Epoch: 2, Loss: 0.019983267411589622 Updates: 81/16001, Avg Grad: 0.01938822865486145\n",
            "Epoch: 2, Loss: 0.00032890503644011915 Updates: 86/17001, Avg Grad: 0.016485851258039474\n",
            "Epoch: 2, Loss: 0.0014140105340629816 Updates: 91/18001, Avg Grad: 0.020747318863868713\n",
            "Epoch: 2, Loss: 8.532206265954301e-05 Updates: 96/19001, Avg Grad: 0.016617070883512497\n",
            "Epoch 1 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.412 | Acc: 88.279% (354/401)\n",
            "Test accuracy: 0.8827930092811584\n",
            "Epoch: 2, Loss: 0.0002403315738774836 Updates: 101/20001, Avg Grad: 0.011520272120833397\n",
            "Epoch: 2, Loss: 0.00019216505461372435 Updates: 106/21001, Avg Grad: 0.01654656231403351\n",
            "Epoch: 2, Loss: 0.0011009458685293794 Updates: 111/22001, Avg Grad: 0.01625550165772438\n",
            "Epoch: 2, Loss: 0.00037217867793515325 Updates: 116/23001, Avg Grad: 0.019647978246212006\n",
            "Epoch: 2, Loss: 6.14752498222515e-05 Updates: 121/24001, Avg Grad: 0.007729268167167902\n",
            "Epoch 1 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.390 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 2, Loss: 0.002373279072344303 Updates: 126/25001, Avg Grad: 0.013390900567173958\n",
            "Epoch: 2, Loss: 0.0005597959388978779 Updates: 131/26001, Avg Grad: 0.020252350717782974\n",
            "Epoch: 2, Loss: 0.0010336465202271938 Updates: 136/27001, Avg Grad: 0.011912229470908642\n",
            "Epoch: 2, Loss: 0.03593054413795471 Updates: 141/28001, Avg Grad: 0.016478026285767555\n",
            "Epoch: 2, Loss: 0.000607251946348697 Updates: 146/29001, Avg Grad: 0.016896778717637062\n",
            "Epoch 1 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.368 | Acc: 89.277% (358/401)\n",
            "Test accuracy: 0.8927680850028992\n",
            "Epoch: 2, Loss: 1.0960093277390115e-06 Updates: 151/30001, Avg Grad: 0.013436156325042248\n",
            "Epoch: 2, Loss: 0.0018281510565429926 Updates: 156/31001, Avg Grad: 0.01875908486545086\n",
            "Epoch: 2, Loss: 5.116716420161538e-05 Updates: 161/32001, Avg Grad: 0.016948096454143524\n",
            "Epoch: 2, Loss: 2.1560725144809112e-05 Updates: 166/33001, Avg Grad: 0.019962523132562637\n",
            "Epoch: 2, Loss: 0.005644149146974087 Updates: 171/34001, Avg Grad: 0.01779249869287014\n",
            "Epoch 1 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.294 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 2, Loss: 0.015724139288067818 Updates: 176/35001, Avg Grad: 0.01624341681599617\n",
            "Epoch: 2, Loss: 9.107054938795045e-05 Updates: 181/36001, Avg Grad: 0.01742522418498993\n",
            "Epoch: 2, Loss: 0.00015095769776962698 Updates: 186/37001, Avg Grad: 0.01676596887409687\n",
            "Epoch: 2, Loss: 0.0015577285084873438 Updates: 191/38001, Avg Grad: 0.014661276713013649\n",
            "Epoch: 2, Loss: 0.00013846138608641922 Updates: 196/39001, Avg Grad: 0.013888532295823097\n",
            "Epoch 1 iteration 0 Loss: 0.077 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.405 | Acc: 88.279% (354/401)\n",
            "Test accuracy: 0.8827930092811584\n",
            "Epoch: 2, Loss: 0.0001102399910450913 Updates: 201/40001, Avg Grad: 0.026097770780324936\n",
            "Epoch: 2, Loss: 0.0005666195065714419 Updates: 206/41001, Avg Grad: 0.011307531967759132\n",
            "Epoch: 2, Loss: 2.4922264856286347e-05 Updates: 211/42001, Avg Grad: 0.016407180577516556\n",
            "Epoch: 2, Loss: 1.539658842375502e-05 Updates: 216/43001, Avg Grad: 0.01831675134599209\n",
            "Epoch: 2, Loss: 0.00022545430692844093 Updates: 221/44001, Avg Grad: 0.01522847730666399\n",
            "Epoch 1 iteration 0 Loss: 1.190 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 400 Loss: 0.334 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 2, Loss: 0.0011197049170732498 Updates: 226/45001, Avg Grad: 0.010911756195127964\n",
            "Epoch: 2, Loss: 4.105711923330091e-05 Updates: 231/46001, Avg Grad: 0.014980104751884937\n",
            "Epoch: 2, Loss: 0.003574497066438198 Updates: 236/47001, Avg Grad: 0.016676919534802437\n",
            "Epoch: 2, Loss: 1.5215943676594179e-05 Updates: 241/48001, Avg Grad: 0.01488819532096386\n",
            "Epoch: 2, Loss: 0.002330193994566798 Updates: 246/49001, Avg Grad: 0.013317090459167957\n",
            "Epoch 1 iteration 0 Loss: 0.408 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.323 | Acc: 88.778% (356/401)\n",
            "Test accuracy: 0.8877805471420288\n",
            "Epoch: 2, Loss: 0.0001045683748088777 Updates: 251/50001, Avg Grad: 0.019644279032945633\n",
            "Epoch: 2, Loss: 0.003042993601411581 Updates: 256/51001, Avg Grad: 0.012062719091773033\n",
            "Epoch: 2, Loss: 0.00597894424572587 Updates: 261/52001, Avg Grad: 0.023600585758686066\n",
            "Epoch: 2, Loss: 0.0016163937980309129 Updates: 266/53001, Avg Grad: 0.012820333242416382\n",
            "Epoch: 2, Loss: 0.0011387820122763515 Updates: 271/54001, Avg Grad: 0.016362519934773445\n",
            "Epoch 1 iteration 0 Loss: 0.026 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 400 Loss: 0.377 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 2, Loss: 0.001558863208629191 Updates: 276/55001, Avg Grad: 0.021993951871991158\n",
            "Epoch: 2, Loss: 0.0006264483090490103 Updates: 281/56001, Avg Grad: 0.02306228317320347\n",
            "Epoch: 2, Loss: 0.0010468849213793874 Updates: 286/57001, Avg Grad: 0.01329694502055645\n",
            "Epoch: 2, Loss: 0.00020964909344911575 Updates: 291/58001, Avg Grad: 0.021794281899929047\n",
            "Epoch: 2, Loss: 0.004303452093154192 Updates: 296/59001, Avg Grad: 0.01594780944287777\n",
            "Epoch 2 iteration 0 Loss: 0.214 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.389 | Acc: 87.781% (352/401)\n",
            "Test accuracy: 0.8778054714202881\n",
            "Epoch: 3, Loss: 4.829727913602255e-05 Updates: 1/1, Avg Grad: 1.358098779746797e-05\n",
            "Epoch: 3, Loss: 0.005697460845112801 Updates: 6/1001, Avg Grad: 0.020275751128792763\n",
            "Epoch: 3, Loss: 9.271611634176224e-05 Updates: 11/2001, Avg Grad: 0.010628396645188332\n",
            "Epoch: 3, Loss: 9.73349506239174e-06 Updates: 16/3001, Avg Grad: 0.015285457484424114\n",
            "Epoch: 3, Loss: 0.00033656798768788576 Updates: 21/4001, Avg Grad: 0.02022795006632805\n",
            "Epoch 2 iteration 0 Loss: 0.892 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 400 Loss: 0.364 | Acc: 89.277% (358/401)\n",
            "Test accuracy: 0.8927680850028992\n",
            "Epoch: 3, Loss: 0.006435272749513388 Updates: 26/5001, Avg Grad: 0.020077986642718315\n",
            "Epoch: 3, Loss: 0.0044476427137851715 Updates: 31/6001, Avg Grad: 0.02000558003783226\n",
            "Epoch: 3, Loss: 0.00031466048676520586 Updates: 36/7001, Avg Grad: 0.0222232174128294\n",
            "Epoch: 3, Loss: 0.0024285807739943266 Updates: 41/8001, Avg Grad: 0.018834134563803673\n",
            "Epoch: 3, Loss: 0.013147001154720783 Updates: 46/9001, Avg Grad: 0.026028161868453026\n",
            "Epoch 2 iteration 0 Loss: 2.205 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 400 Loss: 0.365 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 3, Loss: 4.248164259479381e-05 Updates: 51/10001, Avg Grad: 0.017444007098674774\n",
            "Epoch: 3, Loss: 0.003408353077247739 Updates: 56/11001, Avg Grad: 0.01578417979180813\n",
            "Epoch: 3, Loss: 0.00693640997633338 Updates: 61/12001, Avg Grad: 0.02422369457781315\n",
            "Epoch: 3, Loss: 0.0003018557035829872 Updates: 66/13001, Avg Grad: 0.021432198584079742\n",
            "Epoch: 3, Loss: 1.930439429997932e-05 Updates: 71/14001, Avg Grad: 0.016134921461343765\n",
            "Epoch 2 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.426 | Acc: 87.282% (350/401)\n",
            "Test accuracy: 0.8728179335594177\n",
            "Epoch: 3, Loss: 0.0007153864135034382 Updates: 76/15001, Avg Grad: 0.014119182713329792\n",
            "Epoch: 3, Loss: 9.43425766308792e-06 Updates: 81/16001, Avg Grad: 0.01326665561646223\n",
            "Epoch: 3, Loss: 2.1147647203179076e-05 Updates: 86/17001, Avg Grad: 0.01539582945406437\n",
            "Epoch: 3, Loss: 4.575542334350757e-06 Updates: 91/18001, Avg Grad: 0.017625803127884865\n",
            "Epoch: 3, Loss: 0.003970143850892782 Updates: 96/19001, Avg Grad: 0.02389889396727085\n",
            "Epoch 2 iteration 0 Loss: 0.057 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.378 | Acc: 87.282% (350/401)\n",
            "Test accuracy: 0.8728179335594177\n",
            "Epoch: 3, Loss: 0.01849120482802391 Updates: 101/20001, Avg Grad: 0.019442936405539513\n",
            "Epoch: 3, Loss: 2.7374878754926613e-06 Updates: 106/21001, Avg Grad: 0.019175870344042778\n",
            "Epoch: 3, Loss: 0.0023337930906563997 Updates: 111/22001, Avg Grad: 0.015929516404867172\n",
            "Epoch: 3, Loss: 0.0012155418517068028 Updates: 116/23001, Avg Grad: 0.01772318035364151\n",
            "Epoch: 3, Loss: 0.001039187773130834 Updates: 121/24001, Avg Grad: 0.023961033672094345\n",
            "Epoch 2 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.365 | Acc: 88.529% (355/401)\n",
            "Test accuracy: 0.885286808013916\n",
            "Epoch: 3, Loss: 0.0003705196431837976 Updates: 126/25001, Avg Grad: 0.015376189723610878\n",
            "Epoch: 3, Loss: 0.004349704831838608 Updates: 131/26001, Avg Grad: 0.014810753986239433\n",
            "Epoch: 3, Loss: 0.003729606978595257 Updates: 136/27001, Avg Grad: 0.02966720424592495\n",
            "Epoch: 3, Loss: 0.0016390406526625156 Updates: 141/28001, Avg Grad: 0.02156335487961769\n",
            "Epoch: 3, Loss: 0.001416470273397863 Updates: 146/29001, Avg Grad: 0.01860911026597023\n",
            "Epoch 2 iteration 0 Loss: 0.051 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.291 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 3, Loss: 0.004412975162267685 Updates: 151/30001, Avg Grad: 0.021232936531305313\n",
            "Epoch: 3, Loss: 6.072408723412082e-06 Updates: 156/31001, Avg Grad: 0.01837902143597603\n",
            "Epoch: 3, Loss: 0.005077681038528681 Updates: 161/32001, Avg Grad: 0.011709288693964481\n",
            "Epoch: 3, Loss: 4.6475226554321125e-05 Updates: 166/33001, Avg Grad: 0.021962834522128105\n",
            "Epoch: 3, Loss: 0.00024203096108976752 Updates: 171/34001, Avg Grad: 0.02500534988939762\n",
            "Epoch 2 iteration 0 Loss: 0.829 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.251 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 3, Loss: 0.0034824758768081665 Updates: 176/35001, Avg Grad: 0.02599877491593361\n",
            "Epoch: 3, Loss: 6.641899381065741e-05 Updates: 181/36001, Avg Grad: 0.020379099994897842\n",
            "Epoch: 3, Loss: 3.160184496664442e-05 Updates: 186/37001, Avg Grad: 0.012423541396856308\n",
            "Epoch: 3, Loss: 0.0013798773288726807 Updates: 191/38001, Avg Grad: 0.015994843095541\n",
            "Epoch: 3, Loss: 0.00028586664120666683 Updates: 196/39001, Avg Grad: 0.016788052394986153\n",
            "Epoch 2 iteration 0 Loss: 0.024 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.351 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 3, Loss: 0.024325082078576088 Updates: 201/40001, Avg Grad: 0.014920240268111229\n",
            "Epoch: 3, Loss: 0.0014058531960472465 Updates: 206/41001, Avg Grad: 0.017176859080791473\n",
            "Epoch: 3, Loss: 3.4594486351124942e-06 Updates: 211/42001, Avg Grad: 0.02183135412633419\n",
            "Epoch: 3, Loss: 0.013090742751955986 Updates: 216/43001, Avg Grad: 0.021590230986475945\n",
            "Epoch: 3, Loss: 4.6305744035635144e-05 Updates: 221/44001, Avg Grad: 0.01668829284608364\n",
            "Epoch 2 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.329 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 3, Loss: 0.00028896008734591305 Updates: 226/45001, Avg Grad: 0.017654433846473694\n",
            "Epoch: 3, Loss: 1.8734361219685525e-05 Updates: 231/46001, Avg Grad: 0.01734079234302044\n",
            "Epoch: 3, Loss: 0.0022775456309318542 Updates: 236/47001, Avg Grad: 0.01857016608119011\n",
            "Epoch: 3, Loss: 0.0013650371693074703 Updates: 241/48001, Avg Grad: 0.015543622896075249\n",
            "Epoch: 3, Loss: 0.02155713364481926 Updates: 246/49001, Avg Grad: 0.020292378962039948\n",
            "Epoch 2 iteration 0 Loss: 0.072 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.303 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 3, Loss: 0.0004562910180538893 Updates: 251/50001, Avg Grad: 0.013622710481286049\n",
            "Epoch: 3, Loss: 1.9458766473690048e-05 Updates: 256/51001, Avg Grad: 0.016603577882051468\n",
            "Epoch: 3, Loss: 0.0020450535230338573 Updates: 261/52001, Avg Grad: 0.016604648903012276\n",
            "Epoch: 3, Loss: 4.1543036786606535e-05 Updates: 266/53001, Avg Grad: 0.020627819001674652\n",
            "Epoch: 3, Loss: 0.00013535517791751772 Updates: 271/54001, Avg Grad: 0.01219668798148632\n",
            "Epoch 2 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 400 Loss: 0.310 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 3, Loss: 0.00027869545738212764 Updates: 276/55001, Avg Grad: 0.01560383290052414\n",
            "Epoch: 3, Loss: 0.00011124332377221435 Updates: 281/56001, Avg Grad: 0.018741730600595474\n",
            "Epoch: 3, Loss: 6.96999704814516e-05 Updates: 286/57001, Avg Grad: 0.01331051904708147\n",
            "Epoch: 3, Loss: 2.8193231628392823e-05 Updates: 291/58001, Avg Grad: 0.013965293765068054\n",
            "Epoch: 3, Loss: 3.9545258914586157e-05 Updates: 296/59001, Avg Grad: 0.02144494093954563\n",
            "Epoch 3 iteration 0 Loss: 0.051 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.365 | Acc: 88.030% (353/401)\n",
            "Test accuracy: 0.8802992701530457\n",
            "Epoch: 4, Loss: 4.2038955143652856e-05 Updates: 1/1, Avg Grad: 1.8272054148837924e-05\n",
            "Epoch: 4, Loss: 0.001534435898065567 Updates: 6/1001, Avg Grad: 0.024232128635048866\n",
            "Epoch: 4, Loss: 0.0020077757071703672 Updates: 11/2001, Avg Grad: 0.01287259440869093\n",
            "Epoch: 4, Loss: 0.0007558593642897904 Updates: 16/3001, Avg Grad: 0.01766779273748398\n",
            "Epoch: 4, Loss: 0.00013617309741675854 Updates: 21/4001, Avg Grad: 0.02481815777719021\n",
            "Epoch 3 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.278 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 4, Loss: 0.0001553602487547323 Updates: 26/5001, Avg Grad: 0.01744776964187622\n",
            "Epoch: 4, Loss: 2.644981213961728e-05 Updates: 31/6001, Avg Grad: 0.01880732737481594\n",
            "Epoch: 4, Loss: 3.4016029530903324e-05 Updates: 36/7001, Avg Grad: 0.010111015290021896\n",
            "Epoch: 4, Loss: 0.0003057724970858544 Updates: 41/8001, Avg Grad: 0.010881398804485798\n",
            "Epoch: 4, Loss: 2.6571948183118366e-05 Updates: 46/9001, Avg Grad: 0.02613433077931404\n",
            "Epoch 3 iteration 0 Loss: 0.184 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.309 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 4, Loss: 0.0035154030192643404 Updates: 51/10001, Avg Grad: 0.016856422647833824\n",
            "Epoch: 4, Loss: 9.367558232042938e-05 Updates: 56/11001, Avg Grad: 0.01745796762406826\n",
            "Epoch: 4, Loss: 2.5367262423969805e-06 Updates: 61/12001, Avg Grad: 0.016550760716199875\n",
            "Epoch: 4, Loss: 7.821161489118822e-06 Updates: 66/13001, Avg Grad: 0.017682934179902077\n",
            "Epoch: 4, Loss: 0.0007498902268707752 Updates: 71/14001, Avg Grad: 0.01832524687051773\n",
            "Epoch 3 iteration 0 Loss: 0.007 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.381 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 4, Loss: 0.002388235181570053 Updates: 76/15001, Avg Grad: 0.019047031179070473\n",
            "Epoch: 4, Loss: 0.00015544460620731115 Updates: 81/16001, Avg Grad: 0.021128054708242416\n",
            "Epoch: 4, Loss: 0.0001092510501621291 Updates: 86/17001, Avg Grad: 0.022497203201055527\n",
            "Epoch: 4, Loss: 3.67018292308785e-05 Updates: 91/18001, Avg Grad: 0.01746271178126335\n",
            "Epoch: 4, Loss: 3.458257424426847e-06 Updates: 96/19001, Avg Grad: 0.021052684634923935\n",
            "Epoch 3 iteration 0 Loss: 0.415 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.285 | Acc: 89.776% (360/401)\n",
            "Test accuracy: 0.8977556228637695\n",
            "Epoch: 4, Loss: 8.28305201139301e-05 Updates: 101/20001, Avg Grad: 0.02651389315724373\n",
            "Epoch: 4, Loss: 0.0004431281122379005 Updates: 106/21001, Avg Grad: 0.02531186304986477\n",
            "Epoch: 4, Loss: 0.00047518315841443837 Updates: 111/22001, Avg Grad: 0.013992640189826488\n",
            "Epoch: 4, Loss: 7.99154004198499e-05 Updates: 116/23001, Avg Grad: 0.020852331072092056\n",
            "Epoch: 4, Loss: 3.2221934816334397e-05 Updates: 121/24001, Avg Grad: 0.026619672775268555\n",
            "Epoch 3 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.290 | Acc: 91.022% (365/401)\n",
            "Test accuracy: 0.910224437713623\n",
            "Epoch: 4, Loss: 5.764446541434154e-05 Updates: 126/25001, Avg Grad: 0.014741318300366402\n",
            "Epoch: 4, Loss: 0.00012630978017114103 Updates: 131/26001, Avg Grad: 0.014313419349491596\n",
            "Epoch: 4, Loss: 2.7246607714914717e-05 Updates: 136/27001, Avg Grad: 0.019068030640482903\n",
            "Epoch: 4, Loss: 1.1857386198244058e-05 Updates: 141/28001, Avg Grad: 0.020129580050706863\n",
            "Epoch: 4, Loss: 0.0010552670573815703 Updates: 146/29001, Avg Grad: 0.015356311574578285\n",
            "Epoch 3 iteration 0 Loss: 0.032 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.284 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 4, Loss: 0.00043760088738054037 Updates: 151/30001, Avg Grad: 0.013536172918975353\n",
            "Epoch: 4, Loss: 3.075835047638975e-05 Updates: 156/31001, Avg Grad: 0.020690681412816048\n",
            "Epoch: 4, Loss: 0.003006482031196356 Updates: 161/32001, Avg Grad: 0.02390948124229908\n",
            "Epoch: 4, Loss: 6.0284033679636195e-05 Updates: 166/33001, Avg Grad: 0.009509826079010963\n",
            "Epoch: 4, Loss: 0.0059469882398843765 Updates: 171/34001, Avg Grad: 0.017150720581412315\n",
            "Epoch 3 iteration 0 Loss: 0.187 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.264 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 4, Loss: 3.152687668261933e-06 Updates: 176/35001, Avg Grad: 0.013572072610259056\n",
            "Epoch: 4, Loss: 0.00492516765370965 Updates: 181/36001, Avg Grad: 0.01641995832324028\n",
            "Epoch: 4, Loss: 0.0006667548441328108 Updates: 186/37001, Avg Grad: 0.009552661329507828\n",
            "Epoch: 4, Loss: 0.008825235068798065 Updates: 191/38001, Avg Grad: 0.015403592959046364\n",
            "Epoch: 4, Loss: 6.171231689222623e-06 Updates: 196/39001, Avg Grad: 0.015315885655581951\n",
            "Epoch 3 iteration 0 Loss: 0.107 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.274 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 4, Loss: 0.00019653492199722677 Updates: 201/40001, Avg Grad: 0.0206903088837862\n",
            "Epoch: 4, Loss: 0.006490540690720081 Updates: 206/41001, Avg Grad: 0.01901828497648239\n",
            "Epoch: 4, Loss: 0.0005516464007087052 Updates: 211/42001, Avg Grad: 0.01591368019580841\n",
            "Epoch: 4, Loss: 6.442466838052496e-05 Updates: 216/43001, Avg Grad: 0.021985800936818123\n",
            "Epoch: 4, Loss: 0.000556952552869916 Updates: 221/44001, Avg Grad: 0.0176222063601017\n",
            "Epoch 3 iteration 0 Loss: 0.016 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.318 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 4, Loss: 4.490771243581548e-05 Updates: 226/45001, Avg Grad: 0.017801422625780106\n",
            "Epoch: 4, Loss: 0.0023950443137437105 Updates: 231/46001, Avg Grad: 0.012307627126574516\n",
            "Epoch: 4, Loss: 0.0003056385030504316 Updates: 236/47001, Avg Grad: 0.030981924384832382\n",
            "Epoch: 4, Loss: 2.4163844045688165e-06 Updates: 241/48001, Avg Grad: 0.011914254166185856\n",
            "Epoch: 4, Loss: 4.5690350816585124e-05 Updates: 246/49001, Avg Grad: 0.02485102228820324\n",
            "Epoch 3 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.343 | Acc: 90.524% (363/401)\n",
            "Test accuracy: 0.9052368998527527\n",
            "Epoch: 4, Loss: 0.00401368597522378 Updates: 251/50001, Avg Grad: 0.023934418335556984\n",
            "Epoch: 4, Loss: 0.0013708332553505898 Updates: 256/51001, Avg Grad: 0.01806546561419964\n",
            "Epoch: 4, Loss: 3.8507925637532026e-05 Updates: 261/52001, Avg Grad: 0.01840798370540142\n",
            "Epoch: 4, Loss: 1.57857830345165e-05 Updates: 266/53001, Avg Grad: 0.02456495724618435\n",
            "Epoch: 4, Loss: 3.585623926483095e-05 Updates: 271/54001, Avg Grad: 0.02094978466629982\n",
            "Epoch 3 iteration 0 Loss: 0.023 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 400 Loss: 0.273 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 4, Loss: 8.343953936673643e-07 Updates: 276/55001, Avg Grad: 0.01710224524140358\n",
            "Epoch: 4, Loss: 0.008421186357736588 Updates: 281/56001, Avg Grad: 0.020591571927070618\n",
            "Epoch: 4, Loss: 0.0015768982702866197 Updates: 286/57001, Avg Grad: 0.020988063886761665\n",
            "Epoch: 4, Loss: 3.7477273053809768e-06 Updates: 291/58001, Avg Grad: 0.027736473828554153\n",
            "Epoch: 4, Loss: 7.395615102723241e-05 Updates: 296/59001, Avg Grad: 0.02495039440691471\n",
            "Epoch 4 iteration 0 Loss: 0.427 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.317 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 5, Loss: 0.0002144194149877876 Updates: 1/1, Avg Grad: 5.3627765737473965e-05\n",
            "Epoch: 5, Loss: 0.00025757751427590847 Updates: 6/1001, Avg Grad: 0.017878657206892967\n",
            "Epoch: 5, Loss: 0.0016120221698656678 Updates: 11/2001, Avg Grad: 0.01913035660982132\n",
            "Epoch: 5, Loss: 1.916011024150066e-05 Updates: 16/3001, Avg Grad: 0.019230429083108902\n",
            "Epoch: 5, Loss: 0.0007708186167292297 Updates: 21/4001, Avg Grad: 0.019829852506518364\n",
            "Epoch 4 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.202 | Acc: 94.763% (380/401)\n",
            "Test accuracy: 0.9476309418678284\n",
            "Epoch: 5, Loss: 3.3724852528393967e-06 Updates: 26/5001, Avg Grad: 0.020055610686540604\n",
            "Epoch: 5, Loss: 3.140460830763914e-05 Updates: 31/6001, Avg Grad: 0.02502434514462948\n",
            "Epoch: 5, Loss: 0.00010267212928738445 Updates: 36/7001, Avg Grad: 0.012173802591860294\n",
            "Epoch: 5, Loss: 6.078957085264847e-06 Updates: 41/8001, Avg Grad: 0.015986789017915726\n",
            "Epoch: 5, Loss: 5.488676833920181e-05 Updates: 46/9001, Avg Grad: 0.02315303310751915\n",
            "Epoch 4 iteration 0 Loss: 6.263 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 400 Loss: 0.400 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 5, Loss: 0.00026098437956534326 Updates: 51/10001, Avg Grad: 0.01558920182287693\n",
            "Epoch: 5, Loss: 0.005452319979667664 Updates: 56/11001, Avg Grad: 0.020062517374753952\n",
            "Epoch: 5, Loss: 9.385473276779521e-06 Updates: 61/12001, Avg Grad: 0.014286552555859089\n",
            "Epoch: 5, Loss: 0.00012194672308396548 Updates: 66/13001, Avg Grad: 0.028522301465272903\n",
            "Epoch: 5, Loss: 0.0002537003601901233 Updates: 71/14001, Avg Grad: 0.014002551324665546\n",
            "Epoch 4 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.252 | Acc: 90.524% (363/401)\n",
            "Test accuracy: 0.9052368998527527\n",
            "Epoch: 5, Loss: 1.5929783785395557e-06 Updates: 76/15001, Avg Grad: 0.022685199975967407\n",
            "Epoch: 5, Loss: 6.0021598073944915e-06 Updates: 81/16001, Avg Grad: 0.01836881972849369\n",
            "Epoch: 5, Loss: 0.00011000385711668059 Updates: 86/17001, Avg Grad: 0.015415249392390251\n",
            "Epoch: 5, Loss: 8.948776667239144e-06 Updates: 91/18001, Avg Grad: 0.012434032745659351\n",
            "Epoch: 5, Loss: 1.1425068805692717e-05 Updates: 96/19001, Avg Grad: 0.011428607627749443\n",
            "Epoch 4 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.297 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 5, Loss: 0.009187458083033562 Updates: 101/20001, Avg Grad: 0.022835317999124527\n",
            "Epoch: 5, Loss: 5.463031629915349e-05 Updates: 106/21001, Avg Grad: 0.014955705031752586\n",
            "Epoch: 5, Loss: 6.495421257568523e-05 Updates: 111/22001, Avg Grad: 0.018675897270441055\n",
            "Epoch: 5, Loss: 0.01888320967555046 Updates: 116/23001, Avg Grad: 0.01650955155491829\n",
            "Epoch: 5, Loss: 0.00010325369657948613 Updates: 121/24001, Avg Grad: 0.01231882069259882\n",
            "Epoch 4 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.321 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 5, Loss: 2.3745449652778916e-05 Updates: 126/25001, Avg Grad: 0.017966410145163536\n",
            "Epoch: 5, Loss: 1.6732201402192004e-05 Updates: 131/26001, Avg Grad: 0.0253013726323843\n",
            "Epoch: 5, Loss: 2.5819515940384008e-05 Updates: 136/27001, Avg Grad: 0.015608884394168854\n",
            "Epoch: 5, Loss: 7.521539941990341e-07 Updates: 141/28001, Avg Grad: 0.018022365868091583\n",
            "Epoch: 5, Loss: 0.0004943961976096034 Updates: 146/29001, Avg Grad: 0.027026167139410973\n",
            "Epoch 4 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.214 | Acc: 93.516% (375/401)\n",
            "Test accuracy: 0.9351620674133301\n",
            "Epoch: 5, Loss: 0.00033077149419113994 Updates: 151/30001, Avg Grad: 0.02064106985926628\n",
            "Epoch: 5, Loss: 7.753913450869732e-06 Updates: 156/31001, Avg Grad: 0.012270679697394371\n",
            "Epoch: 5, Loss: 4.050079223816283e-05 Updates: 161/32001, Avg Grad: 0.024504397064447403\n",
            "Epoch: 5, Loss: 0.016053320840001106 Updates: 166/33001, Avg Grad: 0.012254760600626469\n",
            "Epoch: 5, Loss: 0.00014394181198440492 Updates: 171/34001, Avg Grad: 0.03421209007501602\n",
            "Epoch 4 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.314 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 5, Loss: 1.949438956216909e-05 Updates: 176/35001, Avg Grad: 0.020289573818445206\n",
            "Epoch: 5, Loss: 2.7286918339086697e-05 Updates: 181/36001, Avg Grad: 0.02510533295571804\n",
            "Epoch: 5, Loss: 9.23019524634583e-06 Updates: 186/37001, Avg Grad: 0.021578889340162277\n",
            "Epoch: 5, Loss: 0.006066781468689442 Updates: 191/38001, Avg Grad: 0.024774083867669106\n",
            "Epoch: 5, Loss: 5.170696385903284e-05 Updates: 196/39001, Avg Grad: 0.012297013774514198\n",
            "Epoch 4 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.320 | Acc: 90.025% (361/401)\n",
            "Test accuracy: 0.9002493619918823\n",
            "Epoch: 5, Loss: 0.003107162192463875 Updates: 201/40001, Avg Grad: 0.02202654257416725\n",
            "Epoch: 5, Loss: 0.004466468468308449 Updates: 206/41001, Avg Grad: 0.018155992031097412\n",
            "Epoch: 5, Loss: 2.8524536901386455e-05 Updates: 211/42001, Avg Grad: 0.022950923070311546\n",
            "Epoch: 5, Loss: 2.899745413742494e-05 Updates: 216/43001, Avg Grad: 0.02356989122927189\n",
            "Epoch: 5, Loss: 0.019155267626047134 Updates: 221/44001, Avg Grad: 0.022325946018099785\n",
            "Epoch 4 iteration 0 Loss: 0.062 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.303 | Acc: 88.279% (354/401)\n",
            "Test accuracy: 0.8827930092811584\n",
            "Epoch: 5, Loss: 0.0005765737150795758 Updates: 226/45001, Avg Grad: 0.01563459075987339\n",
            "Epoch: 5, Loss: 8.577967673772946e-05 Updates: 231/46001, Avg Grad: 0.020461954176425934\n",
            "Epoch: 5, Loss: 0.00014328618999570608 Updates: 236/47001, Avg Grad: 0.016752034425735474\n",
            "Epoch: 5, Loss: 0.0001296893460676074 Updates: 241/48001, Avg Grad: 0.0155930295586586\n",
            "Epoch: 5, Loss: 0.0003022119926754385 Updates: 246/49001, Avg Grad: 0.02416183613240719\n",
            "Epoch 4 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.289 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 5, Loss: 3.9894148358143866e-05 Updates: 251/50001, Avg Grad: 0.016553666442632675\n",
            "Epoch: 5, Loss: 3.101781112491153e-05 Updates: 256/51001, Avg Grad: 0.016842424869537354\n",
            "Epoch: 5, Loss: 0.00018769607413560152 Updates: 261/52001, Avg Grad: 0.015555773861706257\n",
            "Epoch: 5, Loss: 0.0006698930519632995 Updates: 266/53001, Avg Grad: 0.022573206573724747\n",
            "Epoch: 5, Loss: 1.0519113402551739e-06 Updates: 271/54001, Avg Grad: 0.023590365424752235\n",
            "Epoch 4 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 400 Loss: 0.306 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 5, Loss: 9.364056495542172e-06 Updates: 276/55001, Avg Grad: 0.01673893816769123\n",
            "Epoch: 5, Loss: 0.00017333940195385367 Updates: 281/56001, Avg Grad: 0.015516906976699829\n",
            "Epoch: 5, Loss: 0.0014903025003150105 Updates: 286/57001, Avg Grad: 0.02280302904546261\n",
            "Epoch: 5, Loss: 5.187450005905703e-05 Updates: 291/58001, Avg Grad: 0.025678161531686783\n",
            "Epoch: 5, Loss: 0.00016954961756709963 Updates: 296/59001, Avg Grad: 0.014992108568549156\n",
            "Epoch 5 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.292 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 6, Loss: 0.00015493843238800764 Updates: 1/1, Avg Grad: 7.523007661802694e-05\n",
            "Epoch: 6, Loss: 1.1687911865010392e-05 Updates: 6/1001, Avg Grad: 0.021784167736768723\n",
            "Epoch: 6, Loss: 2.0779821170435753e-06 Updates: 11/2001, Avg Grad: 0.016231197863817215\n",
            "Epoch: 6, Loss: 1.658546898397617e-05 Updates: 16/3001, Avg Grad: 0.01675916276872158\n",
            "Epoch: 6, Loss: 3.0259532650234178e-05 Updates: 21/4001, Avg Grad: 0.027986079454421997\n",
            "Epoch 5 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.320 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 6, Loss: 0.00021801503316964954 Updates: 26/5001, Avg Grad: 0.021199023351073265\n",
            "Epoch: 6, Loss: 3.212007140973583e-05 Updates: 31/6001, Avg Grad: 0.020836438983678818\n",
            "Epoch: 6, Loss: 0.0005675528664141893 Updates: 36/7001, Avg Grad: 0.02112601138651371\n",
            "Epoch: 6, Loss: 0.00014670756354462355 Updates: 41/8001, Avg Grad: 0.014550061896443367\n",
            "Epoch: 6, Loss: 3.41592967743054e-05 Updates: 46/9001, Avg Grad: 0.015912819653749466\n",
            "Epoch 5 iteration 0 Loss: 0.009 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.339 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 6, Loss: 0.0007151926984079182 Updates: 51/10001, Avg Grad: 0.018796995282173157\n",
            "Epoch: 6, Loss: 0.013406608253717422 Updates: 56/11001, Avg Grad: 0.01940028741955757\n",
            "Epoch: 6, Loss: 4.535013795248233e-05 Updates: 61/12001, Avg Grad: 0.02424456924200058\n",
            "Epoch: 6, Loss: 8.13656242826255e-06 Updates: 66/13001, Avg Grad: 0.03070937469601631\n",
            "Epoch: 6, Loss: 0.001654464635066688 Updates: 71/14001, Avg Grad: 0.028053397312760353\n",
            "Epoch 5 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.283 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 6, Loss: 0.005898833740502596 Updates: 76/15001, Avg Grad: 0.01672055758535862\n",
            "Epoch: 6, Loss: 1.1520251064212061e-06 Updates: 81/16001, Avg Grad: 0.012462345883250237\n",
            "Epoch: 6, Loss: 1.8135164282284677e-05 Updates: 86/17001, Avg Grad: 0.01692495122551918\n",
            "Epoch: 6, Loss: 9.710889571579173e-06 Updates: 91/18001, Avg Grad: 0.02061617374420166\n",
            "Epoch: 6, Loss: 0.0003149895346723497 Updates: 96/19001, Avg Grad: 0.0162961445748806\n",
            "Epoch 5 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.280 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 6, Loss: 0.0008884381968528032 Updates: 101/20001, Avg Grad: 0.013940675184130669\n",
            "Epoch: 6, Loss: 0.00462690694257617 Updates: 106/21001, Avg Grad: 0.016233930364251137\n",
            "Epoch: 6, Loss: 0.0033255473244935274 Updates: 111/22001, Avg Grad: 0.015030530281364918\n",
            "Epoch: 6, Loss: 4.713480666396208e-05 Updates: 116/23001, Avg Grad: 0.014379305765032768\n",
            "Epoch: 6, Loss: 9.727546057547443e-06 Updates: 121/24001, Avg Grad: 0.020513493567705154\n",
            "Epoch 5 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.342 | Acc: 89.526% (359/401)\n",
            "Test accuracy: 0.895261824131012\n",
            "Epoch: 6, Loss: 2.1739047042501625e-06 Updates: 126/25001, Avg Grad: 0.020729368552565575\n",
            "Epoch: 6, Loss: 0.0006232163868844509 Updates: 131/26001, Avg Grad: 0.016963694244623184\n",
            "Epoch: 6, Loss: 0.0002925643348135054 Updates: 136/27001, Avg Grad: 0.026406440883874893\n",
            "Epoch: 6, Loss: 0.0005138373817317188 Updates: 141/28001, Avg Grad: 0.013605071231722832\n",
            "Epoch: 6, Loss: 1.0215330803475808e-05 Updates: 146/29001, Avg Grad: 0.02431933954358101\n",
            "Epoch 5 iteration 0 Loss: 0.384 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.290 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 6, Loss: 4.067993722856045e-05 Updates: 151/30001, Avg Grad: 0.017733076587319374\n",
            "Epoch: 6, Loss: 2.255767503811512e-05 Updates: 156/31001, Avg Grad: 0.02085193060338497\n",
            "Epoch: 6, Loss: 0.00039918749826028943 Updates: 161/32001, Avg Grad: 0.02399432845413685\n",
            "Epoch: 6, Loss: 0.003375058062374592 Updates: 166/33001, Avg Grad: 0.0213763527572155\n",
            "Epoch: 6, Loss: 2.534096347517334e-05 Updates: 171/34001, Avg Grad: 0.024998527020215988\n",
            "Epoch 5 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.301 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 6, Loss: 7.939682109281421e-05 Updates: 176/35001, Avg Grad: 0.011347168125212193\n",
            "Epoch: 6, Loss: 8.509661711286753e-06 Updates: 181/36001, Avg Grad: 0.021281147375702858\n",
            "Epoch: 6, Loss: 8.088955837592948e-06 Updates: 186/37001, Avg Grad: 0.012997150421142578\n",
            "Epoch: 6, Loss: 9.97463648673147e-05 Updates: 191/38001, Avg Grad: 0.020234625786542892\n",
            "Epoch: 6, Loss: 7.699271372985095e-05 Updates: 196/39001, Avg Grad: 0.016147080808877945\n",
            "Epoch 5 iteration 0 Loss: 0.021 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.253 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 6, Loss: 1.4320351510832552e-05 Updates: 201/40001, Avg Grad: 0.01724378392100334\n",
            "Epoch: 6, Loss: 0.0007673116633668542 Updates: 206/41001, Avg Grad: 0.020187456160783768\n",
            "Epoch: 6, Loss: 0.0001127849318436347 Updates: 211/42001, Avg Grad: 0.019089367240667343\n",
            "Epoch: 6, Loss: 1.3053647307970095e-05 Updates: 216/43001, Avg Grad: 0.01967668905854225\n",
            "Epoch: 6, Loss: 8.597887790529057e-05 Updates: 221/44001, Avg Grad: 0.012175028212368488\n",
            "Epoch 5 iteration 0 Loss: 0.503 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.221 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 6, Loss: 3.0847807011014083e-06 Updates: 226/45001, Avg Grad: 0.018773619085550308\n",
            "Epoch: 6, Loss: 0.022361068055033684 Updates: 231/46001, Avg Grad: 0.01315341331064701\n",
            "Epoch: 6, Loss: 4.4006199459545314e-05 Updates: 236/47001, Avg Grad: 0.02011962980031967\n",
            "Epoch: 6, Loss: 3.024020998054766e-06 Updates: 241/48001, Avg Grad: 0.029519086703658104\n",
            "Epoch: 6, Loss: 0.00026431880542077124 Updates: 246/49001, Avg Grad: 0.017370732501149178\n",
            "Epoch 5 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.255 | Acc: 93.516% (375/401)\n",
            "Test accuracy: 0.9351620674133301\n",
            "Epoch: 6, Loss: 0.0022407756187021732 Updates: 251/50001, Avg Grad: 0.019421445205807686\n",
            "Epoch: 6, Loss: 0.009980830363929272 Updates: 256/51001, Avg Grad: 0.013309073634445667\n",
            "Epoch: 6, Loss: 3.5582706914283335e-07 Updates: 261/52001, Avg Grad: 0.02831175923347473\n",
            "Epoch: 6, Loss: 2.283477078890428e-05 Updates: 266/53001, Avg Grad: 0.014323343522846699\n",
            "Epoch: 6, Loss: 0.00041568276355974376 Updates: 271/54001, Avg Grad: 0.01161598227918148\n",
            "Epoch 5 iteration 0 Loss: 0.008 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 400 Loss: 0.326 | Acc: 90.773% (364/401)\n",
            "Test accuracy: 0.9077306985855103\n",
            "Epoch: 6, Loss: 2.0192563169985078e-05 Updates: 276/55001, Avg Grad: 0.01570848561823368\n",
            "Epoch: 6, Loss: 1.2650565622607246e-05 Updates: 281/56001, Avg Grad: 0.021462006494402885\n",
            "Epoch: 6, Loss: 0.0035555108916014433 Updates: 286/57001, Avg Grad: 0.02414974570274353\n",
            "Epoch: 6, Loss: 5.893808520340826e-06 Updates: 291/58001, Avg Grad: 0.017528682947158813\n",
            "Epoch: 6, Loss: 7.839014870114625e-06 Updates: 296/59001, Avg Grad: 0.01620713621377945\n",
            "Epoch 6 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.309 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 7, Loss: 2.930915252363775e-05 Updates: 1/1, Avg Grad: 1.195834374811966e-05\n",
            "Epoch: 7, Loss: 8.811632142169401e-05 Updates: 6/1001, Avg Grad: 0.020764080807566643\n",
            "Epoch: 7, Loss: 5.084561780677177e-05 Updates: 11/2001, Avg Grad: 0.017597338184714317\n",
            "Epoch: 7, Loss: 0.0012438190169632435 Updates: 16/3001, Avg Grad: 0.02316179871559143\n",
            "Epoch: 7, Loss: 0.0003483370819594711 Updates: 21/4001, Avg Grad: 0.015886517241597176\n",
            "Epoch 6 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.252 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 7, Loss: 0.00012496999988798052 Updates: 26/5001, Avg Grad: 0.01767880842089653\n",
            "Epoch: 7, Loss: 3.249780775149702e-06 Updates: 31/6001, Avg Grad: 0.013872010633349419\n",
            "Epoch: 7, Loss: 9.68114454735769e-06 Updates: 36/7001, Avg Grad: 0.020198099315166473\n",
            "Epoch: 7, Loss: 4.845622925131465e-07 Updates: 41/8001, Avg Grad: 0.020826855674386024\n",
            "Epoch: 7, Loss: 0.002203378826379776 Updates: 46/9001, Avg Grad: 0.027174215763807297\n",
            "Epoch 6 iteration 0 Loss: 2.599 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 400 Loss: 0.249 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 7, Loss: 8.915454236557707e-05 Updates: 51/10001, Avg Grad: 0.021099384874105453\n",
            "Epoch: 7, Loss: 3.936134817195125e-05 Updates: 56/11001, Avg Grad: 0.014825348742306232\n",
            "Epoch: 7, Loss: 1.1284720130788628e-05 Updates: 61/12001, Avg Grad: 0.015922322869300842\n",
            "Epoch: 7, Loss: 8.062689448706806e-05 Updates: 66/13001, Avg Grad: 0.016948003321886063\n",
            "Epoch: 7, Loss: 0.0013611887115985155 Updates: 71/14001, Avg Grad: 0.01524757593870163\n",
            "Epoch 6 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.280 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 7, Loss: 4.668663677875884e-05 Updates: 76/15001, Avg Grad: 0.013632141053676605\n",
            "Epoch: 7, Loss: 7.489926065318286e-05 Updates: 81/16001, Avg Grad: 0.013855187222361565\n",
            "Epoch: 7, Loss: 0.00014884928532410413 Updates: 86/17001, Avg Grad: 0.017264792695641518\n",
            "Epoch: 7, Loss: 0.0006869414355605841 Updates: 91/18001, Avg Grad: 0.013506598770618439\n",
            "Epoch: 7, Loss: 0.00013769775978289545 Updates: 96/19001, Avg Grad: 0.02224901132285595\n",
            "Epoch 6 iteration 0 Loss: 3.197 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 400 Loss: 0.324 | Acc: 89.277% (358/401)\n",
            "Test accuracy: 0.8927680850028992\n",
            "Epoch: 7, Loss: 2.0577823306666687e-05 Updates: 101/20001, Avg Grad: 0.01941710151731968\n",
            "Epoch: 7, Loss: 3.1025512726046145e-05 Updates: 106/21001, Avg Grad: 0.025486260652542114\n",
            "Epoch: 7, Loss: 0.007992000319063663 Updates: 111/22001, Avg Grad: 0.012691900134086609\n",
            "Epoch: 7, Loss: 1.6656723573760246e-06 Updates: 116/23001, Avg Grad: 0.024994805455207825\n",
            "Epoch: 7, Loss: 1.5119412637432106e-06 Updates: 121/24001, Avg Grad: 0.01955336704850197\n",
            "Epoch 6 iteration 0 Loss: 0.270 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.252 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 7, Loss: 6.5778122007031925e-06 Updates: 126/25001, Avg Grad: 0.013063502497971058\n",
            "Epoch: 7, Loss: 0.00020552979549393058 Updates: 131/26001, Avg Grad: 0.015592704527080059\n",
            "Epoch: 7, Loss: 2.167230013583321e-05 Updates: 136/27001, Avg Grad: 0.02529827132821083\n",
            "Epoch: 7, Loss: 2.8096030291635543e-05 Updates: 141/28001, Avg Grad: 0.02948354184627533\n",
            "Epoch: 7, Loss: 2.632044243000564e-06 Updates: 146/29001, Avg Grad: 0.025977706536650658\n",
            "Epoch 6 iteration 0 Loss: 0.042 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.262 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 7, Loss: 5.213345502852462e-05 Updates: 151/30001, Avg Grad: 0.025076931342482567\n",
            "Epoch: 7, Loss: 7.533258030889556e-05 Updates: 156/31001, Avg Grad: 0.020255055278539658\n",
            "Epoch: 7, Loss: 0.004147961735725403 Updates: 161/32001, Avg Grad: 0.022657956928014755\n",
            "Epoch: 7, Loss: 4.2819515329028945e-06 Updates: 166/33001, Avg Grad: 0.015284925699234009\n",
            "Epoch: 7, Loss: 7.592037491122028e-06 Updates: 171/34001, Avg Grad: 0.017177799716591835\n",
            "Epoch 6 iteration 0 Loss: 0.097 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.256 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 7, Loss: 2.290677912242245e-06 Updates: 176/35001, Avg Grad: 0.023030322045087814\n",
            "Epoch: 7, Loss: 0.0015357696684077382 Updates: 181/36001, Avg Grad: 0.02325613796710968\n",
            "Epoch: 7, Loss: 3.54597250407096e-05 Updates: 186/37001, Avg Grad: 0.013777187094092369\n",
            "Epoch: 7, Loss: 6.064669378247345e-06 Updates: 191/38001, Avg Grad: 0.01647956296801567\n",
            "Epoch: 7, Loss: 4.975703177478863e-06 Updates: 196/39001, Avg Grad: 0.020113693550229073\n",
            "Epoch 6 iteration 0 Loss: 0.010 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.245 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 7, Loss: 0.0034324044827371836 Updates: 201/40001, Avg Grad: 0.019877314567565918\n",
            "Epoch: 7, Loss: 0.00027304142713546753 Updates: 206/41001, Avg Grad: 0.017508288845419884\n",
            "Epoch: 7, Loss: 0.0025840920861810446 Updates: 211/42001, Avg Grad: 0.015697672963142395\n",
            "Epoch: 7, Loss: 0.02734629437327385 Updates: 216/43001, Avg Grad: 0.01575564220547676\n",
            "Epoch: 7, Loss: 0.00335307396017015 Updates: 221/44001, Avg Grad: 0.015462410636246204\n",
            "Epoch 6 iteration 0 Loss: 0.620 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.272 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 7, Loss: 6.6134896769654e-05 Updates: 226/45001, Avg Grad: 0.019054347649216652\n",
            "Epoch: 7, Loss: 8.922019105739309e-07 Updates: 231/46001, Avg Grad: 0.013900640420615673\n",
            "Epoch: 7, Loss: 6.871943583064422e-07 Updates: 236/47001, Avg Grad: 0.015351136215031147\n",
            "Epoch: 7, Loss: 3.711007593665272e-05 Updates: 241/48001, Avg Grad: 0.018459634855389595\n",
            "Epoch: 7, Loss: 5.0907568947877735e-05 Updates: 246/49001, Avg Grad: 0.011837111786007881\n",
            "Epoch 6 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 400 Loss: 0.242 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 7, Loss: 6.923147157067433e-05 Updates: 251/50001, Avg Grad: 0.014711080119013786\n",
            "Epoch: 7, Loss: 0.001962754176929593 Updates: 256/51001, Avg Grad: 0.018625419586896896\n",
            "Epoch: 7, Loss: 2.8728612733175396e-07 Updates: 261/52001, Avg Grad: 0.026925235986709595\n",
            "Epoch: 7, Loss: 0.0001180141480290331 Updates: 266/53001, Avg Grad: 0.023978909477591515\n",
            "Epoch: 7, Loss: 8.790912602307799e-07 Updates: 271/54001, Avg Grad: 0.027239982038736343\n",
            "Epoch 6 iteration 0 Loss: 2.721 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 400 Loss: 0.246 | Acc: 91.521% (367/401)\n",
            "Test accuracy: 0.9152119755744934\n",
            "Epoch: 7, Loss: 0.0041327397339046 Updates: 276/55001, Avg Grad: 0.024194378405809402\n",
            "Epoch: 7, Loss: 1.629495818633586e-05 Updates: 281/56001, Avg Grad: 0.015666980296373367\n",
            "Epoch: 7, Loss: 0.00015232813893817365 Updates: 286/57001, Avg Grad: 0.019484538584947586\n",
            "Epoch: 7, Loss: 0.00048133821110241115 Updates: 291/58001, Avg Grad: 0.019046535715460777\n",
            "Epoch: 7, Loss: 0.0013975715264678001 Updates: 296/59001, Avg Grad: 0.020986750721931458\n",
            "Epoch 7 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.380 | Acc: 89.027% (357/401)\n",
            "Test accuracy: 0.8902742862701416\n",
            "Epoch: 8, Loss: 6.959956408536527e-06 Updates: 1/1, Avg Grad: 3.124056092929095e-06\n",
            "Epoch: 8, Loss: 1.8270568034495227e-05 Updates: 6/1001, Avg Grad: 0.023210782557725906\n",
            "Epoch: 8, Loss: 0.0011281173210591078 Updates: 11/2001, Avg Grad: 0.01196230761706829\n",
            "Epoch: 8, Loss: 0.00020809359557460994 Updates: 16/3001, Avg Grad: 0.028030093759298325\n",
            "Epoch: 8, Loss: 0.00013106736878398806 Updates: 21/4001, Avg Grad: 0.013275775127112865\n",
            "Epoch 7 iteration 0 Loss: 0.468 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.236 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 8, Loss: 5.006071205571061e-06 Updates: 26/5001, Avg Grad: 0.013754317536950111\n",
            "Epoch: 8, Loss: 2.5773859306355007e-05 Updates: 31/6001, Avg Grad: 0.022134069353342056\n",
            "Epoch: 8, Loss: 4.226532837492414e-05 Updates: 36/7001, Avg Grad: 0.03151785954833031\n",
            "Epoch: 8, Loss: 0.003063468262553215 Updates: 41/8001, Avg Grad: 0.021436646580696106\n",
            "Epoch: 8, Loss: 0.00019592112221289426 Updates: 46/9001, Avg Grad: 0.021037181839346886\n",
            "Epoch 7 iteration 0 Loss: 0.044 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.270 | Acc: 91.771% (368/401)\n",
            "Test accuracy: 0.9177057147026062\n",
            "Epoch: 8, Loss: 0.00015100801829248667 Updates: 51/10001, Avg Grad: 0.02715364657342434\n",
            "Epoch: 8, Loss: 0.00143366155680269 Updates: 56/11001, Avg Grad: 0.015100988559424877\n",
            "Epoch: 8, Loss: 0.00020362567738629878 Updates: 61/12001, Avg Grad: 0.017173688858747482\n",
            "Epoch: 8, Loss: 2.7519292416400276e-05 Updates: 66/13001, Avg Grad: 0.016347551718354225\n",
            "Epoch: 8, Loss: 5.8396835811436176e-05 Updates: 71/14001, Avg Grad: 0.02200462482869625\n",
            "Epoch 7 iteration 0 Loss: 0.534 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.239 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 8, Loss: 0.0003368159814272076 Updates: 76/15001, Avg Grad: 0.01623757742345333\n",
            "Epoch: 8, Loss: 6.298625976342009e-06 Updates: 81/16001, Avg Grad: 0.0179794542491436\n",
            "Epoch: 8, Loss: 6.6879347286885604e-06 Updates: 86/17001, Avg Grad: 0.019607333466410637\n",
            "Epoch: 8, Loss: 9.651661821408197e-05 Updates: 91/18001, Avg Grad: 0.028194185346364975\n",
            "Epoch: 8, Loss: 0.0007752373930998147 Updates: 96/19001, Avg Grad: 0.012222303077578545\n",
            "Epoch 7 iteration 0 Loss: 0.007 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.288 | Acc: 90.274% (362/401)\n",
            "Test accuracy: 0.9027431607246399\n",
            "Epoch: 8, Loss: 0.0009370425250381231 Updates: 101/20001, Avg Grad: 0.018986055627465248\n",
            "Epoch: 8, Loss: 0.016826188191771507 Updates: 106/21001, Avg Grad: 0.03149763494729996\n",
            "Epoch: 8, Loss: 0.00011101597192464396 Updates: 111/22001, Avg Grad: 0.017231635749340057\n",
            "Epoch: 8, Loss: 0.01782240718603134 Updates: 116/23001, Avg Grad: 0.015642866492271423\n",
            "Epoch: 8, Loss: 0.0005771100986748934 Updates: 121/24001, Avg Grad: 0.010208828374743462\n",
            "Epoch 7 iteration 0 Loss: 0.019 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.254 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 8, Loss: 2.429355663480237e-05 Updates: 126/25001, Avg Grad: 0.020256077870726585\n",
            "Epoch: 8, Loss: 0.0006550308899022639 Updates: 131/26001, Avg Grad: 0.02970210276544094\n",
            "Epoch: 8, Loss: 0.0005971883656457067 Updates: 136/27001, Avg Grad: 0.014948916621506214\n",
            "Epoch: 8, Loss: 0.0001269031345145777 Updates: 141/28001, Avg Grad: 0.020313844084739685\n",
            "Epoch: 8, Loss: 1.7263258996536024e-05 Updates: 146/29001, Avg Grad: 0.01369232963770628\n",
            "Epoch 7 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.230 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 8, Loss: 2.3341690393863246e-06 Updates: 151/30001, Avg Grad: 0.02062910422682762\n",
            "Epoch: 8, Loss: 7.271840786415851e-06 Updates: 156/31001, Avg Grad: 0.01943664625287056\n",
            "Epoch: 8, Loss: 1.7115524997279863e-06 Updates: 161/32001, Avg Grad: 0.020093491300940514\n",
            "Epoch: 8, Loss: 0.00120070471893996 Updates: 166/33001, Avg Grad: 0.016830911859869957\n",
            "Epoch: 8, Loss: 4.4142081605969e-05 Updates: 171/34001, Avg Grad: 0.008162916637957096\n",
            "Epoch 7 iteration 0 Loss: 0.024 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.206 | Acc: 93.516% (375/401)\n",
            "Test accuracy: 0.9351620674133301\n",
            "Epoch: 8, Loss: 5.018459319217072e-07 Updates: 176/35001, Avg Grad: 0.01441203337162733\n",
            "Epoch: 8, Loss: 6.389094778569415e-05 Updates: 181/36001, Avg Grad: 0.013043935410678387\n",
            "Epoch: 8, Loss: 9.246853551303502e-06 Updates: 186/37001, Avg Grad: 0.020017297938466072\n",
            "Epoch: 8, Loss: 5.328199677023804e-06 Updates: 191/38001, Avg Grad: 0.02167203091084957\n",
            "Epoch: 8, Loss: 0.0002857079089153558 Updates: 196/39001, Avg Grad: 0.02499845065176487\n",
            "Epoch 7 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.241 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 8, Loss: 0.0013538813218474388 Updates: 201/40001, Avg Grad: 0.015124465338885784\n",
            "Epoch: 8, Loss: 4.2833868064917624e-05 Updates: 206/41001, Avg Grad: 0.01978793367743492\n",
            "Epoch: 8, Loss: 5.638692528009415e-05 Updates: 211/42001, Avg Grad: 0.016365712508559227\n",
            "Epoch: 8, Loss: 0.00606435164809227 Updates: 216/43001, Avg Grad: 0.020664364099502563\n",
            "Epoch: 8, Loss: 0.0009170513367280364 Updates: 221/44001, Avg Grad: 0.015611909329891205\n",
            "Epoch 7 iteration 0 Loss: 0.012 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.264 | Acc: 92.269% (370/401)\n",
            "Test accuracy: 0.9226932525634766\n",
            "Epoch: 8, Loss: 7.949704922793899e-06 Updates: 226/45001, Avg Grad: 0.012064541690051556\n",
            "Epoch: 8, Loss: 4.748830178868957e-06 Updates: 231/46001, Avg Grad: 0.01508194487541914\n",
            "Epoch: 8, Loss: 0.0012518722796812654 Updates: 236/47001, Avg Grad: 0.017205577343702316\n",
            "Epoch: 8, Loss: 0.0006335324142128229 Updates: 241/48001, Avg Grad: 0.022845763713121414\n",
            "Epoch: 8, Loss: 1.7931821503225365e-06 Updates: 246/49001, Avg Grad: 0.01941591314971447\n",
            "Epoch 7 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.217 | Acc: 94.015% (377/401)\n",
            "Test accuracy: 0.9401496052742004\n",
            "Epoch: 8, Loss: 2.5480453587078955e-06 Updates: 251/50001, Avg Grad: 0.02341299131512642\n",
            "Epoch: 8, Loss: 0.0002683374332264066 Updates: 256/51001, Avg Grad: 0.01486387848854065\n",
            "Epoch: 8, Loss: 0.008552392944693565 Updates: 261/52001, Avg Grad: 0.021239925175905228\n",
            "Epoch: 8, Loss: 1.140544281952316e-05 Updates: 266/53001, Avg Grad: 0.020099138841032982\n",
            "Epoch: 8, Loss: 0.002148388884961605 Updates: 271/54001, Avg Grad: 0.010521317832171917\n",
            "Epoch 7 iteration 0 Loss: 0.075 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 400 Loss: 0.244 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 8, Loss: 5.452638561109779e-06 Updates: 276/55001, Avg Grad: 0.015735788270831108\n",
            "Epoch: 8, Loss: 1.9839928427245468e-05 Updates: 281/56001, Avg Grad: 0.01665019430220127\n",
            "Epoch: 8, Loss: 1.9684377548401244e-05 Updates: 286/57001, Avg Grad: 0.013594219461083412\n",
            "Epoch: 8, Loss: 0.00011008607543772087 Updates: 291/58001, Avg Grad: 0.017582576721906662\n",
            "Epoch: 8, Loss: 3.662905146484263e-05 Updates: 296/59001, Avg Grad: 0.015722021460533142\n",
            "Epoch 8 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.282 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 9, Loss: 1.3330673937161919e-05 Updates: 1/1, Avg Grad: 6.060038231225917e-06\n",
            "Epoch: 9, Loss: 1.8731392628978938e-05 Updates: 6/1001, Avg Grad: 0.03315284103155136\n",
            "Epoch: 9, Loss: 1.959888504643459e-05 Updates: 11/2001, Avg Grad: 0.017570000141859055\n",
            "Epoch: 9, Loss: 0.00084836152382195 Updates: 16/3001, Avg Grad: 0.015595386736094952\n",
            "Epoch: 9, Loss: 2.7762093850469682e-06 Updates: 21/4001, Avg Grad: 0.011054969392716885\n",
            "Epoch 8 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.203 | Acc: 93.017% (373/401)\n",
            "Test accuracy: 0.9301745891571045\n",
            "Epoch: 9, Loss: 9.381309610034805e-06 Updates: 26/5001, Avg Grad: 0.01598159596323967\n",
            "Epoch: 9, Loss: 1.854136644396931e-05 Updates: 31/6001, Avg Grad: 0.01859506219625473\n",
            "Epoch: 9, Loss: 8.990452624857426e-05 Updates: 36/7001, Avg Grad: 0.028179341927170753\n",
            "Epoch: 9, Loss: 4.187856120552169e-06 Updates: 41/8001, Avg Grad: 0.012316593900322914\n",
            "Epoch: 9, Loss: 0.00022588788124267012 Updates: 46/9001, Avg Grad: 0.023220429196953773\n",
            "Epoch 8 iteration 0 Loss: 0.025 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.318 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 9, Loss: 5.532713475986384e-05 Updates: 51/10001, Avg Grad: 0.013642927631735802\n",
            "Epoch: 9, Loss: 3.0317592973005958e-05 Updates: 56/11001, Avg Grad: 0.016910653561353683\n",
            "Epoch: 9, Loss: 0.0012220019707456231 Updates: 61/12001, Avg Grad: 0.019461244344711304\n",
            "Epoch: 9, Loss: 0.00015417963732033968 Updates: 66/13001, Avg Grad: 0.020041290670633316\n",
            "Epoch: 9, Loss: 2.0850871806032956e-05 Updates: 71/14001, Avg Grad: 0.020638465881347656\n",
            "Epoch 8 iteration 0 Loss: 0.091 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.223 | Acc: 94.015% (377/401)\n",
            "Test accuracy: 0.9401496052742004\n",
            "Epoch: 9, Loss: 3.778791608510801e-07 Updates: 76/15001, Avg Grad: 0.027199843898415565\n",
            "Epoch: 9, Loss: 7.150422334234463e-06 Updates: 81/16001, Avg Grad: 0.0125922542065382\n",
            "Epoch: 9, Loss: 0.033304374665021896 Updates: 86/17001, Avg Grad: 0.014395122416317463\n",
            "Epoch: 9, Loss: 4.9470098019810393e-05 Updates: 91/18001, Avg Grad: 0.018355686217546463\n",
            "Epoch: 9, Loss: 3.926999852410518e-06 Updates: 96/19001, Avg Grad: 0.02243189886212349\n",
            "Epoch 8 iteration 0 Loss: 0.023 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.244 | Acc: 93.766% (376/401)\n",
            "Test accuracy: 0.9376558661460876\n",
            "Epoch: 9, Loss: 0.0007657168316654861 Updates: 101/20001, Avg Grad: 0.016102038323879242\n",
            "Epoch: 9, Loss: 0.0022122168447822332 Updates: 106/21001, Avg Grad: 0.014972494915127754\n",
            "Epoch: 9, Loss: 1.0067215043818578e-05 Updates: 111/22001, Avg Grad: 0.020823629572987556\n",
            "Epoch: 9, Loss: 2.159605855922564e-06 Updates: 116/23001, Avg Grad: 0.025966156274080276\n",
            "Epoch: 9, Loss: 5.885473910893779e-06 Updates: 121/24001, Avg Grad: 0.018404025584459305\n",
            "Epoch 8 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.193 | Acc: 94.763% (380/401)\n",
            "Test accuracy: 0.9476309418678284\n",
            "Epoch: 9, Loss: 0.0008338825427927077 Updates: 126/25001, Avg Grad: 0.02034536749124527\n",
            "Epoch: 9, Loss: 4.437663301359862e-05 Updates: 131/26001, Avg Grad: 0.013773011043667793\n",
            "Epoch: 9, Loss: 6.602218036277918e-06 Updates: 136/27001, Avg Grad: 0.01279679499566555\n",
            "Epoch: 9, Loss: 5.042298880653107e-07 Updates: 141/28001, Avg Grad: 0.022803938016295433\n",
            "Epoch: 9, Loss: 1.0769086657091975e-05 Updates: 146/29001, Avg Grad: 0.022830236703157425\n",
            "Epoch 8 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.259 | Acc: 93.267% (374/401)\n",
            "Test accuracy: 0.9326683282852173\n",
            "Epoch: 9, Loss: 2.3208553102449514e-05 Updates: 151/30001, Avg Grad: 0.01295878179371357\n",
            "Epoch: 9, Loss: 5.5532600526930764e-06 Updates: 156/31001, Avg Grad: 0.019761022180318832\n",
            "Epoch: 9, Loss: 0.00021798934903927147 Updates: 161/32001, Avg Grad: 0.01656501553952694\n",
            "Epoch: 9, Loss: 5.66578637517523e-06 Updates: 166/33001, Avg Grad: 0.02099970355629921\n",
            "Epoch: 9, Loss: 1.6255748050753027e-05 Updates: 171/34001, Avg Grad: 0.017630595713853836\n",
            "Epoch 8 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.319 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 9, Loss: 4.883325527771376e-05 Updates: 176/35001, Avg Grad: 0.020202424377202988\n",
            "Epoch: 9, Loss: 1.8226028259960003e-05 Updates: 181/36001, Avg Grad: 0.021048229187726974\n",
            "Epoch: 9, Loss: 2.818505208779243e-06 Updates: 186/37001, Avg Grad: 0.013808953575789928\n",
            "Epoch: 9, Loss: 0.00012463222083169967 Updates: 191/38001, Avg Grad: 0.03205234929919243\n",
            "Epoch: 9, Loss: 1.96238215721678e-05 Updates: 196/39001, Avg Grad: 0.020188886672258377\n",
            "Epoch 8 iteration 0 Loss: 0.046 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.254 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 9, Loss: 4.973685281584039e-05 Updates: 201/40001, Avg Grad: 0.020094092935323715\n",
            "Epoch: 9, Loss: 6.0779279010603204e-05 Updates: 206/41001, Avg Grad: 0.023097630590200424\n",
            "Epoch: 9, Loss: 2.7525220502866432e-05 Updates: 211/42001, Avg Grad: 0.02229105681180954\n",
            "Epoch: 9, Loss: 9.205022797686979e-05 Updates: 216/43001, Avg Grad: 0.021564457565546036\n",
            "Epoch: 9, Loss: 0.007600728422403336 Updates: 221/44001, Avg Grad: 0.014866748824715614\n",
            "Epoch 8 iteration 0 Loss: 0.002 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.156 | Acc: 95.761% (384/401)\n",
            "Test accuracy: 0.9576059579849243\n",
            "Epoch: 9, Loss: 6.352202490234049e-06 Updates: 226/45001, Avg Grad: 0.02393893338739872\n",
            "Epoch: 9, Loss: 0.0006332383491098881 Updates: 231/46001, Avg Grad: 0.013470470905303955\n",
            "Epoch: 9, Loss: 2.6700010494096205e-05 Updates: 236/47001, Avg Grad: 0.021470680832862854\n",
            "Epoch: 9, Loss: 8.857074863044545e-05 Updates: 241/48001, Avg Grad: 0.017485611140727997\n",
            "Epoch: 9, Loss: 9.52810951275751e-05 Updates: 246/49001, Avg Grad: 0.01527547836303711\n",
            "Epoch 8 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.305 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 9, Loss: 0.0039847237057983875 Updates: 251/50001, Avg Grad: 0.01212679035961628\n",
            "Epoch: 9, Loss: 0.0023028897121548653 Updates: 256/51001, Avg Grad: 0.02144506946206093\n",
            "Epoch: 9, Loss: 3.3027943118213443e-06 Updates: 261/52001, Avg Grad: 0.013725915923714638\n",
            "Epoch: 9, Loss: 1.7091690551751526e-06 Updates: 266/53001, Avg Grad: 0.016835544258356094\n",
            "Epoch: 9, Loss: 0.00015770792379043996 Updates: 271/54001, Avg Grad: 0.018099777400493622\n",
            "Epoch 8 iteration 0 Loss: 0.354 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 400 Loss: 0.254 | Acc: 92.020% (369/401)\n",
            "Test accuracy: 0.9201995134353638\n",
            "Epoch: 9, Loss: 1.2493010217440315e-05 Updates: 276/55001, Avg Grad: 0.020792251452803612\n",
            "Epoch: 9, Loss: 0.0001216523814946413 Updates: 281/56001, Avg Grad: 0.01891452632844448\n",
            "Epoch: 9, Loss: 5.972572762402706e-05 Updates: 286/57001, Avg Grad: 0.01596313714981079\n",
            "Epoch: 9, Loss: 1.8855354255720158e-06 Updates: 291/58001, Avg Grad: 0.01664394699037075\n",
            "Epoch: 9, Loss: 4.8070000048028305e-05 Updates: 296/59001, Avg Grad: 0.014126503840088844\n",
            "Epoch 9 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.214 | Acc: 94.763% (380/401)\n",
            "Test accuracy: 0.9476309418678284\n",
            "Epoch: 10, Loss: 1.2017340850434266e-05 Updates: 1/1, Avg Grad: 4.597075076162582e-06\n",
            "Epoch: 10, Loss: 2.7582716938923113e-05 Updates: 6/1001, Avg Grad: 0.024957844987511635\n",
            "Epoch: 10, Loss: 0.006249827332794666 Updates: 11/2001, Avg Grad: 0.016407350078225136\n",
            "Epoch: 10, Loss: 9.303966362494975e-05 Updates: 16/3001, Avg Grad: 0.010433022864162922\n",
            "Epoch: 10, Loss: 9.224041423294693e-05 Updates: 21/4001, Avg Grad: 0.021065859124064445\n",
            "Epoch 9 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.231 | Acc: 94.514% (379/401)\n",
            "Test accuracy: 0.9451371431350708\n",
            "Epoch: 10, Loss: 0.0030117537826299667 Updates: 26/5001, Avg Grad: 0.022913139313459396\n",
            "Epoch: 10, Loss: 4.0149585402105004e-05 Updates: 31/6001, Avg Grad: 0.02355707623064518\n",
            "Epoch: 10, Loss: 4.532070761342766e-06 Updates: 36/7001, Avg Grad: 0.020274538546800613\n",
            "Epoch: 10, Loss: 0.0002759256458375603 Updates: 41/8001, Avg Grad: 0.01394263468682766\n",
            "Epoch: 10, Loss: 7.744391041342169e-06 Updates: 46/9001, Avg Grad: 0.015251305885612965\n",
            "Epoch 9 iteration 0 Loss: 0.054 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.197 | Acc: 94.264% (378/401)\n",
            "Test accuracy: 0.942643404006958\n",
            "Epoch: 10, Loss: 0.00010487656982149929 Updates: 51/10001, Avg Grad: 0.021679652854800224\n",
            "Epoch: 10, Loss: 0.0016545047983527184 Updates: 56/11001, Avg Grad: 0.013756191357970238\n",
            "Epoch: 10, Loss: 0.00013328892237041146 Updates: 61/12001, Avg Grad: 0.019747581332921982\n",
            "Epoch: 10, Loss: 3.125000876025297e-05 Updates: 66/13001, Avg Grad: 0.015774954110383987\n",
            "Epoch: 10, Loss: 0.0008771993452683091 Updates: 71/14001, Avg Grad: 0.016471289098262787\n",
            "Epoch 9 iteration 0 Loss: 0.817 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 400 Loss: 0.200 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 10, Loss: 3.541060141287744e-05 Updates: 76/15001, Avg Grad: 0.02244635857641697\n",
            "Epoch: 10, Loss: 2.0994309579691617e-06 Updates: 81/16001, Avg Grad: 0.019884828478097916\n",
            "Epoch: 10, Loss: 0.019597135484218597 Updates: 86/17001, Avg Grad: 0.019956419244408607\n",
            "Epoch: 10, Loss: 4.625613655662164e-05 Updates: 91/18001, Avg Grad: 0.014827053062617779\n",
            "Epoch: 10, Loss: 5.3088351705810055e-05 Updates: 96/19001, Avg Grad: 0.024773452430963516\n",
            "Epoch 9 iteration 0 Loss: 0.138 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.199 | Acc: 94.514% (379/401)\n",
            "Test accuracy: 0.9451371431350708\n",
            "Epoch: 10, Loss: 6.649838724115398e-06 Updates: 101/20001, Avg Grad: 0.02234649658203125\n",
            "Epoch: 10, Loss: 0.003279393306002021 Updates: 106/21001, Avg Grad: 0.017161674797534943\n",
            "Epoch: 10, Loss: 2.224437048425898e-05 Updates: 111/22001, Avg Grad: 0.021486204117536545\n",
            "Epoch: 10, Loss: 1.2434743439371232e-05 Updates: 116/23001, Avg Grad: 0.01724829152226448\n",
            "Epoch: 10, Loss: 1.734465087110948e-07 Updates: 121/24001, Avg Grad: 0.018350085243582726\n",
            "Epoch 9 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.221 | Acc: 92.768% (372/401)\n",
            "Test accuracy: 0.9276807904243469\n",
            "Epoch: 10, Loss: 0.0007186176371760666 Updates: 126/25001, Avg Grad: 0.010218912735581398\n",
            "Epoch: 10, Loss: 0.00011841173545690253 Updates: 131/26001, Avg Grad: 0.022102322429418564\n",
            "Epoch: 10, Loss: 0.000250954064540565 Updates: 136/27001, Avg Grad: 0.015111212618649006\n",
            "Epoch: 10, Loss: 3.056582136196084e-05 Updates: 141/28001, Avg Grad: 0.017548050731420517\n",
            "Epoch: 10, Loss: 3.4252236218890175e-05 Updates: 146/29001, Avg Grad: 0.024986382573843002\n",
            "Epoch 9 iteration 0 Loss: 3.513 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 400 Loss: 0.259 | Acc: 93.766% (376/401)\n",
            "Test accuracy: 0.9376558661460876\n",
            "Epoch: 10, Loss: 0.0001862968347268179 Updates: 151/30001, Avg Grad: 0.016310544684529305\n",
            "Epoch: 10, Loss: 0.0019346323097124696 Updates: 156/31001, Avg Grad: 0.028455128893256187\n",
            "Epoch: 10, Loss: 9.573838906362653e-05 Updates: 161/32001, Avg Grad: 0.01809919811785221\n",
            "Epoch: 10, Loss: 2.9603053917526267e-05 Updates: 166/33001, Avg Grad: 0.017421096563339233\n",
            "Epoch: 10, Loss: 7.840821490390226e-05 Updates: 171/34001, Avg Grad: 0.013685946352779865\n",
            "Epoch 9 iteration 0 Loss: 0.011 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.286 | Acc: 91.272% (366/401)\n",
            "Test accuracy: 0.9127181768417358\n",
            "Epoch: 10, Loss: 0.0006367251044139266 Updates: 176/35001, Avg Grad: 0.02428564801812172\n",
            "Epoch: 10, Loss: 0.00031432128162123263 Updates: 181/36001, Avg Grad: 0.01092758122831583\n",
            "Epoch: 10, Loss: 1.2338009014456475e-07 Updates: 186/37001, Avg Grad: 0.016134154051542282\n",
            "Epoch: 10, Loss: 0.0014907732838764787 Updates: 191/38001, Avg Grad: 0.022458121180534363\n",
            "Epoch: 10, Loss: 0.0002579030115157366 Updates: 196/39001, Avg Grad: 0.019156888127326965\n",
            "Epoch 9 iteration 0 Loss: 0.090 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.249 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 10, Loss: 0.005328691564500332 Updates: 201/40001, Avg Grad: 0.017829880118370056\n",
            "Epoch: 10, Loss: 0.008731208741664886 Updates: 206/41001, Avg Grad: 0.017817692831158638\n",
            "Epoch: 10, Loss: 0.01703469827771187 Updates: 211/42001, Avg Grad: 0.019467072561383247\n",
            "Epoch: 10, Loss: 8.867858923622407e-06 Updates: 216/43001, Avg Grad: 0.013829728588461876\n",
            "Epoch: 10, Loss: 2.373299139435403e-05 Updates: 221/44001, Avg Grad: 0.01528968196362257\n",
            "Epoch 9 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.208 | Acc: 95.012% (381/401)\n",
            "Test accuracy: 0.9501246809959412\n",
            "Epoch: 10, Loss: 7.267673936439678e-06 Updates: 226/45001, Avg Grad: 0.018568743020296097\n",
            "Epoch: 10, Loss: 5.434495324152522e-05 Updates: 231/46001, Avg Grad: 0.013585212640464306\n",
            "Epoch: 10, Loss: 4.966771939507453e-06 Updates: 236/47001, Avg Grad: 0.013911774381995201\n",
            "Epoch: 10, Loss: 3.3867806905618636e-06 Updates: 241/48001, Avg Grad: 0.01832120306789875\n",
            "Epoch: 10, Loss: 0.0001155889403889887 Updates: 246/49001, Avg Grad: 0.024204540997743607\n",
            "Epoch 9 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.218 | Acc: 94.514% (379/401)\n",
            "Test accuracy: 0.9451371431350708\n",
            "Epoch: 10, Loss: 1.0679870683816262e-05 Updates: 251/50001, Avg Grad: 0.016834059730172157\n",
            "Epoch: 10, Loss: 0.0008468180312775075 Updates: 256/51001, Avg Grad: 0.014985951595008373\n",
            "Epoch: 10, Loss: 2.937050112450379e-06 Updates: 261/52001, Avg Grad: 0.015669476240873337\n",
            "Epoch: 10, Loss: 2.4619188479846343e-05 Updates: 266/53001, Avg Grad: 0.012766905128955841\n",
            "Epoch: 10, Loss: 8.737277426007495e-07 Updates: 271/54001, Avg Grad: 0.01878213882446289\n",
            "Epoch 9 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 400 Loss: 0.211 | Acc: 92.519% (371/401)\n",
            "Test accuracy: 0.9251870512962341\n",
            "Epoch: 10, Loss: 1.2209991837153211e-05 Updates: 276/55001, Avg Grad: 0.017227474600076675\n",
            "Epoch: 10, Loss: 0.0013684614095836878 Updates: 281/56001, Avg Grad: 0.017167383804917336\n",
            "Epoch: 10, Loss: 0.0002047935122391209 Updates: 286/57001, Avg Grad: 0.01914040930569172\n",
            "Epoch: 10, Loss: 0.0007057370967231691 Updates: 291/58001, Avg Grad: 0.017910636961460114\n",
            "Epoch: 10, Loss: 6.276004569372162e-06 Updates: 296/59001, Avg Grad: 0.0179988332092762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 10,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.1,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], train_config['num_epochs'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss, c3f1_thr_per_batch = net_trainer_thresh(net, train_loader, test_loader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-77QvPVO8xQl",
        "outputId": "29835876-1549-4287-f420-9d665125e18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 4.043 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 300 Loss: 3.299 | Acc: 11.296% (34/301)\n",
            "Test accuracy: 0.11295680701732635\n",
            "Epoch: 1, Loss: 0.012542717158794403 Updates: 0/1, Avg Grad: 0.04362868517637253, Threshold: 0.04799661785364151\n",
            "Epoch: 1, Loss: 0.01890617422759533 Updates: 3/1001, Avg Grad: 0.04069722443819046, Threshold: 0.02095271833240986\n",
            "Epoch: 1, Loss: 0.003106564050540328 Updates: 5/2001, Avg Grad: 0.017455432564020157, Threshold: 0.017455432564020157\n",
            "Epoch: 1, Loss: 0.0068766032345592976 Updates: 8/3001, Avg Grad: 0.029509222134947777, Threshold: 0.01559042651206255\n",
            "Epoch: 1, Loss: 0.015387486666440964 Updates: 10/4001, Avg Grad: 0.016506098210811615, Threshold: 0.016506098210811615\n",
            "Epoch: 1, Loss: 0.013609370216727257 Updates: 13/5001, Avg Grad: 0.02910209633409977, Threshold: 0.015135715715587139\n",
            "Epoch: 1, Loss: 0.018048569560050964 Updates: 15/6001, Avg Grad: 0.013680703938007355, Threshold: 0.013680703938007355\n",
            "Epoch: 1, Loss: 0.012401833198964596 Updates: 18/7001, Avg Grad: 0.029551703482866287, Threshold: 0.012886455282568932\n",
            "Epoch: 1, Loss: 0.01143465843051672 Updates: 20/8001, Avg Grad: 0.011761301197111607, Threshold: 0.011761301197111607\n",
            "Epoch: 1, Loss: 0.012192070484161377 Updates: 23/9001, Avg Grad: 0.021855415776371956, Threshold: 0.010895232670009136\n",
            "Epoch 0 iteration 0 Loss: 1.397 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 300 Loss: 1.827 | Acc: 47.176% (142/301)\n",
            "Test accuracy: 0.4717608094215393\n",
            "Epoch: 1, Loss: 0.011313281022012234 Updates: 25/10001, Avg Grad: 0.01114732213318348, Threshold: 0.01114732213318348\n",
            "Epoch: 1, Loss: 0.013358484953641891 Updates: 28/11001, Avg Grad: 0.01612502709031105, Threshold: 0.008532151579856873\n",
            "Epoch: 1, Loss: 0.007592252921313047 Updates: 30/12001, Avg Grad: 0.008610033430159092, Threshold: 0.008610033430159092\n",
            "Epoch: 1, Loss: 0.006702608894556761 Updates: 33/13001, Avg Grad: 0.016218746080994606, Threshold: 0.007869148626923561\n",
            "Epoch: 1, Loss: 0.004132437985390425 Updates: 35/14001, Avg Grad: 0.009212274104356766, Threshold: 0.009212274104356766\n",
            "Epoch: 1, Loss: 0.007551407441496849 Updates: 38/15001, Avg Grad: 0.020363233983516693, Threshold: 0.009467586874961853\n",
            "Epoch: 1, Loss: 0.0043780263513326645 Updates: 40/16001, Avg Grad: 0.011461948044598103, Threshold: 0.011461948044598103\n",
            "Epoch: 1, Loss: 0.011547762900590897 Updates: 43/17001, Avg Grad: 0.025122735649347305, Threshold: 0.01264454610645771\n",
            "Epoch: 1, Loss: 0.012289579957723618 Updates: 45/18001, Avg Grad: 0.011619050055742264, Threshold: 0.011619050055742264\n",
            "Epoch: 1, Loss: 0.005382504314184189 Updates: 48/19001, Avg Grad: 0.03440336138010025, Threshold: 0.020168762654066086\n",
            "Epoch 0 iteration 0 Loss: 1.929 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 300 Loss: 1.434 | Acc: 59.136% (178/301)\n",
            "Test accuracy: 0.5913621187210083\n",
            "Epoch: 1, Loss: 0.0022925930097699165 Updates: 50/20001, Avg Grad: 0.008798331022262573, Threshold: 0.008798331022262573\n",
            "Epoch: 1, Loss: 0.006489289924502373 Updates: 53/21001, Avg Grad: 0.019983500242233276, Threshold: 0.013143962249159813\n",
            "Epoch: 1, Loss: 0.008951140567660332 Updates: 55/22001, Avg Grad: 0.016680294647812843, Threshold: 0.016680294647812843\n",
            "Epoch: 1, Loss: 0.004974779672920704 Updates: 58/23001, Avg Grad: 0.023954590782523155, Threshold: 0.017015865072607994\n",
            "Epoch: 1, Loss: 0.01118734572082758 Updates: 60/24001, Avg Grad: 0.01225045882165432, Threshold: 0.01225045882165432\n",
            "Epoch: 1, Loss: 0.001668307464569807 Updates: 63/25001, Avg Grad: 0.02840084210038185, Threshold: 0.015506422147154808\n",
            "Epoch: 1, Loss: 0.013049794360995293 Updates: 65/26001, Avg Grad: 0.012848371639847755, Threshold: 0.012848371639847755\n",
            "Epoch: 1, Loss: 0.002775835106149316 Updates: 68/27001, Avg Grad: 0.016592616215348244, Threshold: 0.011461677961051464\n",
            "Epoch: 1, Loss: 0.004823818802833557 Updates: 70/28001, Avg Grad: 0.01339657325297594, Threshold: 0.01339657325297594\n",
            "Epoch: 1, Loss: 0.0019479903858155012 Updates: 73/29001, Avg Grad: 0.01805642433464527, Threshold: 0.011808265931904316\n",
            "Epoch 0 iteration 0 Loss: 3.514 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 300 Loss: 1.055 | Acc: 69.103% (208/301)\n",
            "Test accuracy: 0.6910299062728882\n",
            "Epoch: 1, Loss: 0.004548202734440565 Updates: 75/30001, Avg Grad: 0.010640798136591911, Threshold: 0.010640798136591911\n",
            "Epoch: 1, Loss: 0.003758710576221347 Updates: 78/31001, Avg Grad: 0.01665261946618557, Threshold: 0.011044600047171116\n",
            "Epoch: 1, Loss: 0.005024897400289774 Updates: 80/32001, Avg Grad: 0.012037042528390884, Threshold: 0.012037042528390884\n",
            "Epoch: 1, Loss: 0.004966073669493198 Updates: 83/33001, Avg Grad: 0.02725991979241371, Threshold: 0.010531585663557053\n",
            "Epoch: 1, Loss: 0.0014781584031879902 Updates: 85/34001, Avg Grad: 0.016841605305671692, Threshold: 0.016841605305671692\n",
            "Epoch: 1, Loss: 0.008672565221786499 Updates: 88/35001, Avg Grad: 0.019414730370044708, Threshold: 0.016052646562457085\n",
            "Epoch: 1, Loss: 0.00020054710330441594 Updates: 90/36001, Avg Grad: 0.016565371304750443, Threshold: 0.016565371304750443\n",
            "Epoch: 1, Loss: 0.004712829366326332 Updates: 93/37001, Avg Grad: 0.020497892051935196, Threshold: 0.01588568277657032\n",
            "Epoch: 1, Loss: 0.00019702773715835065 Updates: 95/38001, Avg Grad: 0.01953759230673313, Threshold: 0.01953759230673313\n",
            "Epoch: 1, Loss: 0.0009000372956506908 Updates: 98/39001, Avg Grad: 0.023669015616178513, Threshold: 0.013343890197575092\n",
            "Epoch 0 iteration 0 Loss: 0.131 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 300 Loss: 0.877 | Acc: 78.073% (235/301)\n",
            "Test accuracy: 0.7807309031486511\n",
            "Epoch: 1, Loss: 0.01043446734547615 Updates: 100/40001, Avg Grad: 0.017249327152967453, Threshold: 0.017249327152967453\n",
            "Epoch: 1, Loss: 0.0002641808532644063 Updates: 103/41001, Avg Grad: 0.02341652661561966, Threshold: 0.0158688984811306\n",
            "Epoch: 1, Loss: 0.000662194739561528 Updates: 105/42001, Avg Grad: 0.02038673125207424, Threshold: 0.02038673125207424\n",
            "Epoch: 1, Loss: 0.005350635852664709 Updates: 108/43001, Avg Grad: 0.02739979885518551, Threshold: 0.01532572228461504\n",
            "Epoch: 1, Loss: 0.000413537141866982 Updates: 110/44001, Avg Grad: 0.008098231628537178, Threshold: 0.008098231628537178\n",
            "Epoch: 1, Loss: 0.0009257430792786181 Updates: 113/45001, Avg Grad: 0.0252254456281662, Threshold: 0.015157679095864296\n",
            "Epoch: 1, Loss: 0.005166569724678993 Updates: 115/46001, Avg Grad: 0.014380937442183495, Threshold: 0.014380937442183495\n",
            "Epoch: 1, Loss: 0.000179716240381822 Updates: 118/47001, Avg Grad: 0.033604975789785385, Threshold: 0.013834089040756226\n",
            "Epoch: 1, Loss: 0.00038948384462855756 Updates: 120/48001, Avg Grad: 0.010771023109555244, Threshold: 0.010771023109555244\n",
            "Epoch: 1, Loss: 0.0006332818884402514 Updates: 123/49001, Avg Grad: 0.02443760633468628, Threshold: 0.02244909480214119\n",
            "Epoch 0 iteration 0 Loss: 0.031 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 300 Loss: 0.863 | Acc: 74.086% (223/301)\n",
            "Test accuracy: 0.7408638000488281\n",
            "Epoch: 1, Loss: 0.0012616184540092945 Updates: 125/50001, Avg Grad: 0.013399329967796803, Threshold: 0.013399329967796803\n",
            "Epoch: 1, Loss: 0.0029027077835053205 Updates: 128/51001, Avg Grad: 0.01720147393643856, Threshold: 0.012196417897939682\n",
            "Epoch: 1, Loss: 0.003650070633739233 Updates: 130/52001, Avg Grad: 0.01914753019809723, Threshold: 0.01914753019809723\n",
            "Epoch: 1, Loss: 0.001315291621722281 Updates: 133/53001, Avg Grad: 0.02685672417283058, Threshold: 0.020380137488245964\n",
            "Epoch: 1, Loss: 0.0034439926967024803 Updates: 135/54001, Avg Grad: 0.02167852595448494, Threshold: 0.02167852595448494\n",
            "Epoch: 1, Loss: 0.0020972215570509434 Updates: 138/55001, Avg Grad: 0.03129347786307335, Threshold: 0.019627224653959274\n",
            "Epoch: 1, Loss: 0.006684246007353067 Updates: 140/56001, Avg Grad: 0.01128348708152771, Threshold: 0.01128348708152771\n",
            "Epoch: 1, Loss: 0.0002488006139174104 Updates: 143/57001, Avg Grad: 0.026217639446258545, Threshold: 0.016503704711794853\n",
            "Epoch: 1, Loss: 0.0038431978318840265 Updates: 145/58001, Avg Grad: 0.02265316992998123, Threshold: 0.02265316992998123\n",
            "Epoch: 1, Loss: 0.0028021547477692366 Updates: 148/59001, Avg Grad: 0.02720325067639351, Threshold: 0.01526449341326952\n",
            "Epoch 1 iteration 0 Loss: 0.043 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 300 Loss: 0.561 | Acc: 80.399% (242/301)\n",
            "Test accuracy: 0.8039867281913757\n",
            "Epoch: 2, Loss: 0.0001318378053838387 Updates: 0/1, Avg Grad: 0.04441766068339348, Threshold: 0.048859428614377975\n",
            "Epoch: 2, Loss: 0.006939290557056665 Updates: 3/1001, Avg Grad: 0.046589285135269165, Threshold: 0.027460342273116112\n",
            "Epoch: 2, Loss: 0.0004526156699284911 Updates: 5/2001, Avg Grad: 0.02776060625910759, Threshold: 0.02776060625910759\n",
            "Epoch: 2, Loss: 0.00686570955440402 Updates: 8/3001, Avg Grad: 0.041478145867586136, Threshold: 0.015523532405495644\n",
            "Epoch: 2, Loss: 0.00024577134172432125 Updates: 10/4001, Avg Grad: 0.02656593546271324, Threshold: 0.02656593546271324\n",
            "Epoch: 2, Loss: 0.0006131348200142384 Updates: 13/5001, Avg Grad: 0.028665268793702126, Threshold: 0.022326990962028503\n",
            "Epoch: 2, Loss: 5.1046219596173614e-05 Updates: 15/6001, Avg Grad: 0.01134638860821724, Threshold: 0.01134638860821724\n",
            "Epoch: 2, Loss: 0.006490043364465237 Updates: 18/7001, Avg Grad: 0.04525725916028023, Threshold: 0.023033108562231064\n",
            "Epoch: 2, Loss: 0.00018891012587118894 Updates: 20/8001, Avg Grad: 0.013325179927051067, Threshold: 0.013325179927051067\n",
            "Epoch: 2, Loss: 0.0003996480954810977 Updates: 23/9001, Avg Grad: 0.02059946581721306, Threshold: 0.01328296959400177\n",
            "Epoch 1 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 300 Loss: 0.529 | Acc: 85.382% (257/301)\n",
            "Test accuracy: 0.8538206219673157\n",
            "Epoch: 2, Loss: 0.0014374712482094765 Updates: 25/10001, Avg Grad: 0.01742611825466156, Threshold: 0.01742611825466156\n",
            "Epoch: 2, Loss: 0.011284499429166317 Updates: 28/11001, Avg Grad: 0.02477467805147171, Threshold: 0.019815852865576744\n",
            "Epoch: 2, Loss: 0.005111349746584892 Updates: 30/12001, Avg Grad: 0.018091635778546333, Threshold: 0.018091635778546333\n",
            "Epoch: 2, Loss: 0.0012212279252707958 Updates: 33/13001, Avg Grad: 0.02393764816224575, Threshold: 0.0149816470220685\n",
            "Epoch: 2, Loss: 8.671001705806702e-05 Updates: 35/14001, Avg Grad: 0.019489457830786705, Threshold: 0.019489457830786705\n",
            "Epoch: 2, Loss: 4.058829654240981e-05 Updates: 38/15001, Avg Grad: 0.02518177404999733, Threshold: 0.019394902512431145\n",
            "Epoch: 2, Loss: 0.001688559539616108 Updates: 40/16001, Avg Grad: 0.021421628072857857, Threshold: 0.021421628072857857\n",
            "Epoch: 2, Loss: 0.00297718308866024 Updates: 43/17001, Avg Grad: 0.024995634332299232, Threshold: 0.013242760673165321\n",
            "Epoch: 2, Loss: 0.005532167386263609 Updates: 45/18001, Avg Grad: 0.018587801605463028, Threshold: 0.018587801605463028\n",
            "Epoch: 2, Loss: 0.0002858322986867279 Updates: 48/19001, Avg Grad: 0.029177600517868996, Threshold: 0.02059106156229973\n",
            "Epoch 1 iteration 0 Loss: 4.059 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 300 Loss: 0.581 | Acc: 83.056% (250/301)\n",
            "Test accuracy: 0.8305647969245911\n",
            "Epoch: 2, Loss: 0.0015471663791686296 Updates: 50/20001, Avg Grad: 0.016078146174550056, Threshold: 0.016078146174550056\n",
            "Epoch: 2, Loss: 0.0005722495261579752 Updates: 53/21001, Avg Grad: 0.029357945546507835, Threshold: 0.014064298942685127\n",
            "Epoch: 2, Loss: 0.00568314827978611 Updates: 55/22001, Avg Grad: 0.02233992889523506, Threshold: 0.02233992889523506\n",
            "Epoch: 2, Loss: 2.115773531841114e-05 Updates: 58/23001, Avg Grad: 0.024134347215294838, Threshold: 0.01445109210908413\n",
            "Epoch: 2, Loss: 0.0004208518657833338 Updates: 60/24001, Avg Grad: 0.018031243234872818, Threshold: 0.018031243234872818\n",
            "Epoch: 2, Loss: 0.0004081879451405257 Updates: 63/25001, Avg Grad: 0.03487174212932587, Threshold: 0.019976750016212463\n",
            "Epoch: 2, Loss: 0.0001853343128459528 Updates: 65/26001, Avg Grad: 0.02285803109407425, Threshold: 0.02285803109407425\n",
            "Epoch: 2, Loss: 0.0012859703274443746 Updates: 68/27001, Avg Grad: 0.031357910484075546, Threshold: 0.01828346587717533\n",
            "Epoch: 2, Loss: 0.00010177688818657771 Updates: 70/28001, Avg Grad: 0.014526328071951866, Threshold: 0.014526328071951866\n",
            "Epoch: 2, Loss: 5.9083708038087934e-05 Updates: 73/29001, Avg Grad: 0.03176784887909889, Threshold: 0.018941324204206467\n",
            "Epoch 1 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 300 Loss: 0.530 | Acc: 84.053% (253/301)\n",
            "Test accuracy: 0.840531587600708\n",
            "Epoch: 2, Loss: 0.0007368992082774639 Updates: 75/30001, Avg Grad: 0.020023029297590256, Threshold: 0.020023029297590256\n",
            "Epoch: 2, Loss: 0.00018874136731028557 Updates: 78/31001, Avg Grad: 0.0343208946287632, Threshold: 0.01694844476878643\n",
            "Epoch: 2, Loss: 8.117290417430922e-05 Updates: 80/32001, Avg Grad: 0.01871020719408989, Threshold: 0.01871020719408989\n",
            "Epoch: 2, Loss: 0.0021849304903298616 Updates: 83/33001, Avg Grad: 0.02317807823419571, Threshold: 0.015110030770301819\n",
            "Epoch: 2, Loss: 3.394380837562494e-05 Updates: 85/34001, Avg Grad: 0.012861961498856544, Threshold: 0.012861961498856544\n",
            "Epoch: 2, Loss: 0.00017301294428762048 Updates: 88/35001, Avg Grad: 0.03458762541413307, Threshold: 0.019587337970733643\n",
            "Epoch: 2, Loss: 0.00656327186152339 Updates: 90/36001, Avg Grad: 0.01854967512190342, Threshold: 0.01854967512190342\n",
            "Epoch: 2, Loss: 0.008226516656577587 Updates: 93/37001, Avg Grad: 0.02211974561214447, Threshold: 0.015348142012953758\n",
            "Epoch: 2, Loss: 0.0023683502804487944 Updates: 95/38001, Avg Grad: 0.015024331398308277, Threshold: 0.015024331398308277\n",
            "Epoch: 2, Loss: 0.0017721771728247404 Updates: 98/39001, Avg Grad: 0.02061682939529419, Threshold: 0.009018199518322945\n",
            "Epoch 1 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 300 Loss: 0.504 | Acc: 87.375% (263/301)\n",
            "Test accuracy: 0.8737541437149048\n",
            "Epoch: 2, Loss: 7.538541831308976e-05 Updates: 100/40001, Avg Grad: 0.014639295637607574, Threshold: 0.014639295637607574\n",
            "Epoch: 2, Loss: 0.00011757927131839097 Updates: 103/41001, Avg Grad: 0.03719758987426758, Threshold: 0.021167796105146408\n",
            "Epoch: 2, Loss: 0.0006939637823961675 Updates: 105/42001, Avg Grad: 0.011798037216067314, Threshold: 0.011798037216067314\n",
            "Epoch: 2, Loss: 1.49770548887318e-05 Updates: 108/43001, Avg Grad: 0.022230446338653564, Threshold: 0.012601668015122414\n",
            "Epoch: 2, Loss: 0.00010607763397274539 Updates: 110/44001, Avg Grad: 0.019088374450802803, Threshold: 0.019088374450802803\n",
            "Epoch: 2, Loss: 0.00023654675169382244 Updates: 113/45001, Avg Grad: 0.024686092510819435, Threshold: 0.01438748836517334\n",
            "Epoch: 2, Loss: 0.0033631029073148966 Updates: 115/46001, Avg Grad: 0.014368188567459583, Threshold: 0.014368188567459583\n",
            "Epoch: 2, Loss: 9.579335164744407e-05 Updates: 118/47001, Avg Grad: 0.02427295781672001, Threshold: 0.017364133149385452\n",
            "Epoch: 2, Loss: 0.0009483771864324808 Updates: 120/48001, Avg Grad: 0.01298806257545948, Threshold: 0.01298806257545948\n",
            "Epoch: 2, Loss: 0.0037824618630111217 Updates: 123/49001, Avg Grad: 0.027994895353913307, Threshold: 0.017307069152593613\n",
            "Epoch 1 iteration 0 Loss: 0.042 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 300 Loss: 0.489 | Acc: 87.375% (263/301)\n",
            "Test accuracy: 0.8737541437149048\n",
            "Epoch: 2, Loss: 0.0003159811603836715 Updates: 125/50001, Avg Grad: 0.01899939402937889, Threshold: 0.01899939402937889\n",
            "Epoch: 2, Loss: 7.746987830614671e-05 Updates: 128/51001, Avg Grad: 0.027626868337392807, Threshold: 0.022078488022089005\n",
            "Epoch: 2, Loss: 0.0010995062766596675 Updates: 130/52001, Avg Grad: 0.014459928497672081, Threshold: 0.014459928497672081\n",
            "Epoch: 2, Loss: 0.0005589341162703931 Updates: 133/53001, Avg Grad: 0.02031441032886505, Threshold: 0.01680079475045204\n",
            "Epoch: 2, Loss: 0.00013127116835676134 Updates: 135/54001, Avg Grad: 0.018875377252697945, Threshold: 0.018875377252697945\n",
            "Epoch: 2, Loss: 0.000995706650428474 Updates: 138/55001, Avg Grad: 0.028699124231934547, Threshold: 0.011683215387165546\n",
            "Epoch: 2, Loss: 0.0007408958044834435 Updates: 140/56001, Avg Grad: 0.0130122946575284, Threshold: 0.0130122946575284\n",
            "Epoch: 2, Loss: 0.0017461298266425729 Updates: 143/57001, Avg Grad: 0.02329610474407673, Threshold: 0.018384505063295364\n",
            "Epoch: 2, Loss: 1.1480969078547787e-05 Updates: 145/58001, Avg Grad: 0.012517419643700123, Threshold: 0.012517419643700123\n",
            "Epoch: 2, Loss: 0.005304345395416021 Updates: 148/59001, Avg Grad: 0.019507596269249916, Threshold: 0.013462500646710396\n",
            "Epoch 2 iteration 0 Loss: 0.808 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 300 Loss: 0.533 | Acc: 84.718% (255/301)\n",
            "Test accuracy: 0.8471760749816895\n",
            "Epoch: 3, Loss: 0.012784196063876152 Updates: 0/1, Avg Grad: 0.02380932867527008, Threshold: 0.02619026228785515\n",
            "Epoch: 3, Loss: 0.00016579616931267083 Updates: 3/1001, Avg Grad: 0.035308826714754105, Threshold: 0.02449689246714115\n",
            "Epoch: 3, Loss: 0.0004809538659173995 Updates: 5/2001, Avg Grad: 0.023971419781446457, Threshold: 0.023971419781446457\n",
            "Epoch: 3, Loss: 0.004546577576547861 Updates: 8/3001, Avg Grad: 0.028313279151916504, Threshold: 0.013524144887924194\n",
            "Epoch: 3, Loss: 0.006132000591605902 Updates: 10/4001, Avg Grad: 0.021910974755883217, Threshold: 0.021910974755883217\n",
            "Epoch: 3, Loss: 1.298825372941792e-05 Updates: 13/5001, Avg Grad: 0.026870880275964737, Threshold: 0.018324384465813637\n",
            "Epoch: 3, Loss: 5.2521569159580395e-05 Updates: 15/6001, Avg Grad: 0.014657306484878063, Threshold: 0.014657306484878063\n",
            "Epoch: 3, Loss: 0.012794844806194305 Updates: 18/7001, Avg Grad: 0.01953817345201969, Threshold: 0.012340595945715904\n",
            "Epoch: 3, Loss: 0.004358821548521519 Updates: 20/8001, Avg Grad: 0.015415106900036335, Threshold: 0.015415106900036335\n",
            "Epoch: 3, Loss: 0.00017790577840059996 Updates: 23/9001, Avg Grad: 0.027523186057806015, Threshold: 0.01878201588988304\n",
            "Epoch 2 iteration 0 Loss: 0.032 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 300 Loss: 0.493 | Acc: 86.711% (261/301)\n",
            "Test accuracy: 0.8671096563339233\n",
            "Epoch: 3, Loss: 0.0014003574615344405 Updates: 25/10001, Avg Grad: 0.02499553933739662, Threshold: 0.02499553933739662\n",
            "Epoch: 3, Loss: 3.5512395697878674e-05 Updates: 28/11001, Avg Grad: 0.02772035077214241, Threshold: 0.018208397552371025\n",
            "Epoch: 3, Loss: 0.000771477585658431 Updates: 30/12001, Avg Grad: 0.016174975782632828, Threshold: 0.016174975782632828\n",
            "Epoch: 3, Loss: 9.163883805740625e-05 Updates: 33/13001, Avg Grad: 0.029228370636701584, Threshold: 0.020183231681585312\n",
            "Epoch: 3, Loss: 8.710366091690958e-05 Updates: 35/14001, Avg Grad: 0.01685166358947754, Threshold: 0.01685166358947754\n",
            "Epoch: 3, Loss: 0.003928311634808779 Updates: 38/15001, Avg Grad: 0.029803548008203506, Threshold: 0.022271903231739998\n",
            "Epoch: 3, Loss: 0.0005180409643799067 Updates: 40/16001, Avg Grad: 0.015105660073459148, Threshold: 0.015105660073459148\n",
            "Epoch: 3, Loss: 5.963031071587466e-05 Updates: 43/17001, Avg Grad: 0.022378962486982346, Threshold: 0.01253952831029892\n",
            "Epoch: 3, Loss: 0.0022872446570545435 Updates: 45/18001, Avg Grad: 0.017687510699033737, Threshold: 0.017687510699033737\n",
            "Epoch: 3, Loss: 4.2269461118848994e-05 Updates: 48/19001, Avg Grad: 0.03339322656393051, Threshold: 0.022442970424890518\n",
            "Epoch 2 iteration 0 Loss: 0.016 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 300 Loss: 0.450 | Acc: 88.372% (266/301)\n",
            "Test accuracy: 0.8837209343910217\n",
            "Epoch: 3, Loss: 0.00028090423438698053 Updates: 50/20001, Avg Grad: 0.01914987713098526, Threshold: 0.01914987713098526\n",
            "Epoch: 3, Loss: 0.002481757663190365 Updates: 53/21001, Avg Grad: 0.03165848180651665, Threshold: 0.0211110170930624\n",
            "Epoch: 3, Loss: 2.719681287999265e-05 Updates: 55/22001, Avg Grad: 0.01864505186676979, Threshold: 0.01864505186676979\n",
            "Epoch: 3, Loss: 9.683816460892558e-05 Updates: 57/23001, Avg Grad: 0.014697018079459667, Threshold: 0.014697018079459667\n",
            "Epoch: 3, Loss: 0.0042864661663770676 Updates: 60/24001, Avg Grad: 0.03542476147413254, Threshold: 0.025134697556495667\n",
            "Epoch: 3, Loss: 0.0005741713684983552 Updates: 62/25001, Avg Grad: 0.010612326674163342, Threshold: 0.010612326674163342\n",
            "Epoch: 3, Loss: 8.483920100843534e-05 Updates: 65/26001, Avg Grad: 0.025482693687081337, Threshold: 0.022273775190114975\n",
            "Epoch: 3, Loss: 3.086320430156775e-05 Updates: 67/27001, Avg Grad: 0.011523505672812462, Threshold: 0.011523505672812462\n",
            "Epoch: 3, Loss: 8.939811232266948e-05 Updates: 70/28001, Avg Grad: 0.028808046132326126, Threshold: 0.015455788001418114\n",
            "Epoch: 3, Loss: 0.0005102363647893071 Updates: 72/29001, Avg Grad: 0.015026780776679516, Threshold: 0.015026780776679516\n",
            "Epoch 2 iteration 0 Loss: 0.019 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 300 Loss: 0.465 | Acc: 85.382% (257/301)\n",
            "Test accuracy: 0.8538206219673157\n",
            "Epoch: 3, Loss: 0.006620179396122694 Updates: 75/30001, Avg Grad: 0.027609366923570633, Threshold: 0.014052925631403923\n",
            "Epoch: 3, Loss: 0.00019841825996991247 Updates: 77/31001, Avg Grad: 0.021260317414999008, Threshold: 0.021260317414999008\n",
            "Epoch: 3, Loss: 0.0006384759326465428 Updates: 80/32001, Avg Grad: 0.023484010249376297, Threshold: 0.016544874757528305\n",
            "Epoch: 3, Loss: 0.0070947702042758465 Updates: 82/33001, Avg Grad: 0.016121957451105118, Threshold: 0.016121957451105118\n",
            "Epoch: 3, Loss: 0.002771436469629407 Updates: 85/34001, Avg Grad: 0.024599384516477585, Threshold: 0.01687566563487053\n",
            "Epoch: 3, Loss: 0.00038953564944677055 Updates: 87/35001, Avg Grad: 0.01724075712263584, Threshold: 0.01724075712263584\n",
            "Epoch: 3, Loss: 0.016036594286561012 Updates: 90/36001, Avg Grad: 0.030815143138170242, Threshold: 0.02542358636856079\n",
            "Epoch: 3, Loss: 9.408739424543455e-05 Updates: 92/37001, Avg Grad: 0.01074404176324606, Threshold: 0.01074404176324606\n",
            "Epoch: 3, Loss: 0.00031519215553998947 Updates: 95/38001, Avg Grad: 0.021880337968468666, Threshold: 0.011955673806369305\n",
            "Epoch: 3, Loss: 0.0002931556955445558 Updates: 97/39001, Avg Grad: 0.018237626180052757, Threshold: 0.018237626180052757\n",
            "Epoch 2 iteration 0 Loss: 0.014 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 300 Loss: 0.448 | Acc: 88.372% (266/301)\n",
            "Test accuracy: 0.8837209343910217\n",
            "Epoch: 3, Loss: 0.0014822821831330657 Updates: 100/40001, Avg Grad: 0.0312584713101387, Threshold: 0.021951602771878242\n",
            "Epoch: 3, Loss: 0.02489089034497738 Updates: 102/41001, Avg Grad: 0.018192727118730545, Threshold: 0.018192727118730545\n",
            "Epoch: 3, Loss: 0.0055937860161066055 Updates: 105/42001, Avg Grad: 0.034548934549093246, Threshold: 0.015893545001745224\n",
            "Epoch: 3, Loss: 0.012921304441988468 Updates: 107/43001, Avg Grad: 0.016858305782079697, Threshold: 0.016858305782079697\n",
            "Epoch: 3, Loss: 0.0006554669816978276 Updates: 110/44001, Avg Grad: 0.020461920648813248, Threshold: 0.013884087093174458\n",
            "Epoch: 3, Loss: 0.00014842170639894903 Updates: 112/45001, Avg Grad: 0.019480567425489426, Threshold: 0.019480567425489426\n",
            "Epoch: 3, Loss: 6.232946907402948e-05 Updates: 115/46001, Avg Grad: 0.028388475999236107, Threshold: 0.017343584448099136\n",
            "Epoch: 3, Loss: 4.904515662929043e-05 Updates: 117/47001, Avg Grad: 0.02076876536011696, Threshold: 0.02076876536011696\n",
            "Epoch: 3, Loss: 0.0003254855109844357 Updates: 120/48001, Avg Grad: 0.030173499137163162, Threshold: 0.019050585106015205\n",
            "Epoch: 3, Loss: 0.00021518282301258296 Updates: 122/49001, Avg Grad: 0.017889801412820816, Threshold: 0.017889801412820816\n",
            "Epoch 2 iteration 0 Loss: 0.104 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 300 Loss: 0.451 | Acc: 87.708% (264/301)\n",
            "Test accuracy: 0.8770763874053955\n",
            "Epoch: 3, Loss: 0.0034504381474107504 Updates: 125/50001, Avg Grad: 0.03316361457109451, Threshold: 0.02407912164926529\n",
            "Epoch: 3, Loss: 0.00021132294205017388 Updates: 127/51001, Avg Grad: 0.019185839220881462, Threshold: 0.019185839220881462\n",
            "Epoch: 3, Loss: 2.7100182705908082e-05 Updates: 130/52001, Avg Grad: 0.026448363438248634, Threshold: 0.021932872012257576\n",
            "Epoch: 3, Loss: 2.5884744900395162e-05 Updates: 132/53001, Avg Grad: 0.015630142763257027, Threshold: 0.015630142763257027\n",
            "Epoch: 3, Loss: 9.19367084861733e-05 Updates: 135/54001, Avg Grad: 0.02795277163386345, Threshold: 0.019695986062288284\n",
            "Epoch: 3, Loss: 0.005799778737127781 Updates: 137/55001, Avg Grad: 0.014604225754737854, Threshold: 0.014604225754737854\n",
            "Epoch: 3, Loss: 0.0005782629596069455 Updates: 140/56001, Avg Grad: 0.038822244852781296, Threshold: 0.02521349862217903\n",
            "Epoch: 3, Loss: 0.0019292623037472367 Updates: 142/57001, Avg Grad: 0.022017929702997208, Threshold: 0.022017929702997208\n",
            "Epoch: 3, Loss: 3.2705160265322775e-05 Updates: 145/58001, Avg Grad: 0.029201623052358627, Threshold: 0.018024135380983353\n",
            "Epoch: 3, Loss: 3.706392817548476e-05 Updates: 147/59001, Avg Grad: 0.022808518260717392, Threshold: 0.022808518260717392\n",
            "Epoch 3 iteration 0 Loss: 0.136 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 300 Loss: 0.367 | Acc: 89.369% (269/301)\n",
            "Test accuracy: 0.8936877250671387\n",
            "Epoch: 4, Loss: 4.903335138806142e-05 Updates: 0/1, Avg Grad: 0.012761106714606285, Threshold: 0.013821393251419067\n",
            "Epoch: 4, Loss: 0.012451283633708954 Updates: 3/1001, Avg Grad: 0.053687769919633865, Threshold: 0.019443495199084282\n",
            "Epoch: 4, Loss: 4.984838960808702e-05 Updates: 5/2001, Avg Grad: 0.020061833783984184, Threshold: 0.020061833783984184\n",
            "Epoch: 4, Loss: 0.0038358098827302456 Updates: 8/3001, Avg Grad: 0.030658314004540443, Threshold: 0.016357598826289177\n",
            "Epoch: 4, Loss: 0.0030109647195786238 Updates: 10/4001, Avg Grad: 0.024418944492936134, Threshold: 0.024418944492936134\n",
            "Epoch: 4, Loss: 0.034484557807445526 Updates: 13/5001, Avg Grad: 0.02028178609907627, Threshold: 0.01645083911716938\n",
            "Epoch: 4, Loss: 0.00639151968061924 Updates: 15/6001, Avg Grad: 0.016161346808075905, Threshold: 0.016161346808075905\n",
            "Epoch: 4, Loss: 0.00027622233028523624 Updates: 18/7001, Avg Grad: 0.027196701616048813, Threshold: 0.013742022216320038\n",
            "Epoch: 4, Loss: 0.0006234063766896725 Updates: 20/8001, Avg Grad: 0.014242971315979958, Threshold: 0.014242971315979958\n",
            "Epoch: 4, Loss: 0.0003991698904428631 Updates: 22/9001, Avg Grad: 0.020228752866387367, Threshold: 0.020228752866387367\n",
            "Epoch 3 iteration 0 Loss: 0.942 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 300 Loss: 0.383 | Acc: 86.379% (260/301)\n",
            "Test accuracy: 0.8637873530387878\n",
            "Epoch: 4, Loss: 0.008371411822736263 Updates: 25/10001, Avg Grad: 0.02842436358332634, Threshold: 0.02255641110241413\n",
            "Epoch: 4, Loss: 2.7212816348765045e-05 Updates: 27/11001, Avg Grad: 0.01652410998940468, Threshold: 0.01652410998940468\n",
            "Epoch: 4, Loss: 0.00026066359714604914 Updates: 30/12001, Avg Grad: 0.030312690883874893, Threshold: 0.017185447737574577\n",
            "Epoch: 4, Loss: 0.003113097744062543 Updates: 32/13001, Avg Grad: 0.016304360702633858, Threshold: 0.016304360702633858\n",
            "Epoch: 4, Loss: 0.0001599384704604745 Updates: 34/14001, Avg Grad: 0.01672028936445713, Threshold: 0.01672028936445713\n",
            "Epoch: 4, Loss: 0.0005158506100997329 Updates: 37/15001, Avg Grad: 0.02684663236141205, Threshold: 0.017664095386862755\n",
            "Epoch: 4, Loss: 0.0004292517260182649 Updates: 39/16001, Avg Grad: 0.01476280391216278, Threshold: 0.01476280391216278\n",
            "Epoch: 4, Loss: 0.0009022243320941925 Updates: 42/17001, Avg Grad: 0.019516928121447563, Threshold: 0.009379206225275993\n",
            "Epoch: 4, Loss: 0.0003816304379142821 Updates: 44/18001, Avg Grad: 0.01905142515897751, Threshold: 0.01905142515897751\n",
            "Epoch: 4, Loss: 0.00013862950436305255 Updates: 47/19001, Avg Grad: 0.018907146528363228, Threshold: 0.01413814164698124\n",
            "Epoch 3 iteration 0 Loss: 0.017 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 300 Loss: 0.313 | Acc: 89.037% (268/301)\n",
            "Test accuracy: 0.8903654217720032\n",
            "Epoch: 4, Loss: 0.00013271607167553157 Updates: 49/20001, Avg Grad: 0.017701424658298492, Threshold: 0.013913827016949654\n",
            "Epoch: 4, Loss: 6.354785000439733e-05 Updates: 51/21001, Avg Grad: 0.01249580830335617, Threshold: 0.01249580830335617\n",
            "Epoch: 4, Loss: 0.00023179635172709823 Updates: 54/22001, Avg Grad: 0.03069423697888851, Threshold: 0.015378326177597046\n",
            "Epoch: 4, Loss: 0.00015617662575095892 Updates: 56/23001, Avg Grad: 0.015652066096663475, Threshold: 0.015652066096663475\n",
            "Epoch: 4, Loss: 0.010567584075033665 Updates: 59/24001, Avg Grad: 0.0257837176322937, Threshold: 0.019482864066958427\n",
            "Epoch: 4, Loss: 0.01662176288664341 Updates: 61/25001, Avg Grad: 0.01301337219774723, Threshold: 0.01301337219774723\n",
            "Epoch: 4, Loss: 0.001897858688607812 Updates: 64/26001, Avg Grad: 0.03932752460241318, Threshold: 0.021734565496444702\n",
            "Epoch: 4, Loss: 0.00041745026828721166 Updates: 66/27001, Avg Grad: 0.010119798593223095, Threshold: 0.010119798593223095\n",
            "Epoch: 4, Loss: 6.493832916021347e-05 Updates: 69/28001, Avg Grad: 0.04054804891347885, Threshold: 0.027180572971701622\n",
            "Epoch: 4, Loss: 0.0016178978839889169 Updates: 71/29001, Avg Grad: 0.013790098950266838, Threshold: 0.013790098950266838\n",
            "Epoch 3 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 300 Loss: 0.435 | Acc: 87.043% (262/301)\n",
            "Test accuracy: 0.8704319000244141\n",
            "Epoch: 4, Loss: 3.625271710916422e-05 Updates: 74/30001, Avg Grad: 0.032314058393239975, Threshold: 0.013757346197962761\n",
            "Epoch: 4, Loss: 0.0029359448235481977 Updates: 76/31001, Avg Grad: 0.01917927898466587, Threshold: 0.014081875793635845\n",
            "Epoch: 4, Loss: 0.0003464615729171783 Updates: 78/32001, Avg Grad: 0.015908999368548393, Threshold: 0.015908999368548393\n",
            "Epoch: 4, Loss: 0.000405170168960467 Updates: 81/33001, Avg Grad: 0.035777024924755096, Threshold: 0.02867799438536167\n",
            "Epoch: 4, Loss: 0.004312606528401375 Updates: 83/34001, Avg Grad: 0.0197642482817173, Threshold: 0.0197642482817173\n",
            "Epoch: 4, Loss: 0.006989607587456703 Updates: 86/35001, Avg Grad: 0.03326381370425224, Threshold: 0.01819145679473877\n",
            "Epoch: 4, Loss: 5.730859993491322e-05 Updates: 88/36001, Avg Grad: 0.01896105706691742, Threshold: 0.01896105706691742\n",
            "Epoch: 4, Loss: 0.0002061738632619381 Updates: 91/37001, Avg Grad: 0.02545749582350254, Threshold: 0.01440785638988018\n",
            "Epoch: 4, Loss: 0.00022772830561734736 Updates: 93/38001, Avg Grad: 0.020456809550523758, Threshold: 0.020456809550523758\n",
            "Epoch: 4, Loss: 3.662491144496016e-05 Updates: 96/39001, Avg Grad: 0.0173281729221344, Threshold: 0.013336819596588612\n",
            "Epoch 3 iteration 0 Loss: 0.513 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 300 Loss: 0.477 | Acc: 86.047% (259/301)\n",
            "Test accuracy: 0.8604651093482971\n",
            "Epoch: 4, Loss: 0.0002653030096553266 Updates: 98/40001, Avg Grad: 0.02240954339504242, Threshold: 0.02240954339504242\n",
            "Epoch: 4, Loss: 0.00014927798474673182 Updates: 101/41001, Avg Grad: 0.026530945673584938, Threshold: 0.02196546271443367\n",
            "Epoch: 4, Loss: 0.004622718784958124 Updates: 103/42001, Avg Grad: 0.024563582614064217, Threshold: 0.024563582614064217\n",
            "Epoch: 4, Loss: 0.00033574376720935106 Updates: 106/43001, Avg Grad: 0.03373020142316818, Threshold: 0.019208647310733795\n",
            "Epoch: 4, Loss: 0.000858794548548758 Updates: 108/44001, Avg Grad: 0.01953745074570179, Threshold: 0.01953745074570179\n",
            "Epoch: 4, Loss: 0.005382027477025986 Updates: 111/45001, Avg Grad: 0.023607496172189713, Threshold: 0.013888674788177013\n",
            "Epoch: 4, Loss: 0.008973986841738224 Updates: 113/46001, Avg Grad: 0.01716250739991665, Threshold: 0.01716250739991665\n",
            "Epoch: 4, Loss: 0.0011100067058578134 Updates: 116/47001, Avg Grad: 0.030440175905823708, Threshold: 0.01572374440729618\n",
            "Epoch: 4, Loss: 0.0025792738888412714 Updates: 118/48001, Avg Grad: 0.027889948338270187, Threshold: 0.01659795269370079\n",
            "Epoch: 4, Loss: 0.0006206057732924819 Updates: 120/49001, Avg Grad: 0.017300009727478027, Threshold: 0.017300009727478027\n",
            "Epoch 3 iteration 0 Loss: 0.045 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 300 Loss: 0.434 | Acc: 85.050% (256/301)\n",
            "Test accuracy: 0.8504983186721802\n",
            "Epoch: 4, Loss: 0.0004830626421608031 Updates: 123/50001, Avg Grad: 0.023184828460216522, Threshold: 0.017963901162147522\n",
            "Epoch: 4, Loss: 0.0018770667957141995 Updates: 125/51001, Avg Grad: 0.015626365318894386, Threshold: 0.015626365318894386\n",
            "Epoch: 4, Loss: 0.0007559132063761353 Updates: 128/52001, Avg Grad: 0.03516191989183426, Threshold: 0.020280132070183754\n",
            "Epoch: 4, Loss: 0.0014114470686763525 Updates: 130/53001, Avg Grad: 0.014787857420742512, Threshold: 0.014787857420742512\n",
            "Epoch: 4, Loss: 0.0002880497486330569 Updates: 133/54001, Avg Grad: 0.027215968817472458, Threshold: 0.02004336193203926\n",
            "Epoch: 4, Loss: 8.198037539841607e-05 Updates: 135/55001, Avg Grad: 0.01669204980134964, Threshold: 0.01669204980134964\n",
            "Epoch: 4, Loss: 0.0019992825109511614 Updates: 138/56001, Avg Grad: 0.027074608951807022, Threshold: 0.013950088992714882\n",
            "Epoch: 4, Loss: 3.394795203348622e-05 Updates: 140/57001, Avg Grad: 0.024968504905700684, Threshold: 0.016818074509501457\n",
            "Epoch: 4, Loss: 5.339855488273315e-05 Updates: 142/58001, Avg Grad: 0.020161816850304604, Threshold: 0.020161816850304604\n",
            "Epoch: 4, Loss: 0.0006896448903717101 Updates: 145/59001, Avg Grad: 0.025728311389684677, Threshold: 0.019094953313469887\n",
            "Epoch 4 iteration 0 Loss: 0.578 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 300 Loss: 0.422 | Acc: 87.375% (263/301)\n",
            "Test accuracy: 0.8737541437149048\n",
            "Epoch: 5, Loss: 5.6009717809502035e-05 Updates: 0/1, Avg Grad: 0.1298852264881134, Threshold: 0.14287374913692474\n",
            "Epoch: 5, Loss: 3.651426231954247e-05 Updates: 0/1001, Avg Grad: 0.12271450459957123, Threshold: 0.14287374913692474\n",
            "Epoch: 5, Loss: 2.5230237952200696e-06 Updates: 1/2001, Avg Grad: 0.14649079740047455, Threshold: 0.14287374913692474\n",
            "Epoch: 5, Loss: 6.002314330544323e-05 Updates: 3/3001, Avg Grad: 0.04200385883450508, Threshold: 0.04200385883450508\n",
            "Epoch: 5, Loss: 0.00014270926476456225 Updates: 6/4001, Avg Grad: 0.06947252154350281, Threshold: 0.040206778794527054\n",
            "Epoch: 5, Loss: 0.0003163711226079613 Updates: 8/5001, Avg Grad: 0.03592449799180031, Threshold: 0.03592449799180031\n",
            "Epoch: 5, Loss: 0.02449365332722664 Updates: 11/6001, Avg Grad: 0.045342642813920975, Threshold: 0.02396455779671669\n",
            "Epoch: 5, Loss: 0.00227280892431736 Updates: 13/7001, Avg Grad: 0.017993131652474403, Threshold: 0.017993131652474403\n",
            "Epoch: 5, Loss: 5.32564299646765e-05 Updates: 16/8001, Avg Grad: 0.04927394539117813, Threshold: 0.026463091373443604\n",
            "Epoch: 5, Loss: 7.331243250519037e-05 Updates: 18/9001, Avg Grad: 0.016811128705739975, Threshold: 0.016811128705739975\n",
            "Epoch 4 iteration 0 Loss: 0.319 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 300 Loss: 0.450 | Acc: 89.369% (269/301)\n",
            "Test accuracy: 0.8936877250671387\n",
            "Epoch: 5, Loss: 0.00011090171756222844 Updates: 21/10001, Avg Grad: 0.027648944407701492, Threshold: 0.019752640277147293\n",
            "Epoch: 5, Loss: 2.868544470402412e-06 Updates: 23/11001, Avg Grad: 0.018681447952985764, Threshold: 0.018681447952985764\n",
            "Epoch: 5, Loss: 6.096594006521627e-05 Updates: 26/12001, Avg Grad: 0.04140504077076912, Threshold: 0.027147868648171425\n",
            "Epoch: 5, Loss: 0.00034962670179083943 Updates: 28/13001, Avg Grad: 0.01922203041613102, Threshold: 0.01922203041613102\n",
            "Epoch: 5, Loss: 0.00039131849189288914 Updates: 31/14001, Avg Grad: 0.033689942210912704, Threshold: 0.016282176598906517\n",
            "Epoch: 5, Loss: 0.00019767004414461553 Updates: 33/15001, Avg Grad: 0.018881335854530334, Threshold: 0.018881335854530334\n",
            "Epoch: 5, Loss: 0.0001612452615518123 Updates: 36/16001, Avg Grad: 0.04173626750707626, Threshold: 0.02225230261683464\n",
            "Epoch: 5, Loss: 0.0011998852714896202 Updates: 38/17001, Avg Grad: 0.01987174153327942, Threshold: 0.01987174153327942\n",
            "Epoch: 5, Loss: 4.8430098104290664e-05 Updates: 41/18001, Avg Grad: 0.04355984926223755, Threshold: 0.028474699705839157\n",
            "Epoch: 5, Loss: 0.00043193792225793004 Updates: 43/19001, Avg Grad: 0.01686711236834526, Threshold: 0.01686711236834526\n",
            "Epoch 4 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 300 Loss: 0.369 | Acc: 88.040% (265/301)\n",
            "Test accuracy: 0.880398690700531\n",
            "Epoch: 5, Loss: 0.0015490635996684432 Updates: 46/20001, Avg Grad: 0.02597052976489067, Threshold: 0.025649001821875572\n",
            "Epoch: 5, Loss: 7.09518208168447e-05 Updates: 48/21001, Avg Grad: 0.02210783027112484, Threshold: 0.02210783027112484\n",
            "Epoch: 5, Loss: 2.800297261273954e-05 Updates: 51/22001, Avg Grad: 0.029809609055519104, Threshold: 0.01730780489742756\n",
            "Epoch: 5, Loss: 9.12429368327139e-06 Updates: 53/23001, Avg Grad: 0.030143097043037415, Threshold: 0.030143097043037415\n",
            "Epoch: 5, Loss: 0.0007467587129212916 Updates: 56/24001, Avg Grad: 0.040370114147663116, Threshold: 0.020790766924619675\n",
            "Epoch: 5, Loss: 0.0009076541755348444 Updates: 58/25001, Avg Grad: 0.017672305926680565, Threshold: 0.017672305926680565\n",
            "Epoch: 5, Loss: 0.00037319608964025974 Updates: 61/26001, Avg Grad: 0.03608272224664688, Threshold: 0.016318971291184425\n",
            "Epoch: 5, Loss: 0.00048477997188456357 Updates: 63/27001, Avg Grad: 0.018870782107114792, Threshold: 0.018870782107114792\n",
            "Epoch: 5, Loss: 0.001143372617661953 Updates: 66/28001, Avg Grad: 0.02571137621998787, Threshold: 0.01676110364496708\n",
            "Epoch: 5, Loss: 5.421082732937066e-06 Updates: 68/29001, Avg Grad: 0.019081024453043938, Threshold: 0.019081024453043938\n",
            "Epoch 4 iteration 0 Loss: 0.012 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 300 Loss: 0.349 | Acc: 89.701% (270/301)\n",
            "Test accuracy: 0.8970099687576294\n",
            "Epoch: 5, Loss: 0.0002616716956254095 Updates: 71/30001, Avg Grad: 0.043947115540504456, Threshold: 0.023771408945322037\n",
            "Epoch: 5, Loss: 8.959775004768744e-05 Updates: 73/31001, Avg Grad: 0.01928500272333622, Threshold: 0.01928500272333622\n",
            "Epoch: 5, Loss: 0.00023793996660970151 Updates: 76/32001, Avg Grad: 0.03760860487818718, Threshold: 0.020745856687426567\n",
            "Epoch: 5, Loss: 0.002506826538592577 Updates: 78/33001, Avg Grad: 0.02373032458126545, Threshold: 0.02373032458126545\n",
            "Epoch: 5, Loss: 1.3226048395154066e-05 Updates: 81/34001, Avg Grad: 0.03605334833264351, Threshold: 0.02094225212931633\n",
            "Epoch: 5, Loss: 0.0010956499027088284 Updates: 83/35001, Avg Grad: 0.040326692163944244, Threshold: 0.03779027983546257\n",
            "Epoch: 5, Loss: 0.0003863526799250394 Updates: 85/36001, Avg Grad: 0.02306893840432167, Threshold: 0.02306893840432167\n",
            "Epoch: 5, Loss: 0.00868026353418827 Updates: 88/37001, Avg Grad: 0.03267577290534973, Threshold: 0.019216936081647873\n",
            "Epoch: 5, Loss: 4.405226263770601e-06 Updates: 90/38001, Avg Grad: 0.013746583834290504, Threshold: 0.013746583834290504\n",
            "Epoch: 5, Loss: 0.011612605303525925 Updates: 93/39001, Avg Grad: 0.026152903214097023, Threshold: 0.014518974348902702\n",
            "Epoch 4 iteration 0 Loss: 0.029 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 300 Loss: 0.379 | Acc: 89.701% (270/301)\n",
            "Test accuracy: 0.8970099687576294\n",
            "Epoch: 5, Loss: 0.01677911914885044 Updates: 95/40001, Avg Grad: 0.020243871957063675, Threshold: 0.020243871957063675\n",
            "Epoch: 5, Loss: 0.0017179260030388832 Updates: 98/41001, Avg Grad: 0.03004009649157524, Threshold: 0.02207079902291298\n",
            "Epoch: 5, Loss: 0.0009448248310945928 Updates: 100/42001, Avg Grad: 0.019762735813856125, Threshold: 0.019762735813856125\n",
            "Epoch: 5, Loss: 0.00016797296120785177 Updates: 103/43001, Avg Grad: 0.025357335805892944, Threshold: 0.018512843176722527\n",
            "Epoch: 5, Loss: 2.418263284198474e-05 Updates: 105/44001, Avg Grad: 0.017894607037305832, Threshold: 0.017894607037305832\n",
            "Epoch: 5, Loss: 0.0006920525920577347 Updates: 108/45001, Avg Grad: 0.021764373406767845, Threshold: 0.017429271712899208\n",
            "Epoch: 5, Loss: 0.000104330807516817 Updates: 110/46001, Avg Grad: 0.0233149416744709, Threshold: 0.0233149416744709\n",
            "Epoch: 5, Loss: 2.542683432693593e-06 Updates: 113/47001, Avg Grad: 0.023335836827754974, Threshold: 0.022563857957720757\n",
            "Epoch: 5, Loss: 0.00022311031352728605 Updates: 115/48001, Avg Grad: 0.013440331444144249, Threshold: 0.013440331444144249\n",
            "Epoch: 5, Loss: 0.00255412794649601 Updates: 118/49001, Avg Grad: 0.032745640724897385, Threshold: 0.02217879332602024\n",
            "Epoch 4 iteration 0 Loss: 1.745 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 300 Loss: 0.454 | Acc: 88.704% (267/301)\n",
            "Test accuracy: 0.8870431780815125\n",
            "Epoch: 5, Loss: 0.0001623134157853201 Updates: 120/50001, Avg Grad: 0.020291101187467575, Threshold: 0.020291101187467575\n",
            "Epoch: 5, Loss: 0.0008990902570076287 Updates: 123/51001, Avg Grad: 0.03004644252359867, Threshold: 0.020589465275406837\n",
            "Epoch: 5, Loss: 0.0007888002437539399 Updates: 125/52001, Avg Grad: 0.017365025356411934, Threshold: 0.017365025356411934\n",
            "Epoch: 5, Loss: 0.0008653676486574113 Updates: 128/53001, Avg Grad: 0.03527724742889404, Threshold: 0.022488418966531754\n",
            "Epoch: 5, Loss: 1.788275403669104e-05 Updates: 130/54001, Avg Grad: 0.019246071577072144, Threshold: 0.019246071577072144\n",
            "Epoch: 5, Loss: 0.0007406845688819885 Updates: 133/55001, Avg Grad: 0.02043168619275093, Threshold: 0.011907005682587624\n",
            "Epoch: 5, Loss: 0.006068476475775242 Updates: 135/56001, Avg Grad: 0.01566319540143013, Threshold: 0.01566319540143013\n",
            "Epoch: 5, Loss: 0.0006230517174117267 Updates: 138/57001, Avg Grad: 0.03462424501776695, Threshold: 0.01917872205376625\n",
            "Epoch: 5, Loss: 0.01422860100865364 Updates: 140/58001, Avg Grad: 0.013356933370232582, Threshold: 0.013356933370232582\n",
            "Epoch: 5, Loss: 3.092244151048362e-05 Updates: 142/59001, Avg Grad: 0.01935720257461071, Threshold: 0.01935720257461071\n",
            "Epoch 5 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 300 Loss: 0.363 | Acc: 89.701% (270/301)\n",
            "Test accuracy: 0.8970099687576294\n",
            "Epoch: 6, Loss: 0.00016720635176170617 Updates: 0/1, Avg Grad: 0.00906538125127554, Threshold: 0.009284569881856441\n",
            "Epoch: 6, Loss: 9.428565590496873e-07 Updates: 3/1001, Avg Grad: 0.031163031235337257, Threshold: 0.02007112465798855\n",
            "Epoch: 6, Loss: 0.0002677013399079442 Updates: 5/2001, Avg Grad: 0.015417340211570263, Threshold: 0.015417340211570263\n",
            "Epoch: 6, Loss: 0.00035524979466572404 Updates: 8/3001, Avg Grad: 0.031220361590385437, Threshold: 0.01895971968770027\n",
            "Epoch: 6, Loss: 5.880743628949858e-05 Updates: 10/4001, Avg Grad: 0.028654228895902634, Threshold: 0.028654228895902634\n",
            "Epoch: 6, Loss: 0.00019854486163239926 Updates: 13/5001, Avg Grad: 0.034777965396642685, Threshold: 0.017352024093270302\n",
            "Epoch: 6, Loss: 9.191680874209851e-05 Updates: 15/6001, Avg Grad: 0.017780553549528122, Threshold: 0.017780553549528122\n",
            "Epoch: 6, Loss: 0.0002453791967127472 Updates: 18/7001, Avg Grad: 0.03267388790845871, Threshold: 0.0180690698325634\n",
            "Epoch: 6, Loss: 2.532732287363615e-05 Updates: 20/8001, Avg Grad: 0.020628202706575394, Threshold: 0.020628202706575394\n",
            "Epoch: 6, Loss: 0.0010100919753313065 Updates: 23/9001, Avg Grad: 0.038831692188978195, Threshold: 0.020123623311519623\n",
            "Epoch 5 iteration 0 Loss: 0.189 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 300 Loss: 0.305 | Acc: 91.030% (274/301)\n",
            "Test accuracy: 0.9102990031242371\n",
            "Epoch: 6, Loss: 0.0002000661042984575 Updates: 25/10001, Avg Grad: 0.0224325992166996, Threshold: 0.0224325992166996\n",
            "Epoch: 6, Loss: 2.2381444068741985e-05 Updates: 28/11001, Avg Grad: 0.03193061798810959, Threshold: 0.021344847977161407\n",
            "Epoch: 6, Loss: 0.014384741894900799 Updates: 30/12001, Avg Grad: 0.016181157901883125, Threshold: 0.016181157901883125\n",
            "Epoch: 6, Loss: 3.588103936635889e-06 Updates: 33/13001, Avg Grad: 0.018304506316781044, Threshold: 0.015158584341406822\n",
            "Epoch: 6, Loss: 0.0012722904793918133 Updates: 35/14001, Avg Grad: 0.020265396684408188, Threshold: 0.020265396684408188\n",
            "Epoch: 6, Loss: 5.3054151067044586e-05 Updates: 38/15001, Avg Grad: 0.019223909825086594, Threshold: 0.013865607790648937\n",
            "Epoch: 6, Loss: 0.002428297186270356 Updates: 40/16001, Avg Grad: 0.01538488082587719, Threshold: 0.01538488082587719\n",
            "Epoch: 6, Loss: 0.00010354912228649482 Updates: 43/17001, Avg Grad: 0.03635028377175331, Threshold: 0.021404724568128586\n",
            "Epoch: 6, Loss: 2.909819522756152e-05 Updates: 45/18001, Avg Grad: 0.020930735394358635, Threshold: 0.020930735394358635\n",
            "Epoch: 6, Loss: 1.8569871826912276e-05 Updates: 48/19001, Avg Grad: 0.03493095561861992, Threshold: 0.019738225266337395\n",
            "Epoch 5 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 300 Loss: 0.387 | Acc: 87.043% (262/301)\n",
            "Test accuracy: 0.8704319000244141\n",
            "Epoch: 6, Loss: 0.0014999976847320795 Updates: 50/20001, Avg Grad: 0.015974003821611404, Threshold: 0.015974003821611404\n",
            "Epoch: 6, Loss: 0.00014099838153924793 Updates: 53/21001, Avg Grad: 0.024958327412605286, Threshold: 0.01421098131686449\n",
            "Epoch: 6, Loss: 8.993672236101702e-05 Updates: 55/22001, Avg Grad: 0.020681824535131454, Threshold: 0.020681824535131454\n",
            "Epoch: 6, Loss: 3.5168533941032365e-05 Updates: 58/23001, Avg Grad: 0.028803616762161255, Threshold: 0.020853407680988312\n",
            "Epoch: 6, Loss: 0.0009391611674800515 Updates: 60/24001, Avg Grad: 0.01723276823759079, Threshold: 0.01723276823759079\n",
            "Epoch: 6, Loss: 0.0024595633149147034 Updates: 63/25001, Avg Grad: 0.022047583013772964, Threshold: 0.01701558381319046\n",
            "Epoch: 6, Loss: 7.249768532346934e-05 Updates: 65/26001, Avg Grad: 0.014598900452256203, Threshold: 0.014598900452256203\n",
            "Epoch: 6, Loss: 1.2353881174931303e-05 Updates: 68/27001, Avg Grad: 0.02777809463441372, Threshold: 0.016218742355704308\n",
            "Epoch: 6, Loss: 5.426441020972561e-06 Updates: 70/28001, Avg Grad: 0.01943613961338997, Threshold: 0.01943613961338997\n",
            "Epoch: 6, Loss: 0.0006664079264737666 Updates: 73/29001, Avg Grad: 0.03591824695467949, Threshold: 0.026969587430357933\n",
            "Epoch 5 iteration 0 Loss: 2.161 | Acc: 0.000% (0/1)\n",
            "Epoch 5 iteration 300 Loss: 0.388 | Acc: 88.704% (267/301)\n",
            "Test accuracy: 0.8870431780815125\n",
            "Epoch: 6, Loss: 7.501845539081842e-05 Updates: 75/30001, Avg Grad: 0.021303391084074974, Threshold: 0.021303391084074974\n",
            "Epoch: 6, Loss: 0.00019309301569592208 Updates: 78/31001, Avg Grad: 0.026244426146149635, Threshold: 0.021776316687464714\n",
            "Epoch: 6, Loss: 0.00011011638707714155 Updates: 80/32001, Avg Grad: 0.013144535943865776, Threshold: 0.013144535943865776\n",
            "Epoch: 6, Loss: 0.00011786686081904918 Updates: 83/33001, Avg Grad: 0.02777349390089512, Threshold: 0.017314154654741287\n",
            "Epoch: 6, Loss: 0.000351552153006196 Updates: 85/34001, Avg Grad: 0.01917414925992489, Threshold: 0.01917414925992489\n",
            "Epoch: 6, Loss: 0.00010237898823106661 Updates: 88/35001, Avg Grad: 0.02763410285115242, Threshold: 0.013057321310043335\n",
            "Epoch: 6, Loss: 0.00040186679689213634 Updates: 90/36001, Avg Grad: 0.015812641009688377, Threshold: 0.015812641009688377\n",
            "Epoch: 6, Loss: 0.00038127810694277287 Updates: 93/37001, Avg Grad: 0.03173741325736046, Threshold: 0.02248523011803627\n",
            "Epoch: 6, Loss: 0.0013088330160826445 Updates: 95/38001, Avg Grad: 0.01383819617331028, Threshold: 0.01383819617331028\n",
            "Epoch: 6, Loss: 2.697509444260504e-05 Updates: 98/39001, Avg Grad: 0.030119284987449646, Threshold: 0.01833849772810936\n",
            "Epoch 5 iteration 0 Loss: 0.008 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 300 Loss: 0.400 | Acc: 87.375% (263/301)\n",
            "Test accuracy: 0.8737541437149048\n",
            "Epoch: 6, Loss: 0.002733294852077961 Updates: 100/40001, Avg Grad: 0.01347908191382885, Threshold: 0.01347908191382885\n",
            "Epoch: 6, Loss: 8.712064300198108e-05 Updates: 103/41001, Avg Grad: 0.027855703607201576, Threshold: 0.014422672800719738\n",
            "Epoch: 6, Loss: 0.0016917858738452196 Updates: 105/42001, Avg Grad: 0.029955953359603882, Threshold: 0.029955953359603882\n",
            "Epoch: 6, Loss: 1.0595412277325522e-05 Updates: 108/43001, Avg Grad: 0.036525558680295944, Threshold: 0.018652047961950302\n",
            "Epoch: 6, Loss: 0.023639746010303497 Updates: 110/44001, Avg Grad: 0.01609506458044052, Threshold: 0.01609506458044052\n",
            "Epoch: 6, Loss: 0.0011208663927391171 Updates: 113/45001, Avg Grad: 0.047626011073589325, Threshold: 0.026637965813279152\n",
            "Epoch: 6, Loss: 0.0002688215463422239 Updates: 115/46001, Avg Grad: 0.018085569143295288, Threshold: 0.018085569143295288\n",
            "Epoch: 6, Loss: 0.00031349450000561774 Updates: 118/47001, Avg Grad: 0.03621082007884979, Threshold: 0.017330635339021683\n",
            "Epoch: 6, Loss: 8.145087485900149e-05 Updates: 120/48001, Avg Grad: 0.019390631467103958, Threshold: 0.019390631467103958\n",
            "Epoch: 6, Loss: 0.002237840788438916 Updates: 123/49001, Avg Grad: 0.023454049602150917, Threshold: 0.012633104808628559\n",
            "Epoch 5 iteration 0 Loss: 0.029 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 300 Loss: 0.344 | Acc: 90.033% (271/301)\n",
            "Test accuracy: 0.9003322124481201\n",
            "Epoch: 6, Loss: 6.075808414607309e-05 Updates: 125/50001, Avg Grad: 0.01611979678273201, Threshold: 0.01611979678273201\n",
            "Epoch: 6, Loss: 4.384548810776323e-05 Updates: 128/51001, Avg Grad: 0.02870001830160618, Threshold: 0.022759946063160896\n",
            "Epoch: 6, Loss: 0.0018867732724174857 Updates: 130/52001, Avg Grad: 0.017960498109459877, Threshold: 0.017960498109459877\n",
            "Epoch: 6, Loss: 0.0003606919199228287 Updates: 132/53001, Avg Grad: 0.016421999782323837, Threshold: 0.016421999782323837\n",
            "Epoch: 6, Loss: 0.002627416281029582 Updates: 135/54001, Avg Grad: 0.027093227952718735, Threshold: 0.024986300617456436\n",
            "Epoch: 6, Loss: 3.311787804705091e-05 Updates: 137/55001, Avg Grad: 0.017722779884934425, Threshold: 0.017722779884934425\n",
            "Epoch: 6, Loss: 0.0006490270025096834 Updates: 140/56001, Avg Grad: 0.045050814747810364, Threshold: 0.031301867216825485\n",
            "Epoch: 6, Loss: 6.31291322861216e-06 Updates: 142/57001, Avg Grad: 0.024525094777345657, Threshold: 0.024525094777345657\n",
            "Epoch: 6, Loss: 1.217193766933633e-05 Updates: 145/58001, Avg Grad: 0.033367205411195755, Threshold: 0.020709779113531113\n",
            "Epoch: 6, Loss: 0.012334058061242104 Updates: 147/59001, Avg Grad: 0.01705285906791687, Threshold: 0.01705285906791687\n",
            "Epoch 6 iteration 0 Loss: 0.367 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 300 Loss: 0.362 | Acc: 89.037% (268/301)\n",
            "Test accuracy: 0.8903654217720032\n",
            "Epoch: 7, Loss: 0.00011210540105821565 Updates: 0/1, Avg Grad: 0.0032354691065847874, Threshold: 0.003270889865234494\n",
            "Epoch: 7, Loss: 0.000247536925598979 Updates: 3/1001, Avg Grad: 0.020397141575813293, Threshold: 0.01429099403321743\n",
            "Epoch: 7, Loss: 0.0001179052924271673 Updates: 5/2001, Avg Grad: 0.015545864589512348, Threshold: 0.015545864589512348\n",
            "Epoch: 7, Loss: 0.0008574977400712669 Updates: 8/3001, Avg Grad: 0.028422778472304344, Threshold: 0.018380913883447647\n",
            "Epoch: 7, Loss: 0.00023180944845080376 Updates: 10/4001, Avg Grad: 0.009298103861510754, Threshold: 0.009298103861510754\n",
            "Epoch: 7, Loss: 2.519625923014246e-05 Updates: 13/5001, Avg Grad: 0.030343085527420044, Threshold: 0.017956271767616272\n",
            "Epoch: 7, Loss: 6.413398659788072e-05 Updates: 15/6001, Avg Grad: 0.010954966768622398, Threshold: 0.010954966768622398\n",
            "Epoch: 7, Loss: 0.009648885577917099 Updates: 18/7001, Avg Grad: 0.027376508340239525, Threshold: 0.025217663496732712\n",
            "Epoch: 7, Loss: 0.0006202777731232345 Updates: 20/8001, Avg Grad: 0.01975567638874054, Threshold: 0.01975567638874054\n",
            "Epoch: 7, Loss: 0.00016825018974486738 Updates: 23/9001, Avg Grad: 0.034348972141742706, Threshold: 0.023997094482183456\n",
            "Epoch 6 iteration 0 Loss: 1.028 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 300 Loss: 0.293 | Acc: 90.365% (272/301)\n",
            "Test accuracy: 0.9036544561386108\n",
            "Epoch: 7, Loss: 1.0286703400197439e-06 Updates: 25/10001, Avg Grad: 0.020761679857969284, Threshold: 0.020761679857969284\n",
            "Epoch: 7, Loss: 4.778544825967401e-05 Updates: 28/11001, Avg Grad: 0.033749260008335114, Threshold: 0.02094220742583275\n",
            "Epoch: 7, Loss: 0.0003238768258597702 Updates: 30/12001, Avg Grad: 0.011379946023225784, Threshold: 0.011379946023225784\n",
            "Epoch: 7, Loss: 3.100122557953e-05 Updates: 33/13001, Avg Grad: 0.019508864730596542, Threshold: 0.01576339825987816\n",
            "Epoch: 7, Loss: 4.873879788647173e-06 Updates: 35/14001, Avg Grad: 0.021333228796720505, Threshold: 0.021333228796720505\n",
            "Epoch: 7, Loss: 0.0006684326217509806 Updates: 38/15001, Avg Grad: 0.018604908138513565, Threshold: 0.016640309244394302\n",
            "Epoch: 7, Loss: 0.0001903459196910262 Updates: 40/16001, Avg Grad: 0.01715046912431717, Threshold: 0.01715046912431717\n",
            "Epoch: 7, Loss: 0.0005271219997666776 Updates: 43/17001, Avg Grad: 0.03105388954281807, Threshold: 0.023865308612585068\n",
            "Epoch: 7, Loss: 3.3436423109378666e-05 Updates: 45/18001, Avg Grad: 0.02190917357802391, Threshold: 0.02190917357802391\n",
            "Epoch: 7, Loss: 0.0007024053484201431 Updates: 48/19001, Avg Grad: 0.0263840202242136, Threshold: 0.01746201701462269\n",
            "Epoch 6 iteration 0 Loss: 0.050 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 300 Loss: 0.364 | Acc: 89.369% (269/301)\n",
            "Test accuracy: 0.8936877250671387\n",
            "Epoch: 7, Loss: 2.247186102977139e-06 Updates: 50/20001, Avg Grad: 0.014281856827437878, Threshold: 0.014281856827437878\n",
            "Epoch: 7, Loss: 0.0008320473716594279 Updates: 53/21001, Avg Grad: 0.03162163123488426, Threshold: 0.023516755551099777\n",
            "Epoch: 7, Loss: 8.197855095204432e-06 Updates: 55/22001, Avg Grad: 0.01847243495285511, Threshold: 0.01847243495285511\n",
            "Epoch: 7, Loss: 6.504658085759729e-05 Updates: 58/23001, Avg Grad: 0.03735345974564552, Threshold: 0.02415001206099987\n",
            "Epoch: 7, Loss: 8.333701407536864e-05 Updates: 60/24001, Avg Grad: 0.018293265253305435, Threshold: 0.018293265253305435\n",
            "Epoch: 7, Loss: 6.660603685304523e-05 Updates: 63/25001, Avg Grad: 0.026337003335356712, Threshold: 0.01531960628926754\n",
            "Epoch: 7, Loss: 0.002169357379898429 Updates: 65/26001, Avg Grad: 0.016932843253016472, Threshold: 0.016932843253016472\n",
            "Epoch: 7, Loss: 0.00019760816940106452 Updates: 68/27001, Avg Grad: 0.02085127867758274, Threshold: 0.019154146313667297\n",
            "Epoch: 7, Loss: 0.0005924197612330317 Updates: 70/28001, Avg Grad: 0.016976026818156242, Threshold: 0.016976026818156242\n",
            "Epoch: 7, Loss: 0.006144877523183823 Updates: 73/29001, Avg Grad: 0.02033996768295765, Threshold: 0.018303269520401955\n",
            "Epoch 6 iteration 0 Loss: 0.003 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 300 Loss: 0.355 | Acc: 90.365% (272/301)\n",
            "Test accuracy: 0.9036544561386108\n",
            "Epoch: 7, Loss: 2.3297547159017995e-05 Updates: 75/30001, Avg Grad: 0.013612044043838978, Threshold: 0.013612044043838978\n",
            "Epoch: 7, Loss: 5.189466719457414e-06 Updates: 78/31001, Avg Grad: 0.02178237959742546, Threshold: 0.021262122318148613\n",
            "Epoch: 7, Loss: 1.660031921346672e-05 Updates: 80/32001, Avg Grad: 0.022344160825014114, Threshold: 0.022344160825014114\n",
            "Epoch: 7, Loss: 0.0005102917784824967 Updates: 82/33001, Avg Grad: 0.011574131436645985, Threshold: 0.011574131436645985\n",
            "Epoch: 7, Loss: 4.795547283720225e-05 Updates: 85/34001, Avg Grad: 0.03017500601708889, Threshold: 0.020002728328108788\n",
            "Epoch: 7, Loss: 0.00023052323376759887 Updates: 87/35001, Avg Grad: 0.02082267589867115, Threshold: 0.02082267589867115\n",
            "Epoch: 7, Loss: 4.786868885275908e-05 Updates: 90/36001, Avg Grad: 0.03777969628572464, Threshold: 0.027568737044930458\n",
            "Epoch: 7, Loss: 3.7215206702967407e-06 Updates: 92/37001, Avg Grad: 0.021499546244740486, Threshold: 0.021499546244740486\n",
            "Epoch: 7, Loss: 0.00010242103599011898 Updates: 95/38001, Avg Grad: 0.029653843492269516, Threshold: 0.018708311021327972\n",
            "Epoch: 7, Loss: 0.0038838284090161324 Updates: 97/39001, Avg Grad: 0.01944013312458992, Threshold: 0.01944013312458992\n",
            "Epoch 6 iteration 0 Loss: 0.011 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 300 Loss: 0.401 | Acc: 89.369% (269/301)\n",
            "Test accuracy: 0.8936877250671387\n",
            "Epoch: 7, Loss: 0.00015173779684118927 Updates: 100/40001, Avg Grad: 0.027298491448163986, Threshold: 0.015098175033926964\n",
            "Epoch: 7, Loss: 0.002700638724491 Updates: 102/41001, Avg Grad: 0.015490228310227394, Threshold: 0.015490228310227394\n",
            "Epoch: 7, Loss: 0.0015311785973608494 Updates: 105/42001, Avg Grad: 0.030034512281417847, Threshold: 0.024568675085902214\n",
            "Epoch: 7, Loss: 7.783074579492677e-06 Updates: 107/43001, Avg Grad: 0.020373264327645302, Threshold: 0.020373264327645302\n",
            "Epoch: 7, Loss: 6.852012302260846e-05 Updates: 110/44001, Avg Grad: 0.026387181133031845, Threshold: 0.01284960936754942\n",
            "Epoch: 7, Loss: 0.00017604176537133753 Updates: 112/45001, Avg Grad: 0.027595173567533493, Threshold: 0.027595173567533493\n",
            "Epoch: 7, Loss: 0.01124176662415266 Updates: 115/46001, Avg Grad: 0.03343280404806137, Threshold: 0.02094537578523159\n",
            "Epoch: 7, Loss: 0.00010277665569446981 Updates: 117/47001, Avg Grad: 0.02204165793955326, Threshold: 0.02204165793955326\n",
            "Epoch: 7, Loss: 6.418694101739675e-05 Updates: 120/48001, Avg Grad: 0.030609991401433945, Threshold: 0.025078540667891502\n",
            "Epoch: 7, Loss: 6.615019083255902e-05 Updates: 122/49001, Avg Grad: 0.014057794585824013, Threshold: 0.014057794585824013\n",
            "Epoch 6 iteration 0 Loss: 1.893 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 300 Loss: 0.307 | Acc: 90.698% (273/301)\n",
            "Test accuracy: 0.9069767594337463\n",
            "Epoch: 7, Loss: 0.00030397740192711353 Updates: 125/50001, Avg Grad: 0.028881948441267014, Threshold: 0.016030143946409225\n",
            "Epoch: 7, Loss: 7.270446803886443e-05 Updates: 127/51001, Avg Grad: 0.01411898247897625, Threshold: 0.01411898247897625\n",
            "Epoch: 7, Loss: 0.00174126704223454 Updates: 130/52001, Avg Grad: 0.03447677940130234, Threshold: 0.017520006746053696\n",
            "Epoch: 7, Loss: 1.2151127521065064e-05 Updates: 132/53001, Avg Grad: 0.02444101683795452, Threshold: 0.02444101683795452\n",
            "Epoch: 7, Loss: 0.0007713054656051099 Updates: 135/54001, Avg Grad: 0.021989675238728523, Threshold: 0.017598507925868034\n",
            "Epoch: 7, Loss: 0.00015114738198462874 Updates: 137/55001, Avg Grad: 0.019164670258760452, Threshold: 0.019164670258760452\n",
            "Epoch: 7, Loss: 0.0001536062773084268 Updates: 140/56001, Avg Grad: 0.02726704254746437, Threshold: 0.018216578289866447\n",
            "Epoch: 7, Loss: 0.0002008671872317791 Updates: 142/57001, Avg Grad: 0.019819216802716255, Threshold: 0.019819216802716255\n",
            "Epoch: 7, Loss: 3.297577131888829e-05 Updates: 145/58001, Avg Grad: 0.03318072110414505, Threshold: 0.020357254892587662\n",
            "Epoch: 7, Loss: 3.384398041816894e-06 Updates: 147/59001, Avg Grad: 0.013636233285069466, Threshold: 0.013636233285069466\n",
            "Epoch 7 iteration 0 Loss: 0.014 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 300 Loss: 0.313 | Acc: 92.027% (277/301)\n",
            "Test accuracy: 0.920265793800354\n",
            "Epoch: 8, Loss: 5.088986654300243e-05 Updates: 0/1, Avg Grad: 0.006570660974830389, Threshold: 0.0066155195236206055\n",
            "Epoch: 8, Loss: 3.9739814383210614e-05 Updates: 3/1001, Avg Grad: 0.02294529601931572, Threshold: 0.014977914281189442\n",
            "Epoch: 8, Loss: 0.000592955038882792 Updates: 5/2001, Avg Grad: 0.012390698306262493, Threshold: 0.012390698306262493\n",
            "Epoch: 8, Loss: 0.016418881714344025 Updates: 8/3001, Avg Grad: 0.01957712695002556, Threshold: 0.0168224535882473\n",
            "Epoch: 8, Loss: 4.531470040092245e-05 Updates: 10/4001, Avg Grad: 0.01946857199072838, Threshold: 0.01946857199072838\n",
            "Epoch: 8, Loss: 0.0003601950011216104 Updates: 13/5001, Avg Grad: 0.02756652608513832, Threshold: 0.014293422922492027\n",
            "Epoch: 8, Loss: 2.0200281142024323e-05 Updates: 15/6001, Avg Grad: 0.03754813224077225, Threshold: 0.02420867048203945\n",
            "Epoch: 8, Loss: 0.0006837837863713503 Updates: 17/7001, Avg Grad: 0.025851208716630936, Threshold: 0.025851208716630936\n",
            "Epoch: 8, Loss: 2.223131559730973e-05 Updates: 20/8001, Avg Grad: 0.032213952392339706, Threshold: 0.020058685913681984\n",
            "Epoch: 8, Loss: 0.0002540244022384286 Updates: 22/9001, Avg Grad: 0.01658480614423752, Threshold: 0.01658480614423752\n",
            "Epoch 7 iteration 0 Loss: 0.062 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 300 Loss: 0.351 | Acc: 90.698% (273/301)\n",
            "Test accuracy: 0.9069767594337463\n",
            "Epoch: 8, Loss: 1.720801810733974e-05 Updates: 25/10001, Avg Grad: 0.02522306516766548, Threshold: 0.017978396266698837\n",
            "Epoch: 8, Loss: 3.021865450136829e-05 Updates: 27/11001, Avg Grad: 0.011703597381711006, Threshold: 0.011703597381711006\n",
            "Epoch: 8, Loss: 0.0006683054380118847 Updates: 30/12001, Avg Grad: 0.024521777406334877, Threshold: 0.015185214579105377\n",
            "Epoch: 8, Loss: 0.0004847290983889252 Updates: 32/13001, Avg Grad: 0.010979397222399712, Threshold: 0.010979397222399712\n",
            "Epoch: 8, Loss: 0.005969948600977659 Updates: 35/14001, Avg Grad: 0.02597183920443058, Threshold: 0.01885262317955494\n",
            "Epoch: 8, Loss: 5.4517116950592026e-05 Updates: 37/15001, Avg Grad: 0.010400554165244102, Threshold: 0.010400554165244102\n",
            "Epoch: 8, Loss: 1.8451102732797153e-05 Updates: 40/16001, Avg Grad: 0.022171014919877052, Threshold: 0.011145494878292084\n",
            "Epoch: 8, Loss: 0.0008946176385506988 Updates: 42/17001, Avg Grad: 0.013477606698870659, Threshold: 0.013477606698870659\n",
            "Epoch: 8, Loss: 0.00013783868052996695 Updates: 45/18001, Avg Grad: 0.017168013378977776, Threshold: 0.013822510838508606\n",
            "Epoch: 8, Loss: 7.226606612675823e-06 Updates: 47/19001, Avg Grad: 0.013683803379535675, Threshold: 0.013683803379535675\n",
            "Epoch 7 iteration 0 Loss: 0.481 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 300 Loss: 0.295 | Acc: 92.691% (279/301)\n",
            "Test accuracy: 0.9269102811813354\n",
            "Epoch: 8, Loss: 1.0424705578770954e-05 Updates: 50/20001, Avg Grad: 0.020814847201108932, Threshold: 0.014962611719965935\n",
            "Epoch: 8, Loss: 0.00010773751273518428 Updates: 52/21001, Avg Grad: 0.021014738827943802, Threshold: 0.017383592203259468\n",
            "Epoch: 8, Loss: 0.00025273088249377906 Updates: 54/22001, Avg Grad: 0.01232227124273777, Threshold: 0.01232227124273777\n",
            "Epoch: 8, Loss: 0.00031498618773184717 Updates: 57/23001, Avg Grad: 0.025617212057113647, Threshold: 0.01804778352379799\n",
            "Epoch: 8, Loss: 0.0003011295339092612 Updates: 59/24001, Avg Grad: 0.018043717369437218, Threshold: 0.018043717369437218\n",
            "Epoch: 8, Loss: 0.009974208660423756 Updates: 62/25001, Avg Grad: 0.03980063647031784, Threshold: 0.022784218192100525\n",
            "Epoch: 8, Loss: 0.00011182330490555614 Updates: 64/26001, Avg Grad: 0.014376978389918804, Threshold: 0.014376978389918804\n",
            "Epoch: 8, Loss: 6.0583188314922154e-05 Updates: 67/27001, Avg Grad: 0.029814422130584717, Threshold: 0.017931070178747177\n",
            "Epoch: 8, Loss: 0.002733041299507022 Updates: 69/28001, Avg Grad: 0.009785527363419533, Threshold: 0.009785527363419533\n",
            "Epoch: 8, Loss: 0.009692230261862278 Updates: 72/29001, Avg Grad: 0.03182632848620415, Threshold: 0.02366539090871811\n",
            "Epoch 7 iteration 0 Loss: 0.113 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 300 Loss: 0.294 | Acc: 92.359% (278/301)\n",
            "Test accuracy: 0.9235880374908447\n",
            "Epoch: 8, Loss: 0.00010789850784931332 Updates: 74/30001, Avg Grad: 0.021669549867510796, Threshold: 0.021669549867510796\n",
            "Epoch: 8, Loss: 3.147331517538987e-05 Updates: 77/31001, Avg Grad: 0.025018978863954544, Threshold: 0.017954396083950996\n",
            "Epoch: 8, Loss: 2.8005935746477917e-05 Updates: 79/32001, Avg Grad: 0.01483168639242649, Threshold: 0.01483168639242649\n",
            "Epoch: 8, Loss: 0.0001618165842955932 Updates: 82/33001, Avg Grad: 0.02929438091814518, Threshold: 0.019087759777903557\n",
            "Epoch: 8, Loss: 4.259984052623622e-05 Updates: 84/34001, Avg Grad: 0.014097863808274269, Threshold: 0.014097863808274269\n",
            "Epoch: 8, Loss: 5.425828567240387e-05 Updates: 87/35001, Avg Grad: 0.025375213474035263, Threshold: 0.016035694628953934\n",
            "Epoch: 8, Loss: 0.008303964510560036 Updates: 89/36001, Avg Grad: 0.020404670387506485, Threshold: 0.020404670387506485\n",
            "Epoch: 8, Loss: 6.846898031653836e-05 Updates: 92/37001, Avg Grad: 0.04202223941683769, Threshold: 0.02297127991914749\n",
            "Epoch: 8, Loss: 0.0003764366265386343 Updates: 94/38001, Avg Grad: 0.011355080641806126, Threshold: 0.011355080641806126\n",
            "Epoch: 8, Loss: 0.00023459015937987715 Updates: 97/39001, Avg Grad: 0.025521595031023026, Threshold: 0.013078292831778526\n",
            "Epoch 7 iteration 0 Loss: 0.132 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 300 Loss: 0.435 | Acc: 87.375% (263/301)\n",
            "Test accuracy: 0.8737541437149048\n",
            "Epoch: 8, Loss: 0.00016721442807465792 Updates: 99/40001, Avg Grad: 0.01583263650536537, Threshold: 0.01583263650536537\n",
            "Epoch: 8, Loss: 0.0013515647733584046 Updates: 102/41001, Avg Grad: 0.02097238227725029, Threshold: 0.015067590400576591\n",
            "Epoch: 8, Loss: 0.000590521318372339 Updates: 104/42001, Avg Grad: 0.00951398629695177, Threshold: 0.00951398629695177\n",
            "Epoch: 8, Loss: 0.0043355608358979225 Updates: 107/43001, Avg Grad: 0.025967683643102646, Threshold: 0.017776766791939735\n",
            "Epoch: 8, Loss: 0.002251067664474249 Updates: 109/44001, Avg Grad: 0.027064930647611618, Threshold: 0.027064930647611618\n",
            "Epoch: 8, Loss: 0.0004059594066347927 Updates: 112/45001, Avg Grad: 0.02701651118695736, Threshold: 0.016696196049451828\n",
            "Epoch: 8, Loss: 0.0001613474014448002 Updates: 114/46001, Avg Grad: 0.022022176533937454, Threshold: 0.022022176533937454\n",
            "Epoch: 8, Loss: 0.006478197406977415 Updates: 117/47001, Avg Grad: 0.02874898537993431, Threshold: 0.025829697027802467\n",
            "Epoch: 8, Loss: 4.862565674557118e-06 Updates: 119/48001, Avg Grad: 0.018614619970321655, Threshold: 0.018614619970321655\n",
            "Epoch: 8, Loss: 0.0005880350363440812 Updates: 122/49001, Avg Grad: 0.036023207008838654, Threshold: 0.026484057307243347\n",
            "Epoch 7 iteration 0 Loss: 0.006 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 300 Loss: 0.288 | Acc: 91.030% (274/301)\n",
            "Test accuracy: 0.9102990031242371\n",
            "Epoch: 8, Loss: 7.119451038306579e-05 Updates: 124/50001, Avg Grad: 0.010342707857489586, Threshold: 0.010342707857489586\n",
            "Epoch: 8, Loss: 0.0010792592074722052 Updates: 127/51001, Avg Grad: 0.02710847556591034, Threshold: 0.011711510829627514\n",
            "Epoch: 8, Loss: 0.0015306170098483562 Updates: 129/52001, Avg Grad: 0.01436946727335453, Threshold: 0.01436946727335453\n",
            "Epoch: 8, Loss: 8.904914284357801e-05 Updates: 132/53001, Avg Grad: 0.03132670372724533, Threshold: 0.014178427867591381\n",
            "Epoch: 8, Loss: 7.794381417625118e-06 Updates: 134/54001, Avg Grad: 0.017154989764094353, Threshold: 0.017154989764094353\n",
            "Epoch: 8, Loss: 7.193309284048155e-05 Updates: 136/55001, Avg Grad: 0.02572997845709324, Threshold: 0.027009937912225723\n",
            "Epoch: 8, Loss: 0.0003573828435037285 Updates: 139/56001, Avg Grad: 0.020698240026831627, Threshold: 0.013168074190616608\n",
            "Epoch: 8, Loss: 4.7305449697887525e-05 Updates: 141/57001, Avg Grad: 0.019269326701760292, Threshold: 0.019269326701760292\n",
            "Epoch: 8, Loss: 8.344580493258036e-08 Updates: 144/58001, Avg Grad: 0.026675676926970482, Threshold: 0.01852456107735634\n",
            "Epoch: 8, Loss: 0.00397760234773159 Updates: 146/59001, Avg Grad: 0.016133666038513184, Threshold: 0.016133666038513184\n",
            "Epoch 8 iteration 0 Loss: 0.001 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 300 Loss: 0.225 | Acc: 93.023% (280/301)\n",
            "Test accuracy: 0.930232584476471\n",
            "Epoch: 9, Loss: 5.1469811296556145e-05 Updates: 0/1, Avg Grad: 0.0028383766766637564, Threshold: 0.002842520596459508\n",
            "Epoch: 9, Loss: 1.123179117712425e-05 Updates: 3/1001, Avg Grad: 0.027942921966314316, Threshold: 0.019673537462949753\n",
            "Epoch: 9, Loss: 0.001962123904377222 Updates: 5/2001, Avg Grad: 0.015375095419585705, Threshold: 0.015375095419585705\n",
            "Epoch: 9, Loss: 0.001426351023837924 Updates: 8/3001, Avg Grad: 0.0298006534576416, Threshold: 0.019088217988610268\n",
            "Epoch: 9, Loss: 0.00021240615751594305 Updates: 10/4001, Avg Grad: 0.020576130598783493, Threshold: 0.020576130598783493\n",
            "Epoch: 9, Loss: 7.464499503839761e-05 Updates: 13/5001, Avg Grad: 0.037501260638237, Threshold: 0.020335610955953598\n",
            "Epoch: 9, Loss: 0.004230906255543232 Updates: 15/6001, Avg Grad: 0.020063458010554314, Threshold: 0.020063458010554314\n",
            "Epoch: 9, Loss: 2.946617860288825e-05 Updates: 18/7001, Avg Grad: 0.02886151149868965, Threshold: 0.011323099955916405\n",
            "Epoch: 9, Loss: 0.0001171152398455888 Updates: 20/8001, Avg Grad: 0.014254097826778889, Threshold: 0.014254097826778889\n",
            "Epoch: 9, Loss: 4.996464485884644e-05 Updates: 23/9001, Avg Grad: 0.03758925572037697, Threshold: 0.017898108810186386\n",
            "Epoch 8 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 300 Loss: 0.254 | Acc: 93.023% (280/301)\n",
            "Test accuracy: 0.930232584476471\n",
            "Epoch: 9, Loss: 0.0093075605109334 Updates: 25/10001, Avg Grad: 0.016544634476304054, Threshold: 0.016544634476304054\n",
            "Epoch: 9, Loss: 6.69195142108947e-05 Updates: 28/11001, Avg Grad: 0.02395588904619217, Threshold: 0.016975242644548416\n",
            "Epoch: 9, Loss: 0.004640236496925354 Updates: 30/12001, Avg Grad: 0.0171593576669693, Threshold: 0.0171593576669693\n",
            "Epoch: 9, Loss: 0.0014052349142730236 Updates: 33/13001, Avg Grad: 0.03946346789598465, Threshold: 0.020101511850953102\n",
            "Epoch: 9, Loss: 0.0011316615855321288 Updates: 35/14001, Avg Grad: 0.015926670283079147, Threshold: 0.015926670283079147\n",
            "Epoch: 9, Loss: 0.0010265153832733631 Updates: 38/15001, Avg Grad: 0.028235474601387978, Threshold: 0.01372906006872654\n",
            "Epoch: 9, Loss: 9.173079888569191e-06 Updates: 40/16001, Avg Grad: 0.022345902398228645, Threshold: 0.019147271290421486\n",
            "Epoch: 9, Loss: 1.7745558579917997e-05 Updates: 42/17001, Avg Grad: 0.015241247601807117, Threshold: 0.015241247601807117\n",
            "Epoch: 9, Loss: 2.6749810785986483e-05 Updates: 45/18001, Avg Grad: 0.01702839508652687, Threshold: 0.014641515910625458\n",
            "Epoch: 9, Loss: 0.004995021503418684 Updates: 47/19001, Avg Grad: 0.022640813142061234, Threshold: 0.022640813142061234\n",
            "Epoch 8 iteration 0 Loss: 0.013 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 300 Loss: 0.341 | Acc: 88.040% (265/301)\n",
            "Test accuracy: 0.880398690700531\n",
            "Epoch: 9, Loss: 0.00011253784759901464 Updates: 50/20001, Avg Grad: 0.025338029488921165, Threshold: 0.018110178411006927\n",
            "Epoch: 9, Loss: 0.0023844155948609114 Updates: 52/21001, Avg Grad: 0.026022348552942276, Threshold: 0.026022348552942276\n",
            "Epoch: 9, Loss: 2.1404044673545286e-05 Updates: 55/22001, Avg Grad: 0.03146896883845329, Threshold: 0.021311091259121895\n",
            "Epoch: 9, Loss: 0.000668271561153233 Updates: 57/23001, Avg Grad: 0.016169335693120956, Threshold: 0.016169335693120956\n",
            "Epoch: 9, Loss: 3.241854574298486e-05 Updates: 60/24001, Avg Grad: 0.030602101236581802, Threshold: 0.01948654279112816\n",
            "Epoch: 9, Loss: 0.0001607812155270949 Updates: 62/25001, Avg Grad: 0.015976380556821823, Threshold: 0.015976380556821823\n",
            "Epoch: 9, Loss: 6.36184704490006e-05 Updates: 65/26001, Avg Grad: 0.03694772720336914, Threshold: 0.01824256405234337\n",
            "Epoch: 9, Loss: 0.0025825852062553167 Updates: 67/27001, Avg Grad: 0.016926122829318047, Threshold: 0.016926122829318047\n",
            "Epoch: 9, Loss: 0.00446911109611392 Updates: 70/28001, Avg Grad: 0.03051609918475151, Threshold: 0.020118558779358864\n",
            "Epoch: 9, Loss: 8.430522029811982e-06 Updates: 72/29001, Avg Grad: 0.016356710344552994, Threshold: 0.016356710344552994\n",
            "Epoch 8 iteration 0 Loss: 0.030 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 300 Loss: 0.252 | Acc: 92.027% (277/301)\n",
            "Test accuracy: 0.920265793800354\n",
            "Epoch: 9, Loss: 3.936528628400993e-06 Updates: 75/30001, Avg Grad: 0.02572026289999485, Threshold: 0.018443679437041283\n",
            "Epoch: 9, Loss: 0.00031557102920487523 Updates: 77/31001, Avg Grad: 0.018780622631311417, Threshold: 0.018780622631311417\n",
            "Epoch: 9, Loss: 0.0028038357850164175 Updates: 80/32001, Avg Grad: 0.02492394857108593, Threshold: 0.018327120691537857\n",
            "Epoch: 9, Loss: 6.636547186644748e-05 Updates: 82/33001, Avg Grad: 0.012553779408335686, Threshold: 0.012553779408335686\n",
            "Epoch: 9, Loss: 0.0003111768455710262 Updates: 85/34001, Avg Grad: 0.03770945593714714, Threshold: 0.02122354507446289\n",
            "Epoch: 9, Loss: 0.018469106405973434 Updates: 87/35001, Avg Grad: 0.01418447308242321, Threshold: 0.01418447308242321\n",
            "Epoch: 9, Loss: 2.137199589924421e-05 Updates: 90/36001, Avg Grad: 0.021843310445547104, Threshold: 0.010933018289506435\n",
            "Epoch: 9, Loss: 0.0006241098162718117 Updates: 92/37001, Avg Grad: 0.018601197749376297, Threshold: 0.018601197749376297\n",
            "Epoch: 9, Loss: 8.766114660829771e-06 Updates: 95/38001, Avg Grad: 0.01880144514143467, Threshold: 0.016349349170923233\n",
            "Epoch: 9, Loss: 5.63482717552688e-06 Updates: 97/39001, Avg Grad: 0.012049173004925251, Threshold: 0.012049173004925251\n",
            "Epoch 8 iteration 0 Loss: 0.549 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 300 Loss: 0.335 | Acc: 89.701% (270/301)\n",
            "Test accuracy: 0.8970099687576294\n",
            "Epoch: 9, Loss: 0.000648624962195754 Updates: 100/40001, Avg Grad: 0.037199653685092926, Threshold: 0.018914837390184402\n",
            "Epoch: 9, Loss: 3.776911398745142e-06 Updates: 102/41001, Avg Grad: 0.013969885185360909, Threshold: 0.013969885185360909\n",
            "Epoch: 9, Loss: 0.00040135305607691407 Updates: 105/42001, Avg Grad: 0.023787599056959152, Threshold: 0.0188596248626709\n",
            "Epoch: 9, Loss: 3.327478771097958e-05 Updates: 107/43001, Avg Grad: 0.015862856060266495, Threshold: 0.015862856060266495\n",
            "Epoch: 9, Loss: 4.893773075309582e-05 Updates: 110/44001, Avg Grad: 0.030057525262236595, Threshold: 0.02782316878437996\n",
            "Epoch: 9, Loss: 4.685708972829161e-06 Updates: 112/45001, Avg Grad: 0.01857280358672142, Threshold: 0.01857280358672142\n",
            "Epoch: 9, Loss: 2.8144037059973925e-05 Updates: 115/46001, Avg Grad: 0.02647015079855919, Threshold: 0.018929507583379745\n",
            "Epoch: 9, Loss: 1.3580935046775267e-05 Updates: 117/47001, Avg Grad: 0.011838614009320736, Threshold: 0.011838614009320736\n",
            "Epoch: 9, Loss: 0.00035287425271235406 Updates: 120/48001, Avg Grad: 0.03766121342778206, Threshold: 0.020981648936867714\n",
            "Epoch: 9, Loss: 6.001564997859532e-06 Updates: 122/49001, Avg Grad: 0.01568567380309105, Threshold: 0.01568567380309105\n",
            "Epoch 8 iteration 0 Loss: 0.079 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 300 Loss: 0.379 | Acc: 88.372% (266/301)\n",
            "Test accuracy: 0.8837209343910217\n",
            "Epoch: 9, Loss: 0.0003753142664209008 Updates: 125/50001, Avg Grad: 0.02667926251888275, Threshold: 0.018352201208472252\n",
            "Epoch: 9, Loss: 0.00031380518339574337 Updates: 127/51001, Avg Grad: 0.020160896703600883, Threshold: 0.020160896703600883\n",
            "Epoch: 9, Loss: 0.0005195738631300628 Updates: 130/52001, Avg Grad: 0.021625924855470657, Threshold: 0.01767594926059246\n",
            "Epoch: 9, Loss: 0.0013130768202245235 Updates: 132/53001, Avg Grad: 0.01687309332191944, Threshold: 0.01687309332191944\n",
            "Epoch: 9, Loss: 6.575960287591442e-05 Updates: 135/54001, Avg Grad: 0.02703249081969261, Threshold: 0.02001391351222992\n",
            "Epoch: 9, Loss: 9.313209739048034e-05 Updates: 137/55001, Avg Grad: 0.012589678168296814, Threshold: 0.012589678168296814\n",
            "Epoch: 9, Loss: 1.1152695151395164e-05 Updates: 140/56001, Avg Grad: 0.029784124344587326, Threshold: 0.012977930717170238\n",
            "Epoch: 9, Loss: 0.0002558482810854912 Updates: 142/57001, Avg Grad: 0.017331065610051155, Threshold: 0.017331065610051155\n",
            "Epoch: 9, Loss: 1.1013725043085287e-06 Updates: 145/58001, Avg Grad: 0.017744870856404305, Threshold: 0.013438517227768898\n",
            "Epoch: 9, Loss: 0.0005049476749263704 Updates: 147/59001, Avg Grad: 0.011998417787253857, Threshold: 0.011998417787253857\n",
            "Epoch 9 iteration 0 Loss: 0.000 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 300 Loss: 0.272 | Acc: 91.362% (275/301)\n",
            "Test accuracy: 0.9136212468147278\n",
            "Epoch: 10, Loss: 0.014060493558645248 Updates: 0/1, Avg Grad: 0.0879279375076294, Threshold: 0.0879359245300293\n",
            "Epoch: 10, Loss: 0.004416994284838438 Updates: 3/1001, Avg Grad: 0.07769723236560822, Threshold: 0.04041540250182152\n",
            "Epoch: 10, Loss: 0.00013317458797246218 Updates: 5/2001, Avg Grad: 0.024933915585279465, Threshold: 0.024933915585279465\n",
            "Epoch: 10, Loss: 1.9055012671742588e-05 Updates: 8/3001, Avg Grad: 0.043048229068517685, Threshold: 0.02193584106862545\n",
            "Epoch: 10, Loss: 5.897591472603381e-05 Updates: 10/4001, Avg Grad: 0.016085103154182434, Threshold: 0.016085103154182434\n",
            "Epoch: 10, Loss: 8.31508259580005e-06 Updates: 13/5001, Avg Grad: 0.036826521158218384, Threshold: 0.02147907391190529\n",
            "Epoch: 10, Loss: 0.0007638979004696012 Updates: 15/6001, Avg Grad: 0.020538371056318283, Threshold: 0.020538371056318283\n",
            "Epoch: 10, Loss: 4.349842583906138e-06 Updates: 18/7001, Avg Grad: 0.037470970302820206, Threshold: 0.02314295805990696\n",
            "Epoch: 10, Loss: 0.00017899568774737418 Updates: 20/8001, Avg Grad: 0.015612262301146984, Threshold: 0.015612262301146984\n",
            "Epoch: 10, Loss: 3.7638083085766993e-06 Updates: 23/9001, Avg Grad: 0.04356231540441513, Threshold: 0.02743656374514103\n",
            "Epoch 9 iteration 0 Loss: 0.224 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 300 Loss: 0.281 | Acc: 92.027% (277/301)\n",
            "Test accuracy: 0.920265793800354\n",
            "Epoch: 10, Loss: 7.082958472892642e-05 Updates: 25/10001, Avg Grad: 0.011360694654285908, Threshold: 0.011360694654285908\n",
            "Epoch: 10, Loss: 0.00016782080638222396 Updates: 28/11001, Avg Grad: 0.04107487574219704, Threshold: 0.021671075373888016\n",
            "Epoch: 10, Loss: 0.00021324468252714723 Updates: 30/12001, Avg Grad: 0.023765234276652336, Threshold: 0.023765234276652336\n",
            "Epoch: 10, Loss: 1.8148823073715903e-05 Updates: 33/13001, Avg Grad: 0.01680266484618187, Threshold: 0.010705766268074512\n",
            "Epoch: 10, Loss: 0.00013691953790839761 Updates: 35/14001, Avg Grad: 0.0181033406406641, Threshold: 0.0181033406406641\n",
            "Epoch: 10, Loss: 5.357374448067276e-06 Updates: 38/15001, Avg Grad: 0.03293970972299576, Threshold: 0.01995997317135334\n",
            "Epoch: 10, Loss: 6.430168286897242e-05 Updates: 40/16001, Avg Grad: 0.011794717982411385, Threshold: 0.011794717982411385\n",
            "Epoch: 10, Loss: 0.0013765409821644425 Updates: 43/17001, Avg Grad: 0.0368066243827343, Threshold: 0.018111402168869972\n",
            "Epoch: 10, Loss: 1.026291647576727e-05 Updates: 45/18001, Avg Grad: 0.011242625303566456, Threshold: 0.011242625303566456\n",
            "Epoch: 10, Loss: 4.567853466141969e-05 Updates: 48/19001, Avg Grad: 0.023249397054314613, Threshold: 0.015542281791567802\n",
            "Epoch 9 iteration 0 Loss: 0.005 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 300 Loss: 0.395 | Acc: 89.369% (269/301)\n",
            "Test accuracy: 0.8936877250671387\n",
            "Epoch: 10, Loss: 0.0026937639340758324 Updates: 50/20001, Avg Grad: 0.013447153382003307, Threshold: 0.013447153382003307\n",
            "Epoch: 10, Loss: 3.7429624626383884e-06 Updates: 53/21001, Avg Grad: 0.018610382452607155, Threshold: 0.013934746384620667\n",
            "Epoch: 10, Loss: 0.0002049314061878249 Updates: 55/22001, Avg Grad: 0.019771885126829147, Threshold: 0.019771885126829147\n",
            "Epoch: 10, Loss: 0.00022244389401748776 Updates: 57/23001, Avg Grad: 0.014171645045280457, Threshold: 0.014960752800107002\n",
            "Epoch: 10, Loss: 0.00017497768567409366 Updates: 60/24001, Avg Grad: 0.02943478897213936, Threshold: 0.017606183886528015\n",
            "Epoch: 10, Loss: 2.70652053586673e-05 Updates: 62/25001, Avg Grad: 0.016005197539925575, Threshold: 0.016005197539925575\n",
            "Epoch: 10, Loss: 0.0010896081803366542 Updates: 65/26001, Avg Grad: 0.02245766669511795, Threshold: 0.011202971450984478\n",
            "Epoch: 10, Loss: 0.001865705824457109 Updates: 67/27001, Avg Grad: 0.015811115503311157, Threshold: 0.015811115503311157\n",
            "Epoch: 10, Loss: 0.0002145970065612346 Updates: 70/28001, Avg Grad: 0.018780920654535294, Threshold: 0.01240281667560339\n",
            "Epoch: 10, Loss: 4.58143804280553e-05 Updates: 72/29001, Avg Grad: 0.016757136210799217, Threshold: 0.016757136210799217\n",
            "Epoch 9 iteration 0 Loss: 1.716 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 300 Loss: 0.215 | Acc: 94.020% (283/301)\n",
            "Test accuracy: 0.9401993155479431\n",
            "Epoch: 10, Loss: 6.324592686723918e-05 Updates: 75/30001, Avg Grad: 0.0157049261033535, Threshold: 0.009496004320681095\n",
            "Epoch: 10, Loss: 0.012347500771284103 Updates: 77/31001, Avg Grad: 0.014307886362075806, Threshold: 0.014307886362075806\n",
            "Epoch: 10, Loss: 0.00013410078827291727 Updates: 80/32001, Avg Grad: 0.02281736582517624, Threshold: 0.013425816781818867\n",
            "Epoch: 10, Loss: 8.164262544596568e-05 Updates: 82/33001, Avg Grad: 0.019966812804341316, Threshold: 0.019966812804341316\n",
            "Epoch: 10, Loss: 0.00023957369558047503 Updates: 85/34001, Avg Grad: 0.030588682740926743, Threshold: 0.015060827136039734\n",
            "Epoch: 10, Loss: 3.101366382907145e-05 Updates: 87/35001, Avg Grad: 0.017965679988265038, Threshold: 0.017965679988265038\n",
            "Epoch: 10, Loss: 3.402267566343653e-06 Updates: 90/36001, Avg Grad: 0.029454588890075684, Threshold: 0.020462842658162117\n",
            "Epoch: 10, Loss: 9.45270039665047e-06 Updates: 92/37001, Avg Grad: 0.019338805228471756, Threshold: 0.019338805228471756\n",
            "Epoch: 10, Loss: 0.0035949775483459234 Updates: 95/38001, Avg Grad: 0.017412936314940453, Threshold: 0.013422364369034767\n",
            "Epoch: 10, Loss: 0.0021228163968771696 Updates: 97/39001, Avg Grad: 0.021242722868919373, Threshold: 0.021242722868919373\n",
            "Epoch 9 iteration 0 Loss: 0.015 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 300 Loss: 0.400 | Acc: 88.372% (266/301)\n",
            "Test accuracy: 0.8837209343910217\n",
            "Epoch: 10, Loss: 5.98787210037699e-06 Updates: 100/40001, Avg Grad: 0.030952204018831253, Threshold: 0.01988353207707405\n",
            "Epoch: 10, Loss: 8.792294465820305e-06 Updates: 102/41001, Avg Grad: 0.017038637772202492, Threshold: 0.017038637772202492\n",
            "Epoch: 10, Loss: 3.135840597678907e-05 Updates: 105/42001, Avg Grad: 0.025573739781975746, Threshold: 0.020913545042276382\n",
            "Epoch: 10, Loss: 0.0003324956924188882 Updates: 107/43001, Avg Grad: 0.021016716957092285, Threshold: 0.021016716957092285\n",
            "Epoch: 10, Loss: 4.893065124633722e-05 Updates: 110/44001, Avg Grad: 0.026870086789131165, Threshold: 0.014542952179908752\n",
            "Epoch: 10, Loss: 1.7616672266740352e-05 Updates: 112/45001, Avg Grad: 0.016209794208407402, Threshold: 0.016209794208407402\n",
            "Epoch: 10, Loss: 4.065924440510571e-05 Updates: 115/46001, Avg Grad: 0.02427997626364231, Threshold: 0.012780621647834778\n",
            "Epoch: 10, Loss: 7.873531103541609e-06 Updates: 117/47001, Avg Grad: 0.021436497569084167, Threshold: 0.021436497569084167\n",
            "Epoch: 10, Loss: 3.7161603358981665e-06 Updates: 120/48001, Avg Grad: 0.022647539153695107, Threshold: 0.02043045125901699\n",
            "Epoch: 10, Loss: 4.03316953452304e-05 Updates: 122/49001, Avg Grad: 0.01808289811015129, Threshold: 0.01808289811015129\n",
            "Epoch 9 iteration 0 Loss: 0.004 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 300 Loss: 0.275 | Acc: 92.359% (278/301)\n",
            "Test accuracy: 0.9235880374908447\n",
            "Epoch: 10, Loss: 0.00035691948141902685 Updates: 124/50001, Avg Grad: 0.02385754883289337, Threshold: 0.024270443245768547\n",
            "Epoch: 10, Loss: 4.794779670191929e-05 Updates: 127/51001, Avg Grad: 0.024860749021172523, Threshold: 0.017822852358222008\n",
            "Epoch: 10, Loss: 8.863750554155558e-05 Updates: 129/52001, Avg Grad: 0.012936590239405632, Threshold: 0.012936590239405632\n",
            "Epoch: 10, Loss: 0.0017947140149772167 Updates: 132/53001, Avg Grad: 0.01939029060304165, Threshold: 0.0146133778616786\n",
            "Epoch: 10, Loss: 0.02488343045115471 Updates: 134/54001, Avg Grad: 0.020214740186929703, Threshold: 0.020214740186929703\n",
            "Epoch: 10, Loss: 4.600278043653816e-05 Updates: 137/55001, Avg Grad: 0.0236650500446558, Threshold: 0.017125671729445457\n",
            "Epoch: 10, Loss: 0.0003022815799340606 Updates: 139/56001, Avg Grad: 0.017884310334920883, Threshold: 0.017884310334920883\n",
            "Epoch: 10, Loss: 1.8383403585175984e-05 Updates: 142/57001, Avg Grad: 0.02491009421646595, Threshold: 0.010602889582514763\n",
            "Epoch: 10, Loss: 0.00010544095130171627 Updates: 144/58001, Avg Grad: 0.01594574563205242, Threshold: 0.01594574563205242\n",
            "Epoch: 10, Loss: 0.0001922132505569607 Updates: 147/59001, Avg Grad: 0.03365163877606392, Threshold: 0.011923544108867645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## in-situ robustness"
      ],
      "metadata": {
        "id": "2Bw4ky766b_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = net_base"
      ],
      "metadata": {
        "id": "W7VmOENQ6hy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b"
      ],
      "metadata": {
        "id": "mKLenRyX6dq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wire_R = [(i+1)*2 for i in range(40)]\n",
        "p_stuck = [0.01*(i) for i in range(40)]\n",
        "R_std = [1e2*i for i in range(1, 101, 4)]\n",
        "\n",
        "\n",
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b\n",
        "\n",
        "err_org = network_tester(net, test_loader, 800, 0, False).item()\n",
        "print(\"original accuracy: \",err_org,\"%\")\n",
        "\n",
        "result = []\n",
        "\n",
        "for std in R_std:\n",
        "    result_wire = []\n",
        "    for var in p_stuck:\n",
        "        device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": std,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": std*10,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": var,\n",
        "                 \"p_stuck_off\": var}\n",
        "        crb_new = crossbar(device_params)\n",
        "\n",
        "        net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "        net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "        net.fc1.cb = crb_new\n",
        "        net.fc1.remap()\n",
        "\n",
        "        err = network_tester(net, test_loader, 800, 0, False).item()\n",
        "        result_wire.append(err/err_org)\n",
        "\n",
        "    result.append(result_wire)\n",
        "\n",
        "print(result)\n",
        "with open(\"result_non_ideal_CNN_base.txt\", 'w') as writefile:\n",
        "    writefile.write(str(result))\n",
        "\n",
        "ax = sns.heatmap(result, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "nOazYVUo-pJt",
        "outputId": "b01069d6-4aaa-4478-ad39-45578b21074f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original accuracy:  0.9250936508178711 %\n",
            "[[0.7260458686371879, 0.7058029588745102, 0.6720647329827604, 0.7125506169390463, 0.681511467159297, 0.7004048710814397, 0.6936572130169035, 0.682860972999832, 0.6842105432712975, 0.6788123910472964, 0.7031038827625097, 0.6869095549523675, 0.7004048710814397, 0.6869095549523675, 0.7058029588745102, 0.7125506169390463, 0.7152496286201163, 0.6545208993320832, 0.7125506169390463, 0.6963562891289041, 0.6990553008099741, 0.7004048710814397, 0.7165991988915819, 0.7058029588745102, 0.7031038827625097, 0.6990553008099741, 0.688259125223833, 0.7179487047321168, 0.7165991988915819, 0.7031038827625097, 0.7071524647150452, 0.7004048710814397, 0.6869095549523675, 0.6950067188574386, 0.6842105432712975, 0.6653171393491548, 0.7246963627966528, 0.7192982105726518, 0.7139001227795813, 0.7192982105726518], [0.7651821823220083, 0.7543859423049366, 0.7719298403865443, 0.7624831062100077, 0.7098515408270457, 0.7584345242574722, 0.7462887783998656, 0.740890690606795, 0.7300944505897234, 0.7543859423049366, 0.7476382842404006, 0.7516868661929361, 0.7597840300980072, 0.7314439564302584, 0.7395411203353295, 0.7395411203353295, 0.7449392725593306, 0.743589702287865, 0.7462887783998656, 0.7233467925251873, 0.7139001227795813, 0.7422401964473301, 0.7058029588745102, 0.7206477808441174, 0.7179487047321168, 0.7584345242574722, 0.7381916144947945, 0.7611336003694728, 0.8016194198948281, 0.7570850184169372, 0.7287449447491884, 0.7381916144947945, 0.740890690606795, 0.7206477808441174, 0.7058029588745102, 0.7422401964473301, 0.7314439564302584, 0.7557354481454717, 0.7071524647150452, 0.6720647329827604], [0.8825910589455389, 0.9203778667898243, 0.844804315532184, 0.844804315532184, 0.9149797145658232, 0.8582995672303256, 0.8137651657524347, 0.8596491375017912, 0.7597840300980072, 0.8272604174505763, 0.8731443891999328, 0.8812415531050038, 0.8582995672303256, 0.8893387170100749, 0.8421052394201834, 0.8542509852777901, 0.950067446298108, 0.9757085082847867, 0.8340080755151124, 0.9244264487423598, 0.8609986433423261, 0.839406163308183, 0.8272604174505763, 0.8002699140542932, 0.7975708379422926, 0.8124156599118997, 0.7773279281796148, 0.8340080755151124, 0.8529014794372551, 0.9163292848372887, 0.8421052394201834, 0.897435880915146, 0.8083670779593641, 0.8272604174505763, 0.7314439564302584, 0.8717948833593978, 0.7759784223390799, 0.8218623296575058, 0.7719298403865443, 0.8434547452607185], [0.5695006783288369, 0.8097165837998992, 0.7894736740372215, 0.8690958072473972, 0.8906882228506099, 0.8650472252948617, 0.8731443891999328, 0.839406163308183, 0.6518218232200826, 0.8083670779593641, 0.7759784223390799, 0.8542509852777901, 0.8353575813556474, 0.9190282965183587, 0.7759784223390799, 0.7665316881625432, 0.9014844628676816, 0.9149797145658232, 0.7192982105726518, 0.8785424769930034, 0.8191632535455052, 0.6896086310643681, 0.8947368048031454, 0.9217273726303592, 0.898785386755681, 0.8191632535455052, 0.9257759545828947, 0.950067446298108, 0.956815104362644, 0.8380566574676479, 0.9190282965183587, 0.896086375074611, 0.8636977194543267, 0.6018893339491213, 0.7139001227795813, 0.9149797145658232, 0.8650472252948617, 0.7624831062100077, 0.9284750306948953, 0.8785424769930034], [0.31713899478470275, 0.6248313198237995, 0.6221322437117989, 0.4116059177490204, 0.4574898572829116, 0.496626170967732, 0.5317139027000168, 0.1012145729749874, 0.6099864978541922, 0.43319836556769836, 0.40620782995594984, 0.5303643646440165, 0.5533063505186948, 0.4601889333949121, 0.534412946596552, 0.7773279281796148, 0.44399460558477, 0.36977056016766474, 0.3994601718914138, 0.793522255989757, 0.5897435880915146, 0.6923077071763686, 0.6234817495523339, 0.6342779895694055, 0.4493926933778405, 0.7071524647150452, 0.5816464241864435, 0.4170040377575562, 0.6977057949694391, 0.475033723149054, 0.267206473298276, 0.5695006783288369, 0.36842105432712974, 0.5492577685661593, 0.44534411142530494, 0.5087719168253386, 0.7638326120505428, 0.7557354481454717, 0.4035087538439493, 0.3522267265169876], [0.2753036372033471, 0.30229420503056087, 0.30229420503056087, 0.3900135021458077, 0.3441295626119165, 0.3846153821372719, 0.23481780157025908, 0.3076922928236314, 0.1794871761830292, 0.4804318431575898, 0.3913630079863427, 0.2253711157169204, 0.36302293431859395, 0.16599189226942232, 0.475033723149054, 0.2834008011084182, 0.2753036372033471, 0.4183535758135565, 0.2577597713372047, 0.24696356353559834, 0.19568150399317136, 0.3900135021458077, 0.24426450353133045, 0.13090418469873644, 0.30904183087963166, 0.1187584307872635, 0.20647772790251034, 0.12145749079153141, 0.475033723149054, 0.2968960850220251, 0.36302293431859395, 0.22132253376438485, 0.2753036372033471, 0.12010796078939745, 0.3522267265169876, 0.2051282059542427, 0.2510121454881339, 0.21592442986358168, 0.17273953422622576, 0.33873144260338073], [0.10391362492538898, 0.10526315492752293, 0.09851552102458581, 0.1012145729749874, 0.10526315492752293, 0.09041834906564841, 0.21862347376011695, 0.13225370664700406, 0.1187584307872635, 0.2510121454881339, 0.25236166743640154, 0.17543859423049366, 0.12820512469446851, 0.11740890078512954, 0.12280701273979903, 0.12820512469446851, 0.10796221493179084, 0.15249662446354806, 0.259109309393205, 0.2024291459499748, 0.2658569513500084, 0.19298246009663608, 0.15789472836435126, 0.1646423703211547, 0.11470984883472796, 0.2834008011084182, 0.15114710251528043, 0.16329284837288705, 0.11336031883259401, 0.38596492019327217, 0.2604588313414726, 0.1295546546966025, 0.0958164610203179, 0.12280701273979903, 0.12010796078939745, 0.1012145729749874, 0.11336031883259401, 0.25236166743640154, 0.26990553330254397, 0.1187584307872635], [0.09311740906991632, 0.24426450353133045, 0.11740890078512954, 0.11605937883686192, 0.16329284837288705, 0.11066126688219242, 0.1673414303254226, 0.09851552102458581, 0.13225370664700406, 0.09716599102245185, 0.15249662446354806, 0.1470985205627449, 0.10796221493179084, 0.10526315492752293, 0.1997300859457069, 0.2145748918075814, 0.10526315492752293, 0.08097165515844339, 0.10796221493179084, 0.10796221493179084, 0.13900134054994118, 0.17408907228222603, 0.09176787906778237, 0.09986504297285345, 0.08906882711738079, 0.12685559469233457, 0.10391362492538898, 0.11605937883686192, 0.18623481813983264, 0.10661268492965689, 0.09716599102245185, 0.08771929711524683, 0.11201079688432637, 0.10526315492752293, 0.14574898250674462, 0.11201079688432637, 0.10661268492965689, 0.10661268492965689, 0.0958164610203179, 0.09716599102245185], [0.124156542741933, 0.13225370664700406, 0.08636976711311288, 0.10256410297712135, 0.10256410297712135, 0.21322536985931378, 0.11470984883472796, 0.08636976711311288, 0.1012145729749874, 0.11470984883472796, 0.10256410297712135, 0.09041834906564841, 0.11740890078512954, 0.12010796078939745, 0.12010796078939745, 0.13090418469873644, 0.09446693101818396, 0.08636976711311288, 0.12820512469446851, 0.06747638332563599, 0.11201079688432637, 0.12685559469233457, 0.07557355125764022, 0.12280701273979903, 0.09986504297285345, 0.09446693101818396, 0.1646423703211547, 0.12010796078939745, 0.20377866789824242, 0.11201079688432637, 0.09851552102458581, 0.17543859423049366, 0.10391362492538898, 0.11201079688432637, 0.10661268492965689, 0.11336031883259401, 0.08097165515844339, 0.09311740906991632, 0.09446693101818396, 0.10661268492965689], [0.16194331031688677, 0.13765181860167355, 0.10391362492538898, 0.10256410297712135, 0.09851552102458581, 0.11740890078512954, 0.0958164610203179, 0.144399460558477, 0.11066126688219242, 0.2145748918075814, 0.10526315492752293, 0.10661268492965689, 0.10391362492538898, 0.12145749079153141, 0.1295546546966025, 0.11066126688219242, 0.10661268492965689, 0.1538461464118157, 0.08771929711524683, 0.15789472836435126, 0.11066126688219242, 0.11336031883259401, 0.11066126688219242, 0.10256410297712135, 0.15249662446354806, 0.09176787906778237, 0.09851552102458581, 0.1012145729749874, 0.09986504297285345, 0.09716599102245185, 0.09986504297285345, 0.07692307320590785, 0.08906882711738079, 0.11740890078512954, 0.10526315492752293, 0.10661268492965689, 0.13900134054994118, 0.11201079688432637, 0.11201079688432637, 0.13900134054994118], [0.08097165515844339, 0.10661268492965689, 0.11201079688432637, 0.13495276665127198, 0.08502024516484524, 0.09716599102245185, 0.11201079688432637, 0.10526315492752293, 0.09041834906564841, 0.10796221493179084, 0.1417004005542091, 0.12685559469233457, 0.11336031883259401, 0.12280701273979903, 0.10526315492752293, 0.11336031883259401, 0.13900134054994118, 0.09986504297285345, 0.0958164610203179, 0.08636976711311288, 0.12010796078939745, 0.06342779734616728, 0.10661268492965689, 0.10661268492965689, 0.11066126688219242, 0.13225370664700406, 0.10391362492538898, 0.12010796078939745, 0.09311740906991632, 0.09716599102245185, 0.10796221493179084, 0.09176787906778237, 0.11740890078512954, 0.07692307320590785, 0.11336031883259401, 0.10796221493179084, 0.1187584307872635, 0.09851552102458581, 0.09716599102245185, 0.12280701273979903], [0.17273953422622576, 0.11336031883259401, 0.10661268492965689, 0.124156542741933, 0.10796221493179084, 0.1295546546966025, 0.11201079688432637, 0.10661268492965689, 0.10796221493179084, 0.1187584307872635, 0.0836707151627113, 0.10796221493179084, 0.12010796078939745, 0.09716599102245185, 0.10391362492538898, 0.10931173688005848, 0.11066126688219242, 0.07962213321017576, 0.11336031883259401, 0.07422402125550627, 0.10256410297712135, 0.10661268492965689, 0.12010796078939745, 0.09311740906991632, 0.13630229665340593, 0.10256410297712135, 0.11066126688219242, 0.10391362492538898, 0.1012145729749874, 0.07017543930297072, 0.10661268492965689, 0.12820512469446851, 0.1012145729749874, 0.08097165515844339, 0.10256410297712135, 0.12010796078939745, 0.09986504297285345, 0.13090418469873644, 0.08097165515844339, 0.09851552102458581], [0.12280701273979903, 0.09311740906991632, 0.0782726032080418, 0.0958164610203179, 0.10256410297712135, 0.10391362492538898, 0.10526315492752293, 0.10256410297712135, 0.09986504297285345, 0.10391362492538898, 0.08232118516057735, 0.12145749079153141, 0.10526315492752293, 0.12820512469446851, 0.09311740906991632, 0.10661268492965689, 0.12280701273979903, 0.08771929711524683, 0.09446693101818396, 0.124156542741933, 0.1012145729749874, 0.10661268492965689, 0.10931173688005848, 0.08636976711311288, 0.08502024516484524, 0.12550607274406694, 0.15789472836435126, 0.10526315492752293, 0.11740890078512954, 0.10931173688005848, 0.10256410297712135, 0.1012145729749874, 0.10796221493179084, 0.10391362492538898, 0.11605937883686192, 0.15114710251528043, 0.10661268492965689, 0.10526315492752293, 0.11740890078512954, 0.13630229665340593], [0.10796221493179084, 0.09986504297285345, 0.12145749079153141, 0.09311740906991632, 0.10661268492965689, 0.09446693101818396, 0.1902834000923682, 0.1012145729749874, 0.09851552102458581, 0.09041834906564841, 0.09716599102245185, 0.10661268492965689, 0.10526315492752293, 0.10931173688005848, 0.12145749079153141, 0.09851552102458581, 0.11740890078512954, 0.11470984883472796, 0.11336031883259401, 0.08906882711738079, 0.10661268492965689, 0.09716599102245185, 0.1012145729749874, 0.0958164610203179, 0.10796221493179084, 0.09176787906778237, 0.08232118516057735, 0.11336031883259401, 0.08906882711738079, 0.15789472836435126, 0.11201079688432637, 0.07557355125764022, 0.11470984883472796, 0.11066126688219242, 0.12280701273979903, 0.09851552102458581, 0.09311740906991632, 0.09851552102458581, 0.10256410297712135, 0.08771929711524683], [0.11336031883259401, 0.09176787906778237, 0.1295546546966025, 0.11066126688219242, 0.12280701273979903, 0.09851552102458581, 0.08771929711524683, 0.09716599102245185, 0.0836707151627113, 0.12145749079153141, 0.15249662446354806, 0.10391362492538898, 0.0958164610203179, 0.08771929711524683, 0.0958164610203179, 0.09851552102458581, 0.10526315492752293, 0.09446693101818396, 0.11336031883259401, 0.12010796078939745, 0.0958164610203179, 0.08232118516057735, 0.11201079688432637, 0.10796221493179084, 0.13630229665340593, 0.11470984883472796, 0.10661268492965689, 0.09176787906778237, 0.11201079688432637, 0.10391362492538898, 0.124156542741933, 0.08771929711524683, 0.12820512469446851, 0.09716599102245185, 0.11740890078512954, 0.08502024516484524, 0.07422402125550627, 0.12010796078939745, 0.09176787906778237, 0.09851552102458581], [0.1012145729749874, 0.08906882711738079, 0.13630229665340593, 0.08636976711311288, 0.10391362492538898, 0.11336031883259401, 0.10796221493179084, 0.11201079688432637, 0.13495276665127198, 0.10661268492965689, 0.0958164610203179, 0.11336031883259401, 0.1012145729749874, 0.12145749079153141, 0.10526315492752293, 0.10796221493179084, 0.09716599102245185, 0.10256410297712135, 0.09446693101818396, 0.12280701273979903, 0.0958164610203179, 0.14574898250674462, 0.07692307320590785, 0.12280701273979903, 0.12280701273979903, 0.11336031883259401, 0.14304992250247672, 0.10256410297712135, 0.11066126688219242, 0.11470984883472796, 0.09041834906564841, 0.18488528008383237, 0.1295546546966025, 0.11740890078512954, 0.09851552102458581, 0.1012145729749874, 0.10931173688005848, 0.10391362492538898, 0.09041834906564841, 0.1187584307872635], [0.11470984883472796, 0.1417004005542091, 0.11201079688432637, 0.09311740906991632, 0.0958164610203179, 0.11740890078512954, 0.09851552102458581, 0.10391362492538898, 0.09986504297285345, 0.10391362492538898, 0.12820512469446851, 0.09176787906778237, 0.0958164610203179, 0.08906882711738079, 0.09176787906778237, 0.10391362492538898, 0.0958164610203179, 0.09311740906991632, 0.09716599102245185, 0.06882590930083678, 0.10796221493179084, 0.1470985205627449, 0.10661268492965689, 0.11740890078512954, 0.09986504297285345, 0.09851552102458581, 0.12010796078939745, 0.1012145729749874, 0.10256410297712135, 0.19568150399317136, 0.07017543930297072, 0.10526315492752293, 0.11201079688432637, 0.09311740906991632, 0.19703104204917163, 0.12145749079153141, 0.11066126688219242, 0.09311740906991632, 0.09311740906991632, 0.05533063344109621], [0.08502024516484524, 0.12010796078939745, 0.11470984883472796, 0.1417004005542091, 0.124156542741933, 0.11201079688432637, 0.10256410297712135, 0.01619433183707531, 0.11066126688219242, 0.1187584307872635, 0.11740890078512954, 0.11740890078512954, 0.11470984883472796, 0.15519568446781598, 0.14035087860594145, 0.13900134054994118, 0.04723346550909198, 0.11066126688219242, 0.10661268492965689, 0.1187584307872635, 0.09041834906564841, 0.10256410297712135, 0.10526315492752293, 0.1187584307872635, 0.10391362492538898, 0.09986504297285345, 0.1538461464118157, 0.11201079688432637, 0.07287449125337231, 0.16194331031688677, 0.07692307320590785, 0.1012145729749874, 0.10661268492965689, 0.0958164610203179, 0.08906882711738079, 0.08906882711738079, 0.09041834906564841, 0.11066126688219242, 0.10931173688005848, 0.10931173688005848], [0.09446693101818396, 0.07017543930297072, 0.0958164610203179, 0.1417004005542091, 0.144399460558477, 0.08771929711524683, 0.0958164610203179, 0.09446693101818396, 0.1187584307872635, 0.12010796078939745, 0.10391362492538898, 0.12010796078939745, 0.09446693101818396, 0.08771929711524683, 0.10931173688005848, 0.12550607274406694, 0.07287449125337231, 0.09851552102458581, 0.11336031883259401, 0.124156542741933, 0.08097165515844339, 0.09446693101818396, 0.11201079688432637, 0.10526315492752293, 0.06882590930083678, 0.10931173688005848, 0.10661268492965689, 0.07422402125550627, 0.09986504297285345, 0.0958164610203179, 0.11605937883686192, 0.1187584307872635, 0.10796221493179084, 0.11336031883259401, 0.15114710251528043, 0.10256410297712135, 0.10256410297712135, 0.09041834906564841, 0.14574898250674462, 0.09176787906778237], [0.09986504297285345, 0.09851552102458581, 0.11605937883686192, 0.11066126688219242, 0.10391362492538898, 0.12820512469446851, 0.1187584307872635, 0.10661268492965689, 0.11201079688432637, 0.11470984883472796, 0.10796221493179084, 0.1187584307872635, 0.11470984883472796, 0.12010796078939745, 0.10796221493179084, 0.11201079688432637, 0.10661268492965689, 0.13090418469873644, 0.10796221493179084, 0.09176787906778237, 0.07017543930297072, 0.12145749079153141, 0.12820512469446851, 0.10931173688005848, 0.10661268492965689, 0.0958164610203179, 0.09851552102458581, 0.10796221493179084, 0.12280701273979903, 0.10256410297712135, 0.10526315492752293, 0.12145749079153141, 0.08097165515844339, 0.09311740906991632, 0.09311740906991632, 0.11201079688432637, 0.0958164610203179, 0.124156542741933, 0.11740890078512954, 0.09851552102458581], [0.1012145729749874, 0.09986504297285345, 0.0958164610203179, 0.09311740906991632, 0.08906882711738079, 0.133603236649138, 0.11605937883686192, 0.0958164610203179, 0.09041834906564841, 0.10256410297712135, 0.11605937883686192, 0.10931173688005848, 0.10661268492965689, 0.09716599102245185, 0.0958164610203179, 0.10661268492965689, 0.10796221493179084, 0.11201079688432637, 0.09041834906564841, 0.1012145729749874, 0.11336031883259401, 0.06477732734830124, 0.10931173688005848, 0.08502024516484524, 0.11605937883686192, 0.1470985205627449, 0.09986504297285345, 0.1012145729749874, 0.10391362492538898, 0.11740890078512954, 0.11336031883259401, 0.10256410297712135, 0.10796221493179084, 0.09851552102458581, 0.09176787906778237, 0.04048582757922169, 0.09176787906778237, 0.11201079688432637, 0.1187584307872635, 0.08771929711524683], [0.11201079688432637, 0.09446693101818396, 0.11605937883686192, 0.12685559469233457, 0.10526315492752293, 0.09716599102245185, 0.13225370664700406, 0.10931173688005848, 0.11740890078512954, 0.13630229665340593, 0.08771929711524683, 0.13765181860167355, 0.08502024516484524, 0.10391362492538898, 0.12280701273979903, 0.10526315492752293, 0.09851552102458581, 0.12280701273979903, 0.08771929711524683, 0.1470985205627449, 0.08771929711524683, 0.09851552102458581, 0.09986504297285345, 0.09716599102245185, 0.12010796078939745, 0.11336031883259401, 0.10796221493179084, 0.10796221493179084, 0.11336031883259401, 0.09716599102245185, 0.09851552102458581, 0.10796221493179084, 0.10661268492965689, 0.09176787906778237, 0.09446693101818396, 0.1012145729749874, 0.11470984883472796, 0.12280701273979903, 0.11605937883686192, 0.10256410297712135], [0.1417004005542091, 0.11470984883472796, 0.10796221493179084, 0.09311740906991632, 0.07962213321017576, 0.12010796078939745, 0.1012145729749874, 0.10796221493179084, 0.1187584307872635, 0.09176787906778237, 0.07692307320590785, 0.09851552102458581, 0.08502024516484524, 0.15519568446781598, 0.09176787906778237, 0.1012145729749874, 0.10526315492752293, 0.10796221493179084, 0.11066126688219242, 0.10526315492752293, 0.1187584307872635, 0.11066126688219242, 0.0836707151627113, 0.09851552102458581, 0.11066126688219242, 0.11201079688432637, 0.09986504297285345, 0.09446693101818396, 0.10526315492752293, 0.10391362492538898, 0.10391362492538898, 0.10661268492965689, 0.1187584307872635, 0.10391362492538898, 0.10391362492538898, 0.10796221493179084, 0.09311740906991632, 0.0958164610203179, 0.10931173688005848, 0.1012145729749874], [0.10526315492752293, 0.1187584307872635, 0.09986504297285345, 0.0958164610203179, 0.10526315492752293, 0.09446693101818396, 0.09851552102458581, 0.11066126688219242, 0.09041834906564841, 0.14844804251101254, 0.11470984883472796, 0.1012145729749874, 0.11605937883686192, 0.12280701273979903, 0.08636976711311288, 0.124156542741933, 0.09446693101818396, 0.10661268492965689, 0.10661268492965689, 0.07962213321017576, 0.09986504297285345, 0.08771929711524683, 0.13495276665127198, 0.12145749079153141, 0.10391362492538898, 0.10256410297712135, 0.09311740906991632, 0.056680159416297005, 0.08771929711524683, 0.12145749079153141, 0.11201079688432637, 0.12550607274406694, 0.09851552102458581, 0.124156542741933, 0.11605937883686192, 0.09176787906778237, 0.12010796078939745, 0.11605937883686192, 0.1538461464118157, 0.09716599102245185], [0.10526315492752293, 0.11201079688432637, 0.10796221493179084, 0.10661268492965689, 0.07422402125550627, 0.13090418469873644, 0.09311740906991632, 0.09446693101818396, 0.11201079688432637, 0.07287449125337231, 0.14035087860594145, 0.10391362492538898, 0.10796221493179084, 0.0782726032080418, 0.1794871761830292, 0.12010796078939745, 0.1187584307872635, 0.10256410297712135, 0.10391362492538898, 0.0958164610203179, 0.09176787906778237, 0.09986504297285345, 0.10796221493179084, 0.11201079688432637, 0.1673414303254226, 0.11740890078512954, 0.10796221493179084, 0.10526315492752293, 0.07692307320590785, 0.11066126688219242, 0.10391362492538898, 0.13900134054994118, 0.10661268492965689, 0.12145749079153141, 0.12145749079153141, 0.10931173688005848, 0.124156542741933, 0.12145749079153141, 0.11201079688432637, 0.11066126688219242]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wire_R = [(i+1)*2 for i in range(40)]\n",
        "p_stuck = [0.01*(i) for i in range(40)]\n",
        "R_std = [1e2*i for i in range(1, 101, 4)]\n",
        "\n",
        "\n",
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b\n",
        "\n",
        "err_org = network_tester(net, test_loader, 800, 0, False).item()\n",
        "print(\"original accuracy: \",err_org,\"%\")\n",
        "\n",
        "result = []\n",
        "\n",
        "for std in R_std:\n",
        "    result_wire = []\n",
        "    for var in p_stuck:\n",
        "        device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": std,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": std*10,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": var,\n",
        "                 \"p_stuck_off\": var}\n",
        "        crb_new = crossbar(device_params)\n",
        "\n",
        "        net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "        net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "        net.fc1.cb = crb_new\n",
        "        net.fc1.remap()\n",
        "\n",
        "        err = network_tester(net, test_loader, 800, 0, False).item()\n",
        "        result_wire.append(err/err_org)\n",
        "\n",
        "    result.append(result_wire)\n",
        "\n",
        "print(result)\n",
        "with open(\"result_non_ideal_CNN_thresh.txt\", 'w') as writefile:\n",
        "    writefile.write(str(result))\n",
        "\n",
        "ax = sns.heatmap(result, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "uzX1TZRPKELD",
        "outputId": "032e4c63-1ebd-4dd4-c058-b66f9465e352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original accuracy:  0.9250936508178711 %\n",
            "[[0.5991902578371207, 0.5978407519965857, 0.6032388397896562, 0.5883940822509796, 0.5829959300269785, 0.5910930939320496, 0.6126855739661928, 0.5856950061389791, 0.6099864978541922, 0.6018893339491213, 0.5910930939320496, 0.6113360036947273, 0.5978407519965857, 0.5964912461560506, 0.5937921700440502, 0.6167341559187284, 0.6018893339491213, 0.5748987661219075, 0.5735492602813724, 0.6072874217421917, 0.5748987661219075, 0.6153845856472628, 0.5748987661219075, 0.6005398281085862, 0.5951416758845851, 0.5641025261048358, 0.6423751534744766, 0.5641025261048358, 0.6099864978541922, 0.6086369920136573, 0.5951416758845851, 0.6140350798067278, 0.6032388397896562, 0.6005398281085862, 0.6086369920136573, 0.5587044383117653, 0.6086369920136573, 0.5641025261048358, 0.5668016022168364, 0.6234817495523339], [0.7354925383827939, 0.7179487047321168, 0.7557354481454717, 0.7152496286201163, 0.7759784223390799, 0.7665316881625432, 0.7786774340201499, 0.7381916144947945, 0.7179487047321168, 0.8097165837998992, 0.7368421086542595, 0.7004048710814397, 0.6936572130169035, 0.7611336003694728, 0.7611336003694728, 0.7638326120505428, 0.7489878545118661, 0.8313089994031119, 0.8178137477049703, 0.7584345242574722, 0.688259125223833, 0.7327935267017239, 0.6545208993320832, 0.7246963627966528, 0.6896086310643681, 0.8272604174505763, 0.7786774340201499, 0.786774597925221, 0.7058029588745102, 0.7476382842404006, 0.7112010466675808, 0.7638326120505428, 0.7584345242574722, 0.7139001227795813, 0.7611336003694728, 0.7800270042916154, 0.8299594935625768, 0.7543859423049366, 0.7597840300980072, 0.7476382842404006], [0.9068825506607521, 0.8407557335796485, 0.7085020349865108, 0.6990553008099741, 0.7705802701150788, 0.8056680018473636, 0.6923077071763686, 0.8164642418644352, 0.8056680018473636, 0.7206477808441174, 0.7962213321017576, 0.8151146715929697, 0.8137651657524347, 0.8137651657524347, 0.7827260159726854, 0.8151146715929697, 0.7692307642745438, 0.6801619613187619, 0.7948717618302921, 0.8731443891999328, 0.8218623296575058, 0.8650472252948617, 0.740890690606795, 0.8151146715929697, 0.8178137477049703, 0.8893387170100749, 0.8542509852777901, 0.8434547452607185, 0.7449392725593306, 0.7921727501492221, 0.8002699140542932, 0.8650472252948617, 0.8569500613897907, 0.690958136904903, 0.8205128238169708, 0.6653171393491548, 0.8407557335796485, 0.8178137477049703, 0.8744938950404678, 0.7139001227795813], [0.7651821823220083, 0.6342779895694055, 0.8313089994031119, 0.6113360036947273, 0.5398110666050879, 0.8380566574676479, 0.41565449970155594, 0.7300944505897234, 0.8529014794372551, 0.7921727501492221, 0.6734143032542259, 0.6801619613187619, 0.7881241681966865, 0.8353575813556474, 0.7219972866846524, 0.7206477808441174, 0.6747638090947609, 0.682860972999832, 0.7381916144947945, 0.7624831062100077, 0.7152496286201163, 0.7692307642745438, 0.7219972866846524, 0.5937921700440502, 0.8272604174505763, 0.8299594935625768, 0.5897435880915146, 0.6275303315048695, 0.8245614057695063, 0.3846153821372719, 0.8286099877220419, 0.5614035144237658, 0.8245614057695063, 0.5236167387949457, 0.7786774340201499, 0.6693657213016904, 0.341430486499916, 0.7719298403865443, 0.6545208993320832, 0.6342779895694055], [0.4170040377575562, 0.41430499386102093, 0.41430499386102093, 0.16869095227369021, 0.20647772790251034, 0.32388662063377355, 0.6923077071763686, 0.3805668001847364, 0.5573549324712302, 0.4507422314338408, 0.45209176948984103, 0.473684217308519, 0.31039136893563196, 0.341430486499916, 0.49257758901519644, 0.267206473298276, 0.578947348074443, 0.13495276665127198, 0.5020242909762678, 0.40890687385248514, 0.3508771884609873, 0.31713899478470275, 0.5290148265880162, 0.5155195426744094, 0.30904183087963166, 0.4885290070626609, 0.4493926933778405, 0.49932521486426723, 0.24966260743213362, 0.4588393953389118, 0.5168690807304096, 0.422402157766092, 0.39406208409834326, 0.526315782691481, 0.341430486499916, 0.30229420503056087, 0.5641025261048358, 0.6464237354270121, 0.3049932489270961, 0.3211875767372383], [0.2820512630524179, 0.20377866789824242, 0.22402159376865274, 0.2510121454881339, 0.43859648557623415, 0.18893386203636792, 0.10796221493179084, 0.16599189226942232, 0.09851552102458581, 0.2051282059542427, 0.237516861574527, 0.10526315492752293, 0.3144399508881675, 0.21052630985504586, 0.19703104204917163, 0.34682860650845176, 0.2280701757211883, 0.2834008011084182, 0.19568150399317136, 0.14304992250247672, 0.17543859423049366, 0.32253711479323854, 0.20107962400170717, 0.23076921961772356, 0.27800268109988235, 0.1538461464118157, 0.4588393953389118, 0.3049932489270961, 0.3130904128321672, 0.20647772790251034, 0.1943319820449037, 0.31174087477616697, 0.2753036372033471, 0.16329284837288705, 0.17543859423049366, 0.3346828606508452, 0.2766531752593474, 0.13090418469873644, 0.2024291459499748, 0.2280701757211883], [0.11740890078512954, 0.10931173688005848, 0.11336031883259401, 0.24966260743213362, 0.1417004005542091, 0.09041834906564841, 0.1646423703211547, 0.12280701273979903, 0.17543859423049366, 0.11066126688219242, 0.1902834000923682, 0.1767881161787613, 0.19703104204917163, 0.10931173688005848, 0.0958164610203179, 0.12280701273979903, 0.21052630985504586, 0.11066126688219242, 0.10796221493179084, 0.133603236649138, 0.10256410297712135, 0.13900134054994118, 0.14304992250247672, 0.09311740906991632, 0.14035087860594145, 0.11066126688219242, 0.12145749079153141, 0.08636976711311288, 0.1012145729749874, 0.11740890078512954, 0.1767881161787613, 0.124156542741933, 0.20917678790677824, 0.19298246009663608, 0.23346827962199146, 0.13495276665127198, 0.11201079688432637, 0.11336031883259401, 0.17004049032969049, 0.18488528008383237], [0.09851552102458581, 0.15924426642035153, 0.13495276665127198, 0.11605937883686192, 0.1187584307872635, 0.10796221493179084, 0.13630229665340593, 0.12010796078939745, 0.1012145729749874, 0.13630229665340593, 0.09851552102458581, 0.14979756445928016, 0.11336031883259401, 0.10796221493179084, 0.10256410297712135, 0.11201079688432637, 0.11066126688219242, 0.133603236649138, 0.14844804251101254, 0.12145749079153141, 0.12280701273979903, 0.11066126688219242, 0.09311740906991632, 0.16059378836861915, 0.1794871761830292, 0.12280701273979903, 0.1187584307872635, 0.11201079688432637, 0.09446693101818396, 0.11066126688219242, 0.10931173688005848, 0.10526315492752293, 0.08906882711738079, 0.10391362492538898, 0.124156542741933, 0.09851552102458581, 0.1794871761830292, 0.12145749079153141, 0.10796221493179084, 0.13090418469873644], [0.03778677562882011, 0.11201079688432637, 0.09986504297285345, 0.11336031883259401, 0.11201079688432637, 0.13495276665127198, 0.14304992250247672, 0.10931173688005848, 0.09716599102245185, 0.05398110746589542, 0.11605937883686192, 0.15114710251528043, 0.11201079688432637, 0.11336031883259401, 0.12820512469446851, 0.09176787906778237, 0.10796221493179084, 0.11336031883259401, 0.124156542741933, 0.13090418469873644, 0.11740890078512954, 0.09716599102245185, 0.052631577463761466, 0.1187584307872635, 0.13225370664700406, 0.09311740906991632, 0.10391362492538898, 0.09446693101818396, 0.11336031883259401, 0.11201079688432637, 0.10526315492752293, 0.10526315492752293, 0.10796221493179084, 0.20377866789824242, 0.10661268492965689, 0.12550607274406694, 0.124156542741933, 0.10256410297712135, 0.11605937883686192, 0.13900134054994118], [0.09311740906991632, 0.11740890078512954, 0.10661268492965689, 0.10931173688005848, 0.048582995511225927, 0.12550607274406694, 0.11066126688219242, 0.12145749079153141, 0.11201079688432637, 0.10931173688005848, 0.10796221493179084, 0.10661268492965689, 0.11470984883472796, 0.12145749079153141, 0.10931173688005848, 0.11336031883259401, 0.11605937883686192, 0.03508771965148536, 0.10931173688005848, 0.13225370664700406, 0.10526315492752293, 0.11336031883259401, 0.1012145729749874, 0.12010796078939745, 0.1012145729749874, 0.12820512469446851, 0.10526315492752293, 0.09716599102245185, 0.07287449125337231, 0.10391362492538898, 0.0958164610203179, 0.12145749079153141, 0.1417004005542091, 0.16194331031688677, 0.11066126688219242, 0.10931173688005848, 0.10256410297712135, 0.12685559469233457, 0.124156542741933, 0.11336031883259401], [0.1012145729749874, 0.14574898250674462, 0.124156542741933, 0.09986504297285345, 0.0958164610203179, 0.11470984883472796, 0.1012145729749874, 0.10526315492752293, 0.12820512469446851, 0.10931173688005848, 0.10931173688005848, 0.11470984883472796, 0.1012145729749874, 0.12010796078939745, 0.12550607274406694, 0.12820512469446851, 0.12685559469233457, 0.07017543930297072, 0.124156542741933, 0.11605937883686192, 0.09986504297285345, 0.08097165515844339, 0.10391362492538898, 0.10796221493179084, 0.0958164610203179, 0.12010796078939745, 0.09986504297285345, 0.13090418469873644, 0.12145749079153141, 0.10526315492752293, 0.11066126688219242, 0.12280701273979903, 0.10661268492965689, 0.10526315492752293, 0.08097165515844339, 0.1187584307872635, 0.12685559469233457, 0.10256410297712135, 0.10931173688005848, 0.12280701273979903], [0.0958164610203179, 0.11201079688432637, 0.09716599102245185, 0.10931173688005848, 0.10661268492965689, 0.11066126688219242, 0.07557355125764022, 0.09986504297285345, 0.09446693101818396, 0.08502024516484524, 0.10796221493179084, 0.10256410297712135, 0.11066126688219242, 0.09851552102458581, 0.10796221493179084, 0.1538461464118157, 0.1187584307872635, 0.1187584307872635, 0.17408907228222603, 0.10256410297712135, 0.1187584307872635, 0.12685559469233457, 0.12280701273979903, 0.08232118516057735, 0.11470984883472796, 0.11066126688219242, 0.124156542741933, 0.11605937883686192, 0.11470984883472796, 0.09716599102245185, 0.11336031883259401, 0.10661268492965689, 0.09311740906991632, 0.10391362492538898, 0.0958164610203179, 0.09311740906991632, 0.12145749079153141, 0.16059378836861915, 0.10391362492538898, 0.12010796078939745], [0.10256410297712135, 0.12010796078939745, 0.0958164610203179, 0.12550607274406694, 0.10391362492538898, 0.12145749079153141, 0.11740890078512954, 0.06477732734830124, 0.12550607274406694, 0.08636976711311288, 0.12280701273979903, 0.11066126688219242, 0.09986504297285345, 0.11605937883686192, 0.10661268492965689, 0.09716599102245185, 0.07557355125764022, 0.11605937883686192, 0.10796221493179084, 0.18083669813129682, 0.1417004005542091, 0.08771929711524683, 0.12145749079153141, 0.11470984883472796, 0.09986504297285345, 0.11066126688219242, 0.10796221493179084, 0.11066126688219242, 0.12010796078939745, 0.12280701273979903, 0.12550607274406694, 0.11740890078512954, 0.14035087860594145, 0.12685559469233457, 0.0620782713709665, 0.11066126688219242, 0.12685559469233457, 0.11336031883259401, 0.09041834906564841, 0.11336031883259401], [0.11470984883472796, 0.12550607274406694, 0.1012145729749874, 0.10931173688005848, 0.11740890078512954, 0.11740890078512954, 0.09986504297285345, 0.13090418469873644, 0.08906882711738079, 0.10661268492965689, 0.11066126688219242, 0.12550607274406694, 0.12145749079153141, 0.124156542741933, 0.09716599102245185, 0.11336031883259401, 0.09716599102245185, 0.09986504297285345, 0.12145749079153141, 0.11740890078512954, 0.07017543930297072, 0.1187584307872635, 0.12280701273979903, 0.11605937883686192, 0.10661268492965689, 0.124156542741933, 0.11066126688219242, 0.10256410297712135, 0.12550607274406694, 0.10526315492752293, 0.11336031883259401, 0.09986504297285345, 0.18893386203636792, 0.11336031883259401, 0.1565452064160836, 0.10526315492752293, 0.09041834906564841, 0.12145749079153141, 0.12550607274406694, 0.11470984883472796], [0.1187584307872635, 0.11066126688219242, 0.1012145729749874, 0.09851552102458581, 0.09986504297285345, 0.029689607696815876, 0.11740890078512954, 0.12685559469233457, 0.11740890078512954, 0.144399460558477, 0.09041834906564841, 0.133603236649138, 0.10391362492538898, 0.14035087860594145, 0.09176787906778237, 0.1295546546966025, 0.11201079688432637, 0.1187584307872635, 0.10391362492538898, 0.14979756445928016, 0.1417004005542091, 0.11605937883686192, 0.10931173688005848, 0.11470984883472796, 0.07152496125123836, 0.11605937883686192, 0.09986504297285345, 0.12280701273979903, 0.09716599102245185, 0.12550607274406694, 0.10256410297712135, 0.1187584307872635, 0.13495276665127198, 0.09986504297285345, 0.11336031883259401, 0.10931173688005848, 0.09176787906778237, 0.13495276665127198, 0.10526315492752293, 0.11201079688432637], [0.12145749079153141, 0.07152496125123836, 0.11605937883686192, 0.10931173688005848, 0.10256410297712135, 0.14844804251101254, 0.11605937883686192, 0.10931173688005848, 0.13630229665340593, 0.05398110746589542, 0.10526315492752293, 0.10796221493179084, 0.124156542741933, 0.1565452064160836, 0.03778677562882011, 0.0958164610203179, 0.11066126688219242, 0.13090418469873644, 0.17813765423476158, 0.12145749079153141, 0.09446693101818396, 0.06747638332563599, 0.11605937883686192, 0.11470984883472796, 0.0958164610203179, 0.1187584307872635, 0.12820512469446851, 0.11336031883259401, 0.10661268492965689, 0.14304992250247672, 0.09311740906991632, 0.09851552102458581, 0.1012145729749874, 0.1295546546966025, 0.1012145729749874, 0.12685559469233457, 0.09716599102245185, 0.07692307320590785, 0.1538461464118157, 0.09716599102245185], [0.124156542741933, 0.12145749079153141, 0.11740890078512954, 0.10526315492752293, 0.1187584307872635, 0.1767881161787613, 0.1012145729749874, 0.12010796078939745, 0.08771929711524683, 0.0958164610203179, 0.08636976711311288, 0.10391362492538898, 0.09851552102458581, 0.10391362492538898, 0.11201079688432637, 0.09716599102245185, 0.13225370664700406, 0.11740890078512954, 0.1187584307872635, 0.11740890078512954, 0.11066126688219242, 0.13225370664700406, 0.09851552102458581, 0.11336031883259401, 0.11066126688219242, 0.11336031883259401, 0.1187584307872635, 0.08232118516057735, 0.06612685332350203, 0.10661268492965689, 0.10391362492538898, 0.07422402125550627, 0.09986504297285345, 0.12280701273979903, 0.09851552102458581, 0.10526315492752293, 0.1187584307872635, 0.13225370664700406, 0.12685559469233457, 0.12145749079153141], [0.11470984883472796, 0.13225370664700406, 0.12685559469233457, 0.13225370664700406, 0.12550607274406694, 0.10931173688005848, 0.09311740906991632, 0.09986504297285345, 0.0836707151627113, 0.10526315492752293, 0.10931173688005848, 0.07422402125550627, 0.16194331031688677, 0.10256410297712135, 0.10661268492965689, 0.10661268492965689, 0.11201079688432637, 0.11201079688432637, 0.12685559469233457, 0.11066126688219242, 0.0958164610203179, 0.12820512469446851, 0.10391362492538898, 0.09986504297285345, 0.10256410297712135, 0.09851552102458581, 0.1187584307872635, 0.11605937883686192, 0.12010796078939745, 0.11740890078512954, 0.10391362492538898, 0.11336031883259401, 0.05398110746589542, 0.133603236649138, 0.18623481813983264, 0.11201079688432637, 0.124156542741933, 0.10526315492752293, 0.124156542741933, 0.09311740906991632], [0.124156542741933, 0.10526315492752293, 0.10256410297712135, 0.0958164610203179, 0.12280701273979903, 0.10661268492965689, 0.11066126688219242, 0.025641025744280337, 0.11470984883472796, 0.14035087860594145, 0.10796221493179084, 0.124156542741933, 0.09851552102458581, 0.12145749079153141, 0.11336031883259401, 0.12685559469233457, 0.124156542741933, 0.10526315492752293, 0.09851552102458581, 0.11336031883259401, 0.09446693101818396, 0.124156542741933, 0.09041834906564841, 0.08502024516484524, 0.11470984883472796, 0.1794871761830292, 0.1295546546966025, 0.13090418469873644, 0.0958164610203179, 0.09311740906991632, 0.10256410297712135, 0.10931173688005848, 0.124156542741933, 0.044534413558690394, 0.1012145729749874, 0.09446693101818396, 0.1012145729749874, 0.09716599102245185, 0.10256410297712135, 0.06882590930083678], [0.11066126688219242, 0.11470984883472796, 0.12010796078939745, 0.09716599102245185, 0.08502024516484524, 0.08906882711738079, 0.10391362492538898, 0.09311740906991632, 0.10256410297712135, 0.13765181860167355, 0.10931173688005848, 0.11336031883259401, 0.16329284837288705, 0.1295546546966025, 0.133603236649138, 0.11470984883472796, 0.11336031883259401, 0.11066126688219242, 0.10931173688005848, 0.11740890078512954, 0.11201079688432637, 0.1187584307872635, 0.12685559469233457, 0.11201079688432637, 0.09851552102458581, 0.12010796078939745, 0.09986504297285345, 0.14844804251101254, 0.08232118516057735, 0.07287449125337231, 0.1012145729749874, 0.0836707151627113, 0.10526315492752293, 0.12010796078939745, 0.11336031883259401, 0.1767881161787613, 0.1187584307872635, 0.1187584307872635, 0.10256410297712135, 0.09716599102245185], [0.12145749079153141, 0.11740890078512954, 0.09851552102458581, 0.07287449125337231, 0.0958164610203179, 0.11201079688432637, 0.08906882711738079, 0.10391362492538898, 0.11740890078512954, 0.08771929711524683, 0.11066126688219242, 0.12820512469446851, 0.12010796078939745, 0.10661268492965689, 0.10796221493179084, 0.12145749079153141, 0.11605937883686192, 0.09851552102458581, 0.14574898250674462, 0.21997301181611723, 0.11336031883259401, 0.09851552102458581, 0.08097165515844339, 0.12010796078939745, 0.10526315492752293, 0.048582995511225927, 0.10661268492965689, 0.14979756445928016, 0.09176787906778237, 0.11605937883686192, 0.11201079688432637, 0.1012145729749874, 0.11470984883472796, 0.124156542741933, 0.06882590930083678, 0.10391362492538898, 0.11740890078512954, 0.10931173688005848, 0.13495276665127198, 0.11336031883259401], [0.09986504297285345, 0.1417004005542091, 0.11740890078512954, 0.12820512469446851, 0.07692307320590785, 0.11201079688432637, 0.10391362492538898, 0.12145749079153141, 0.08502024516484524, 0.13090418469873644, 0.09851552102458581, 0.05398110746589542, 0.10391362492538898, 0.12010796078939745, 0.124156542741933, 0.09986504297285345, 0.10931173688005848, 0.11740890078512954, 0.12280701273979903, 0.07962213321017576, 0.13090418469873644, 0.10796221493179084, 0.11201079688432637, 0.11605937883686192, 0.09986504297285345, 0.12550607274406694, 0.08097165515844339, 0.10256410297712135, 0.10661268492965689, 0.10526315492752293, 0.12685559469233457, 0.08636976711311288, 0.12280701273979903, 0.06882590930083678, 0.09446693101818396, 0.11336031883259401, 0.1295546546966025, 0.09446693101818396, 0.12010796078939745, 0.11336031883259401], [0.12010796078939745, 0.048582995511225927, 0.13495276665127198, 0.11740890078512954, 0.11740890078512954, 0.11201079688432637, 0.1012145729749874, 0.12685559469233457, 0.15789472836435126, 0.12280701273979903, 0.13090418469873644, 0.09176787906778237, 0.09446693101818396, 0.10391362492538898, 0.13900134054994118, 0.08502024516484524, 0.11740890078512954, 0.10391362492538898, 0.10796221493179084, 0.10661268492965689, 0.11066126688219242, 0.11066126688219242, 0.12010796078939745, 0.14035087860594145, 0.09311740906991632, 0.16329284837288705, 0.144399460558477, 0.12145749079153141, 0.12145749079153141, 0.08906882711738079, 0.12010796078939745, 0.10661268492965689, 0.10391362492538898, 0.13090418469873644, 0.08232118516057735, 0.09176787906778237, 0.09446693101818396, 0.10661268492965689, 0.11066126688219242, 0.0958164610203179], [0.10796221493179084, 0.1187584307872635, 0.11470984883472796, 0.08232118516057735, 0.10796221493179084, 0.11336031883259401, 0.10661268492965689, 0.09311740906991632, 0.1295546546966025, 0.10796221493179084, 0.14304992250247672, 0.08906882711738079, 0.09851552102458581, 0.1295546546966025, 0.07287449125337231, 0.1565452064160836, 0.1012145729749874, 0.12685559469233457, 0.11201079688432637, 0.09986504297285345, 0.12280701273979903, 0.144399460558477, 0.16329284837288705, 0.10931173688005848, 0.12145749079153141, 0.10256410297712135, 0.12010796078939745, 0.124156542741933, 0.0958164610203179, 0.124156542741933, 0.10256410297712135, 0.08771929711524683, 0.11470984883472796, 0.09446693101818396, 0.10661268492965689, 0.1012145729749874, 0.13090418469873644, 0.09446693101818396, 0.1417004005542091, 0.10796221493179084], [0.10391362492538898, 0.09986504297285345, 0.17004049032969049, 0.10256410297712135, 0.11201079688432637, 0.09311740906991632, 0.10256410297712135, 0.09716599102245185, 0.10526315492752293, 0.11201079688432637, 0.13090418469873644, 0.12550607274406694, 0.10796221493179084, 0.1187584307872635, 0.09851552102458581, 0.14574898250674462, 0.11201079688432637, 0.10931173688005848, 0.10796221493179084, 0.07287449125337231, 0.08906882711738079, 0.10931173688005848, 0.0958164610203179, 0.10256410297712135, 0.12280701273979903, 0.13900134054994118, 0.10661268492965689, 0.16194331031688677, 0.13765181860167355, 0.12820512469446851, 0.12550607274406694, 0.12550607274406694, 0.08906882711738079, 0.11201079688432637, 0.11336031883259401, 0.08502024516484524, 0.10661268492965689, 0.13090418469873644, 0.12145749079153141, 0.21052630985504586]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wire_R = [(i+1)*2 for i in range(40)]\n",
        "p_stuck = [0.01*(i) for i in range(40)]\n",
        "R_std = [1e2*i for i in range(1, 101, 4)]\n",
        "\n",
        "\n",
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b\n",
        "\n",
        "err_org = network_tester(net, test_loader, 800, 0, False).item()\n",
        "print(\"original accuracy: \",err_org,\"%\")\n",
        "\n",
        "result = []\n",
        "\n",
        "for std in R_std:\n",
        "    result_wire = []\n",
        "    for var in p_stuck:\n",
        "        device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": std,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": std*10,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": var,\n",
        "                 \"p_stuck_off\": var}\n",
        "        crb_new = crossbar(device_params)\n",
        "\n",
        "        net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "        net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "        net.fc1.cb = crb_new\n",
        "        net.fc1.remap()\n",
        "\n",
        "        err = network_tester(net, test_loader, 800, 0, False).item()\n",
        "        result_wire.append(err/err_org)\n",
        "\n",
        "    result.append(result_wire)\n",
        "\n",
        "print(result)\n",
        "with open(\"result_non_ideal_CNN_manh.txt\", 'w') as writefile:\n",
        "    writefile.write(str(result))\n",
        "\n",
        "ax = sns.heatmap(result, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "TpQq6NpEnHZk",
        "outputId": "1bbe36bc-cee1-4813-cd73-4bc6882ad51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original accuracy:  0.942571759223938 %\n",
            "[[0.7576159451312245, 0.7523178908454868, 0.7748344634694029, 0.7668874452769839, 0.7350993725073085, 0.7774834906122717, 0.7589404270845651, 0.7602649090379057, 0.7602649090379057, 0.7655629633236435, 0.7509934088921463, 0.7536424360350151, 0.7390728816035179, 0.7470198997959369, 0.7443708726530679, 0.7496688637026179, 0.7483443817492774, 0.7456953546064085, 0.7907284998542405, 0.7192053361224708, 0.7324503453644396, 0.7721854363265339, 0.7947020089504501, 0.7496688637026179, 0.7523178908454868, 0.7562913999416963, 0.7629139361807745, 0.7324503453644396, 0.7748344634694029, 0.7708609543731935, 0.6980132454518952, 0.7523178908454868, 0.7721854363265339, 0.7390728816035179, 0.7642384181341151, 0.7443708726530679, 0.7496688637026179, 0.7337748273177802, 0.7033112997376331, 0.7682119272303245], [0.7205298180758113, 0.8066225362390783, 0.7576159451312245, 0.8172185815743661, 0.847682172390701, 0.8211920906705755, 0.8463576272011727, 0.8543046453935915, 0.852980163440251, 0.8582781544898009, 0.8370861270554132, 0.8437086632944916, 0.7960264909037905, 0.8317880727696755, 0.7668874452769839, 0.880794727113717, 0.8596026996793293, 0.7986755180466595, 0.7867549907580311, 0.8370861270554132, 0.7854305088046906, 0.8278145636734661, 0.8291391088629944, 0.8251655997667849, 0.8344370999125443, 0.8569536725364604, 0.8198675454810471, 0.8, 0.8437086632944916, 0.8066225362390783, 0.8516556814869104, 0.8225165726239161, 0.8251655997667849, 0.8596026996793293, 0.8609271816326698, 0.8437086632944916, 0.852980163440251, 0.8172185815743661, 0.8185430635277067, 0.8490066543440415], [0.9152318270262614, 0.8463576272011727, 0.8596026996793293, 0.9417219087463868, 0.9523178908454868, 0.8052980542857378, 0.8940397363556859, 0.9350993725073086, 0.7470198997959369, 0.8728477089212981, 0.908609290787183, 0.8728477089212981, 0.8794702451603764, 0.8874172633527954, 0.8940397363556859, 0.8622516635860104, 0.8781456999708481, 0.9178807909329425, 0.8596026996793293, 0.8940397363556859, 0.875496736064167, 0.9072848088338424, 0.9231788452186802, 0.8980132454518953, 0.8688741998250887, 0.913907281836733, 0.8953642815452142, 0.8662251726822198, 0.8145695544314971, 0.883443754256586, 0.8794702451603764, 0.8966887634985548, 0.8966887634985548, 0.8503311362973821, 0.913907281836733, 0.9549669179883556, 0.9258278091253613, 0.9099337727405236, 0.9072848088338424, 0.8768212180175076], [0.6317880727696754, 0.7033112997376331, 0.761589454227434, 0.7218543000291519, 0.4754967044460731, 0.7761589454227434, 0.4993377590233297, 0.7390728816035179, 0.7364238544606491, 0.5827814816618218, 0.7364238544606491, 0.6688741998250887, 0.4993377590233297, 0.8013245451895283, 0.5894040179009, 0.6649006907288793, 0.5735099182798745, 0.4900662272594764, 0.5033112681195392, 0.4715231953498637, 0.6317880727696754, 0.8794702451603764, 0.8039735090962095, 0.8503311362973821, 0.3403973635568585, 0.6132450724781566, 0.455629158965026, 0.4649006907288793, 0.7231788452186803, 0.6066225362390784, 0.6781456999708482, 0.6410596361516226, 0.7655629633236435, 0.5589404270845652, 0.8026490271428689, 0.313907281836733, 0.31788079093294247, 0.6980132454518952, 0.6715232269679576, 0.6450331452478321], [0.2927152544023453, 0.5245033271720208, 0.38410596361516225, 0.5231788136005864, 0.39470200895045005, 0.26887416820699483, 0.39602649090379055, 0.4635761771574448, 0.4980132454518953, 0.37880797256561227, 0.480794727113717, 0.3258278091253613, 0.4781456999708481, 0.33377485893587405, 0.38145696809038726, 0.4304635908163349, 0.6132450724781566, 0.566887445276984, 0.4741721908746387, 0.3271523226967958, 0.5615893909912462, 0.40662253623907835, 0.5125827998833925, 0.3072847772157486, 0.7470198997959369, 0.4529801318221571, 0.29403973635568587, 0.2437086158673507, 0.5496688637026179, 0.4238410861953505, 0.3205298180758113, 0.41721854995627217, 0.5761589454227434, 0.4821192090670576, 0.28476820459183255, 0.39470200895045005, 0.40662253623907835, 0.2675496862536543, 0.3231788136005863, 0.2450331136297382], [0.13907284998542405, 0.4, 0.2437086158673507, 0.38675499075803116, 0.3668874136588901, 0.19337749537901558, 0.36556293170554954, 0.27019868177842926, 0.14834438174927736, 0.11125827840743455, 0.22384107038630355, 0.21456953862245023, 0.539072881603518, 0.3761589454227434, 0.35364240441692124, 0.3430463590816335, 0.36821192723032453, 0.2980132454518953, 0.5019867545481047, 0.286092718163267, 0.3205298180758113, 0.5417218455101991, 0.3231788136005863, 0.27019868177842926, 0.19470199314140307, 0.36821192723032453, 0.4145695544314972, 0.39602649090379055, 0.39205298180758114, 0.2437086158673507, 0.3311258317930052, 0.19735100447522502, 0.4145695544314972, 0.18013245451895282, 0.6119205272886283, 0.12317881360058633, 0.3642384181341151, 0.4794702135422826, 0.3231788136005863, 0.2092715317638533], [0.15761589770408374, 0.3258278091253613, 0.141721861319246, 0.22649006591107854, 0.16291390456268065, 0.21854304771865968, 0.22913907724490049, 0.26887416820699483, 0.14966887951166485, 0.38145696809038726, 0.24900662272594765, 0.2807946954956231, 0.3072847772157486, 0.19470199314140307, 0.19602649090379057, 0.20132451357143444, 0.2317880885787224, 0.16953642499271204, 0.3642384181341151, 0.14701986817784293, 0.25298013182215706, 0.1192053045043769, 0.2437086158673507, 0.18543046137754976, 0.141721861319246, 0.3403973635568585, 0.22649006591107854, 0.14966887951166485, 0.10066225678571722, 0.2503311362973821, 0.31788079093294247, 0.25695365672741344, 0.3668874136588901, 0.2622516635860104, 0.2198675612900941, 0.23576159767493185, 0.10860927497813609, 0.37218543632653395, 0.34834438174927734, 0.09668874768950779], [0.2211920590524816, 0.3019867545481047, 0.19072848404519363, 0.11125827840743455, 0.15099337727405235, 0.15761589770408374, 0.19602649090379057, 0.2211920590524816, 0.16158940680029316, 0.18145695228134032, 0.20794701819241887, 0.21854304771865968, 0.1536423886078743, 0.2105960295262408, 0.1536423886078743, 0.16821192723032455, 0.11125827840743455, 0.20132451357143444, 0.24238411810496321, 0.14039734774781154, 0.19470199314140307, 0.2609271658236229, 0.1774834431851309, 0.2317880885787224, 0.18013245451895282, 0.15629139994169625, 0.12317881360058633, 0.1642384181341151, 0.21721854995627218, 0.16688742946793703, 0.20397350909620943, 0.17086093856414647, 0.15761589770408374, 0.14039734774781154, 0.227814579482513, 0.1258278170298848, 0.2410596045335288, 0.1311258317930052, 0.19337749537901558, 0.1642384181341151], [0.13774835222303655, 0.15894039546647123, 0.13907284998542405, 0.1523178908454868, 0.14437087265306794, 0.11788079883746592, 0.12715232269679577, 0.14966887951166485, 0.19072848404519363, 0.15761589770408374, 0.13774835222303655, 0.16158940680029316, 0.1549668863702618, 0.15629139994169625, 0.14569537041545544, 0.16158940680029316, 0.14701986817784293, 0.15761589770408374, 0.11788079883746592, 0.1536423886078743, 0.16026490903790566, 0.23046357500728798, 0.15099337727405235, 0.21324504086006274, 0.14701986817784293, 0.14437087265306794, 0.12847682836370672, 0.1774834431851309, 0.20397350909620943, 0.09403973635568585, 0.2410596045335288, 0.18013245451895282, 0.14701986817784293, 0.13509934088921463, 0.12715232269679577, 0.12185430793367535, 0.16688742946793703, 0.22516556814869104, 0.13377484312682714, 0.22384107038630355], [0.11125827840743455, 0.14834438174927736, 0.1258278170298848, 0.1311258317930052, 0.10198675454810471, 0.1311258317930052, 0.12450331136297382, 0.14569537041545544, 0.18013245451895282, 0.21192054309767525, 0.16026490903790566, 0.12715232269679577, 0.1298013261260942, 0.13774835222303655, 0.13509934088921463, 0.12317881360058633, 0.09139073292638739, 0.15761589770408374, 0.10198675454810471, 0.2158940521938847, 0.18013245451895282, 0.13377484312682714, 0.14437087265306794, 0.1311258317930052, 0.12185430793367535, 0.14437087265306794, 0.12715232269679577, 0.1536423886078743, 0.13377484312682714, 0.15629139994169625, 0.10463576588192665, 0.1139072897412565, 0.14966887951166485, 0.10198675454810471, 0.15629139994169625, 0.1430463590816335, 0.11788079883746592, 0.14039734774781154, 0.12847682836370672, 0.15099337727405235], [0.13642383865160213, 0.13907284998542405, 0.14701986817784293, 0.16688742946793703, 0.13245033745991616, 0.10198675454810471, 0.12317881360058633, 0.1549668863702618, 0.13509934088921463, 0.14701986817784293, 0.14437087265306794, 0.19602649090379057, 0.1298013261260942, 0.13907284998542405, 0.11788079883746592, 0.15894039546647123, 0.10463576588192665, 0.14834438174927736, 0.10066225678571722, 0.14039734774781154, 0.1311258317930052, 0.14569537041545544, 0.16026490903790566, 0.2105960295262408, 0.13774835222303655, 0.14437087265306794, 0.1311258317930052, 0.15629139994169625, 0.1430463590816335, 0.13377484312682714, 0.13377484312682714, 0.14437087265306794, 0.10596027154883762, 0.15761589770408374, 0.20264901133382193, 0.13245033745991616, 0.16158940680029316, 0.14039734774781154, 0.1523178908454868, 0.1298013261260942], [0.07019867387390577, 0.141721861319246, 0.12317881360058633, 0.1298013261260942, 0.1523178908454868, 0.141721861319246, 0.14569537041545544, 0.1523178908454868, 0.15629139994169625, 0.10463576588192665, 0.14437087265306794, 0.13907284998542405, 0.12715232269679577, 0.15894039546647123, 0.11523178750364399, 0.14701986817784293, 0.07549668863702617, 0.1748344476603559, 0.12317881360058633, 0.12185430793367535, 0.11523178750364399, 0.13642383865160213, 0.13377484312682714, 0.141721861319246, 0.141721861319246, 0.1536423886078743, 0.1298013261260942, 0.1205298022667644, 0.13907284998542405, 0.14834438174927736, 0.1311258317930052, 0.1880794727113717, 0.1258278170298848, 0.1205298022667644, 0.13509934088921463, 0.16026490903790566, 0.20132451357143444, 0.09933775111880626, 0.13377484312682714, 0.13377484312682714], [0.15099337727405235, 0.12847682836370672, 0.1430463590816335, 0.14569537041545544, 0.1311258317930052, 0.13774835222303655, 0.11788079883746592, 0.12715232269679577, 0.13642383865160213, 0.1430463590816335, 0.141721861319246, 0.14966887951166485, 0.17218543632653396, 0.141721861319246, 0.12317881360058633, 0.18145695228134032, 0.10596027154883762, 0.11523178750364399, 0.13509934088921463, 0.13377484312682714, 0.14039734774781154, 0.13245033745991616, 0.13642383865160213, 0.13377484312682714, 0.13774835222303655, 0.13642383865160213, 0.14834438174927736, 0.12317881360058633, 0.12185430793367535, 0.1311258317930052, 0.17880795675656533, 0.11258278407434552, 0.10728476931122512, 0.1192053045043769, 0.13377484312682714, 0.13245033745991616, 0.1192053045043769, 0.13245033745991616, 0.16953642499271204, 0.14834438174927736], [0.13377484312682714, 0.1258278170298848, 0.14701986817784293, 0.1430463590816335, 0.1258278170298848, 0.11125827840743455, 0.12317881360058633, 0.16291390456268065, 0.13907284998542405, 0.13907284998542405, 0.14039734774781154, 0.1430463590816335, 0.16821192723032455, 0.13642383865160213, 0.141721861319246, 0.1430463590816335, 0.13377484312682714, 0.22384107038630355, 0.12847682836370672, 0.13245033745991616, 0.12847682836370672, 0.1298013261260942, 0.13245033745991616, 0.1311258317930052, 0.14437087265306794, 0.19072848404519363, 0.14437087265306794, 0.16158940680029316, 0.1298013261260942, 0.13245033745991616, 0.14834438174927736, 0.1298013261260942, 0.1298013261260942, 0.14966887951166485, 0.13907284998542405, 0.14834438174927736, 0.141721861319246, 0.14701986817784293, 0.1298013261260942, 0.14966887951166485], [0.1311258317930052, 0.12847682836370672, 0.18278146585277477, 0.1655629158965026, 0.12317881360058633, 0.1536423886078743, 0.1549668863702618, 0.141721861319246, 0.13377484312682714, 0.11523178750364399, 0.18940398628280614, 0.1549668863702618, 0.14039734774781154, 0.11523178750364399, 0.14834438174927736, 0.16821192723032455, 0.13642383865160213, 0.12317881360058633, 0.18675497494898421, 0.1298013261260942, 0.13245033745991616, 0.14834438174927736, 0.1430463590816335, 0.12450331136297382, 0.1549668863702618, 0.11788079883746592, 0.1549668863702618, 0.14966887951166485, 0.1430463590816335, 0.1311258317930052, 0.11523178750364399, 0.11788079883746592, 0.14569537041545544, 0.1549668863702618, 0.12185430793367535, 0.13774835222303655, 0.13774835222303655, 0.1761589454227434, 0.12715232269679577, 0.13377484312682714], [0.1311258317930052, 0.1258278170298848, 0.12317881360058633, 0.18278146585277477, 0.15894039546647123, 0.13774835222303655, 0.13509934088921463, 0.13907284998542405, 0.13907284998542405, 0.12185430793367535, 0.13907284998542405, 0.1298013261260942, 0.13774835222303655, 0.12847682836370672, 0.1549668863702618, 0.14966887951166485, 0.13377484312682714, 0.14569537041545544, 0.17880795675656533, 0.1311258317930052, 0.13245033745991616, 0.15099337727405235, 0.14039734774781154, 0.13509934088921463, 0.15099337727405235, 0.1748344476603559, 0.1748344476603559, 0.16688742946793703, 0.12317881360058633, 0.1642384181341151, 0.17880795675656533, 0.1311258317930052, 0.16158940680029316, 0.14039734774781154, 0.13642383865160213, 0.14437087265306794, 0.13642383865160213, 0.1430463590816335, 0.13907284998542405, 0.2105960295262408], [0.141721861319246, 0.1298013261260942, 0.1311258317930052, 0.1748344476603559, 0.1205298022667644, 0.14569537041545544, 0.14039734774781154, 0.14966887951166485, 0.16291390456268065, 0.1311258317930052, 0.13377484312682714, 0.16026490903790566, 0.15761589770408374, 0.13245033745991616, 0.1311258317930052, 0.14701986817784293, 0.14966887951166485, 0.13245033745991616, 0.14437087265306794, 0.1536423886078743, 0.1748344476603559, 0.141721861319246, 0.13377484312682714, 0.1139072897412565, 0.13774835222303655, 0.12317881360058633, 0.13907284998542405, 0.12185430793367535, 0.19072848404519363, 0.07947019773323562, 0.19470199314140307, 0.14834438174927736, 0.12317881360058633, 0.1205298022667644, 0.15894039546647123, 0.1430463590816335, 0.15894039546647123, 0.16026490903790566, 0.13377484312682714, 0.10463576588192665], [0.1536423886078743, 0.14569537041545544, 0.1430463590816335, 0.16688742946793703, 0.141721861319246, 0.141721861319246, 0.1523178908454868, 0.14039734774781154, 0.14834438174927736, 0.11788079883746592, 0.1549668863702618, 0.1205298022667644, 0.1139072897412565, 0.12450331136297382, 0.13509934088921463, 0.12715232269679577, 0.1774834431851309, 0.1430463590816335, 0.15629139994169625, 0.13245033745991616, 0.11655629317055495, 0.13509934088921463, 0.12317881360058633, 0.19205298180758112, 0.141721861319246, 0.13509934088921463, 0.1774834431851309, 0.1523178908454868, 0.141721861319246, 0.13509934088921463, 0.13907284998542405, 0.1205298022667644, 0.1430463590816335, 0.1258278170298848, 0.14701986817784293, 0.17086093856414647, 0.13377484312682714, 0.09139073292638739, 0.12185430793367535, 0.14039734774781154], [0.1880794727113717, 0.13509934088921463, 0.15894039546647123, 0.14701986817784293, 0.1655629158965026, 0.09271523068877488, 0.1298013261260942, 0.16953642499271204, 0.11655629317055495, 0.14701986817784293, 0.12847682836370672, 0.1536423886078743, 0.12847682836370672, 0.11258278407434552, 0.12715232269679577, 0.14039734774781154, 0.15629139994169625, 0.13509934088921463, 0.1430463590816335, 0.13907284998542405, 0.11655629317055495, 0.14039734774781154, 0.12715232269679577, 0.1536423886078743, 0.141721861319246, 0.2105960295262408, 0.10463576588192665, 0.14701986817784293, 0.13642383865160213, 0.10463576588192665, 0.1258278170298848, 0.13245033745991616, 0.1642384181341151, 0.12847682836370672, 0.10198675454810471, 0.20132451357143444, 0.1642384181341151, 0.15761589770408374, 0.13245033745991616, 0.13774835222303655], [0.14039734774781154, 0.1298013261260942, 0.14039734774781154, 0.16026490903790566, 0.141721861319246, 0.141721861319246, 0.14569537041545544, 0.09006622725947641, 0.10860927497813609, 0.18410596361516227, 0.13377484312682714, 0.12847682836370672, 0.13377484312682714, 0.1258278170298848, 0.19205298180758112, 0.18013245451895282, 0.13509934088921463, 0.1642384181341151, 0.15629139994169625, 0.13642383865160213, 0.13907284998542405, 0.15894039546647123, 0.19602649090379057, 0.09801324545189528, 0.13245033745991616, 0.12317881360058633, 0.11788079883746592, 0.14966887951166485, 0.12450331136297382, 0.13642383865160213, 0.1311258317930052, 0.1298013261260942, 0.14437087265306794, 0.1192053045043769, 0.1205298022667644, 0.1523178908454868, 0.13907284998542405, 0.12715232269679577, 0.1258278170298848, 0.1523178908454868], [0.1430463590816335, 0.13245033745991616, 0.13509934088921463, 0.12847682836370672, 0.10198675454810471, 0.15099337727405235, 0.13509934088921463, 0.14039734774781154, 0.14966887951166485, 0.1774834431851309, 0.14966887951166485, 0.13642383865160213, 0.12317881360058633, 0.14039734774781154, 0.14834438174927736, 0.16158940680029316, 0.14834438174927736, 0.1774834431851309, 0.13377484312682714, 0.14834438174927736, 0.14437087265306794, 0.1298013261260942, 0.12847682836370672, 0.1139072897412565, 0.1536423886078743, 0.13642383865160213, 0.12185430793367535, 0.13907284998542405, 0.1536423886078743, 0.13642383865160213, 0.11523178750364399, 0.1258278170298848, 0.13245033745991616, 0.17218543632653396, 0.15629139994169625, 0.13774835222303655, 0.141721861319246, 0.12847682836370672, 0.14437087265306794, 0.12715232269679577], [0.13509934088921463, 0.14039734774781154, 0.11788079883746592, 0.13774835222303655, 0.12317881360058633, 0.1536423886078743, 0.1311258317930052, 0.1430463590816335, 0.13907284998542405, 0.14039734774781154, 0.16158940680029316, 0.11523178750364399, 0.15761589770408374, 0.13642383865160213, 0.14966887951166485, 0.14437087265306794, 0.14437087265306794, 0.12317881360058633, 0.14569537041545544, 0.13642383865160213, 0.10993378064504705, 0.141721861319246, 0.141721861319246, 0.13774835222303655, 0.13377484312682714, 0.141721861319246, 0.19072848404519363, 0.1258278170298848, 0.1430463590816335, 0.12450331136297382, 0.14569537041545544, 0.16688742946793703, 0.16291390456268065, 0.141721861319246, 0.12847682836370672, 0.15761589770408374, 0.12185430793367535, 0.1311258317930052, 0.13245033745991616, 0.13377484312682714], [0.11788079883746592, 0.13245033745991616, 0.1311258317930052, 0.1430463590816335, 0.12715232269679577, 0.13245033745991616, 0.14569537041545544, 0.13642383865160213, 0.13245033745991616, 0.14569537041545544, 0.14834438174927736, 0.14834438174927736, 0.14437087265306794, 0.15761589770408374, 0.14437087265306794, 0.1311258317930052, 0.13509934088921463, 0.11788079883746592, 0.13245033745991616, 0.17350993408892146, 0.1298013261260942, 0.16953642499271204, 0.1430463590816335, 0.16158940680029316, 0.14701986817784293, 0.2635761613483979, 0.12450331136297382, 0.14569537041545544, 0.10198675454810471, 0.1549668863702618, 0.1311258317930052, 0.13642383865160213, 0.14834438174927736, 0.1298013261260942, 0.1655629158965026, 0.13774835222303655, 0.1258278170298848, 0.1430463590816335, 0.13245033745991616, 0.13377484312682714], [0.08476821249635602, 0.12847682836370672, 0.13509934088921463, 0.13907284998542405, 0.1192053045043769, 0.16026490903790566, 0.12847682836370672, 0.15629139994169625, 0.13509934088921463, 0.13774835222303655, 0.13509934088921463, 0.14039734774781154, 0.14966887951166485, 0.13907284998542405, 0.1430463590816335, 0.14701986817784293, 0.13774835222303655, 0.13509934088921463, 0.14966887951166485, 0.1642384181341151, 0.11788079883746592, 0.141721861319246, 0.13642383865160213, 0.13377484312682714, 0.17350993408892146, 0.13377484312682714, 0.19735100447522502, 0.1311258317930052, 0.11788079883746592, 0.12450331136297382, 0.12715232269679577, 0.18675497494898421, 0.13509934088921463, 0.09668874768950779, 0.14701986817784293, 0.12715232269679577, 0.1430463590816335, 0.13642383865160213, 0.15761589770408374, 0.1205298022667644], [0.12450331136297382, 0.141721861319246, 0.20794701819241887, 0.12847682836370672, 0.1205298022667644, 0.12317881360058633, 0.14834438174927736, 0.14834438174927736, 0.14039734774781154, 0.18410596361516227, 0.1139072897412565, 0.12317881360058633, 0.1311258317930052, 0.12185430793367535, 0.2158940521938847, 0.1523178908454868, 0.13774835222303655, 0.13907284998542405, 0.1205298022667644, 0.19205298180758112, 0.16026490903790566, 0.13245033745991616, 0.1139072897412565, 0.12317881360058633, 0.16158940680029316, 0.19337749537901558, 0.10728476931122512, 0.11258278407434552, 0.1311258317930052, 0.13509934088921463, 0.1311258317930052, 0.13642383865160213, 0.1258278170298848, 0.1761589454227434, 0.15099337727405235, 0.1205298022667644, 0.13907284998542405, 0.12450331136297382, 0.14437087265306794, 0.10596027154883762]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model_mnistl.pth\")"
      ],
      "metadata": {
        "id": "7T5fFTqTA0uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('model_mnistl.pth')\n",
        "temp_model = Net_ex()\n",
        "temp_model.load_state_dict(torch.load('model_mnistl.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oejwO4WeA6lk",
        "outputId": "9df385ba-fdcf-4401-d5dc-88df92481fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "Running the cell below will end up with a Runtime Error, but all the data needed have been loaded successfully to the model."
      ],
      "metadata": {
        "id": "q--CfamI3nJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('model_mnistl.pth')\n",
        "model = Net()\n",
        "model.load_state_dict(torch.load('model_mnistl.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "I-_nJO0tBkuC",
        "outputId": "9d551699-17fc-497e-b28e-8df3236c7da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-f29fc941f056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_mnistl.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_mnistl.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"fc1.W\", \"fc1.b\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crossbar update\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}\n",
        "crb_new = crossbar(device_params)\n",
        "\n",
        "model.fc1.W = torch.nn.parameter.Parameter(temp_model.fc1.weight.data)\n",
        "model.fc1.b = torch.nn.parameter.Parameter(temp_model.fc1.bias.data)\n",
        "\n",
        "model.fc1.cb = crb_new\n",
        "model.fc1.remap()\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4b7Et7Bo6B",
        "outputId": "12f73824-67b2-424c-cc47-ea17ce369305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(3, 3))\n",
              "  (conv2): Conv2d(16, 64, kernel_size=(3, 3), stride=(3, 3))\n",
              "  (fc1): Linear()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def network_tester(model, test_loader, test_size, epoch, log = True):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            if batch_idx * len(data) > test_size:\n",
        "                break\n",
        "            output = model(data)\n",
        "\n",
        "            pred = output.data.max(1, keepdim=True)[1][0]\n",
        "\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            total+=target.size(0)\n",
        "\n",
        "            if batch_idx % 1000 == 0 and log:\n",
        "              with open('log_baseline_test.csv', 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([batch_idx + test_size * 100, test_loss/(batch_idx+1), correct.item()/total])\n",
        "              print(\"Epoch\", epoch, 'iteration',batch_idx, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    return torch.div(correct, float(total))"
      ],
      "metadata": {
        "id": "J7ka_oHgCaWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = model\n",
        "wire_R = [(i+1)*2 for i in range(40)]\n",
        "p_stuck = [0.01*(i) for i in range(40)]\n",
        "R_std = [1e2*i for i in range(1, 101, 4)]\n",
        "\n",
        "\n",
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b\n",
        "\n",
        "err_org = network_tester(net, test_loader, 800, 0, False).item()\n",
        "print(\"original accuracy: \",err_org,\"%\")\n",
        "\n",
        "result = []\n",
        "\n",
        "for std in R_std:\n",
        "    result_wire = []\n",
        "    for var in p_stuck:\n",
        "        device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": std,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": std*10,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 4,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": var,\n",
        "                 \"p_stuck_off\": var}\n",
        "        crb_new = crossbar(device_params)\n",
        "\n",
        "        net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "        net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "        net.fc1.cb = crb_new\n",
        "        net.fc1.remap()\n",
        "\n",
        "        err = network_tester(net, test_loader, 800, 0, False).item()\n",
        "        result_wire.append(err/err_org)\n",
        "\n",
        "    result.append(result_wire)\n",
        "\n",
        "print(result)\n",
        "with open(\"result_non_ideal_CNN_ex_situ2.txt\", 'w') as writefile:\n",
        "    writefile.write(str(result))\n",
        "\n",
        "ax = sns.heatmap(result, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "qMMAxozfABqk",
        "outputId": "32a4cff1-5bf8-409e-c750-c47df2561e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original accuracy:  0.9288389682769775 %\n",
            "[[0.9986558715427484, 1.0026881285722462, 0.9905913574837525, 0.9838709718820083, 1.0026881285722462, 0.9825268434247566, 1.0080645140589959, 1.001344064286123, 0.9731182650796376, 0.9798387148525104, 1.0, 0.9704300723362629, 1.0174730924041149, 0.9959677429705021, 0.9905913574837525, 0.9973118072566253, 1.004032257029498, 1.0080645140589959, 0.9892472931976294, 0.9973118072566253, 1.010752642631242, 1.0, 1.0067203856017441, 1.0067203856017441, 0.9838709718820083, 0.9973118072566253, 0.9825268434247566, 0.9946236145132504, 1.009408578345119, 0.9905913574837525, 1.0201612851474897, 0.9892472931976294, 1.0026881285722462, 1.0026881285722462, 1.001344064286123, 1.004032257029498, 1.0080645140589959, 0.9973118072566253, 0.9838709718820083, 0.9959677429705021], [1.004032257029498, 1.009408578345119, 1.0215053494336128, 0.9825268434247566, 1.0, 1.0067203856017441, 0.9973118072566253, 1.005376321315621, 0.9932795502271273, 0.9784945863952588, 1.0120967710884938, 1.0161290281179918, 1.0161290281179918, 1.013440835374617, 1.0161290281179918, 0.9973118072566253, 1.005376321315621, 1.0161290281179918, 1.013440835374617, 0.9986558715427484, 0.9919354859410041, 1.01478489966074, 1.0255376064631108, 0.9879032289115062, 0.9919354859410041, 0.9959677429705021, 1.005376321315621, 1.0120967710884938, 1.005376321315621, 1.0080645140589959, 0.9986558715427484, 1.0080645140589959, 1.0174730924041149, 0.9973118072566253, 1.001344064286123, 1.010752642631242, 1.0, 0.9892472931976294, 1.009408578345119, 0.9784945863952588], [0.9771505221091356, 1.004032257029498, 1.001344064286123, 0.9798387148525104, 1.009408578345119, 0.9973118072566253, 0.9932795502271273, 1.01478489966074, 1.01478489966074, 0.9986558715427484, 1.004032257029498, 0.9959677429705021, 1.009408578345119, 0.9986558715427484, 0.9932795502271273, 0.9811827791386335, 1.0, 0.9919354859410041, 1.010752642631242, 1.001344064286123, 0.9986558715427484, 1.004032257029498, 0.9784945863952588, 0.9865591004542545, 0.9932795502271273, 1.004032257029498, 0.9798387148525104, 1.0080645140589959, 1.001344064286123, 1.01478489966074, 0.9959677429705021, 0.9919354859410041, 1.005376321315621, 1.0026881285722462, 0.9973118072566253, 1.0026881285722462, 1.022849413719736, 0.9879032289115062, 1.010752642631242, 1.009408578345119], [1.013440835374617, 0.9892472931976294, 0.9825268434247566, 1.005376321315621, 0.9919354859410041, 1.005376321315621, 1.004032257029498, 0.9986558715427484, 0.9852150361681314, 0.9798387148525104, 0.9892472931976294, 1.022849413719736, 0.9986558715427484, 0.9959677429705021, 0.9986558715427484, 0.9905913574837525, 0.9784945863952588, 1.0026881285722462, 0.9032257671209213, 0.9919354859410041, 0.9704300723362629, 1.013440835374617, 1.010752642631242, 1.0080645140589959, 0.9784945863952588, 1.0026881285722462, 0.9865591004542545, 0.9596774297050207, 0.9677419437640166, 0.9892472931976294, 1.0255376064631108, 0.9879032289115062, 0.9811827791386335, 0.9905913574837525, 0.9798387148525104, 0.9556451726755228, 0.9690860080501398, 1.0067203856017441, 0.9986558715427484, 1.0], [0.9354838233569047, 0.8749999679144358, 0.7795698634926086, 0.9663978153067649, 0.9704300723362629, 0.9852150361681314, 0.9247311807256625, 0.9784945863952588, 0.9408602088436542, 0.7540322570294979, 0.8615591325398189, 0.9825268434247566, 0.9327956947846584, 0.9556451726755228, 0.9314515663274068, 0.8279569478465838, 0.8991935100914233, 0.8978494458053001, 0.9301075020412836, 0.9556451726755228, 0.8991935100914233, 0.9663978153067649, 0.8508064257374482, 0.9395160803864026, 0.9677419437640166, 0.8481182329940734, 0.9139784739232919, 0.9274193092979088, 0.943548401587029, 0.9247311807256625, 0.9677419437640166, 0.9502687871887733, 0.9516129156460249, 0.9637096867345186, 0.9422042731297774, 0.943548401587029, 0.9206989236961646, 0.8897849317463044, 0.956989236961646, 0.9448924658731521], [0.7217742007935145, 0.7943548273244772, 0.8723118393421895, 0.5927419116784524, 0.7258064578230125, 0.8373655903628313, 0.6545698955781729, 0.8602150682536956, 0.712365558277267, 0.7943548273244772, 0.8467741687079503, 0.706989236961646, 0.8373655903628313, 0.4086021526076708, 0.8266128835604606, 0.7701612851474896, 0.7688172208613665, 0.8333333333333334, 0.6787634377551605, 0.806451598412971, 0.8387096546489544, 0.7137096867345186, 0.7956988916106004, 0.9139784739232919, 0.8293010763038354, 0.7647849638318686, 0.45564514058995853, 0.8897849317463044, 0.7271505221091356, 0.7473118072566253, 0.17876343775516046, 0.7849462489793582, 0.6706989236961646, 0.8346773976194565, 0.7446236145132504, 0.6344086104306832, 0.811827919728592, 0.8279569478465838, 0.7634408353746169, 0.8091397911563458], [0.18413977511356364, 0.5806451405899585, 0.2446236465988147, 0.2943548273244772, 0.11559139759070781, 0.14516128514748963, 0.5456988916106004, 0.2432795662699094, 0.17876343775516046, 0.1586021526076708, 0.706989236961646, 0.20698923696164595, 0.5833333333333334, 0.6680107309527898, 0.7043010442182711, 0.20026881927433748, 0.3104838554424689, 0.18548387148525103, 0.7728494778908644, 0.6922042731297774, 0.3629032289115062, 0.40725805623598343, 0.6451612530619254, 0.2432795662699094, 0.19489246587315215, 0.24596774297050208, 0.18279569478465837, 0.21102149399114387, 0.3911290281179917, 0.7352150361681314, 0.25672041768730847, 0.3413978473923292, 0.1276881686792016, 0.25672041768730847, 0.44354836950146476, 0.37096774297050206, 0.2163978473923292, 0.8185483695014648, 0.5913978473923291, 0.11021505221091356], [0.12096774297050207, 0.2271505381519177, 0.20026881927433748, 0.22043010442182712, 0.12634408032890526, 0.19758064257374483, 0.16263440963716874, 0.202956979932148, 0.20698923696164595, 0.20833333333333334, 0.36559138956931675, 0.12903225702949792, 0.21774192772123446, 0.18817203214306158, 0.19086020884365423, 0.2661290281179917, 0.11424730924041149, 0.2325268755103209, 0.1586021526076708, 0.22311828112241977, 0.15591397590707815, 0.19220430521534163, 0.17338710039675725, 0.2432795662699094, 0.18548387148525103, 0.11155914056120989, 0.11290322089011516, 0.23118279518141563, 0.19086020884365423, 0.24193548594100414, 0.21774192772123446, 0.26344085141739904, 0.19758064257374483, 0.11827956626990942, 0.163978489966074, 0.12096774297050207, 0.202956979932148, 0.22177418475073238, 0.10887096386061723, 0.16129031326548135], [0.2620967710884938, 0.1357526907595885, 0.10752688353171196, 0.1276881686792016, 0.1021505381519177, 0.16801074699557195, 0.12231182329940735, 0.1599462329365761, 0.14381720481858437, 0.10752688353171196, 0.21370967069173652, 0.1478494618480823, 0.12096774297050207, 0.12634408032890526, 0.19758064257374483, 0.13037634537979426, 0.11290322089011516, 0.10887096386061723, 0.12096774297050207, 0.11559139759070781, 0.11290322089011516, 0.11962365462020574, 0.11827956626990942, 0.1021505381519177, 0.13306451405899586, 0.18548387148525103, 0.11290322089011516, 0.11155914056120989, 0.13306451405899586, 0.11021505221091356, 0.09677419277212344, 0.16666666666666666, 0.11693547791961309, 0.1276881686792016, 0.1599462329365761, 0.12096774297050207, 0.12365591164970367, 0.13844085141739906, 0.2150537670634239, 0.18548387148525103], [0.1021505381519177, 0.12096774297050207, 0.13306451405899586, 0.11424730924041149, 0.09274193574262551, 0.24731182329940735, 0.12634408032890526, 0.10618279518141563, 0.12096774297050207, 0.11290322089011516, 0.09946236145132505, 0.09677419277212344, 0.125, 0.125, 0.11155914056120989, 0.1021505381519177, 0.11559139759070781, 0.12365591164970367, 0.11693547791961309, 0.13978494778908646, 0.11290322089011516, 0.09139784739232919, 0.09677419277212344, 0.1532257992064855, 0.09811828112241977, 0.10349461848082298, 0.09408601607153079, 0.11155914056120989, 0.10618279518141563, 0.12634408032890526, 0.09408601607153079, 0.12634408032890526, 0.09408601607153079, 0.10618279518141563, 0.13306451405899586, 0.12096774297050207, 0.1048387068311193, 0.12231182329940735, 0.11962365462020574, 0.11424730924041149], [0.12096774297050207, 0.10349461848082298, 0.12903225702949792, 0.09677419277212344, 0.10887096386061723, 0.10887096386061723, 0.11962365462020574, 0.12903225702949792, 0.11021505221091356, 0.10752688353171196, 0.08602150201253493, 0.11290322089011516, 0.12096774297050207, 0.12231182329940735, 0.09811828112241977, 0.11827956626990942, 0.10618279518141563, 0.1021505381519177, 0.11290322089011516, 0.1021505381519177, 0.11155914056120989, 0.09811828112241977, 0.10080644980162137, 0.10618279518141563, 0.09543010442182712, 0.13978494778908646, 0.09543010442182712, 0.11290322089011516, 0.12903225702949792, 0.11424730924041149, 0.13172042570869952, 0.11290322089011516, 0.21774192772123446, 0.09677419277212344, 0.08870967871312758, 0.09543010442182712, 0.08736559036283126, 0.12903225702949792, 0.11827956626990942, 0.12634408032890526], [0.09139784739232919, 0.10887096386061723, 0.17204300402506986, 0.10349461848082298, 0.09139784739232919, 0.08736559036283126, 0.09408601607153079, 0.10080644980162137, 0.1048387068311193, 0.10887096386061723, 0.11827956626990942, 0.11155914056120989, 0.11021505221091356, 0.11021505221091356, 0.11424730924041149, 0.10887096386061723, 0.125, 0.11962365462020574, 0.17338710039675725, 0.10752688353171196, 0.1021505381519177, 0.11424730924041149, 0.18010751808406572, 0.08870967871312758, 0.1021505381519177, 0.11290322089011516, 0.14247310844689698, 0.14112902811799172, 0.1048387068311193, 0.1021505381519177, 0.25, 0.12231182329940735, 0.09677419277212344, 0.13037634537979426, 0.10349461848082298, 0.09677419277212344, 0.09811828112241977, 0.13172042570869952, 0.13844085141739906, 0.14247310844689698], [0.11424730924041149, 0.11290322089011516, 0.11155914056120989, 0.11693547791961309, 0.09408601607153079, 0.10752688353171196, 0.13978494778908646, 0.125, 0.15591397590707815, 0.12365591164970367, 0.12365591164970367, 0.11693547791961309, 0.13172042570869952, 0.1048387068311193, 0.09677419277212344, 0.09811828112241977, 0.125, 0.10752688353171196, 0.11827956626990942, 0.10752688353171196, 0.11827956626990942, 0.1276881686792016, 0.11021505221091356, 0.10349461848082298, 0.11155914056120989, 0.17607526105456778, 0.09946236145132505, 0.09005375904203286, 0.11155914056120989, 0.09408601607153079, 0.07123655422344849, 0.09811828112241977, 0.11693547791961309, 0.17473118072566252, 0.13037634537979426, 0.09946236145132505, 0.11021505221091356, 0.12096774297050207, 0.13440860240929217, 0.10080644980162137], [0.1021505381519177, 0.09811828112241977, 0.11290322089011516, 0.11827956626990942, 0.11827956626990942, 0.11290322089011516, 0.11021505221091356, 0.10618279518141563, 0.09677419277212344, 0.1048387068311193, 0.10080644980162137, 0.09811828112241977, 0.09408601607153079, 0.10752688353171196, 0.15591397590707815, 0.10752688353171196, 0.11155914056120989, 0.10349461848082298, 0.10618279518141563, 0.12096774297050207, 0.08602150201253493, 0.12231182329940735, 0.13978494778908646, 0.10752688353171196, 0.10887096386061723, 0.10618279518141563, 0.0793010763038354, 0.12231182329940735, 0.11021505221091356, 0.10349461848082298, 0.1276881686792016, 0.14650538151917702, 0.12231182329940735, 0.10349461848082298, 0.09677419277212344, 0.10618279518141563, 0.11290322089011516, 0.10349461848082298, 0.1276881686792016, 0.10080644980162137], [0.11021505221091356, 0.081989244983037, 0.09274193574262551, 0.12634408032890526, 0.10080644980162137, 0.12903225702949792, 0.11424730924041149, 0.10080644980162137, 0.13440860240929217, 0.10752688353171196, 0.1357526907595885, 0.11424730924041149, 0.11155914056120989, 0.11021505221091356, 0.1021505381519177, 0.11693547791961309, 0.09005375904203286, 0.12096774297050207, 0.09139784739232919, 0.10349461848082298, 0.19489246587315215, 0.11021505221091356, 0.07392473092404114, 0.10080644980162137, 0.09408601607153079, 0.11962365462020574, 0.11962365462020574, 0.10080644980162137, 0.18010751808406572, 0.09677419277212344, 0.09274193574262551, 0.09811828112241977, 0.10752688353171196, 0.11290322089011516, 0.19758064257374483, 0.09677419277212344, 0.13037634537979426, 0.10080644980162137, 0.12634408032890526, 0.11424730924041149], [0.10080644980162137, 0.10752688353171196, 0.12231182329940735, 0.11155914056120989, 0.09274193574262551, 0.09543010442182712, 0.09543010442182712, 0.09677419277212344, 0.1021505381519177, 0.12903225702949792, 0.10080644980162137, 0.08870967871312758, 0.11290322089011516, 0.11155914056120989, 0.10752688353171196, 0.1021505381519177, 0.09946236145132505, 0.12634408032890526, 0.11559139759070781, 0.11559139759070781, 0.13440860240929217, 0.11827956626990942, 0.1021505381519177, 0.18682795181415632, 0.10618279518141563, 0.10080644980162137, 0.10887096386061723, 0.1048387068311193, 0.11155914056120989, 0.10080644980162137, 0.09811828112241977, 0.11290322089011516, 0.11290322089011516, 0.10618279518141563, 0.1021505381519177, 0.1706989236961646, 0.11021505221091356, 0.10080644980162137, 0.13440860240929217, 0.12231182329940735], [0.10080644980162137, 0.11962365462020574, 0.12903225702949792, 0.10887096386061723, 0.10887096386061723, 0.1048387068311193, 0.09543010442182712, 0.13844085141739906, 0.11155914056120989, 0.10349461848082298, 0.10349461848082298, 0.1048387068311193, 0.11424730924041149, 0.11290322089011516, 0.09543010442182712, 0.13978494778908646, 0.11021505221091356, 0.11827956626990942, 0.11962365462020574, 0.22311828112241977, 0.13440860240929217, 0.12096774297050207, 0.11155914056120989, 0.11559139759070781, 0.11693547791961309, 0.12096774297050207, 0.09139784739232919, 0.10618279518141563, 0.11155914056120989, 0.11021505221091356, 0.08870967871312758, 0.12903225702949792, 0.08467742168362967, 0.1048387068311193, 0.09946236145132505, 0.10080644980162137, 0.10349461848082298, 0.11962365462020574, 0.12903225702949792, 0.08736559036283126], [0.09811828112241977, 0.12231182329940735, 0.10349461848082298, 0.10618279518141563, 0.09274193574262551, 0.10618279518141563, 0.08602150201253493, 0.12096774297050207, 0.11021505221091356, 0.09811828112241977, 0.09811828112241977, 0.10618279518141563, 0.11424730924041149, 0.11827956626990942, 0.13844085141739906, 0.125, 0.12096774297050207, 0.09946236145132505, 0.10349461848082298, 0.08870967871312758, 0.1021505381519177, 0.09543010442182712, 0.11155914056120989, 0.10349461848082298, 0.09005375904203286, 0.1276881686792016, 0.12634408032890526, 0.13440860240929217, 0.14247310844689698, 0.12231182329940735, 0.09677419277212344, 0.11290322089011516, 0.11559139759070781, 0.09543010442182712, 0.10080644980162137, 0.11827956626990942, 0.09811828112241977, 0.10618279518141563, 0.09274193574262551, 0.1021505381519177], [0.13037634537979426, 0.10080644980162137, 0.10887096386061723, 0.10887096386061723, 0.11290322089011516, 0.09946236145132505, 0.11021505221091356, 0.09946236145132505, 0.11559139759070781, 0.08333333333333333, 0.13306451405899586, 0.11021505221091356, 0.1021505381519177, 0.12903225702949792, 0.08467742168362967, 0.12231182329940735, 0.10752688353171196, 0.14112902811799172, 0.16666666666666666, 0.09946236145132505, 0.09677419277212344, 0.11290322089011516, 0.12634408032890526, 0.09677419277212344, 0.1706989236961646, 0.12365591164970367, 0.11827956626990942, 0.10349461848082298, 0.11290322089011516, 0.10080644980162137, 0.12365591164970367, 0.10349461848082298, 0.12096774297050207, 0.12231182329940735, 0.11290322089011516, 0.14650538151917702, 0.09543010442182712, 0.11559139759070781, 0.11559139759070781, 0.09543010442182712], [0.10080644980162137, 0.1276881686792016, 0.09946236145132505, 0.09677419277212344, 0.10887096386061723, 0.11021505221091356, 0.08064515663274067, 0.12365591164970367, 0.11559139759070781, 0.08064515663274067, 0.11559139759070781, 0.11155914056120989, 0.11021505221091356, 0.09677419277212344, 0.09274193574262551, 0.09543010442182712, 0.12096774297050207, 0.10080644980162137, 0.09811828112241977, 0.09543010442182712, 0.1021505381519177, 0.11290322089011516, 0.11021505221091356, 0.09408601607153079, 0.11827956626990942, 0.11424730924041149, 0.12096774297050207, 0.12365591164970367, 0.11962365462020574, 0.12231182329940735, 0.11962365462020574, 0.12231182329940735, 0.11155914056120989, 0.08064515663274067, 0.09946236145132505, 0.11559139759070781, 0.11155914056120989, 0.1048387068311193, 0.1021505381519177, 0.09005375904203286], [0.10349461848082298, 0.09543010442182712, 0.09677419277212344, 0.1021505381519177, 0.15188171887758023, 0.11021505221091356, 0.09005375904203286, 0.09005375904203286, 0.11559139759070781, 0.11827956626990942, 0.11021505221091356, 0.11424730924041149, 0.11290322089011516, 0.11290322089011516, 0.10887096386061723, 0.08736559036283126, 0.10349461848082298, 0.1021505381519177, 0.10349461848082298, 0.11693547791961309, 0.09005375904203286, 0.09677419277212344, 0.11693547791961309, 0.09946236145132505, 0.12096774297050207, 0.11155914056120989, 0.09811828112241977, 0.1021505381519177, 0.1276881686792016, 0.09408601607153079, 0.1021505381519177, 0.09677419277212344, 0.11827956626990942, 0.09946236145132505, 0.1021505381519177, 0.12365591164970367, 0.1276881686792016, 0.09811828112241977, 0.13440860240929217, 0.11424730924041149], [0.11424730924041149, 0.11290322089011516, 0.11962365462020574, 0.12096774297050207, 0.10887096386061723, 0.1276881686792016, 0.10080644980162137, 0.09408601607153079, 0.14112902811799172, 0.16666666666666666, 0.13037634537979426, 0.11290322089011516, 0.1021505381519177, 0.09274193574262551, 0.11559139759070781, 0.11962365462020574, 0.10618279518141563, 0.10887096386061723, 0.1021505381519177, 0.08602150201253493, 0.10618279518141563, 0.09408601607153079, 0.1048387068311193, 0.11559139759070781, 0.10080644980162137, 0.11021505221091356, 0.09811828112241977, 0.11827956626990942, 0.1048387068311193, 0.11962365462020574, 0.10887096386061723, 0.09274193574262551, 0.12096774297050207, 0.10618279518141563, 0.11155914056120989, 0.09946236145132505, 0.10349461848082298, 0.10618279518141563, 0.10752688353171196, 0.09946236145132505], [0.11827956626990942, 0.1021505381519177, 0.10752688353171196, 0.09677419277212344, 0.1048387068311193, 0.09274193574262551, 0.12365591164970367, 0.1021505381519177, 0.2150537670634239, 0.10349461848082298, 0.13306451405899586, 0.12096774297050207, 0.10887096386061723, 0.09946236145132505, 0.10752688353171196, 0.11290322089011516, 0.1586021526076708, 0.09139784739232919, 0.09274193574262551, 0.10080644980162137, 0.12231182329940735, 0.18413977511356364, 0.14381720481858437, 0.09139784739232919, 0.11424730924041149, 0.11693547791961309, 0.09408601607153079, 0.11693547791961309, 0.10887096386061723, 0.11290322089011516, 0.10080644980162137, 0.08333333333333333, 0.11021505221091356, 0.10618279518141563, 0.11155914056120989, 0.09811828112241977, 0.11424730924041149, 0.1021505381519177, 0.11693547791961309, 0.09946236145132505], [0.10752688353171196, 0.11962365462020574, 0.11021505221091356, 0.11155914056120989, 0.12365591164970367, 0.09408601607153079, 0.09139784739232919, 0.11962365462020574, 0.10887096386061723, 0.1021505381519177, 0.10080644980162137, 0.11827956626990942, 0.10887096386061723, 0.11155914056120989, 0.11559139759070781, 0.11021505221091356, 0.09543010442182712, 0.1048387068311193, 0.11693547791961309, 0.09946236145132505, 0.125, 0.11693547791961309, 0.12096774297050207, 0.17338710039675725, 0.09811828112241977, 0.09811828112241977, 0.09005375904203286, 0.09005375904203286, 0.10618279518141563, 0.11290322089011516, 0.1478494618480823, 0.09677419277212344, 0.13978494778908646, 0.10887096386061723, 0.09543010442182712, 0.09139784739232919, 0.09408601607153079, 0.11827956626990942, 0.08467742168362967, 0.11290322089011516], [0.13844085141739906, 0.13037634537979426, 0.10080644980162137, 0.10080644980162137, 0.11559139759070781, 0.11962365462020574, 0.10080644980162137, 0.10752688353171196, 0.11290322089011516, 0.09946236145132505, 0.10080644980162137, 0.1048387068311193, 0.11693547791961309, 0.11290322089011516, 0.09005375904203286, 0.09543010442182712, 0.11155914056120989, 0.11962365462020574, 0.14381720481858437, 0.10618279518141563, 0.09946236145132505, 0.12096774297050207, 0.10080644980162137, 0.09677419277212344, 0.10752688353171196, 0.10752688353171196, 0.14381720481858437, 0.12365591164970367, 0.10887096386061723, 0.09946236145132505, 0.11021505221091356, 0.12096774297050207, 0.10080644980162137, 0.11155914056120989, 0.11827956626990942, 0.1599462329365761, 0.09005375904203286, 0.11962365462020574, 0.09677419277212344, 0.11559139759070781]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "org_weight = copy.deepcopy(net_base.fc1.W)\n",
        "org_acc = test(net_base, device, test_loader)\n",
        "\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for i in range(net_base.fc1.W.shape[0]):\n",
        "        res = []\n",
        "        for j in range(net_base.fc1.W.shape[1]):\n",
        "            net_base.fc1.W = copy.deepcopy(org_weight)\n",
        "            net_base.fc1.W[i, j] = net_base.fc1.W[i, j] + 1\n",
        "\n",
        "            acc_1 = test(net_base, device, test_loader, prints = False)\n",
        "\n",
        "            net_base.fc1.W = copy.deepcopy(org_weight)\n",
        "            net_base.fc1.W[i, j] = net_base.fc1.W[i, j] - 1\n",
        "\n",
        "            acc_2 = test(model, device, test_loader, prints = False)\n",
        "            res.append(((acc_1 + acc_2)/2)/org_acc)\n",
        "        results.append(res)\n",
        "        print(res)\n",
        "    print(results)\n",
        "\n",
        "ax = sns.heatmap(results, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "sETTgdjjV9zc",
        "outputId": "c7d2716e-dcff-47e7-db27-225e44690547"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2260, Accuracy: 9305/10000 (93%)\n",
            "\n",
            "[1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259]\n",
            "[1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259]\n",
            "[1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259, 1.007684040838259]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1b20230ecca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnet_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0macc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mnet_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-27f93077aebd>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, test_loader, prints)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-f6585c218107>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-84e09d3325a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbon\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-84e09d3325a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, ticket, x, W, b)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#return ticket.CODEXvmm(x) + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mticket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCODEXvmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-cc533322f26d>\u001b[0m in \u001b[0;36mCODEXvmm\u001b[0;34m(self, xvector)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#Add encoding vector u to x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxvector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muvect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mpad_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_bits\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mrW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-cc533322f26d>\u001b[0m in \u001b[0;36mprep_vector\u001b[0;34m(self, vector, v_bits)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbin2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mbit_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin2s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mbit_vector\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}