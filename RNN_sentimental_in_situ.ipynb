{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentimental Analysis using RNN\n",
        "with SGD/thresholded update/manhattan learning rule"
      ],
      "metadata": {
        "id": "YXWfTBPP4kCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manhattan Optimizer"
      ],
      "metadata": {
        "id": "291qZQGD4qnD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A23UbwcCMTth"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from typing import List, Optional\n",
        "import numpy as np\n",
        "\n",
        "count_list = []\n",
        "last_layer = []\n",
        "device = 'cpu'\n",
        "\n",
        "class ManhattanSGD(Optimizer):\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, *, maximize=False, foreach: Optional[bool] = None):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        maximize=maximize, foreach=foreach)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(ManhattanSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "            group.setdefault('maximize', False)\n",
        "            group.setdefault('foreach', None)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs the Manhattan Learning rule such that\n",
        "        \\Delta W(i,j) = sgn(\\Delta w(i,j))\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            d_p_list = []\n",
        "            momentum_buffer_list = []\n",
        "            has_sparse_grad = False\n",
        "\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        # buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                        #buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()#.mul_(group['lr'])\n",
        "                        buf = param_state['momentum_buffer'] =  torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device)\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer'].float()\n",
        "                        #buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                        #buf.add_(1 - dampening, torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device))\n",
        "                        #buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()#.mul_(group['lr'])\n",
        "                        buf = param_state['momentum_buffer'] =  torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                else:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "\n",
        "        return loss\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8B3141COYHL"
      },
      "source": [
        "## Crossbar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyP6jPVeOZQS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "crossbar.py\n",
        "Louis Primeau\n",
        "University of Toronto Department of Electrical and Computer Engineering\n",
        "louis.primeau@mail.utoronto.ca\n",
        "July 29th 2020\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import savemat\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from scipy.io import savemat\n",
        "\n",
        "# Implements scipy's minmax scaler except just between 0 and 1 for torch Tensors.\n",
        "# Taken from a ptrblck post on the PyTorch forums. Love that dude.\n",
        "class MinMaxScaler(object):\n",
        "    def __call__(self, tensor):\n",
        "        self.scale = 1.0 / (tensor.max(dim=1, keepdim=True)[0] - tensor.min(dim=1, keepdim=True)[0])\n",
        "        self.min = tensor.min(dim=1, keepdim=True)[0]\n",
        "        tensor.sub_(self.min).mul_(self.scale)\n",
        "        return tensor\n",
        "    def inverse_transform(self, tensor):\n",
        "        tensor.div_(self.scale).add_(self.min)\n",
        "        return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYNBj3F6Ocq9"
      },
      "outputs": [],
      "source": [
        "class ticket:\n",
        "    def __init__(self, row, col, m_rows, m_cols, matrix, mat_scale_factor, crossbar, uvect, decode, inputres, outputres):\n",
        "        self.row, self.col = row, col\n",
        "        self.m_rows, self.m_cols = m_rows, m_cols\n",
        "        self.crossbar = crossbar\n",
        "        self.mat_scale_factor = mat_scale_factor\n",
        "        self.matrix = matrix\n",
        "        self.uvect = uvect\n",
        "        self.inputres = inputres\n",
        "        self.adcres = outputres\n",
        "        self.decode = torch.matmul(self.uvect.t(),self.matrix)\n",
        "\n",
        "    def prep_vector(self, vector, v_bits):\n",
        "\n",
        "        # Scale vector to [0, 2^v_bits]\n",
        "        vect_min = torch.min(vector)\n",
        "        vector = vector - vect_min\n",
        "        vect_scale_factor = torch.max(vector) / (2**v_bits - 1)\n",
        "        vector = vector / vect_scale_factor if vect_scale_factor != 0.0 else vector\n",
        "\n",
        "        # decompose vector by bit\n",
        "        bit_vector = torch.zeros(vector.size(0),v_bits)\n",
        "        bin2s = lambda x : ''.join(reversed( [str((int(x) >> i) & 1) for i in range(v_bits)] ) )\n",
        "        for j in range(vector.size(0)):\n",
        "            bit_vector[j,:] = torch.Tensor([float(i) for i in list(bin2s(vector[j]))])\n",
        "        bit_vector *= self.crossbar.V\n",
        "\n",
        "        # Pad bit vector with unselected voltages\n",
        "        pad_vector = torch.zeros(self.crossbar.size[0], v_bits)\n",
        "\n",
        "        pad_vector[self.row:self.row + self.m_rows,:] = bit_vector\n",
        "\n",
        "        return pad_vector, vect_scale_factor, vect_min\n",
        "\n",
        "    def vmm(self, vector):\n",
        "        # Baseline VMM operation without CODEX\n",
        "        v_bits = self.inputres\n",
        "        assert vector.size(1) == 1, \"vector wrong shape\"\n",
        "\n",
        "        crossbar = self.crossbar\n",
        "        # Rescale vector and convert to bits.\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        # Perform crossbar VMM\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        # Add binary outputs\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "\n",
        "        # Rescale binary outputs\n",
        "        rout = (rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5131 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "        return rout.view(-1,1)\n",
        "\n",
        "    def CODEXvmm(self, xvector):\n",
        "        # CODEX VMM operation\n",
        "        assert xvector.size(1) == 1, \"vector wrong shape\"\n",
        "        v_bits=self.inputres\n",
        "        crossbar = self.crossbar\n",
        "\n",
        "        #Add encoding vector u to x\n",
        "        vector = xvector + self.uvect\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits+1)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        # The rout on the line below this comment contains\n",
        "        # the raw output currents that the ADC will receive.\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "        rout = 2*(rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5231 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "        rout = rout - self.decode\n",
        "        return rout.view(-1,1)\n",
        "\n",
        "    def modified_CODEXvmm(self, xvector):\n",
        "        # CODEX VMM operation\n",
        "        assert xvector.size(1) == 1, \"vector wrong shape\"\n",
        "        v_bits=self.inputres\n",
        "        crossbar = self.crossbar\n",
        "\n",
        "        #Add encoding vector u to x\n",
        "        vector = xvector + self.uvect\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits+1)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        # The rout on the line below this comment contains\n",
        "        # the raw output currents that the ADC will receive.\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "        rout = 2*(rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5231 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "\n",
        "        # do not decode except during inference\n",
        "        # rout = rout - self.decode\n",
        "        return rout.view(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwHVS5NDOlhN"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import random\n",
        "import copy\n",
        "\n",
        "class linear(torch.autograd.Function):\n",
        "    #From Louis: Custom pytorch autograd function for crossbar VMM operation\n",
        "    @staticmethod\n",
        "    def forward(ctx, ticket, x, W, b):\n",
        "        ctx.save_for_backward(x, W, b)\n",
        "        return ticket.CODEXvmm(x) + b\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dx):\n",
        "        x, W, b = ctx.saved_tensors\n",
        "        grad_input = W.t().mm(dx)\n",
        "        grad_weight = dx.mm(x.t())\n",
        "        grad_bias = dx\n",
        "        return (None, grad_input, grad_weight, grad_bias)\n",
        "\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, cb,uvect):\n",
        "        super(Linear, self).__init__()\n",
        "        self.W = torch.nn.parameter.Parameter(torch.rand(output_size, input_size))\n",
        "        self.b = torch.nn.parameter.Parameter(torch.rand(output_size, 1))\n",
        "        self.cb = cb\n",
        "\n",
        "        #Instantiate Linear layer with pool of random encoding vectors to sample from\n",
        "        self.uvectlist = uvect\n",
        "        self.uvectidx = 0\n",
        "        # Decoding vector is calculated ideally here off-chip, but calculating decoding vector on-chip is also possible\n",
        "        self.decode = torch.matmul(self.uvectlist[self.uvectidx].t(),torch.transpose(self.W,0,1)).detach().clone()\n",
        "        self.ticket = cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "        self.f = linear()\n",
        "        self.cbon = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.f.apply(self.ticket, x, self.W, self.b) if self.cbon else self.W.matmul(x) + self.b\n",
        "\n",
        "    def remap(self):\n",
        "        #Should call the remap crossbar function after 1 or a couple update steps\n",
        "        self.cb.clear()\n",
        "        self.ticket = self.cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "\n",
        "    def update_decode(self):\n",
        "        #Update decoding vector by updating U*G.\n",
        "        self.decode = torch.matmul(self.uvectlist[self.uvectidx].t(),torch.transpose(self.W,0,1)).detach().clone()\n",
        "\n",
        "    def resample(self):\n",
        "        #Sample random new uvector from provided uvectlist\n",
        "        self.cb.clear()\n",
        "        self.uvectidx = random.randint(0, len(uvectlist)-1)\n",
        "        self.ticket = self.cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "\n",
        "    def use_cb(self, state):\n",
        "        self.cbon = state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF7JnG4yOo--"
      },
      "outputs": [],
      "source": [
        "# Key Idea is that CODEX allows us to use higher ADC inpt resolution by\n",
        "# Reducing the ADC sensing range.\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 200,\n",
        "                 \"n\": 200,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}\n",
        "\n",
        "\n",
        "class crossbar:\n",
        "    def __init__(self, device_params):\n",
        "\n",
        "        # Power Supply Voltage\n",
        "        self.V = device_params[\"Vdd\"]\n",
        "\n",
        "        # DAC resolution\n",
        "        self.input_resolution = device_params[\"dac_resolution\"]\n",
        "        self.output_resolution = device_params[\"adc_resolution\"]\n",
        "\n",
        "        # Wordline Resistance\n",
        "        self.r_wl = torch.Tensor((device_params[\"r_wl\"],))\n",
        "        # Bitline Resistance\n",
        "        self.r_bl = torch.Tensor((device_params[\"r_bl\"],))\n",
        "\n",
        "        # Number of rows, columns\n",
        "        self.size = device_params[\"m\"], device_params[\"n\"]\n",
        "\n",
        "        # High resistance state\n",
        "        self.g_on = 1 / torch.normal(device_params[\"r_on_mean\"], device_params[\"r_on_stddev\"], size=self.size)\n",
        "        #self.g_on = (1 / device_params[\"r_on_mean\"]) * torch.ones(self.size)\n",
        "\n",
        "        # Low Resistance state\n",
        "        self.g_off = 1 / torch.normal(device_params[\"r_off_mean\"], device_params[\"r_off_stddev\"], size=self.size)\n",
        "        #self.g_off = (1 / device_params[\"r_off_mean\"]) * torch.ones(self.size)\n",
        "\n",
        "        self.g_wl = torch.Tensor((1 / device_params[\"r_wl\"],))\n",
        "        self.g_bl = torch.Tensor((1 / device_params[\"r_bl\"],))\n",
        "\n",
        "        # Resolution\n",
        "        self.resolution = device_params[\"device_resolution\"]\n",
        "        # Conductance tensor, m x n x 2**resolution\n",
        "\n",
        "        # 2**self.resolution - 1 so that there's a conductance state in the middle.\n",
        "        self.conductance_states = torch.cat([torch.cat([torch.linspace(self.g_off[i,j], self.g_on[i,j],2**self.resolution - 1).unsqueeze(0)\n",
        "                                                        for j in range(self.size[1])],dim=0).unsqueeze(0)\n",
        "                                             for i in range(self.size[0])],dim=0)\n",
        "\n",
        "        # Bias Scheme\n",
        "        self.bias_voltage = self.V * device_params[\"bias_scheme\"]\n",
        "\n",
        "        # Tile size (1x1 = 1T1R, nxm = passive, etc.)\n",
        "        self.tile_rows = device_params[\"tile_rows\"]\n",
        "        self.tile_cols = device_params[\"tile_cols\"]\n",
        "        assert self.size[0] % self.tile_rows == 0, \"tile size does not divide crossbar size in row direction\"\n",
        "        assert self.size[1] % self.tile_cols == 0, \"tile size does not divide crossbar size in col direction\"\n",
        "\n",
        "        # Resistance of CMOS lines\n",
        "        self.r_cmos_line = device_params[\"r_cmos_line\"]\n",
        "\n",
        "        # Conductance Matrix; initialize each memristor at the on resstance\n",
        "        self.W = torch.ones(self.size) * self.g_on\n",
        "\n",
        "        # Stuck-on & stuck-on device nonideality\n",
        "        self.p_stuck_on = device_params[\"p_stuck_on\"]\n",
        "        self.p_stuck_off = device_params[\"p_stuck_off\"]\n",
        "        self.devicefaults = False\n",
        "\n",
        "        self.mapped = []\n",
        "\n",
        "        self.saved_tiles = {}\n",
        "\n",
        "    def apply_stuck(self, p_stuck_on, p_stuck_off):\n",
        "\n",
        "        state_dist = torch.distributions.categorical.Categorical(probs=torch.Tensor([p_stuck_on, p_stuck_off, 1 - p_stuck_on - p_stuck_off]))\n",
        "        state_mask = state_dist.sample(self.size)\n",
        "\n",
        "        self.W[state_mask == 0] = self.g_off[state_mask==0]\n",
        "        self.W[state_mask == 1] = self.g_on[state_mask==1]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def map(self, matrix):\n",
        "        assert not(matrix.size(0) > self.size[0] or matrix.size(1)*2 > self.size[1]), \"input too large\"\n",
        "        midpoint = self.conductance_states.size(2) // 2\n",
        "\n",
        "        for i in range(matrix.size(0)):\n",
        "            for j in range(matrix.size(1)):\n",
        "\n",
        "                shifted = self.conductance_states[i,j] - self.conductance_states[i,j,midpoint]\n",
        "                idx = torch.min(torch.abs(shifted - matrix[i,j]), dim=0)[1]\n",
        "\n",
        "                self.W[i,2*j+1] = self.conductance_states[i,j,idx]\n",
        "                self.W[i,2*j] = self.conductance_states[i,j,midpoint-(idx-midpoint)]\n",
        "\n",
        "    def solve(self, voltage):\n",
        "        output = torch.zeros((voltage.size(1), self.size[1]))\n",
        "        for i in range(self.size[0] // self.tile_rows):\n",
        "            for j in range(self.size[1] // self.tile_cols):\n",
        "                for k in range(voltage.size(1)):\n",
        "                    coords = (i*self.tile_rows, (i+1)*self.tile_rows, j*self.tile_cols, (j+1)*self.tile_rows)\n",
        "                    vect = voltage[i*self.tile_rows:(i+1)*self.tile_rows,k]\n",
        "                    solution = self.circuit_solve(coords, vect, torch.zeros(self.size[1]), torch.ones(self.size[1]), torch.zeros(self.size[0]))\n",
        "                    output[k] += torch.cat((torch.zeros(j*self.tile_cols), solution, torch.zeros((self.size[1] // self.tile_cols - j - 1) * self.tile_cols)))\n",
        "        return output\n",
        "\n",
        "    \"\"\"\n",
        "    A Comprehensive Crossbar Array Model With Solutions for Line Resistance and Nonlinear Device Characteristics\n",
        "    An Chen\n",
        "    IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL. 60, NO. 4, APRIL 2013\n",
        "    \"\"\"\n",
        "\n",
        "    def hash_M(self, a, b, c, d):\n",
        "        return str(a) + \"_\" + str(b) + \"_\" + str(c) + \"_\" + str(d)\n",
        "\n",
        "    def make_M(self, a, b, c, d):\n",
        "\n",
        "        conductances = self.W[a:b,c:d]\n",
        "        g_wl, g_bl = self.g_wl, self.g_bl\n",
        "        g_s_wl_in, g_s_wl_out = torch.ones(self.tile_rows) * 1, torch.ones(self.tile_rows) * 1e-9\n",
        "        g_s_bl_in, g_s_bl_out = torch.ones(self.tile_rows) * 1e-9, torch.ones(self.tile_rows) * 1\n",
        "        m, n = self.tile_rows, self.tile_cols\n",
        "\n",
        "        A = torch.block_diag(*tuple(torch.diag(conductances[i,:])\n",
        "                          + torch.diag(torch.cat((g_wl, g_wl * 2 * torch.ones(n-2), g_wl)))\n",
        "                          + torch.diag(g_wl * -1 *torch.ones(n-1), diagonal = 1)\n",
        "                          + torch.diag(g_wl * -1 *torch.ones(n-1), diagonal = -1)\n",
        "                          + torch.diag(torch.cat((g_s_wl_in[i].view(1), torch.zeros(n - 2), g_s_wl_out[i].view(1))))\n",
        "                                   for i in range(m)))\n",
        "\n",
        "        B = torch.block_diag(*tuple(-torch.diag(conductances[i,:]) for i in range(m)))\n",
        "\n",
        "        def makec(j):\n",
        "            c = torch.zeros(m, m*n)\n",
        "            for i in range(m):\n",
        "                c[i,n*(i) + j] = conductances[i,j]\n",
        "            return c\n",
        "\n",
        "        C = torch.cat([makec(j) for j in range(n)],dim=0)\n",
        "\n",
        "        def maked(j):\n",
        "            d = torch.zeros(m, m*n)\n",
        "\n",
        "            def c(k):\n",
        "                return(k - 1)\n",
        "\n",
        "            i = 1\n",
        "            d[c(i),c(j)] = -g_s_bl_in[c(j)] - g_bl - conductances[c(i),c(j)]\n",
        "            d[c(i), n*i + c(j)] = g_bl\n",
        "\n",
        "            i = m\n",
        "            d[c(i), n*(i-2) + c(j)] = g_bl\n",
        "            d[c(i), n*(i-1) + c(j)] = -g_s_bl_out[c(j)] - conductances[c(i),c(j)] - g_bl\n",
        "\n",
        "            for i in range(2, m):\n",
        "                d[c(i), n*(i-2) + c(j)] = g_bl\n",
        "                d[c(i), n*(i-1) + c(j)] = -g_bl - conductances[c(i),c(j)] - g_bl\n",
        "                d[c(i), n*(i+1) + c(j)] = g_bl\n",
        "\n",
        "            return d\n",
        "\n",
        "        D = torch.cat([maked(j) for j in range(1,n+1)], dim=0)\n",
        "\n",
        "        M = torch.cat((torch.cat((A,B),dim=1), torch.cat((C,D),dim=1)), dim=0)\n",
        "\n",
        "        self.saved_tiles[self.hash_M(a,b,c,d)] = M\n",
        "\n",
        "        return torch.inverse(M)\n",
        "\n",
        "    def circuit_solve(self, coords,  v_wl_in, v_bl_in, v_bl_out, v_wl_out):\n",
        "\n",
        "        g_wl, g_bl = self.g_wl, self.g_bl\n",
        "        g_s_wl_in, g_s_wl_out = torch.ones(self.tile_rows) * 1, torch.ones(self.tile_rows) * 1e-9\n",
        "        g_s_bl_in, g_s_bl_out = torch.ones(self.tile_rows) * 1e-9, torch.ones(self.tile_rows) * 1\n",
        "        m, n = self.tile_rows, self.tile_cols\n",
        "\n",
        "\n",
        "        if self.hash_M(*coords) not in self.saved_tiles.keys():\n",
        "            #print(coords)\n",
        "            M = self.make_M(*coords)\n",
        "        else:\n",
        "            M = self.saved_tiles[self.hash_M(*coords)]\n",
        "\n",
        "        E = torch.cat([torch.cat(((v_wl_in[i]*g_s_wl_in[i]).view(1), #EW\n",
        "                                  torch.zeros(n-2),\n",
        "                                  (v_wl_out[i]*g_s_wl_out[i]).view(1)))\n",
        "                                 for i in range(m)] +\n",
        "                      [torch.cat(((-v_bl_in[i]*g_s_bl_in[i]).view(1), #EB\n",
        "                                  torch.zeros(m-2),\n",
        "                                  (-v_bl_in[i]*g_s_bl_out[i]).view(1)))\n",
        "                                 for i in range(n)]\n",
        "        ).view(-1, 1)\n",
        "\n",
        "        V = torch.matmul(M, E)\n",
        "\n",
        "        V = torch.chunk(torch.solve(E, M)[0], 2)\n",
        "\n",
        "        return torch.sum((V[1] - V[0]).view(m,n)*self.W[coords[0]:coords[1],coords[2]:coords[3]],dim=0)\n",
        "\n",
        "    def register_linear(self, matrix, uvectlist, decode, bias=None):\n",
        "\n",
        "        row, col = self.find_space(matrix.size(0), matrix.size(1))\n",
        "        # Need to add checks for bias size and col size\n",
        "\n",
        "        # Scale matrix\n",
        "        mat_scale_factor = torch.max(torch.abs(matrix)) / torch.max(self.g_on) * 2\n",
        "        scaled_matrix = matrix / mat_scale_factor\n",
        "\n",
        "        midpoint = self.conductance_states.size(2) // 2\n",
        "        for i in range(row, row + scaled_matrix.size(0)):\n",
        "            for j in range(col, col + scaled_matrix.size(1)):\n",
        "\n",
        "                shifted = self.conductance_states[i,j] - self.conductance_states[i,j,midpoint]\n",
        "                idx = torch.min(torch.abs(shifted - scaled_matrix[i-row,j-col]), dim=0)[1]\n",
        "                self.W[i,2*j+1] = self.conductance_states[i,j,idx]\n",
        "                self.W[i,2*j] = self.conductance_states[i,j,midpoint-(idx-midpoint)]\n",
        "\n",
        "        return ticket(row, col, matrix.size(0), matrix.size(1), matrix, mat_scale_factor, self, uvectlist, decode, self.input_resolution, self.output_resolution)\n",
        "\n",
        "    def which_tiles(self, row, col, m_row, m_col):\n",
        "        return itertools.product(range(row // self.tile_rows, (row + m_row) // self.tile_rows + 1),\n",
        "                                 range(col // self.tile_cols,(col + m_col) // self.tile_cols + 1),\n",
        "        )\n",
        "\n",
        "    def find_space(self, m_row, m_col):\n",
        "        if not self.mapped:\n",
        "            self.mapped.append((0,0,m_row,m_col))\n",
        "        else:\n",
        "            self.mapped.append((self.mapped[-1][0] + self.mapped[-1][2], self.mapped[-1][1] + self.mapped[-1][3], m_row, m_col))\n",
        "        return self.mapped[-1][0], self.mapped[-1][1]\n",
        "\n",
        "    def clear(self):\n",
        "        self.mapped = []\n",
        "        self.W = torch.ones(self.size) * self.g_on\n",
        "\n",
        "    def conductance_update(self):\n",
        "        self.conductance_states = torch.cat([torch.cat([torch.linspace(self.g_off[i,j], self.g_on[i,j],2**self.resolution - 1).unsqueeze(0)\n",
        "                                                        for j in range(self.size[1])],dim=0).unsqueeze(0)\n",
        "                                             for i in range(self.size[0])],dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVFdpz-OvTC"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJdhv6wYOwzQ",
        "outputId": "6a2177af-f647-4a3b-dece-049316dd69d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:38, 5.43MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:07<00:00, 51483.52it/s]\n",
            "100%|█████████▉| 9999/10000 [00:00<00:00, 50105.91it/s]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# file location (make sure to use your file location)\n",
        "file_dir = '/content/'\n",
        "\n",
        "# load csv file\n",
        "def get_data():\n",
        "    return csv.reader(open(file_dir + \"training.1600000.processed.noemoticon.csv\",\"rt\", encoding=\"latin-1\"))\n",
        "\n",
        "import torchtext\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
        "\n",
        "def split_tweet(tweet):\n",
        "    # separate punctuations\n",
        "    tweet = tweet.replace(\".\", \" . \") \\\n",
        "                 .replace(\",\", \" , \") \\\n",
        "                 .replace(\";\", \" ; \") \\\n",
        "                 .replace(\"?\", \" ? \")\n",
        "    return tweet.lower().split()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_tweet_vectors(glove_vector):\n",
        "    train, valid, test = [], [], []\n",
        "    for i, line in enumerate(get_data()):\n",
        "        tweet = line[-1]\n",
        "        if i % 59 == 0:\n",
        "            # obtain an embedding for the entire tweet\n",
        "            tweet_emb = sum(glove_vector[w] for w in split_tweet(tweet))\n",
        "            # generate a label: 1 = happy, 0 = sad\n",
        "            label = torch.tensor(int(line[0] == \"4\")).long()\n",
        "            # place the data set in either the training, validation, or test set\n",
        "            if i % 5 < 3:\n",
        "                train.append((tweet_emb, label)) # 60% training\n",
        "            elif i % 5 == 4:\n",
        "                valid.append((tweet_emb, label)) # 20% validation\n",
        "            else:\n",
        "                test.append((tweet_emb, label)) # 20% test\n",
        "    return train, valid, test\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcWgfUu3Pk7h"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
        "\n",
        "train, valid, test = get_tweet_vectors(glove)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XB2mMsLPliz"
      },
      "outputs": [],
      "source": [
        "def get_tweet_words(glove_vector):\n",
        "    train, valid, test = [], [], []\n",
        "    for i, line in enumerate(get_data()):\n",
        "        if i % 29 == 0:\n",
        "            tweet = line[-1]\n",
        "            idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
        "                    for w in split_tweet(tweet)\n",
        "                    if w in glove_vector.stoi] # keep words that has an embedding\n",
        "            if not idxs: # ignore tweets without any word with an embedding\n",
        "                continue\n",
        "            idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
        "            label = torch.tensor(int(line[0] == \"4\")).long()\n",
        "            if i % 5 < 3:\n",
        "                train.append((idxs, label))\n",
        "            elif i % 5 == 4:\n",
        "                valid.append((idxs, label))\n",
        "            else:\n",
        "                test.append((idxs, label))\n",
        "    return train, valid, test\n",
        "\n",
        "train, valid, test = get_tweet_words(glove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0KyWJcWPoLe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        i = 0\n",
        "        for tweets, labels in train:\n",
        "            i += 1\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(tweets)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if i % 100 == 1:\n",
        "                train_acc.append(get_accuracy(model, train_loader))\n",
        "                valid_acc.append(get_accuracy(model, valid_loader))\n",
        "                print(\"Epoch %d; Iteration %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "                      epoch+1, i, loss, train_acc[-1], valid_acc[-1]))\n",
        "                with open('log_baseline_test2.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([epoch * 100 + i, loss, valid_acc[-1]])\n",
        "\n",
        "        losses.append(float(loss))\n",
        "\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train_loader))\n",
        "        valid_acc.append(get_accuracy(model, valid_loader))\n",
        "        print(\"End of Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "\n",
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD-only"
      ],
      "metadata": {
        "id": "ut2K-zGt46w_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAcwwoebU5xu"
      },
      "outputs": [],
      "source": [
        "def net_trainer2(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    #ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    #optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def local_sched(learning_rate, gamma, cur_avg_abs_grad, epoch_n):\n",
        "        new_thr = cur_avg_abs_grad*(1+(math.exp(-learning_rate*gamma*epoch_n)))\n",
        "        upper_bound = cur_avg_abs_grad * config['max_thresh_multiplier']\n",
        "\n",
        "        return min(new_thr, upper_bound)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    #thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cur_epoch_updates = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 500, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}0\")\n",
        "                with open('log_baseline_train2.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * 32000, loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "                with open('record.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + 32000 * (j), cur_epoch_updates, cur_avg_abs_grad.item(), 0])\n",
        "\n",
        "                #save_model(model)\n",
        "\n",
        "\n",
        "            output = model(data)\n",
        "            #print(output)\n",
        "            #curr_loss = criterion(output, target) / config['batch_size']\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward(retain_graph=True)\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                #print(\"batch id:\", batch_idx)\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                cur_epoch_updates += 1\n",
        "                # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                optimizer.step()\n",
        "                model.fc1.remap()\n",
        "\n",
        "                #netowrk.fc2.remap()\n",
        "                optimizer.zero_grad()\n",
        "                avg_loss = -1.0\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss, []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thresholded update"
      ],
      "metadata": {
        "id": "JYHUV3_N4yY1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-N24S2oU3dM"
      },
      "outputs": [],
      "source": [
        "def net_trainer(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    threshold_update = False\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def local_sched(learning_rate, gamma, cur_avg_abs_grad, epoch_n):\n",
        "        new_thr = cur_avg_abs_grad*(1+(math.exp(-learning_rate*gamma*epoch_n)))\n",
        "        upper_bound = cur_avg_abs_grad * config['max_thresh_multiplier']\n",
        "\n",
        "        return min(new_thr, upper_bound)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "                               cur_avg_abs_grad, j+1)\n",
        "        thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 0:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 400, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 0:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}, Threshold: {thr}\")\n",
        "                with open('log_baseline_train3.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "                with open('record3.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + config['log_interval'] * (j), cur_epoch_updates, cur_avg_abs_grad.item(), thr.item()])\n",
        "\n",
        "\n",
        "            output = model(data)\n",
        "            #print(output)\n",
        "            #curr_loss = criterion(output, target) / config['batch_size']\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward(retain_graph=True)\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "\n",
        "            if avg_loss == -1.0:\n",
        "                avg_loss = loss\n",
        "            else:\n",
        "                avg_loss += loss\n",
        "            if avg_loss > config['naive_loss_thr']:\n",
        "                thr = init_thr\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                if threshold_update:\n",
        "                    thr = local_sched(config['learning_rate'],\n",
        "                                                    config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                    threshold_update = False\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                if cur_avg_abs_grad > thr:\n",
        "                    cur_epoch_updates += 1\n",
        "                    # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                    # thr = local_sched(config['learning_rate'],\n",
        "                    #                                 config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                    optimizer.step()\n",
        "                    model.fc1.remap()\n",
        "                    #netowrk.fc2.remap()\n",
        "                    optimizer.zero_grad()\n",
        "                    threshold_update = True\n",
        "                avg_loss = -1.0\n",
        "                thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss, thr_per_batch\n",
        "\n",
        "def network_tester(model, test_loader, test_size, epoch, log = True ):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            if batch_idx * len(data) > test_size:\n",
        "                break\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            total+=target.size(0)\n",
        "\n",
        "            if batch_idx % 100 == 0 and log:\n",
        "              with open('log_baseline_test3.csv', 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([batch_idx + epoch * 600, test_loss/(batch_idx+1), correct.item()/total])\n",
        "              print(\"Epoch\", epoch, 'iteration',batch_idx, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    return torch.div(correct, float(total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan Learning Rule"
      ],
      "metadata": {
        "id": "tCbMl48P4_4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net_trainer3(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    #optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def local_sched(learning_rate, gamma, cur_avg_abs_grad, epoch_n):\n",
        "        new_thr = cur_avg_abs_grad*(1+(math.exp(-learning_rate*gamma*epoch_n)))\n",
        "        upper_bound = cur_avg_abs_grad * config['max_thresh_multiplier']\n",
        "\n",
        "        return min(new_thr, upper_bound)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    #thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cur_epoch_updates = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 500, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}0\")\n",
        "                with open('log_baseline_train2.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * 32000, loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "                with open('record.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + 32000 * (j), cur_epoch_updates, cur_avg_abs_grad.item(), 0])\n",
        "\n",
        "                #save_model(model)\n",
        "\n",
        "\n",
        "            output = model(data)\n",
        "            #print(output)\n",
        "            #curr_loss = criterion(output, target) / config['batch_size']\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward(retain_graph=True)\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                #print(\"batch id:\", batch_idx)\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                cur_epoch_updates += 1\n",
        "                # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                optimizer.step()\n",
        "                model.fc1.remap()\n",
        "\n",
        "                #netowrk.fc2.remap()\n",
        "                optimizer.zero_grad()\n",
        "                avg_loss = -1.0\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss, []"
      ],
      "metadata": {
        "id": "dQj0c7BtjPcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig1iE7hFPpgw"
      },
      "outputs": [],
      "source": [
        "class TweetRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TweetRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        #self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self.uvect = [2*(torch.rand(hidden_size,1) - 0.5) for i in range(0,num_classes)] #\n",
        "        crb1 = crossbar(device_params)\n",
        "        # Can test using more than 1 crossbar linear layers.\n",
        "        # Easiest implementation is to create a crossbar for each linear layer\n",
        "        self.fc1 = Linear(hidden_size, num_classes,crb1,self.uvect)\n",
        "        self.fc1.use_cb(True)\n",
        "        #self.fc2 = nn.Linear(64*2*2, 10)\n",
        "        self.traincount = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc1(out.view(self.hidden_size, 1)).t()\n",
        "        out = F.log_softmax(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRo0fW4UP6EW"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# padding\n",
        "tweet_padded = pad_sequence([tweet for tweet, label in train[:10]],\n",
        "                            batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlSEC8rVP7g_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class TweetBatcher:\n",
        "    def __init__(self, tweets, batch_size=32, drop_last=False):\n",
        "        # store tweets by length\n",
        "        self.tweets_by_length = {}\n",
        "        for words, label in tweets:\n",
        "            # compute the length of the tweet\n",
        "            wlen = words.shape[0]\n",
        "            # put the tweet in the correct key inside self.tweet_by_length\n",
        "            if wlen not in self.tweets_by_length:\n",
        "                self.tweets_by_length[wlen] = []\n",
        "            self.tweets_by_length[wlen].append((words, label),)\n",
        "\n",
        "        #  create a DataLoader for each set of tweets of the same length\n",
        "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
        "                                    tweets,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
        "            for wlen, tweets in self.tweets_by_length.items()}\n",
        "\n",
        "    def __iter__(self): # called by Python to create an iterator\n",
        "        # make an iterator for every tweet length\n",
        "        iters = [iter(loader) for loader in self.loaders.values()]\n",
        "        while iters:\n",
        "            # pick an iterator (a length)\n",
        "            im = random.choice(iters)\n",
        "            try:\n",
        "                yield next(im)\n",
        "            except StopIteration:\n",
        "                # no more elements in the iterator, remove it\n",
        "                iters.remove(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYl1uWx2P-VB"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return correct / total\n",
        "\n",
        "test_loader = TweetBatcher(test, batch_size=1, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Code"
      ],
      "metadata": {
        "id": "waYpgNBh5H7g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "WgXHopL2QEne",
        "outputId": "4b724b10-beb6-4556-9745-1b29ea27604a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 0.008 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 2.167 | Acc: 52.475% (53/101)\n",
            "Epoch 0 iteration 200 Loss: 2.156 | Acc: 53.234% (107/201)\n",
            "Epoch 0 iteration 300 Loss: 2.291 | Acc: 50.498% (152/301)\n",
            "Epoch 0 iteration 400 Loss: 2.267 | Acc: 50.873% (204/401)\n",
            "Epoch 0 iteration 500 Loss: 2.308 | Acc: 50.100% (251/501)\n",
            "Test accuracy: 0.5009980201721191\n",
            "Epoch: 1, Loss: 0.050448838621377945 Updates: 1/1, Avg Grad: 0.00375957670621573930\n",
            "Epoch: 1, Loss: 0.0017122204881161451 Updates: 11/1001, Avg Grad: 0.111950680613517760\n",
            "Epoch: 1, Loss: 0.002534326398745179 Updates: 21/2001, Avg Grad: 0.107146434485912320\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-dae2252516fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mvalidloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetBatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetBatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mc3f1_loss_per_epoch2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_update_per_epoch2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_every_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_thr_per_batch2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_trainer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# model = TweetRNN(50, 50, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7973c8a132cc>\u001b[0m in \u001b[0;36mnet_trainer2\u001b[0;34m(model, train_dataloader, test_dataloader, config)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcur_epoch_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_interval'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-85d4f3fd9e8f>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# no more elements in the iterator, remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ndarray'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'memmap'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_config = {\n",
        "    \"num_epochs\" : 25,\n",
        "    \"batch_size\" : 100,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 1,\n",
        "    'learning_rate' : 0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.1,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "\n",
        "with open('log_baseline_train2.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test2.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "with open('record.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"update\", \"grad\", \"threshold\"])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "net2 = TweetRNN(50, 50, 2)\n",
        "trainloader = TweetBatcher(train, batch_size=1, drop_last=True)\n",
        "validloader = TweetBatcher(valid, batch_size=1, drop_last=False)\n",
        "testloader = TweetBatcher(test, batch_size=1, drop_last=False)\n",
        "c3f1_loss_per_epoch2, c3f1_update_per_epoch2, c3f1_every_loss2, c3f1_thr_per_batch2 = net_trainer2(net2, trainloader, validloader, train_config)\n",
        "\n",
        "# model = TweetRNN(50, 50, 2)\n",
        "\n",
        "# train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=2e-4)\n",
        "# get_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = {\n",
        "    \"num_epochs\" : 25,\n",
        "    \"batch_size\" : 50,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 1,\n",
        "    'learning_rate' : 0.0002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.1,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "\n",
        "with open('log_baseline_train3.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test3.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "with open('record3.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"update\", \"grad\", \"threshold\"])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "net = TweetRNN(50, 50, 2)\n",
        "trainloader = TweetBatcher(train, batch_size=1, drop_last=True)\n",
        "validloader = TweetBatcher(valid, batch_size=1, drop_last=False)\n",
        "testloader = TweetBatcher(test, batch_size=1, drop_last=False)\n",
        "c3f1_loss_per_epoch2, c3f1_update_per_epoch2, c3f1_every_loss2, c3f1_thr_per_batch2 = net_trainer(net, trainloader, validloader, train_config)\n",
        "\n",
        "# model = TweetRNN(50, 50, 2)\n",
        "\n",
        "# train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=2e-4)\n",
        "# get_accuracy(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34jffE8pd5Dy",
        "outputId": "9e4ba083-9b5f-46d7-ef76-ef0ca0fe4c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 0.357 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 0.757 | Acc: 55.446% (56/101)\n",
            "Epoch 0 iteration 200 Loss: 0.789 | Acc: 52.239% (105/201)\n",
            "Epoch 0 iteration 300 Loss: 0.810 | Acc: 50.166% (151/301)\n",
            "Epoch 0 iteration 400 Loss: 0.795 | Acc: 51.870% (208/401)\n",
            "Test accuracy: 0.5187032222747803\n",
            "Epoch: 1, Loss: 1.5643587112426758 Updates: 0/0, Avg Grad: 0.2868056893348694, Threshold: 0.31548625230789185\n",
            "Epoch: 1, Loss: 0.003716859733685851 Updates: 9/1000, Avg Grad: 0.016255108639597893, Threshold: 0.017880620434880257\n",
            "Epoch: 1, Loss: 0.01769128255546093 Updates: 19/2000, Avg Grad: 0.03872893005609512, Threshold: 0.020503787323832512\n",
            "Epoch: 1, Loss: 0.013753622770309448 Updates: 27/3000, Avg Grad: 0.03753354400396347, Threshold: 0.05166912078857422\n",
            "Epoch: 1, Loss: 0.007862410508096218 Updates: 35/4000, Avg Grad: 0.031680431216955185, Threshold: 0.03579935431480408\n",
            "Epoch: 1, Loss: 0.01996771991252899 Updates: 40/5000, Avg Grad: 0.025063158944249153, Threshold: 0.026387402787804604\n",
            "Epoch: 1, Loss: 0.012455083429813385 Updates: 50/6000, Avg Grad: 0.027314571663737297, Threshold: 0.03004602901637554\n",
            "Epoch: 1, Loss: 0.016413090750575066 Updates: 59/7000, Avg Grad: 0.04753205180168152, Threshold: 0.032924819737672806\n",
            "Epoch: 1, Loss: 0.012298066169023514 Updates: 66/8000, Avg Grad: 0.028346573933959007, Threshold: 0.031181231141090393\n",
            "Epoch: 1, Loss: 0.01586015149950981 Updates: 73/9000, Avg Grad: 0.04065874218940735, Threshold: 0.04472461715340614\n",
            "Epoch 0 iteration 0 Loss: 0.532 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 0.710 | Acc: 60.396% (61/101)\n",
            "Epoch 0 iteration 200 Loss: 0.708 | Acc: 56.716% (114/201)\n",
            "Epoch 0 iteration 300 Loss: 0.720 | Acc: 53.488% (161/301)\n",
            "Epoch 0 iteration 400 Loss: 0.718 | Acc: 52.618% (211/401)\n",
            "Test accuracy: 0.5261845588684082\n",
            "Epoch: 1, Loss: 0.007834000512957573 Updates: 81/10000, Avg Grad: 0.022615941241383553, Threshold: 0.024877535179257393\n",
            "Epoch: 1, Loss: 0.027912335470318794 Updates: 89/11000, Avg Grad: 0.019735338166356087, Threshold: 0.02170887216925621\n",
            "Epoch: 1, Loss: 0.011213116347789764 Updates: 98/12000, Avg Grad: 0.030133474618196487, Threshold: 0.032222602516412735\n",
            "Epoch: 1, Loss: 0.014240008778870106 Updates: 106/13000, Avg Grad: 0.034911904484033585, Threshold: 0.024726299569010735\n",
            "Epoch: 1, Loss: 0.010029553435742855 Updates: 112/14000, Avg Grad: 0.038999833166599274, Threshold: 0.05186372622847557\n",
            "Epoch: 1, Loss: 0.006041965447366238 Updates: 117/15000, Avg Grad: 0.06494076550006866, Threshold: 0.03268907964229584\n",
            "Epoch: 1, Loss: 0.01076517254114151 Updates: 125/16000, Avg Grad: 0.03258153051137924, Threshold: 0.03473394736647606\n",
            "Epoch: 1, Loss: 0.007676829118281603 Updates: 133/17000, Avg Grad: 0.03276282548904419, Threshold: 0.034588221460580826\n",
            "Epoch: 1, Loss: 0.009931703098118305 Updates: 143/18000, Avg Grad: 0.05782158300280571, Threshold: 0.026924019679427147\n",
            "Epoch: 1, Loss: 0.007977018132805824 Updates: 149/19000, Avg Grad: 0.05417782813310623, Threshold: 0.016935201361775398\n",
            "Epoch 0 iteration 0 Loss: 0.910 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.681 | Acc: 62.376% (63/101)\n",
            "Epoch 0 iteration 200 Loss: 0.705 | Acc: 55.224% (111/201)\n",
            "Epoch 0 iteration 300 Loss: 0.714 | Acc: 54.485% (164/301)\n",
            "Epoch 0 iteration 400 Loss: 0.719 | Acc: 52.618% (211/401)\n",
            "Test accuracy: 0.5261845588684082\n",
            "Epoch: 1, Loss: 0.009903146885335445 Updates: 158/20000, Avg Grad: 0.0426512137055397, Threshold: 0.027325619012117386\n",
            "Epoch: 1, Loss: 0.01421824935823679 Updates: 166/21000, Avg Grad: 0.02127058058977127, Threshold: 0.021870145574212074\n",
            "Epoch: 1, Loss: 0.02505851536989212 Updates: 174/22000, Avg Grad: 0.024176331236958504, Threshold: 0.026458224281668663\n",
            "Epoch: 1, Loss: 0.013320313766598701 Updates: 184/23000, Avg Grad: 0.038290925323963165, Threshold: 0.01918209157884121\n",
            "Epoch: 1, Loss: 0.014267698861658573 Updates: 190/24000, Avg Grad: 0.03336329758167267, Threshold: 0.03392743691802025\n",
            "Epoch: 1, Loss: 0.013527094386518002 Updates: 200/25000, Avg Grad: 0.026044102385640144, Threshold: 0.01803237944841385\n",
            "Epoch: 1, Loss: 0.011609293520450592 Updates: 209/26000, Avg Grad: 0.027160238474607468, Threshold: 0.027486898005008698\n",
            "Epoch: 1, Loss: 0.021000012755393982 Updates: 217/27000, Avg Grad: 0.03748263418674469, Threshold: 0.03267538547515869\n",
            "Epoch: 1, Loss: 0.015492033213376999 Updates: 226/28000, Avg Grad: 0.021410467103123665, Threshold: 0.0167120099067688\n",
            "Epoch: 1, Loss: 0.01715647242963314 Updates: 234/29000, Avg Grad: 0.035213105380535126, Threshold: 0.03546715900301933\n",
            "Epoch 0 iteration 0 Loss: 0.708 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.681 | Acc: 51.485% (52/101)\n",
            "Epoch 0 iteration 200 Loss: 0.700 | Acc: 52.736% (106/201)\n",
            "Epoch 0 iteration 300 Loss: 0.694 | Acc: 55.150% (166/301)\n",
            "Epoch 0 iteration 400 Loss: 0.693 | Acc: 54.364% (218/401)\n",
            "Test accuracy: 0.5436409115791321\n",
            "Epoch: 1, Loss: 0.02499503642320633 Updates: 243/30000, Avg Grad: 0.03178156912326813, Threshold: 0.029553692787885666\n",
            "Epoch: 1, Loss: 0.016305305063724518 Updates: 252/31000, Avg Grad: 0.04068208858370781, Threshold: 0.040890857577323914\n",
            "Epoch: 1, Loss: 0.016057856380939484 Updates: 260/32000, Avg Grad: 0.03218325600028038, Threshold: 0.03955443575978279\n",
            "Epoch 1 iteration 0 Loss: 0.433 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.667 | Acc: 54.455% (55/101)\n",
            "Epoch 1 iteration 200 Loss: 0.695 | Acc: 48.756% (98/201)\n",
            "Epoch 1 iteration 300 Loss: 0.690 | Acc: 50.166% (151/301)\n",
            "Epoch 1 iteration 400 Loss: 0.685 | Acc: 51.122% (205/401)\n",
            "Test accuracy: 0.5112219452857971\n",
            "Epoch: 2, Loss: 0.47588813304901123 Updates: 0/0, Avg Grad: 0.15848121047019958, Threshold: 0.17432934045791626\n",
            "Epoch: 2, Loss: 0.013659044168889523 Updates: 4/1000, Avg Grad: 0.06275003403425217, Threshold: 0.046070393174886703\n",
            "Epoch: 2, Loss: 0.015122516080737114 Updates: 12/2000, Avg Grad: 0.03999382629990578, Threshold: 0.043993208557367325\n",
            "Epoch: 2, Loss: 0.01587921567261219 Updates: 20/3000, Avg Grad: 0.05047152563929558, Threshold: 0.02360890991985798\n",
            "Epoch: 2, Loss: 0.018915055319666862 Updates: 25/4000, Avg Grad: 0.03371695429086685, Threshold: 0.05981908366084099\n",
            "Epoch: 2, Loss: 0.014616826549172401 Updates: 33/5000, Avg Grad: 0.01730543188750744, Threshold: 0.030845971778035164\n",
            "Epoch: 2, Loss: 0.012226882390677929 Updates: 42/6000, Avg Grad: 0.02469298616051674, Threshold: 0.027162285521626472\n",
            "Epoch: 2, Loss: 0.018484408035874367 Updates: 50/7000, Avg Grad: 0.018557963892817497, Threshold: 0.020413760095834732\n",
            "Epoch: 2, Loss: 0.014214904047548771 Updates: 60/8000, Avg Grad: 0.016690311953425407, Threshold: 0.018359344452619553\n",
            "Epoch: 2, Loss: 0.013800566084682941 Updates: 68/9000, Avg Grad: 0.04715898633003235, Threshold: 0.04079977050423622\n",
            "Epoch 1 iteration 0 Loss: 0.609 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.707 | Acc: 54.455% (55/101)\n",
            "Epoch 1 iteration 200 Loss: 0.704 | Acc: 49.254% (99/201)\n",
            "Epoch 1 iteration 300 Loss: 0.698 | Acc: 51.495% (155/301)\n",
            "Epoch 1 iteration 400 Loss: 0.694 | Acc: 54.364% (218/401)\n",
            "Test accuracy: 0.5436409115791321\n",
            "Epoch: 2, Loss: 0.01398343127220869 Updates: 74/10000, Avg Grad: 0.04267646372318268, Threshold: 0.046944111585617065\n",
            "Epoch: 2, Loss: 0.01572125405073166 Updates: 81/11000, Avg Grad: 0.025482213124632835, Threshold: 0.04456265643239021\n",
            "Epoch: 2, Loss: 0.013319429010152817 Updates: 90/12000, Avg Grad: 0.019785769283771515, Threshold: 0.01891055330634117\n",
            "Epoch: 2, Loss: 0.015007095411419868 Updates: 96/13000, Avg Grad: 0.038703858852386475, Threshold: 0.051341503858566284\n",
            "Epoch: 2, Loss: 0.017939571291208267 Updates: 100/14000, Avg Grad: 0.029098069295287132, Threshold: 0.03180064260959625\n",
            "Epoch: 2, Loss: 0.011820487678050995 Updates: 109/15000, Avg Grad: 0.04972001910209656, Threshold: 0.03905239701271057\n",
            "Epoch: 2, Loss: 0.01467969175428152 Updates: 117/16000, Avg Grad: 0.024553347378969193, Threshold: 0.016806738451123238\n",
            "Epoch: 2, Loss: 0.009182976558804512 Updates: 123/17000, Avg Grad: 0.028303690254688263, Threshold: 0.029880642890930176\n",
            "Epoch: 2, Loss: 0.01624188758432865 Updates: 130/18000, Avg Grad: 0.04931577667593956, Threshold: 0.04745957627892494\n",
            "Epoch: 2, Loss: 0.00958038866519928 Updates: 138/19000, Avg Grad: 0.021618491038680077, Threshold: 0.030779125168919563\n",
            "Epoch 1 iteration 0 Loss: 0.736 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 0.700 | Acc: 53.465% (54/101)\n",
            "Epoch 1 iteration 200 Loss: 0.696 | Acc: 50.746% (102/201)\n",
            "Epoch 1 iteration 300 Loss: 0.686 | Acc: 53.488% (161/301)\n",
            "Epoch 1 iteration 400 Loss: 0.686 | Acc: 54.364% (218/401)\n",
            "Test accuracy: 0.5436409115791321\n",
            "Epoch: 2, Loss: 0.01138110738247633 Updates: 147/20000, Avg Grad: 0.01783878728747368, Threshold: 0.018435001373291016\n",
            "Epoch: 2, Loss: 0.028607139363884926 Updates: 157/21000, Avg Grad: 0.029811091721057892, Threshold: 0.0306513924151659\n",
            "Epoch: 2, Loss: 0.008786676451563835 Updates: 166/22000, Avg Grad: 0.023078158497810364, Threshold: 0.032997023314237595\n",
            "Epoch: 2, Loss: 0.014462430030107498 Updates: 174/23000, Avg Grad: 0.0751098170876503, Threshold: 0.05707187578082085\n",
            "Epoch: 2, Loss: 0.010314191691577435 Updates: 183/24000, Avg Grad: 0.015876343473792076, Threshold: 0.016144797205924988\n",
            "Epoch: 2, Loss: 0.011727100238204002 Updates: 193/25000, Avg Grad: 0.022016454488039017, Threshold: 0.022330423817038536\n",
            "Epoch: 2, Loss: 0.005261342506855726 Updates: 202/26000, Avg Grad: 0.025657065212726593, Threshold: 0.025965645909309387\n",
            "Epoch: 2, Loss: 0.019924115389585495 Updates: 211/27000, Avg Grad: 0.023703593760728836, Threshold: 0.023944029584527016\n",
            "Epoch: 2, Loss: 0.012841563671827316 Updates: 220/28000, Avg Grad: 0.051657725125551224, Threshold: 0.025827577337622643\n",
            "Epoch: 2, Loss: 0.013746947981417179 Updates: 229/29000, Avg Grad: 0.01926429755985737, Threshold: 0.03287767618894577\n",
            "Epoch 1 iteration 0 Loss: 1.047 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 0.674 | Acc: 54.455% (55/101)\n",
            "Epoch 1 iteration 200 Loss: 0.685 | Acc: 56.219% (113/201)\n",
            "Epoch 1 iteration 300 Loss: 0.690 | Acc: 55.482% (167/301)\n",
            "Epoch 1 iteration 400 Loss: 0.690 | Acc: 55.362% (222/401)\n",
            "Test accuracy: 0.5536159873008728\n",
            "Epoch: 2, Loss: 0.009479162283241749 Updates: 237/30000, Avg Grad: 0.033217404037714005, Threshold: 0.038962479680776596\n",
            "Epoch: 2, Loss: 0.015358000993728638 Updates: 245/31000, Avg Grad: 0.05925893411040306, Threshold: 0.040476176887750626\n",
            "Epoch: 2, Loss: 0.013102464377880096 Updates: 255/32000, Avg Grad: 0.020195476710796356, Threshold: 0.014261697418987751\n",
            "Epoch 2 iteration 0 Loss: 0.731 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.668 | Acc: 58.416% (59/101)\n",
            "Epoch 2 iteration 200 Loss: 0.680 | Acc: 56.219% (113/201)\n",
            "Epoch 2 iteration 300 Loss: 0.687 | Acc: 54.817% (165/301)\n",
            "Epoch 2 iteration 400 Loss: 0.681 | Acc: 57.107% (229/401)\n",
            "Test accuracy: 0.5710723400115967\n",
            "Epoch: 3, Loss: 0.6722899079322815 Updates: 0/0, Avg Grad: 0.2084198147058487, Threshold: 0.22926180064678192\n",
            "Epoch: 3, Loss: 0.02019711583852768 Updates: 9/1000, Avg Grad: 0.024408984929323196, Threshold: 0.020306533202528954\n",
            "Epoch: 3, Loss: 0.012504045851528645 Updates: 18/2000, Avg Grad: 0.01806563511490822, Threshold: 0.021410970017313957\n",
            "Epoch: 3, Loss: 0.01951894350349903 Updates: 27/3000, Avg Grad: 0.038420114666223526, Threshold: 0.029810620471835136\n",
            "Epoch: 3, Loss: 0.01899361051619053 Updates: 35/4000, Avg Grad: 0.03589645400643349, Threshold: 0.039486099034547806\n",
            "Epoch: 3, Loss: 0.012477431446313858 Updates: 40/5000, Avg Grad: 0.03169933706521988, Threshold: 0.034869272261857986\n",
            "Epoch: 3, Loss: 0.016899166628718376 Updates: 49/6000, Avg Grad: 0.03406506031751633, Threshold: 0.03126253932714462\n",
            "Epoch: 3, Loss: 0.019451312720775604 Updates: 55/7000, Avg Grad: 0.02568085864186287, Threshold: 0.06300967931747437\n",
            "Epoch: 3, Loss: 0.017253823578357697 Updates: 59/8000, Avg Grad: 0.040073320269584656, Threshold: 0.05126848816871643\n",
            "Epoch: 3, Loss: 0.014334904961287975 Updates: 65/9000, Avg Grad: 0.09630367904901505, Threshold: 0.07511719316244125\n",
            "Epoch 2 iteration 0 Loss: 1.265 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.673 | Acc: 61.386% (62/101)\n",
            "Epoch 2 iteration 200 Loss: 0.668 | Acc: 58.706% (118/201)\n",
            "Epoch 2 iteration 300 Loss: 0.662 | Acc: 60.465% (182/301)\n",
            "Epoch 2 iteration 400 Loss: 0.671 | Acc: 58.853% (236/401)\n",
            "Test accuracy: 0.5885286927223206\n",
            "Epoch: 3, Loss: 0.013484298251569271 Updates: 74/10000, Avg Grad: 0.015454242005944252, Threshold: 0.014201963320374489\n",
            "Epoch: 3, Loss: 0.01260851975530386 Updates: 83/11000, Avg Grad: 0.02091880887746811, Threshold: 0.01742205210030079\n",
            "Epoch: 3, Loss: 0.01561666652560234 Updates: 91/12000, Avg Grad: 0.02568981610238552, Threshold: 0.028258798643946648\n",
            "Epoch: 3, Loss: 0.012004091404378414 Updates: 101/13000, Avg Grad: 0.01592010259628296, Threshold: 0.017512112855911255\n",
            "Epoch: 3, Loss: 0.01174630131572485 Updates: 110/14000, Avg Grad: 0.022043166682124138, Threshold: 0.024090494960546494\n",
            "Epoch: 3, Loss: 0.012238061055541039 Updates: 116/15000, Avg Grad: 0.04662791267037392, Threshold: 0.016091659665107727\n",
            "Epoch: 3, Loss: 0.012597395107150078 Updates: 122/16000, Avg Grad: 0.04742364585399628, Threshold: 0.039100807160139084\n",
            "Epoch: 3, Loss: 0.012790159322321415 Updates: 129/17000, Avg Grad: 0.02713879756629467, Threshold: 0.02865084819495678\n",
            "Epoch: 3, Loss: 0.010571821592748165 Updates: 135/18000, Avg Grad: 0.030646054074168205, Threshold: 0.032086081802845\n",
            "Epoch: 3, Loss: 0.0067209769040346146 Updates: 144/19000, Avg Grad: 0.018239380791783333, Threshold: 0.0189621951431036\n",
            "Epoch 2 iteration 0 Loss: 0.766 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.674 | Acc: 57.426% (58/101)\n",
            "Epoch 2 iteration 200 Loss: 0.677 | Acc: 56.716% (114/201)\n",
            "Epoch 2 iteration 300 Loss: 0.676 | Acc: 56.811% (171/301)\n",
            "Epoch 2 iteration 400 Loss: 0.673 | Acc: 56.608% (227/401)\n",
            "Test accuracy: 0.5660848021507263\n",
            "Epoch: 3, Loss: 0.01424808707088232 Updates: 154/20000, Avg Grad: 0.036168307065963745, Threshold: 0.03737713769078255\n",
            "Epoch: 3, Loss: 0.008616072125732899 Updates: 163/21000, Avg Grad: 0.02068139798939228, Threshold: 0.02126435562968254\n",
            "Epoch: 3, Loss: 0.013809413649141788 Updates: 172/22000, Avg Grad: 0.01408844068646431, Threshold: 0.014423361048102379\n",
            "Epoch: 3, Loss: 0.00971498154103756 Updates: 179/23000, Avg Grad: 0.035649728029966354, Threshold: 0.037717536091804504\n",
            "Epoch: 3, Loss: 0.01437984686344862 Updates: 186/24000, Avg Grad: 0.04591798409819603, Threshold: 0.039980873465538025\n",
            "Epoch: 3, Loss: 0.006295653060078621 Updates: 195/25000, Avg Grad: 0.015341022051870823, Threshold: 0.015559795312583447\n",
            "Epoch: 3, Loss: 0.021142534911632538 Updates: 204/26000, Avg Grad: 0.014962349086999893, Threshold: 0.015142302960157394\n",
            "Epoch: 3, Loss: 0.023037564009428024 Updates: 211/27000, Avg Grad: 0.02982783503830433, Threshold: 0.03746059536933899\n",
            "Epoch: 3, Loss: 0.0177993755787611 Updates: 221/28000, Avg Grad: 0.05433815345168114, Threshold: 0.02477366477251053\n",
            "Epoch: 3, Loss: 0.008399936370551586 Updates: 230/29000, Avg Grad: 0.039497505873441696, Threshold: 0.02166052721440792\n",
            "Epoch 2 iteration 0 Loss: 0.623 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 100 Loss: 0.660 | Acc: 65.347% (66/101)\n",
            "Epoch 2 iteration 200 Loss: 0.657 | Acc: 60.697% (122/201)\n",
            "Epoch 2 iteration 300 Loss: 0.650 | Acc: 61.462% (185/301)\n",
            "Epoch 2 iteration 400 Loss: 0.652 | Acc: 62.843% (252/401)\n",
            "Test accuracy: 0.6284289360046387\n",
            "Epoch: 3, Loss: 0.00863939430564642 Updates: 238/30000, Avg Grad: 0.04059915989637375, Threshold: 0.04084619879722595\n",
            "Epoch: 3, Loss: 0.01544276438653469 Updates: 248/31000, Avg Grad: 0.023096151649951935, Threshold: 0.01322377473115921\n",
            "Epoch: 3, Loss: 0.01496768556535244 Updates: 254/32000, Avg Grad: 0.03039364330470562, Threshold: 0.03052518703043461\n",
            "Epoch 3 iteration 0 Loss: 0.528 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 100 Loss: 0.665 | Acc: 63.366% (64/101)\n",
            "Epoch 3 iteration 200 Loss: 0.667 | Acc: 61.194% (123/201)\n",
            "Epoch 3 iteration 300 Loss: 0.672 | Acc: 59.468% (179/301)\n",
            "Epoch 3 iteration 400 Loss: 0.671 | Acc: 59.601% (239/401)\n",
            "Test accuracy: 0.5960099697113037\n",
            "Epoch: 4, Loss: 0.8770402669906616 Updates: 0/0, Avg Grad: 0.22613395750522614, Threshold: 0.24874736368656158\n",
            "Epoch: 4, Loss: 0.017731022089719772 Updates: 8/1000, Avg Grad: 0.022956611588597298, Threshold: 0.020245319232344627\n",
            "Epoch: 4, Loss: 0.01258351095020771 Updates: 16/2000, Avg Grad: 0.08525394648313522, Threshold: 0.03814386948943138\n",
            "Epoch: 4, Loss: 0.010026787407696247 Updates: 25/3000, Avg Grad: 0.04544170945882797, Threshold: 0.021372850984334946\n",
            "Epoch: 4, Loss: 0.010968894697725773 Updates: 34/4000, Avg Grad: 0.027273952960968018, Threshold: 0.026485413312911987\n",
            "Epoch: 4, Loss: 0.011382423341274261 Updates: 40/5000, Avg Grad: 0.03674325719475746, Threshold: 0.03564833849668503\n",
            "Epoch: 4, Loss: 0.014559021219611168 Updates: 49/6000, Avg Grad: 0.06284347921609879, Threshold: 0.03545275330543518\n",
            "Epoch: 4, Loss: 0.012410937808454037 Updates: 55/7000, Avg Grad: 0.04152420163154602, Threshold: 0.05022266134619713\n",
            "Epoch: 4, Loss: 0.01278405636548996 Updates: 62/8000, Avg Grad: 0.017621217295527458, Threshold: 0.01938333921134472\n",
            "Epoch: 4, Loss: 0.009789596311748028 Updates: 72/9000, Avg Grad: 0.018657289445400238, Threshold: 0.02052301913499832\n",
            "Epoch 3 iteration 0 Loss: 0.416 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 100 Loss: 0.677 | Acc: 51.485% (52/101)\n",
            "Epoch 3 iteration 200 Loss: 0.685 | Acc: 54.229% (109/201)\n",
            "Epoch 3 iteration 300 Loss: 0.674 | Acc: 57.143% (172/301)\n",
            "Epoch 3 iteration 400 Loss: 0.669 | Acc: 57.855% (232/401)\n",
            "Test accuracy: 0.5785536170005798\n",
            "Epoch: 4, Loss: 0.011425379663705826 Updates: 79/10000, Avg Grad: 0.03049328364431858, Threshold: 0.03523612767457962\n",
            "Epoch: 4, Loss: 0.006823927629739046 Updates: 88/11000, Avg Grad: 0.0532069206237793, Threshold: 0.058527614921331406\n",
            "Epoch: 4, Loss: 0.008616900071501732 Updates: 95/12000, Avg Grad: 0.038388971239328384, Threshold: 0.027944542467594147\n",
            "Epoch: 4, Loss: 0.00775472167879343 Updates: 105/13000, Avg Grad: 0.0223061703145504, Threshold: 0.017527928575873375\n",
            "Epoch: 4, Loss: 0.0064897602424025536 Updates: 114/14000, Avg Grad: 0.0716433972120285, Threshold: 0.0387418158352375\n",
            "Epoch: 4, Loss: 0.010279051959514618 Updates: 123/15000, Avg Grad: 0.030345922335982323, Threshold: 0.03272295370697975\n",
            "Epoch: 4, Loss: 0.014249587431550026 Updates: 132/16000, Avg Grad: 0.024174602702260017, Threshold: 0.019713161513209343\n",
            "Epoch: 4, Loss: 0.007304179482161999 Updates: 139/17000, Avg Grad: 0.014245141297578812, Threshold: 0.015038815326988697\n",
            "Epoch: 4, Loss: 0.01180074829608202 Updates: 147/18000, Avg Grad: 0.059214264154434204, Threshold: 0.030008118599653244\n",
            "Epoch: 4, Loss: 0.008949128910899162 Updates: 156/19000, Avg Grad: 0.023891575634479523, Threshold: 0.024838382378220558\n",
            "Epoch 3 iteration 0 Loss: 0.795 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.702 | Acc: 49.505% (50/101)\n",
            "Epoch 3 iteration 200 Loss: 0.685 | Acc: 53.234% (107/201)\n",
            "Epoch 3 iteration 300 Loss: 0.677 | Acc: 55.482% (167/301)\n",
            "Epoch 3 iteration 400 Loss: 0.685 | Acc: 56.608% (227/401)\n",
            "Test accuracy: 0.5660848021507263\n",
            "Epoch: 4, Loss: 0.011713133193552494 Updates: 164/20000, Avg Grad: 0.05088486522436142, Threshold: 0.05258555710315704\n",
            "Epoch: 4, Loss: 0.009288299828767776 Updates: 168/21000, Avg Grad: 0.04430563747882843, Threshold: 0.040482375770807266\n",
            "Epoch: 4, Loss: 0.0163448303937912 Updates: 177/22000, Avg Grad: 0.023652205243706703, Threshold: 0.020756740123033524\n",
            "Epoch: 4, Loss: 0.012226633727550507 Updates: 185/23000, Avg Grad: 0.04569133743643761, Threshold: 0.022556031122803688\n",
            "Epoch: 4, Loss: 0.00696167116984725 Updates: 194/24000, Avg Grad: 0.01760784350335598, Threshold: 0.012813755311071873\n",
            "Epoch: 4, Loss: 0.015868980437517166 Updates: 202/25000, Avg Grad: 0.026646222919225693, Threshold: 0.022974632680416107\n",
            "Epoch: 4, Loss: 0.007569483481347561 Updates: 211/26000, Avg Grad: 0.044043928384780884, Threshold: 0.009909836575388908\n",
            "Epoch: 4, Loss: 0.014330311678349972 Updates: 220/27000, Avg Grad: 0.055122651159763336, Threshold: 0.03285559266805649\n",
            "Epoch: 4, Loss: 0.01655573584139347 Updates: 228/28000, Avg Grad: 0.014213954098522663, Threshold: 0.014335550367832184\n",
            "Epoch: 4, Loss: 0.008239432238042355 Updates: 236/29000, Avg Grad: 0.02051713690161705, Threshold: 0.020665163174271584\n",
            "Epoch 3 iteration 0 Loss: 0.766 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.680 | Acc: 47.525% (48/101)\n",
            "Epoch 3 iteration 200 Loss: 0.669 | Acc: 53.731% (108/201)\n",
            "Epoch 3 iteration 300 Loss: 0.669 | Acc: 56.146% (169/301)\n",
            "Epoch 3 iteration 400 Loss: 0.657 | Acc: 60.100% (241/401)\n",
            "Test accuracy: 0.6009975075721741\n",
            "Epoch: 4, Loss: 0.018212340772151947 Updates: 246/30000, Avg Grad: 0.01915426179766655, Threshold: 0.01927081122994423\n",
            "Epoch: 4, Loss: 0.010940668173134327 Updates: 254/31000, Avg Grad: 0.016723450273275375, Threshold: 0.016809269785881042\n",
            "Epoch: 4, Loss: 0.01098442543298006 Updates: 264/32000, Avg Grad: 0.023066964000463486, Threshold: 0.023166798055171967\n",
            "Epoch 4 iteration 0 Loss: 0.451 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 100 Loss: 0.670 | Acc: 64.356% (65/101)\n",
            "Epoch 4 iteration 200 Loss: 0.662 | Acc: 64.179% (129/201)\n",
            "Epoch 4 iteration 300 Loss: 0.658 | Acc: 63.455% (191/301)\n",
            "Epoch 4 iteration 400 Loss: 0.664 | Acc: 62.594% (251/401)\n",
            "Test accuracy: 0.6259351372718811\n",
            "Epoch: 5, Loss: 0.36498838663101196 Updates: 0/0, Avg Grad: 0.11654730141162872, Threshold: 0.12820203602313995\n",
            "Epoch: 5, Loss: 0.021237988024950027 Updates: 9/1000, Avg Grad: 0.020921755582094193, Threshold: 0.01616303063929081\n",
            "Epoch: 5, Loss: 0.025225363671779633 Updates: 17/2000, Avg Grad: 0.04044664278626442, Threshold: 0.018879136070609093\n",
            "Epoch: 5, Loss: 0.015773830935359 Updates: 24/3000, Avg Grad: 0.02054464817047119, Threshold: 0.0225991141051054\n",
            "Epoch: 5, Loss: 0.019136907532811165 Updates: 31/4000, Avg Grad: 0.016053596511483192, Threshold: 0.022012172266840935\n",
            "Epoch: 5, Loss: 0.019529394805431366 Updates: 41/5000, Avg Grad: 0.022652577608823776, Threshold: 0.014879819937050343\n",
            "Epoch: 5, Loss: 0.008541520684957504 Updates: 49/6000, Avg Grad: 0.016573408618569374, Threshold: 0.018230749294161797\n",
            "Epoch: 5, Loss: 0.015164258889853954 Updates: 56/7000, Avg Grad: 0.04287358373403549, Threshold: 0.01463546883314848\n",
            "Epoch: 5, Loss: 0.01666983775794506 Updates: 66/8000, Avg Grad: 0.037421293556690216, Threshold: 0.01933046616613865\n",
            "Epoch: 5, Loss: 0.014909341000020504 Updates: 73/9000, Avg Grad: 0.04310404881834984, Threshold: 0.044594019651412964\n",
            "Epoch 4 iteration 0 Loss: 0.673 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 100 Loss: 0.683 | Acc: 61.386% (62/101)\n",
            "Epoch 4 iteration 200 Loss: 0.647 | Acc: 64.179% (129/201)\n",
            "Epoch 4 iteration 300 Loss: 0.642 | Acc: 65.116% (196/301)\n",
            "Epoch 4 iteration 400 Loss: 0.645 | Acc: 64.589% (259/401)\n",
            "Test accuracy: 0.6458852887153625\n",
            "Epoch: 5, Loss: 0.008106185123324394 Updates: 80/10000, Avg Grad: 0.0653359442949295, Threshold: 0.056543365120887756\n",
            "Epoch: 5, Loss: 0.013236739672720432 Updates: 88/11000, Avg Grad: 0.01632928103208542, Threshold: 0.014729535207152367\n",
            "Epoch: 5, Loss: 0.01635843515396118 Updates: 95/12000, Avg Grad: 0.024154555052518845, Threshold: 0.02657001093029976\n",
            "Epoch: 5, Loss: 0.008797056041657925 Updates: 104/13000, Avg Grad: 0.02776724100112915, Threshold: 0.01896042749285698\n",
            "Epoch: 5, Loss: 0.007937323302030563 Updates: 112/14000, Avg Grad: 0.015242998488247395, Threshold: 0.016658740118145943\n",
            "Epoch: 5, Loss: 0.015764839947223663 Updates: 120/15000, Avg Grad: 0.03547661006450653, Threshold: 0.03825553134083748\n",
            "Epoch: 5, Loss: 0.01695072650909424 Updates: 128/16000, Avg Grad: 0.02042558044195175, Threshold: 0.02985110878944397\n",
            "Epoch: 5, Loss: 0.010024692863225937 Updates: 138/17000, Avg Grad: 0.0491039901971817, Threshold: 0.03544783219695091\n",
            "Epoch: 5, Loss: 0.007140517700463533 Updates: 145/18000, Avg Grad: 0.023211967200040817, Threshold: 0.02430267259478569\n",
            "Epoch: 5, Loss: 0.021833445876836777 Updates: 153/19000, Avg Grad: 0.05456928908824921, Threshold: 0.02625264599919319\n",
            "Epoch 4 iteration 0 Loss: 0.678 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 100 Loss: 0.689 | Acc: 55.446% (56/101)\n",
            "Epoch 4 iteration 200 Loss: 0.665 | Acc: 57.711% (116/201)\n",
            "Epoch 4 iteration 300 Loss: 0.653 | Acc: 59.801% (180/301)\n",
            "Epoch 4 iteration 400 Loss: 0.661 | Acc: 59.601% (239/401)\n",
            "Test accuracy: 0.5960099697113037\n",
            "Epoch: 5, Loss: 0.0076408302411437035 Updates: 157/20000, Avg Grad: 0.0918000191450119, Threshold: 0.04827970638871193\n",
            "Epoch: 5, Loss: 0.01631324365735054 Updates: 164/21000, Avg Grad: 0.0252987053245306, Threshold: 0.028571052476763725\n",
            "Epoch: 5, Loss: 0.004405130632221699 Updates: 173/22000, Avg Grad: 0.06653638184070587, Threshold: 0.06654299050569534\n",
            "Epoch: 5, Loss: 0.02238382026553154 Updates: 180/23000, Avg Grad: 0.04744003340601921, Threshold: 0.04054386541247368\n",
            "Epoch: 5, Loss: 0.0037865296471863985 Updates: 188/24000, Avg Grad: 0.022689247503876686, Threshold: 0.023072900250554085\n",
            "Epoch: 5, Loss: 0.006352473981678486 Updates: 197/25000, Avg Grad: 0.0207071416079998, Threshold: 0.02100243978202343\n",
            "Epoch: 5, Loss: 0.013647951185703278 Updates: 206/26000, Avg Grad: 0.019119760021567345, Threshold: 0.021621812134981155\n",
            "Epoch: 5, Loss: 0.022390883415937424 Updates: 215/27000, Avg Grad: 0.04478958621621132, Threshold: 0.0352792851626873\n",
            "Epoch: 5, Loss: 0.007606430444866419 Updates: 224/28000, Avg Grad: 0.02675929106771946, Threshold: 0.01920553855597973\n",
            "Epoch: 5, Loss: 0.010390915907919407 Updates: 233/29000, Avg Grad: 0.03254537284374237, Threshold: 0.01142972894012928\n",
            "Epoch 4 iteration 0 Loss: 1.155 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.614 | Acc: 68.317% (69/101)\n",
            "Epoch 4 iteration 200 Loss: 0.631 | Acc: 63.682% (128/201)\n",
            "Epoch 4 iteration 300 Loss: 0.641 | Acc: 63.455% (191/301)\n",
            "Epoch 4 iteration 400 Loss: 0.645 | Acc: 62.095% (249/401)\n",
            "Test accuracy: 0.6209476590156555\n",
            "Epoch: 5, Loss: 0.010650659911334515 Updates: 241/30000, Avg Grad: 0.07603047043085098, Threshold: 0.07649309933185577\n",
            "Epoch: 5, Loss: 0.023794302716851234 Updates: 246/31000, Avg Grad: 0.011133554391562939, Threshold: 0.011190688237547874\n",
            "Epoch: 5, Loss: 0.012141079641878605 Updates: 256/32000, Avg Grad: 0.02944716066122055, Threshold: 0.02957460843026638\n",
            "Epoch 5 iteration 0 Loss: 0.531 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.623 | Acc: 64.356% (65/101)\n",
            "Epoch 5 iteration 200 Loss: 0.640 | Acc: 61.692% (124/201)\n",
            "Epoch 5 iteration 300 Loss: 0.660 | Acc: 60.465% (182/301)\n",
            "Epoch 5 iteration 400 Loss: 0.661 | Acc: 61.097% (245/401)\n",
            "Test accuracy: 0.6109725832939148\n",
            "Epoch: 6, Loss: 0.5032268762588501 Updates: 0/0, Avg Grad: 0.16072864830493927, Threshold: 0.17680151760578156\n",
            "Epoch: 6, Loss: 0.010032105259597301 Updates: 9/1000, Avg Grad: 0.03644203767180443, Threshold: 0.04008624330163002\n",
            "Epoch: 6, Loss: 0.02493658848106861 Updates: 17/2000, Avg Grad: 0.02721576765179634, Threshold: 0.02080998569726944\n",
            "Epoch: 6, Loss: 0.015396736562252045 Updates: 22/3000, Avg Grad: 0.01622895337641239, Threshold: 0.017851850017905235\n",
            "Epoch: 6, Loss: 0.015575284138321877 Updates: 31/4000, Avg Grad: 0.025065990164875984, Threshold: 0.02245115302503109\n",
            "Epoch: 6, Loss: 0.00785019900649786 Updates: 39/5000, Avg Grad: 0.030510960146784782, Threshold: 0.0226921234279871\n",
            "Epoch: 6, Loss: 0.01590203121304512 Updates: 46/6000, Avg Grad: 0.035751961171627045, Threshold: 0.05011364445090294\n",
            "Epoch: 6, Loss: 0.013513857498764992 Updates: 54/7000, Avg Grad: 0.04327341169118881, Threshold: 0.04760075360536575\n",
            "Epoch: 6, Loss: 0.016657283529639244 Updates: 58/8000, Avg Grad: 0.025113511830568314, Threshold: 0.02713443897664547\n",
            "Epoch: 6, Loss: 0.013872943818569183 Updates: 67/9000, Avg Grad: 0.03253654018044472, Threshold: 0.01622633822262287\n",
            "Epoch 5 iteration 0 Loss: 0.654 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.646 | Acc: 66.337% (67/101)\n",
            "Epoch 5 iteration 200 Loss: 0.648 | Acc: 61.692% (124/201)\n",
            "Epoch 5 iteration 300 Loss: 0.646 | Acc: 62.126% (187/301)\n",
            "Epoch 5 iteration 400 Loss: 0.665 | Acc: 60.599% (243/401)\n",
            "Test accuracy: 0.6059850454330444\n",
            "Epoch: 6, Loss: 0.01515230629593134 Updates: 74/10000, Avg Grad: 0.03821350261569023, Threshold: 0.042034853249788284\n",
            "Epoch: 6, Loss: 0.010437952354550362 Updates: 83/11000, Avg Grad: 0.04336265102028847, Threshold: 0.045048780739307404\n",
            "Epoch: 6, Loss: 0.00887545570731163 Updates: 88/12000, Avg Grad: 0.016303149983286858, Threshold: 0.02042800933122635\n",
            "Epoch: 6, Loss: 0.007203628309071064 Updates: 98/13000, Avg Grad: 0.03263900801539421, Threshold: 0.021681897342205048\n",
            "Epoch: 6, Loss: 0.005139823537319899 Updates: 105/14000, Avg Grad: 0.016080249100923538, Threshold: 0.01739092729985714\n",
            "Epoch: 6, Loss: 0.00862521305680275 Updates: 114/15000, Avg Grad: 0.025723816826939583, Threshold: 0.025207476690411568\n",
            "Epoch: 6, Loss: 0.0042452202178537846 Updates: 123/16000, Avg Grad: 0.039054024964571, Threshold: 0.03536444902420044\n",
            "Epoch: 6, Loss: 0.01555677130818367 Updates: 132/17000, Avg Grad: 0.025778351351618767, Threshold: 0.016438117250800133\n",
            "Epoch: 6, Loss: 0.008100295439362526 Updates: 138/18000, Avg Grad: 0.023487050086259842, Threshold: 0.018589874729514122\n",
            "Epoch: 6, Loss: 0.010566595941781998 Updates: 146/19000, Avg Grad: 0.017329614609479904, Threshold: 0.018016375601291656\n",
            "Epoch 5 iteration 0 Loss: 0.494 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.610 | Acc: 66.337% (67/101)\n",
            "Epoch 5 iteration 200 Loss: 0.622 | Acc: 65.174% (131/201)\n",
            "Epoch 5 iteration 300 Loss: 0.643 | Acc: 61.462% (185/301)\n",
            "Epoch 5 iteration 400 Loss: 0.646 | Acc: 61.845% (248/401)\n",
            "Test accuracy: 0.618453860282898\n",
            "Epoch: 6, Loss: 0.022929515689611435 Updates: 155/20000, Avg Grad: 0.02465718239545822, Threshold: 0.008779801428318024\n",
            "Epoch: 6, Loss: 0.010853000916540623 Updates: 164/21000, Avg Grad: 0.07374892383813858, Threshold: 0.03549899160861969\n",
            "Epoch: 6, Loss: 0.013018142431974411 Updates: 170/22000, Avg Grad: 0.03497515246272087, Threshold: 0.022874895483255386\n",
            "Epoch: 6, Loss: 0.0035897749476134777 Updates: 180/23000, Avg Grad: 0.047437023371458054, Threshold: 0.022119617089629173\n",
            "Epoch: 6, Loss: 0.014100768603384495 Updates: 189/24000, Avg Grad: 0.035338517278432846, Threshold: 0.031063001602888107\n",
            "Epoch: 6, Loss: 0.005058204755187035 Updates: 198/25000, Avg Grad: 0.019502518698573112, Threshold: 0.019780637696385384\n",
            "Epoch: 6, Loss: 0.010074985213577747 Updates: 207/26000, Avg Grad: 0.032256290316581726, Threshold: 0.017933469265699387\n",
            "Epoch: 6, Loss: 0.011990785598754883 Updates: 215/27000, Avg Grad: 0.009787867777049541, Threshold: 0.00988715048879385\n",
            "Epoch: 6, Loss: 0.014575058594346046 Updates: 224/28000, Avg Grad: 0.022209670394659042, Threshold: 0.022399667650461197\n",
            "Epoch: 6, Loss: 0.02734530158340931 Updates: 233/29000, Avg Grad: 0.037576790899038315, Threshold: 0.03852119669318199\n",
            "Epoch 5 iteration 0 Loss: 0.569 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.670 | Acc: 61.386% (62/101)\n",
            "Epoch 5 iteration 200 Loss: 0.667 | Acc: 62.189% (125/201)\n",
            "Epoch 5 iteration 300 Loss: 0.646 | Acc: 64.120% (193/301)\n",
            "Epoch 5 iteration 400 Loss: 0.648 | Acc: 63.840% (256/401)\n",
            "Test accuracy: 0.6384040117263794\n",
            "Epoch: 6, Loss: 0.00903171394020319 Updates: 243/30000, Avg Grad: 0.02309987135231495, Threshold: 0.023240430280566216\n",
            "Epoch: 6, Loss: 0.014028637669980526 Updates: 253/31000, Avg Grad: 0.03706396371126175, Threshold: 0.02050364576280117\n",
            "Epoch: 6, Loss: 0.018161887302994728 Updates: 262/32000, Avg Grad: 0.026805125176906586, Threshold: 0.026921138167381287\n",
            "Epoch 6 iteration 0 Loss: 0.572 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 100 Loss: 0.671 | Acc: 58.416% (59/101)\n",
            "Epoch 6 iteration 200 Loss: 0.690 | Acc: 57.711% (116/201)\n",
            "Epoch 6 iteration 300 Loss: 0.698 | Acc: 57.807% (174/301)\n",
            "Epoch 6 iteration 400 Loss: 0.680 | Acc: 60.599% (243/401)\n",
            "Test accuracy: 0.6059850454330444\n",
            "Epoch: 7, Loss: 0.7852963805198669 Updates: 0/0, Avg Grad: 0.20449930429458618, Threshold: 0.22494924068450928\n",
            "Epoch: 7, Loss: 0.018310565501451492 Updates: 0/1000, Avg Grad: 0.10930878669023514, Threshold: 0.22494924068450928\n",
            "Epoch: 7, Loss: 0.013810801319777966 Updates: 5/2000, Avg Grad: 0.02502240240573883, Threshold: 0.013679992407560349\n",
            "Epoch: 7, Loss: 0.03788166120648384 Updates: 12/3000, Avg Grad: 0.02291334606707096, Threshold: 0.02445688284933567\n",
            "Epoch: 7, Loss: 0.006495583802461624 Updates: 21/4000, Avg Grad: 0.03697200119495392, Threshold: 0.022094272077083588\n",
            "Epoch: 7, Loss: 0.008826212026178837 Updates: 30/5000, Avg Grad: 0.0367756113409996, Threshold: 0.01753469556570053\n",
            "Epoch: 7, Loss: 0.021000936627388 Updates: 40/6000, Avg Grad: 0.021776633337140083, Threshold: 0.016823140904307365\n",
            "Epoch: 7, Loss: 0.01723564974963665 Updates: 48/7000, Avg Grad: 0.03118146024644375, Threshold: 0.03429960831999779\n",
            "Epoch: 7, Loss: 0.0036281030625104904 Updates: 56/8000, Avg Grad: 0.019810976460576057, Threshold: 0.021792074665427208\n",
            "Epoch: 7, Loss: 0.011548763141036034 Updates: 66/9000, Avg Grad: 0.016232691705226898, Threshold: 0.009588420391082764\n",
            "Epoch 6 iteration 0 Loss: 0.591 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 100 Loss: 0.634 | Acc: 65.347% (66/101)\n",
            "Epoch 6 iteration 200 Loss: 0.653 | Acc: 60.697% (122/201)\n",
            "Epoch 6 iteration 300 Loss: 0.661 | Acc: 59.801% (180/301)\n",
            "Epoch 6 iteration 400 Loss: 0.646 | Acc: 62.095% (249/401)\n",
            "Test accuracy: 0.6209476590156555\n",
            "Epoch: 7, Loss: 0.006309864576905966 Updates: 72/10000, Avg Grad: 0.02464890480041504, Threshold: 0.027113795280456543\n",
            "Epoch: 7, Loss: 0.028741799294948578 Updates: 79/11000, Avg Grad: 0.02223561704158783, Threshold: 0.028941377997398376\n",
            "Epoch: 7, Loss: 0.0054375771433115005 Updates: 87/12000, Avg Grad: 0.04036563262343407, Threshold: 0.03665709123015404\n",
            "Epoch: 7, Loss: 0.01503696944564581 Updates: 93/13000, Avg Grad: 0.04691402614116669, Threshold: 0.044300664216279984\n",
            "Epoch: 7, Loss: 0.006300660781562328 Updates: 101/14000, Avg Grad: 0.04667339846491814, Threshold: 0.05100833624601364\n",
            "Epoch: 7, Loss: 0.017499929293990135 Updates: 111/15000, Avg Grad: 0.01524574775248766, Threshold: 0.01643996499478817\n",
            "Epoch: 7, Loss: 0.007228120695799589 Updates: 119/16000, Avg Grad: 0.02932770736515522, Threshold: 0.02687159925699234\n",
            "Epoch: 7, Loss: 0.006524175405502319 Updates: 127/17000, Avg Grad: 0.08565320819616318, Threshold: 0.04887184500694275\n",
            "Epoch: 7, Loss: 0.010944997891783714 Updates: 135/18000, Avg Grad: 0.06967756152153015, Threshold: 0.03472345694899559\n",
            "Epoch: 7, Loss: 0.02638203091919422 Updates: 144/19000, Avg Grad: 0.0460474006831646, Threshold: 0.04573265090584755\n",
            "Epoch 6 iteration 0 Loss: 0.663 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 100 Loss: 0.676 | Acc: 60.396% (61/101)\n",
            "Epoch 6 iteration 200 Loss: 0.651 | Acc: 64.677% (130/201)\n",
            "Epoch 6 iteration 300 Loss: 0.659 | Acc: 63.787% (192/301)\n",
            "Epoch 6 iteration 400 Loss: 0.686 | Acc: 60.349% (242/401)\n",
            "Test accuracy: 0.6034912467002869\n",
            "Epoch: 7, Loss: 0.014855802990496159 Updates: 153/20000, Avg Grad: 0.017009124159812927, Threshold: 0.017577609047293663\n",
            "Epoch: 7, Loss: 0.005885787773877382 Updates: 162/21000, Avg Grad: 0.02440878003835678, Threshold: 0.038713447749614716\n",
            "Epoch: 7, Loss: 0.008683476597070694 Updates: 171/22000, Avg Grad: 0.011444978415966034, Threshold: 0.011717056855559349\n",
            "Epoch: 7, Loss: 0.015953972935676575 Updates: 180/23000, Avg Grad: 0.02439931407570839, Threshold: 0.015580838546156883\n",
            "Epoch: 7, Loss: 0.01522077340632677 Updates: 189/24000, Avg Grad: 0.014465604908764362, Threshold: 0.014710203744471073\n",
            "Epoch: 7, Loss: 0.005793843884021044 Updates: 199/25000, Avg Grad: 0.05328280106186867, Threshold: 0.03770257160067558\n",
            "Epoch: 7, Loss: 0.013271019794046879 Updates: 209/26000, Avg Grad: 0.01957978494465351, Threshold: 0.015259522944688797\n",
            "Epoch: 7, Loss: 0.009905662387609482 Updates: 218/27000, Avg Grad: 0.027487458661198616, Threshold: 0.0277662742882967\n",
            "Epoch: 7, Loss: 0.0072706216014921665 Updates: 223/28000, Avg Grad: 0.022452853620052338, Threshold: 0.03513452410697937\n",
            "Epoch: 7, Loss: 0.017753569409251213 Updates: 232/29000, Avg Grad: 0.020365551114082336, Threshold: 0.02051248401403427\n",
            "Epoch 6 iteration 0 Loss: 0.207 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 100 Loss: 0.632 | Acc: 65.347% (66/101)\n",
            "Epoch 6 iteration 200 Loss: 0.672 | Acc: 60.199% (121/201)\n",
            "Epoch 6 iteration 300 Loss: 0.660 | Acc: 61.462% (185/301)\n",
            "Epoch 6 iteration 400 Loss: 0.667 | Acc: 59.850% (240/401)\n",
            "Test accuracy: 0.5985037684440613\n",
            "Epoch: 7, Loss: 0.025945086032152176 Updates: 241/30000, Avg Grad: 0.020156070590019226, Threshold: 0.02027871645987034\n",
            "Epoch: 7, Loss: 0.007003485690802336 Updates: 251/31000, Avg Grad: 0.044336844235658646, Threshold: 0.044564370065927505\n",
            "Epoch: 7, Loss: 0.00829858798533678 Updates: 260/32000, Avg Grad: 0.023210057988762856, Threshold: 0.027238965034484863\n",
            "Epoch 7 iteration 0 Loss: 0.354 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.551 | Acc: 76.238% (77/101)\n",
            "Epoch 7 iteration 200 Loss: 0.614 | Acc: 69.652% (140/201)\n",
            "Epoch 7 iteration 300 Loss: 0.630 | Acc: 68.106% (205/301)\n",
            "Epoch 7 iteration 400 Loss: 0.620 | Acc: 69.825% (280/401)\n",
            "Test accuracy: 0.6982543468475342\n",
            "Epoch: 8, Loss: 1.4406386613845825 Updates: 0/0, Avg Grad: 0.28270262479782104, Threshold: 0.3109728991985321\n",
            "Epoch: 8, Loss: 0.03321988508105278 Updates: 0/1000, Avg Grad: 0.18496161699295044, Threshold: 0.30804508924484253\n",
            "Epoch: 8, Loss: 0.017247207462787628 Updates: 0/2000, Avg Grad: 0.28310295939445496, Threshold: 0.30804508924484253\n",
            "Epoch: 8, Loss: 0.01824686862528324 Updates: 8/3000, Avg Grad: 0.056856583803892136, Threshold: 0.06254224479198456\n",
            "Epoch: 8, Loss: 0.00898044928908348 Updates: 16/4000, Avg Grad: 0.015892259776592255, Threshold: 0.01748148538172245\n",
            "Epoch: 8, Loss: 0.0020752609707415104 Updates: 23/5000, Avg Grad: 0.056763406842947006, Threshold: 0.06163368374109268\n",
            "Epoch: 8, Loss: 0.021848883479833603 Updates: 31/6000, Avg Grad: 0.025172358378767967, Threshold: 0.018298989161849022\n",
            "Epoch: 8, Loss: 0.0016068972181528807 Updates: 40/7000, Avg Grad: 0.019421737641096115, Threshold: 0.021363912150263786\n",
            "Epoch: 8, Loss: 0.008783286437392235 Updates: 49/8000, Avg Grad: 0.04392646625638008, Threshold: 0.03327692672610283\n",
            "Epoch: 8, Loss: 0.0036369008012115955 Updates: 58/9000, Avg Grad: 0.016502201557159424, Threshold: 0.018152421340346336\n",
            "Epoch 7 iteration 0 Loss: 1.254 | Acc: 0.000% (0/1)\n",
            "Epoch 7 iteration 100 Loss: 0.631 | Acc: 66.337% (67/101)\n",
            "Epoch 7 iteration 200 Loss: 0.647 | Acc: 65.672% (132/201)\n",
            "Epoch 7 iteration 300 Loss: 0.631 | Acc: 67.110% (202/301)\n",
            "Epoch 7 iteration 400 Loss: 0.641 | Acc: 65.586% (263/401)\n",
            "Test accuracy: 0.6558603644371033\n",
            "Epoch: 8, Loss: 0.02771374210715294 Updates: 68/10000, Avg Grad: 0.03623667359352112, Threshold: 0.0265259500592947\n",
            "Epoch: 8, Loss: 0.006525667384266853 Updates: 76/11000, Avg Grad: 0.016935091465711594, Threshold: 0.018628600984811783\n",
            "Epoch: 8, Loss: 0.010294957086443901 Updates: 83/12000, Avg Grad: 0.028428785502910614, Threshold: 0.03431938588619232\n",
            "Epoch: 8, Loss: 0.017266377806663513 Updates: 90/13000, Avg Grad: 0.03031538426876068, Threshold: 0.05001698434352875\n",
            "Epoch: 8, Loss: 0.0047566452994942665 Updates: 100/14000, Avg Grad: 0.03210362419486046, Threshold: 0.035085346549749374\n",
            "Epoch: 8, Loss: 0.021859588101506233 Updates: 107/15000, Avg Grad: 0.03396802395582199, Threshold: 0.07180618494749069\n",
            "Epoch: 8, Loss: 0.005260992795228958 Updates: 114/16000, Avg Grad: 0.012185140512883663, Threshold: 0.01299012079834938\n",
            "Epoch: 8, Loss: 0.03390461206436157 Updates: 122/17000, Avg Grad: 0.020068537443876266, Threshold: 0.021186664700508118\n",
            "Epoch: 8, Loss: 0.003882751567289233 Updates: 131/18000, Avg Grad: 0.03102365881204605, Threshold: 0.03248142823576927\n",
            "Epoch: 8, Loss: 0.01056276448071003 Updates: 140/19000, Avg Grad: 0.03389141708612442, Threshold: 0.0416145995259285\n",
            "Epoch 7 iteration 0 Loss: 0.532 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.596 | Acc: 66.337% (67/101)\n",
            "Epoch 7 iteration 200 Loss: 0.616 | Acc: 64.179% (129/201)\n",
            "Epoch 7 iteration 300 Loss: 0.629 | Acc: 66.113% (199/301)\n",
            "Epoch 7 iteration 400 Loss: 0.629 | Acc: 66.085% (265/401)\n",
            "Test accuracy: 0.6608479022979736\n",
            "Epoch: 8, Loss: 0.02702360786497593 Updates: 146/20000, Avg Grad: 0.02391655370593071, Threshold: 0.026193872094154358\n",
            "Epoch: 8, Loss: 0.011885484680533409 Updates: 156/21000, Avg Grad: 0.03950847312808037, Threshold: 0.037698280066251755\n",
            "Epoch: 8, Loss: 0.03199245408177376 Updates: 164/22000, Avg Grad: 0.030135735869407654, Threshold: 0.03440828621387482\n",
            "Epoch: 8, Loss: 0.0209798663854599 Updates: 173/23000, Avg Grad: 0.02299843728542328, Threshold: 0.02345954068005085\n",
            "Epoch: 8, Loss: 0.006124235223978758 Updates: 181/24000, Avg Grad: 0.03588375076651573, Threshold: 0.036490507423877716\n",
            "Epoch: 8, Loss: 0.02127278596162796 Updates: 190/25000, Avg Grad: 0.032496239989995956, Threshold: 0.03295965865254402\n",
            "Epoch: 8, Loss: 0.0082127396017313 Updates: 198/26000, Avg Grad: 0.020521018654108047, Threshold: 0.020767828449606895\n",
            "Epoch: 8, Loss: 0.00918111763894558 Updates: 208/27000, Avg Grad: 0.06447290629148483, Threshold: 0.024750325828790665\n",
            "Epoch: 8, Loss: 0.01830802857875824 Updates: 217/28000, Avg Grad: 0.03348061069846153, Threshold: 0.03376702591776848\n",
            "Epoch: 8, Loss: 0.009765064343810081 Updates: 226/29000, Avg Grad: 0.02011730708181858, Threshold: 0.020262449979782104\n",
            "Epoch 7 iteration 0 Loss: 0.901 | Acc: 0.000% (0/1)\n",
            "Epoch 7 iteration 100 Loss: 0.629 | Acc: 67.327% (68/101)\n",
            "Epoch 7 iteration 200 Loss: 0.622 | Acc: 69.154% (139/201)\n",
            "Epoch 7 iteration 300 Loss: 0.625 | Acc: 67.442% (203/301)\n",
            "Epoch 7 iteration 400 Loss: 0.607 | Acc: 69.327% (278/401)\n",
            "Test accuracy: 0.6932668089866638\n",
            "Epoch: 8, Loss: 0.010106963105499744 Updates: 234/30000, Avg Grad: 0.028630606830120087, Threshold: 0.02880481816828251\n",
            "Epoch: 8, Loss: 0.0040330905467271805 Updates: 242/31000, Avg Grad: 0.02528400719165802, Threshold: 0.027361193671822548\n",
            "Epoch: 8, Loss: 0.016273243352770805 Updates: 250/32000, Avg Grad: 0.04178901016712189, Threshold: 0.04178933799266815\n",
            "Epoch 8 iteration 0 Loss: 0.802 | Acc: 0.000% (0/1)\n",
            "Epoch 8 iteration 100 Loss: 0.601 | Acc: 64.356% (65/101)\n",
            "Epoch 8 iteration 200 Loss: 0.619 | Acc: 66.169% (133/201)\n",
            "Epoch 8 iteration 300 Loss: 0.615 | Acc: 66.445% (200/301)\n",
            "Epoch 8 iteration 400 Loss: 0.613 | Acc: 67.581% (271/401)\n",
            "Test accuracy: 0.6758104562759399\n",
            "Epoch: 9, Loss: 0.45635300874710083 Updates: 0/0, Avg Grad: 0.14533965289592743, Threshold: 0.1598736196756363\n",
            "Epoch: 9, Loss: 0.0030727942939847708 Updates: 5/1000, Avg Grad: 0.05730867385864258, Threshold: 0.04457223415374756\n",
            "Epoch: 9, Loss: 0.007709732744842768 Updates: 15/2000, Avg Grad: 0.023797594010829926, Threshold: 0.017757702618837357\n",
            "Epoch: 9, Loss: 0.003827300388365984 Updates: 23/3000, Avg Grad: 0.023818809539079666, Threshold: 0.028165461495518684\n",
            "Epoch: 9, Loss: 0.013524477370083332 Updates: 30/4000, Avg Grad: 0.0155075304210186, Threshold: 0.01705828309059143\n",
            "Epoch: 9, Loss: 0.0042783948592841625 Updates: 38/5000, Avg Grad: 0.024651866406202316, Threshold: 0.02566913701593876\n",
            "Epoch: 9, Loss: 0.005454176105558872 Updates: 48/6000, Avg Grad: 0.025248222053050995, Threshold: 0.024858253076672554\n",
            "Epoch: 9, Loss: 0.013549614697694778 Updates: 57/7000, Avg Grad: 0.02861705794930458, Threshold: 0.03147876262664795\n",
            "Epoch: 9, Loss: 0.01339817140251398 Updates: 66/8000, Avg Grad: 0.01398659311234951, Threshold: 0.015385252423584461\n",
            "Epoch: 9, Loss: 0.011222817935049534 Updates: 76/9000, Avg Grad: 0.022351102903485298, Threshold: 0.020411502569913864\n",
            "Epoch 8 iteration 0 Loss: 0.838 | Acc: 0.000% (0/1)\n",
            "Epoch 8 iteration 100 Loss: 0.762 | Acc: 57.426% (58/101)\n",
            "Epoch 8 iteration 200 Loss: 0.670 | Acc: 62.189% (125/201)\n",
            "Epoch 8 iteration 300 Loss: 0.641 | Acc: 65.116% (196/301)\n",
            "Epoch 8 iteration 400 Loss: 0.636 | Acc: 66.085% (265/401)\n",
            "Test accuracy: 0.6608479022979736\n",
            "Epoch: 9, Loss: 0.002965010004118085 Updates: 84/10000, Avg Grad: 0.03346167877316475, Threshold: 0.036807846277952194\n",
            "Epoch: 9, Loss: 0.015638893470168114 Updates: 94/11000, Avg Grad: 0.08249851316213608, Threshold: 0.027352361008524895\n",
            "Epoch: 9, Loss: 0.009270348586142063 Updates: 104/12000, Avg Grad: 0.028592782095074654, Threshold: 0.01711238920688629\n",
            "Epoch: 9, Loss: 0.015256085433065891 Updates: 113/13000, Avg Grad: 0.0697561576962471, Threshold: 0.05046027526259422\n",
            "Epoch: 9, Loss: 0.017202330753207207 Updates: 121/14000, Avg Grad: 0.02008807845413685, Threshold: 0.021953821182250977\n",
            "Epoch: 9, Loss: 0.004744025878608227 Updates: 130/15000, Avg Grad: 0.020064061507582664, Threshold: 0.02163570187985897\n",
            "Epoch: 9, Loss: 0.006548839621245861 Updates: 138/16000, Avg Grad: 0.10983031988143921, Threshold: 0.05414547771215439\n",
            "Epoch: 9, Loss: 0.022749625146389008 Updates: 144/17000, Avg Grad: 0.017382007092237473, Threshold: 0.020038440823554993\n",
            "Epoch: 9, Loss: 0.015809938311576843 Updates: 150/18000, Avg Grad: 0.025767073035240173, Threshold: 0.04175188019871712\n",
            "Epoch: 9, Loss: 0.002876345533877611 Updates: 158/19000, Avg Grad: 0.02195298671722412, Threshold: 0.022822968661785126\n",
            "Epoch 8 iteration 0 Loss: 0.619 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.643 | Acc: 65.347% (66/101)\n",
            "Epoch 8 iteration 200 Loss: 0.674 | Acc: 62.687% (126/201)\n",
            "Epoch 8 iteration 300 Loss: 0.666 | Acc: 61.462% (185/301)\n",
            "Epoch 8 iteration 400 Loss: 0.654 | Acc: 62.095% (249/401)\n",
            "Test accuracy: 0.6209476590156555\n",
            "Epoch: 9, Loss: 0.01379050686955452 Updates: 167/20000, Avg Grad: 0.05333709716796875, Threshold: 0.04049487039446831\n",
            "Epoch: 9, Loss: 0.020246662199497223 Updates: 175/21000, Avg Grad: 0.030177177861332893, Threshold: 0.031027797609567642\n",
            "Epoch: 9, Loss: 0.005082657560706139 Updates: 185/22000, Avg Grad: 0.028460819274187088, Threshold: 0.012610412202775478\n",
            "Epoch: 9, Loss: 0.002705666469410062 Updates: 194/23000, Avg Grad: 0.01405665185302496, Threshold: 0.014338478446006775\n",
            "Epoch: 9, Loss: 0.005793768912553787 Updates: 202/24000, Avg Grad: 0.022259289398789406, Threshold: 0.0243674349039793\n",
            "Epoch: 9, Loss: 0.00717873265966773 Updates: 209/25000, Avg Grad: 0.027227476239204407, Threshold: 0.03728334605693817\n",
            "Epoch: 9, Loss: 0.014386486262083054 Updates: 215/26000, Avg Grad: 0.023935340344905853, Threshold: 0.027973772957921028\n",
            "Epoch: 9, Loss: 0.018706578761339188 Updates: 225/27000, Avg Grad: 0.024925988167524338, Threshold: 0.019150611013174057\n",
            "Epoch: 9, Loss: 0.008250812068581581 Updates: 235/28000, Avg Grad: 0.019294772297143936, Threshold: 0.012812747620046139\n",
            "Epoch: 9, Loss: 0.01878265105187893 Updates: 243/29000, Avg Grad: 0.0424792654812336, Threshold: 0.02021445520222187\n",
            "Epoch 8 iteration 0 Loss: 0.467 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.538 | Acc: 73.267% (74/101)\n",
            "Epoch 8 iteration 200 Loss: 0.569 | Acc: 71.144% (143/201)\n",
            "Epoch 8 iteration 300 Loss: 0.582 | Acc: 68.771% (207/301)\n",
            "Epoch 8 iteration 400 Loss: 0.584 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 9, Loss: 0.01879306137561798 Updates: 252/30000, Avg Grad: 0.012562086805701256, Threshold: 0.012638524174690247\n",
            "Epoch: 9, Loss: 0.021433964371681213 Updates: 262/31000, Avg Grad: 0.010628197342157364, Threshold: 0.01068273838609457\n",
            "Epoch: 9, Loss: 0.01336502656340599 Updates: 272/32000, Avg Grad: 0.07474293559789658, Threshold: 0.046540044248104095\n",
            "Epoch 9 iteration 0 Loss: 0.133 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 100 Loss: 0.792 | Acc: 52.475% (53/101)\n",
            "Epoch 9 iteration 200 Loss: 0.750 | Acc: 55.721% (112/201)\n",
            "Epoch 9 iteration 300 Loss: 0.740 | Acc: 59.136% (178/301)\n",
            "Epoch 9 iteration 400 Loss: 0.731 | Acc: 58.105% (233/401)\n",
            "Test accuracy: 0.5810473561286926\n",
            "Epoch: 10, Loss: 1.174377679824829 Updates: 0/0, Avg Grad: 0.20770929753780365, Threshold: 0.2284802347421646\n",
            "Epoch: 10, Loss: 0.010555385611951351 Updates: 9/1000, Avg Grad: 0.05228428915143013, Threshold: 0.017305996268987656\n",
            "Epoch: 10, Loss: 0.0036896741949021816 Updates: 18/2000, Avg Grad: 0.017736783251166344, Threshold: 0.019510462880134583\n",
            "Epoch: 10, Loss: 0.014777389355003834 Updates: 27/3000, Avg Grad: 0.017976924777030945, Threshold: 0.016169123351573944\n",
            "Epoch: 10, Loss: 0.007818685844540596 Updates: 35/4000, Avg Grad: 0.03616972267627716, Threshold: 0.01953444816172123\n",
            "Epoch: 10, Loss: 0.021571407094597816 Updates: 41/5000, Avg Grad: 0.042653635144233704, Threshold: 0.047571610659360886\n",
            "Epoch: 10, Loss: 0.007257768418639898 Updates: 50/6000, Avg Grad: 0.03433896228671074, Threshold: 0.03738413751125336\n",
            "Epoch: 10, Loss: 0.009841597639024258 Updates: 58/7000, Avg Grad: 0.02336145006120205, Threshold: 0.02569759637117386\n",
            "Epoch: 10, Loss: 0.010605823248624802 Updates: 66/8000, Avg Grad: 0.026560500264167786, Threshold: 0.0365782156586647\n",
            "Epoch: 10, Loss: 0.016409242525696754 Updates: 73/9000, Avg Grad: 0.060531508177518845, Threshold: 0.046244289726018906\n",
            "Epoch 9 iteration 0 Loss: 0.895 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 100 Loss: 0.624 | Acc: 63.366% (64/101)\n",
            "Epoch 9 iteration 200 Loss: 0.605 | Acc: 68.159% (137/201)\n",
            "Epoch 9 iteration 300 Loss: 0.599 | Acc: 68.771% (207/301)\n",
            "Epoch 9 iteration 400 Loss: 0.616 | Acc: 66.584% (267/401)\n",
            "Test accuracy: 0.665835440158844\n",
            "Epoch: 10, Loss: 0.009750786237418652 Updates: 82/10000, Avg Grad: 0.03440653905272484, Threshold: 0.025972895324230194\n",
            "Epoch: 10, Loss: 0.0053301467560231686 Updates: 89/11000, Avg Grad: 0.026573965325951576, Threshold: 0.024448757991194725\n",
            "Epoch: 10, Loss: 0.005736757069826126 Updates: 95/12000, Avg Grad: 0.022397566586732864, Threshold: 0.030529268085956573\n",
            "Epoch: 10, Loss: 0.023743921890854836 Updates: 104/13000, Avg Grad: 0.02725224941968918, Threshold: 0.029977474361658096\n",
            "Epoch: 10, Loss: 0.011137590743601322 Updates: 111/14000, Avg Grad: 0.024327121675014496, Threshold: 0.02658657915890217\n",
            "Epoch: 10, Loss: 0.01264088973402977 Updates: 120/15000, Avg Grad: 0.027314214035868645, Threshold: 0.031313493847846985\n",
            "Epoch: 10, Loss: 0.015059387311339378 Updates: 130/16000, Avg Grad: 0.018133096396923065, Threshold: 0.019331013783812523\n",
            "Epoch: 10, Loss: 0.010020740330219269 Updates: 138/17000, Avg Grad: 0.021639332175254822, Threshold: 0.0205998457968235\n",
            "Epoch: 10, Loss: 0.005298137199133635 Updates: 147/18000, Avg Grad: 0.03806116431951523, Threshold: 0.03613174706697464\n",
            "Epoch: 10, Loss: 0.00434111338108778 Updates: 155/19000, Avg Grad: 0.023582030087709427, Threshold: 0.024516569450497627\n",
            "Epoch 9 iteration 0 Loss: 0.375 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 100 Loss: 0.593 | Acc: 64.356% (65/101)\n",
            "Epoch 9 iteration 200 Loss: 0.585 | Acc: 67.164% (135/201)\n",
            "Epoch 9 iteration 300 Loss: 0.607 | Acc: 65.116% (196/301)\n",
            "Epoch 9 iteration 400 Loss: 0.619 | Acc: 63.591% (255/401)\n",
            "Test accuracy: 0.6359102129936218\n",
            "Epoch: 10, Loss: 0.008320930413901806 Updates: 165/20000, Avg Grad: 0.023405447602272034, Threshold: 0.024187711998820305\n",
            "Epoch: 10, Loss: 0.004151360597461462 Updates: 172/21000, Avg Grad: 0.0326404869556427, Threshold: 0.060416579246520996\n",
            "Epoch: 10, Loss: 0.005822898354381323 Updates: 182/22000, Avg Grad: 0.07052887231111526, Threshold: 0.045455899089574814\n",
            "Epoch: 10, Loss: 0.0040015969425439835 Updates: 190/23000, Avg Grad: 0.05580127611756325, Threshold: 0.038207441568374634\n",
            "Epoch: 10, Loss: 0.0075345952063798904 Updates: 199/24000, Avg Grad: 0.061046794056892395, Threshold: 0.03827337175607681\n",
            "Epoch: 10, Loss: 0.009417192079126835 Updates: 207/25000, Avg Grad: 0.02694859355688095, Threshold: 0.02951665036380291\n",
            "Epoch: 10, Loss: 0.024448182433843613 Updates: 217/26000, Avg Grad: 0.04862265661358833, Threshold: 0.0341964066028595\n",
            "Epoch: 10, Loss: 0.005394062027335167 Updates: 226/27000, Avg Grad: 0.01797673851251602, Threshold: 0.018159084022045135\n",
            "Epoch: 10, Loss: 0.025971120223402977 Updates: 236/28000, Avg Grad: 0.02103882096707821, Threshold: 0.0212188009172678\n",
            "Epoch: 10, Loss: 0.002500639297068119 Updates: 246/29000, Avg Grad: 0.020858678966760635, Threshold: 0.0210091695189476\n",
            "Epoch 9 iteration 0 Loss: 0.255 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 100 Loss: 0.629 | Acc: 66.337% (67/101)\n",
            "Epoch 9 iteration 200 Loss: 0.643 | Acc: 64.677% (130/201)\n",
            "Epoch 9 iteration 300 Loss: 0.603 | Acc: 66.445% (200/301)\n",
            "Epoch 9 iteration 400 Loss: 0.583 | Acc: 69.327% (278/401)\n",
            "Test accuracy: 0.6932668089866638\n",
            "Epoch: 10, Loss: 0.025771155953407288 Updates: 256/30000, Avg Grad: 0.03111538104712963, Threshold: 0.030396686866879463\n",
            "Epoch: 10, Loss: 0.007594076916575432 Updates: 265/31000, Avg Grad: 0.029447680339217186, Threshold: 0.01525228749960661\n",
            "Epoch: 10, Loss: 0.01621786691248417 Updates: 273/32000, Avg Grad: 0.027028417214751244, Threshold: 0.027145396918058395\n",
            "Epoch 10 iteration 0 Loss: 0.476 | Acc: 100.000% (1/1)\n",
            "Epoch 10 iteration 100 Loss: 0.663 | Acc: 61.386% (62/101)\n",
            "Epoch 10 iteration 200 Loss: 0.644 | Acc: 64.677% (130/201)\n",
            "Epoch 10 iteration 300 Loss: 0.654 | Acc: 62.458% (188/301)\n",
            "Epoch 10 iteration 400 Loss: 0.645 | Acc: 64.339% (258/401)\n",
            "Test accuracy: 0.6433915495872498\n",
            "Epoch: 11, Loss: 0.6033547520637512 Updates: 0/0, Avg Grad: 0.16774064302444458, Threshold: 0.18451471626758575\n",
            "Epoch: 11, Loss: 0.04517683386802673 Updates: 10/1000, Avg Grad: 0.03459834307432175, Threshold: 0.014626378193497658\n",
            "Epoch: 11, Loss: 0.04847987741231918 Updates: 15/2000, Avg Grad: 0.03965364024043083, Threshold: 0.0499769002199173\n",
            "Epoch: 11, Loss: 0.0157771036028862 Updates: 24/3000, Avg Grad: 0.05536163970828056, Threshold: 0.03486456349492073\n",
            "Epoch: 11, Loss: 0.019229751080274582 Updates: 32/4000, Avg Grad: 0.010262392461299896, Threshold: 0.011288631707429886\n",
            "Epoch: 11, Loss: 0.012238878756761551 Updates: 41/5000, Avg Grad: 0.08122920244932175, Threshold: 0.048796042799949646\n",
            "Epoch: 11, Loss: 0.0017779776826500893 Updates: 50/6000, Avg Grad: 0.03981548547744751, Threshold: 0.03801174834370613\n",
            "Epoch: 11, Loss: 0.00819297693669796 Updates: 58/7000, Avg Grad: 0.02094171568751335, Threshold: 0.023035887628793716\n",
            "Epoch: 11, Loss: 0.009665239602327347 Updates: 66/8000, Avg Grad: 0.021346567198634148, Threshold: 0.023481223732233047\n",
            "Epoch: 11, Loss: 0.007123311050236225 Updates: 75/9000, Avg Grad: 0.023740503937005997, Threshold: 0.018366223201155663\n",
            "Epoch 10 iteration 0 Loss: 0.276 | Acc: 100.000% (1/1)\n",
            "Epoch 10 iteration 100 Loss: 0.645 | Acc: 62.376% (63/101)\n",
            "Epoch 10 iteration 200 Loss: 0.624 | Acc: 64.179% (129/201)\n",
            "Epoch 10 iteration 300 Loss: 0.630 | Acc: 65.449% (197/301)\n",
            "Epoch 10 iteration 400 Loss: 0.641 | Acc: 63.092% (253/401)\n",
            "Test accuracy: 0.6309226751327515\n",
            "Epoch: 11, Loss: 0.007344044279307127 Updates: 85/10000, Avg Grad: 0.035413168370723724, Threshold: 0.018141530454158783\n",
            "Epoch: 11, Loss: 0.009262345731258392 Updates: 93/11000, Avg Grad: 0.026751328259706497, Threshold: 0.030056387186050415\n",
            "Epoch: 11, Loss: 0.009464249014854431 Updates: 101/12000, Avg Grad: 0.06374214589595795, Threshold: 0.049148790538311005\n",
            "Epoch: 11, Loss: 0.004400028381496668 Updates: 110/13000, Avg Grad: 0.03921639174222946, Threshold: 0.018527356907725334\n",
            "Epoch: 11, Loss: 0.006694380193948746 Updates: 119/14000, Avg Grad: 0.01814022660255432, Threshold: 0.019825056195259094\n",
            "Epoch: 11, Loss: 0.017005974426865578 Updates: 128/15000, Avg Grad: 0.020251955837011337, Threshold: 0.017971878871321678\n",
            "Epoch: 11, Loss: 0.015965675935149193 Updates: 136/16000, Avg Grad: 0.058008041232824326, Threshold: 0.05561751499772072\n",
            "Epoch: 11, Loss: 0.010218913666903973 Updates: 143/17000, Avg Grad: 0.06133522093296051, Threshold: 0.030827030539512634\n",
            "Epoch: 11, Loss: 0.003532795235514641 Updates: 152/18000, Avg Grad: 0.023662719875574112, Threshold: 0.02477460727095604\n",
            "Epoch: 11, Loss: 0.0024220808409154415 Updates: 161/19000, Avg Grad: 0.02435818873345852, Threshold: 0.02532348781824112\n",
            "Epoch 10 iteration 0 Loss: 0.771 | Acc: 0.000% (0/1)\n",
            "Epoch 10 iteration 100 Loss: 0.536 | Acc: 74.257% (75/101)\n",
            "Epoch 10 iteration 200 Loss: 0.619 | Acc: 66.169% (133/201)\n",
            "Epoch 10 iteration 300 Loss: 0.638 | Acc: 62.791% (189/301)\n",
            "Epoch 10 iteration 400 Loss: 0.637 | Acc: 63.591% (255/401)\n",
            "Test accuracy: 0.6359102129936218\n",
            "Epoch: 11, Loss: 0.010685788467526436 Updates: 169/20000, Avg Grad: 0.03022659942507744, Threshold: 0.01871820166707039\n",
            "Epoch: 11, Loss: 0.0037875575944781303 Updates: 178/21000, Avg Grad: 0.055694133043289185, Threshold: 0.04524153843522072\n",
            "Epoch: 11, Loss: 0.016151489689946175 Updates: 186/22000, Avg Grad: 0.02507735788822174, Threshold: 0.029084915295243263\n",
            "Epoch: 11, Loss: 0.013569729402661324 Updates: 196/23000, Avg Grad: 0.05624854937195778, Threshold: 0.05204962193965912\n",
            "Epoch: 11, Loss: 0.032539352774620056 Updates: 203/24000, Avg Grad: 0.038568951189517975, Threshold: 0.020214799791574478\n",
            "Epoch: 11, Loss: 0.007024670019745827 Updates: 212/25000, Avg Grad: 0.018959471955895424, Threshold: 0.018443915992975235\n",
            "Epoch: 11, Loss: 0.005240472033619881 Updates: 222/26000, Avg Grad: 0.021052632480859756, Threshold: 0.01791718602180481\n",
            "Epoch: 11, Loss: 0.01726163737475872 Updates: 232/27000, Avg Grad: 0.07820159196853638, Threshold: 0.043940313160419464\n",
            "Epoch: 11, Loss: 0.0030173880513757467 Updates: 240/28000, Avg Grad: 0.029636066406965256, Threshold: 0.01647542417049408\n",
            "Epoch: 11, Loss: 0.002513275947421789 Updates: 250/29000, Avg Grad: 0.026227224618196487, Threshold: 0.02487635612487793\n",
            "Epoch 10 iteration 0 Loss: 0.299 | Acc: 100.000% (1/1)\n",
            "Epoch 10 iteration 100 Loss: 0.604 | Acc: 66.337% (67/101)\n",
            "Epoch 10 iteration 200 Loss: 0.632 | Acc: 65.174% (131/201)\n",
            "Epoch 10 iteration 300 Loss: 0.637 | Acc: 65.116% (196/301)\n",
            "Epoch 10 iteration 400 Loss: 0.638 | Acc: 64.838% (260/401)\n",
            "Test accuracy: 0.6483790278434753\n",
            "Epoch: 11, Loss: 0.01256414782255888 Updates: 258/30000, Avg Grad: 0.030956948176026344, Threshold: 0.031145315617322922\n",
            "Epoch: 11, Loss: 0.003487492213025689 Updates: 266/31000, Avg Grad: 0.01157052256166935, Threshold: 0.01162989903241396\n",
            "Epoch: 11, Loss: 0.01609148643910885 Updates: 276/32000, Avg Grad: 0.08605223894119263, Threshold: 0.046774331480264664\n",
            "Epoch 11 iteration 0 Loss: 0.748 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.675 | Acc: 61.386% (62/101)\n",
            "Epoch 11 iteration 200 Loss: 0.665 | Acc: 63.184% (127/201)\n",
            "Epoch 11 iteration 300 Loss: 0.666 | Acc: 62.791% (189/301)\n",
            "Epoch 11 iteration 400 Loss: 0.656 | Acc: 63.342% (254/401)\n",
            "Test accuracy: 0.633416473865509\n",
            "Epoch: 12, Loss: 0.0937400683760643 Updates: 0/0, Avg Grad: 0.03403950855135918, Threshold: 0.037443459033966064\n",
            "Epoch: 12, Loss: 0.01071745716035366 Updates: 8/1000, Avg Grad: 0.02461998164653778, Threshold: 0.027081981301307678\n",
            "Epoch: 12, Loss: 0.013519315980374813 Updates: 17/2000, Avg Grad: 0.07579556107521057, Threshold: 0.08337511867284775\n",
            "Epoch: 12, Loss: 0.0038248393684625626 Updates: 26/3000, Avg Grad: 0.017552921548485756, Threshold: 0.019308215007185936\n",
            "Epoch: 12, Loss: 0.011833811178803444 Updates: 36/4000, Avg Grad: 0.0357690304517746, Threshold: 0.01821262761950493\n",
            "Epoch: 12, Loss: 0.0054915230721235275 Updates: 44/5000, Avg Grad: 0.019679855555295944, Threshold: 0.016768014058470726\n",
            "Epoch: 12, Loss: 0.022532647475600243 Updates: 53/6000, Avg Grad: 0.0148874931037426, Threshold: 0.01637624204158783\n",
            "Epoch: 12, Loss: 0.014418013393878937 Updates: 59/7000, Avg Grad: 0.03225240111351013, Threshold: 0.05069730058312416\n",
            "Epoch: 12, Loss: 0.010445951484143734 Updates: 67/8000, Avg Grad: 0.025171712040901184, Threshold: 0.018150528892874718\n",
            "Epoch: 12, Loss: 0.008334855549037457 Updates: 76/9000, Avg Grad: 0.01521050650626421, Threshold: 0.016731558367609978\n",
            "Epoch 11 iteration 0 Loss: 0.452 | Acc: 100.000% (1/1)\n",
            "Epoch 11 iteration 100 Loss: 0.591 | Acc: 70.297% (71/101)\n",
            "Epoch 11 iteration 200 Loss: 0.596 | Acc: 71.144% (143/201)\n",
            "Epoch 11 iteration 300 Loss: 0.614 | Acc: 68.439% (206/301)\n",
            "Epoch 11 iteration 400 Loss: 0.596 | Acc: 69.825% (280/401)\n",
            "Test accuracy: 0.6982543468475342\n",
            "Epoch: 12, Loss: 0.011351517401635647 Updates: 86/10000, Avg Grad: 0.03481069207191467, Threshold: 0.03829176351428032\n",
            "Epoch: 12, Loss: 0.003543574595823884 Updates: 92/11000, Avg Grad: 0.022177979350090027, Threshold: 0.02439577691257\n",
            "Epoch: 12, Loss: 0.0035178535617887974 Updates: 100/12000, Avg Grad: 0.04385986179113388, Threshold: 0.01771913282573223\n",
            "Epoch: 12, Loss: 0.015221421606838703 Updates: 108/13000, Avg Grad: 0.034219905734062195, Threshold: 0.02306666038930416\n",
            "Epoch: 12, Loss: 0.012937338091433048 Updates: 112/14000, Avg Grad: 0.025921853259205818, Threshold: 0.04184688627719879\n",
            "Epoch: 12, Loss: 0.0069640143774449825 Updates: 121/15000, Avg Grad: 0.01392270065844059, Threshold: 0.015013281255960464\n",
            "Epoch: 12, Loss: 0.003182485932484269 Updates: 130/16000, Avg Grad: 0.022829856723546982, Threshold: 0.024338053539395332\n",
            "Epoch: 12, Loss: 0.008839564397931099 Updates: 136/17000, Avg Grad: 0.03409159556031227, Threshold: 0.04479577764868736\n",
            "Epoch: 12, Loss: 0.008883971720933914 Updates: 144/18000, Avg Grad: 0.035895682871341705, Threshold: 0.030373556539416313\n",
            "Epoch: 12, Loss: 0.008848423138260841 Updates: 150/19000, Avg Grad: 0.016885880380868912, Threshold: 0.017555056139826775\n",
            "Epoch 11 iteration 0 Loss: 1.339 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.636 | Acc: 63.366% (64/101)\n",
            "Epoch 11 iteration 200 Loss: 0.565 | Acc: 71.144% (143/201)\n",
            "Epoch 11 iteration 300 Loss: 0.589 | Acc: 68.771% (207/301)\n",
            "Epoch 11 iteration 400 Loss: 0.589 | Acc: 68.329% (274/401)\n",
            "Test accuracy: 0.6832917928695679\n",
            "Epoch: 12, Loss: 0.0025548837147653103 Updates: 159/20000, Avg Grad: 0.0143569465726614, Threshold: 0.014836789108812809\n",
            "Epoch: 12, Loss: 0.00969388335943222 Updates: 167/21000, Avg Grad: 0.030302658677101135, Threshold: 0.031156815588474274\n",
            "Epoch: 12, Loss: 0.009130914695560932 Updates: 175/22000, Avg Grad: 0.032555319368839264, Threshold: 0.04017059504985809\n",
            "Epoch: 12, Loss: 0.011412518098950386 Updates: 184/23000, Avg Grad: 0.06843055039644241, Threshold: 0.05191732197999954\n",
            "Epoch: 12, Loss: 0.013128037564456463 Updates: 191/24000, Avg Grad: 0.04498844966292381, Threshold: 0.045749157667160034\n",
            "Epoch: 12, Loss: 0.0036787253338843584 Updates: 199/25000, Avg Grad: 0.07416002452373505, Threshold: 0.04460810497403145\n",
            "Epoch: 12, Loss: 0.016808191314339638 Updates: 206/26000, Avg Grad: 0.031865574419498444, Threshold: 0.03224882483482361\n",
            "Epoch: 12, Loss: 0.007775302045047283 Updates: 215/27000, Avg Grad: 0.015182171016931534, Threshold: 0.015867959707975388\n",
            "Epoch: 12, Loss: 0.011408298276364803 Updates: 225/28000, Avg Grad: 0.041812729090452194, Threshold: 0.04217042401432991\n",
            "Epoch: 12, Loss: 0.00818121712654829 Updates: 233/29000, Avg Grad: 0.03618098795413971, Threshold: 0.03644202649593353\n",
            "Epoch 11 iteration 0 Loss: 1.863 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.676 | Acc: 58.416% (59/101)\n",
            "Epoch 11 iteration 200 Loss: 0.627 | Acc: 64.677% (130/201)\n",
            "Epoch 11 iteration 300 Loss: 0.621 | Acc: 64.120% (193/301)\n",
            "Epoch 11 iteration 400 Loss: 0.601 | Acc: 66.085% (265/401)\n",
            "Test accuracy: 0.6608479022979736\n",
            "Epoch: 12, Loss: 0.012161017395555973 Updates: 243/30000, Avg Grad: 0.017158009111881256, Threshold: 0.011353399604558945\n",
            "Epoch: 12, Loss: 0.009013868868350983 Updates: 251/31000, Avg Grad: 0.019504740834236145, Threshold: 0.02092515118420124\n",
            "Epoch: 12, Loss: 0.014048714190721512 Updates: 259/32000, Avg Grad: 0.03179239109158516, Threshold: 0.03192998841404915\n",
            "Epoch 12 iteration 0 Loss: 0.203 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.671 | Acc: 63.366% (64/101)\n",
            "Epoch 12 iteration 200 Loss: 0.670 | Acc: 64.677% (130/201)\n",
            "Epoch 12 iteration 300 Loss: 0.650 | Acc: 64.784% (195/301)\n",
            "Epoch 12 iteration 400 Loss: 0.645 | Acc: 65.586% (263/401)\n",
            "Test accuracy: 0.6558603644371033\n",
            "Epoch: 13, Loss: 0.48158928751945496 Updates: 0/0, Avg Grad: 0.13879430294036865, Threshold: 0.15267373621463776\n",
            "Epoch: 13, Loss: 0.015459644608199596 Updates: 4/1000, Avg Grad: 0.06943242251873016, Threshold: 0.03258276730775833\n",
            "Epoch: 13, Loss: 0.004420735407620668 Updates: 14/2000, Avg Grad: 0.030216321349143982, Threshold: 0.02497837506234646\n",
            "Epoch: 13, Loss: 0.030895071104168892 Updates: 22/3000, Avg Grad: 0.05943407490849495, Threshold: 0.0504520945250988\n",
            "Epoch: 13, Loss: 0.011393708176910877 Updates: 30/4000, Avg Grad: 0.02059934101998806, Threshold: 0.014483525417745113\n",
            "Epoch: 13, Loss: 0.0024815364740788937 Updates: 40/5000, Avg Grad: 0.021563060581684113, Threshold: 0.017829151824116707\n",
            "Epoch: 13, Loss: 0.01437245961278677 Updates: 47/6000, Avg Grad: 0.031207742169499397, Threshold: 0.02527286857366562\n",
            "Epoch: 13, Loss: 0.006996806710958481 Updates: 56/7000, Avg Grad: 0.015838880091905594, Threshold: 0.01745615527033806\n",
            "Epoch: 13, Loss: 0.005062330514192581 Updates: 64/8000, Avg Grad: 0.033042147755622864, Threshold: 0.03634636476635933\n",
            "Epoch: 13, Loss: 0.008040273562073708 Updates: 70/9000, Avg Grad: 0.05198083817958832, Threshold: 0.025658370926976204\n",
            "Epoch 12 iteration 0 Loss: 0.430 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.655 | Acc: 65.347% (66/101)\n",
            "Epoch 12 iteration 200 Loss: 0.634 | Acc: 65.672% (132/201)\n",
            "Epoch 12 iteration 300 Loss: 0.625 | Acc: 67.110% (202/301)\n",
            "Epoch 12 iteration 400 Loss: 0.626 | Acc: 69.077% (277/401)\n",
            "Test accuracy: 0.690773069858551\n",
            "Epoch: 13, Loss: 0.003721709828823805 Updates: 76/10000, Avg Grad: 0.02872426062822342, Threshold: 0.045795079320669174\n",
            "Epoch: 13, Loss: 0.013017999939620495 Updates: 84/11000, Avg Grad: 0.020297640934586525, Threshold: 0.022327406331896782\n",
            "Epoch: 13, Loss: 0.013503444381058216 Updates: 93/12000, Avg Grad: 0.05012011528015137, Threshold: 0.038112811744213104\n",
            "Epoch: 13, Loss: 0.030771786347031593 Updates: 99/13000, Avg Grad: 0.028698451817035675, Threshold: 0.03156829625368118\n",
            "Epoch: 13, Loss: 0.0130907678976655 Updates: 105/14000, Avg Grad: 0.026973078027367592, Threshold: 0.03674321994185448\n",
            "Epoch: 13, Loss: 0.00638455618172884 Updates: 112/15000, Avg Grad: 0.017771955579519272, Threshold: 0.019164051860570908\n",
            "Epoch: 13, Loss: 0.0032304259948432446 Updates: 121/16000, Avg Grad: 0.03052460215985775, Threshold: 0.046930521726608276\n",
            "Epoch: 13, Loss: 0.022070908918976784 Updates: 126/17000, Avg Grad: 0.03806930407881737, Threshold: 0.04285793378949165\n",
            "Epoch: 13, Loss: 0.006450648419559002 Updates: 134/18000, Avg Grad: 0.031629014760255814, Threshold: 0.03288077935576439\n",
            "Epoch: 13, Loss: 0.015886254608631134 Updates: 144/19000, Avg Grad: 0.01731998287141323, Threshold: 0.014561851508915424\n",
            "Epoch 12 iteration 0 Loss: 0.447 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.618 | Acc: 65.347% (66/101)\n",
            "Epoch 12 iteration 200 Loss: 0.636 | Acc: 64.677% (130/201)\n",
            "Epoch 12 iteration 300 Loss: 0.627 | Acc: 64.784% (195/301)\n",
            "Epoch 12 iteration 400 Loss: 0.603 | Acc: 68.080% (273/401)\n",
            "Test accuracy: 0.6807979941368103\n",
            "Epoch: 13, Loss: 0.0302236620336771 Updates: 153/20000, Avg Grad: 0.054547496140003204, Threshold: 0.02203204855322838\n",
            "Epoch: 13, Loss: 0.012234684079885483 Updates: 161/21000, Avg Grad: 0.01696111634373665, Threshold: 0.01755422353744507\n",
            "Epoch: 13, Loss: 0.020640995353460312 Updates: 169/22000, Avg Grad: 0.05963356792926788, Threshold: 0.020785808563232422\n",
            "Epoch: 13, Loss: 0.004230049438774586 Updates: 179/23000, Avg Grad: 0.09571316093206406, Threshold: 0.060757216066122055\n",
            "Epoch: 13, Loss: 0.013805026188492775 Updates: 188/24000, Avg Grad: 0.015993952751159668, Threshold: 0.01626439392566681\n",
            "Epoch: 13, Loss: 0.012388240545988083 Updates: 196/25000, Avg Grad: 0.02192663960158825, Threshold: 0.022239327430725098\n",
            "Epoch: 13, Loss: 0.005412188358604908 Updates: 203/26000, Avg Grad: 0.05743541568517685, Threshold: 0.029622288420796394\n",
            "Epoch: 13, Loss: 0.001876620459370315 Updates: 212/27000, Avg Grad: 0.02410036511719227, Threshold: 0.024344824254512787\n",
            "Epoch: 13, Loss: 0.016917811706662178 Updates: 221/28000, Avg Grad: 0.017971493303775787, Threshold: 0.01812523417174816\n",
            "Epoch: 13, Loss: 0.008805301040410995 Updates: 231/29000, Avg Grad: 0.011348231695592403, Threshold: 0.011430107057094574\n",
            "Epoch 12 iteration 0 Loss: 0.453 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.590 | Acc: 63.366% (64/101)\n",
            "Epoch 12 iteration 200 Loss: 0.587 | Acc: 68.159% (137/201)\n",
            "Epoch 12 iteration 300 Loss: 0.592 | Acc: 68.771% (207/301)\n",
            "Epoch 12 iteration 400 Loss: 0.606 | Acc: 68.828% (276/401)\n",
            "Test accuracy: 0.6882793307304382\n",
            "Epoch: 13, Loss: 0.01604558154940605 Updates: 241/30000, Avg Grad: 0.01497584953904152, Threshold: 0.01506697479635477\n",
            "Epoch: 13, Loss: 0.002656421158462763 Updates: 250/31000, Avg Grad: 0.034508053213357925, Threshold: 0.0373847559094429\n",
            "Epoch: 13, Loss: 0.011448230594396591 Updates: 260/32000, Avg Grad: 0.03875991702079773, Threshold: 0.02370530739426613\n",
            "Epoch 13 iteration 0 Loss: 0.798 | Acc: 0.000% (0/1)\n",
            "Epoch 13 iteration 100 Loss: 0.557 | Acc: 74.257% (75/101)\n",
            "Epoch 13 iteration 200 Loss: 0.583 | Acc: 72.637% (146/201)\n",
            "Epoch 13 iteration 300 Loss: 0.593 | Acc: 70.432% (212/301)\n",
            "Epoch 13 iteration 400 Loss: 0.602 | Acc: 69.825% (280/401)\n",
            "Test accuracy: 0.6982543468475342\n",
            "Epoch: 14, Loss: 0.236223965883255 Updates: 0/0, Avg Grad: 0.0943150743842125, Threshold: 0.10374658554792404\n",
            "Epoch: 14, Loss: 0.003809822490438819 Updates: 1/1000, Avg Grad: 0.031239092350006104, Threshold: 0.034363001585006714\n",
            "Epoch: 14, Loss: 0.006092734169214964 Updates: 10/2000, Avg Grad: 0.0306853000074625, Threshold: 0.033753830939531326\n",
            "Epoch: 14, Loss: 0.024217087775468826 Updates: 18/3000, Avg Grad: 0.017764080315828323, Threshold: 0.019540488719940186\n",
            "Epoch: 14, Loss: 0.009675944223999977 Updates: 26/4000, Avg Grad: 0.02005619741976261, Threshold: 0.026018768548965454\n",
            "Epoch: 14, Loss: 0.005610678344964981 Updates: 35/5000, Avg Grad: 0.01798293925821781, Threshold: 0.019781233742833138\n",
            "Epoch: 14, Loss: 0.013982350006699562 Updates: 45/6000, Avg Grad: 0.04509183391928673, Threshold: 0.028619714081287384\n",
            "Epoch: 14, Loss: 0.0029675792902708054 Updates: 52/7000, Avg Grad: 0.014011422172188759, Threshold: 0.015412564389407635\n",
            "Epoch: 14, Loss: 0.008693743497133255 Updates: 61/8000, Avg Grad: 0.018369926139712334, Threshold: 0.020206918939948082\n",
            "Epoch: 14, Loss: 0.03379246965050697 Updates: 71/9000, Avg Grad: 0.06216185539960861, Threshold: 0.02663741074502468\n",
            "Epoch 13 iteration 0 Loss: 0.994 | Acc: 0.000% (0/1)\n",
            "Epoch 13 iteration 100 Loss: 0.569 | Acc: 70.297% (71/101)\n",
            "Epoch 13 iteration 200 Loss: 0.589 | Acc: 69.652% (140/201)\n",
            "Epoch 13 iteration 300 Loss: 0.608 | Acc: 67.442% (203/301)\n",
            "Epoch 13 iteration 400 Loss: 0.608 | Acc: 68.329% (274/401)\n",
            "Test accuracy: 0.6832917928695679\n",
            "Epoch: 14, Loss: 0.0060784961096942425 Updates: 81/10000, Avg Grad: 0.022178644314408302, Threshold: 0.0181131511926651\n",
            "Epoch: 14, Loss: 0.006701449863612652 Updates: 88/11000, Avg Grad: 0.014588062651455402, Threshold: 0.01604686863720417\n",
            "Epoch: 14, Loss: 0.01239418238401413 Updates: 96/12000, Avg Grad: 0.020625267177820206, Threshold: 0.022687794640660286\n",
            "Epoch: 14, Loss: 0.003363789524883032 Updates: 105/13000, Avg Grad: 0.019591733813285828, Threshold: 0.026995480060577393\n",
            "Epoch: 14, Loss: 0.006572284735739231 Updates: 114/14000, Avg Grad: 0.029251372441649437, Threshold: 0.03196818381547928\n",
            "Epoch: 14, Loss: 0.003509521484375 Updates: 123/15000, Avg Grad: 0.02643812634050846, Threshold: 0.028509054332971573\n",
            "Epoch: 14, Loss: 0.009333128109574318 Updates: 132/16000, Avg Grad: 0.012700007297098637, Threshold: 0.01353900134563446\n",
            "Epoch: 14, Loss: 0.005975875072181225 Updates: 140/17000, Avg Grad: 0.0512479767203331, Threshold: 0.039575353264808655\n",
            "Epoch: 14, Loss: 0.023418819531798363 Updates: 147/18000, Avg Grad: 0.017254849895834923, Threshold: 0.018065636977553368\n",
            "Epoch: 14, Loss: 0.010267935693264008 Updates: 157/19000, Avg Grad: 0.02168080396950245, Threshold: 0.022539999336004257\n",
            "Epoch 13 iteration 0 Loss: 0.579 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.594 | Acc: 71.287% (72/101)\n",
            "Epoch 13 iteration 200 Loss: 0.609 | Acc: 70.647% (142/201)\n",
            "Epoch 13 iteration 300 Loss: 0.620 | Acc: 68.771% (207/301)\n",
            "Epoch 13 iteration 400 Loss: 0.613 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 14, Loss: 0.006772149354219437 Updates: 165/20000, Avg Grad: 0.05225365236401558, Threshold: 0.030882271006703377\n",
            "Epoch: 14, Loss: 0.0051330686546862125 Updates: 175/21000, Avg Grad: 0.037356048822402954, Threshold: 0.03683625906705856\n",
            "Epoch: 14, Loss: 0.006400817073881626 Updates: 184/22000, Avg Grad: 0.0195754487067461, Threshold: 0.01714118756353855\n",
            "Epoch: 14, Loss: 0.01691930927336216 Updates: 192/23000, Avg Grad: 0.0353764109313488, Threshold: 0.03608568385243416\n",
            "Epoch: 14, Loss: 0.014868945814669132 Updates: 202/24000, Avg Grad: 0.022214006632566452, Threshold: 0.02258962392807007\n",
            "Epoch: 14, Loss: 0.005850291345268488 Updates: 211/25000, Avg Grad: 0.03025190532207489, Threshold: 0.034898802638053894\n",
            "Epoch: 14, Loss: 0.005387176759541035 Updates: 221/26000, Avg Grad: 0.025116410106420517, Threshold: 0.025418488308787346\n",
            "Epoch: 14, Loss: 0.006495978217571974 Updates: 231/27000, Avg Grad: 0.023481829091906548, Threshold: 0.023720014840364456\n",
            "Epoch: 14, Loss: 0.004684100858867168 Updates: 241/28000, Avg Grad: 0.03397747501730919, Threshold: 0.013741934671998024\n",
            "Epoch: 14, Loss: 0.0030508842319250107 Updates: 250/29000, Avg Grad: 0.03639960661530495, Threshold: 0.03194853290915489\n",
            "Epoch 13 iteration 0 Loss: 0.501 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.559 | Acc: 69.307% (70/101)\n",
            "Epoch 13 iteration 200 Loss: 0.581 | Acc: 69.154% (139/201)\n",
            "Epoch 13 iteration 300 Loss: 0.581 | Acc: 70.432% (212/301)\n",
            "Epoch 13 iteration 400 Loss: 0.592 | Acc: 69.576% (279/401)\n",
            "Test accuracy: 0.6957606077194214\n",
            "Epoch: 14, Loss: 0.005315692629665136 Updates: 259/30000, Avg Grad: 0.023301444947719574, Threshold: 0.023443229496479034\n",
            "Epoch: 14, Loss: 0.019072838127613068 Updates: 269/31000, Avg Grad: 0.020595336332917213, Threshold: 0.020701026543974876\n",
            "Epoch: 14, Loss: 0.02460554614663124 Updates: 278/32000, Avg Grad: 0.0445670410990715, Threshold: 0.044759929180145264\n",
            "Epoch 14 iteration 0 Loss: 1.343 | Acc: 0.000% (0/1)\n",
            "Epoch 14 iteration 100 Loss: 0.587 | Acc: 74.257% (75/101)\n",
            "Epoch 14 iteration 200 Loss: 0.589 | Acc: 72.139% (145/201)\n",
            "Epoch 14 iteration 300 Loss: 0.582 | Acc: 73.754% (222/301)\n",
            "Epoch 14 iteration 400 Loss: 0.592 | Acc: 72.569% (291/401)\n",
            "Test accuracy: 0.7256857752799988\n",
            "Epoch: 15, Loss: 0.26454833149909973 Updates: 0/0, Avg Grad: 0.09688080847263336, Threshold: 0.10656889528036118\n",
            "Epoch: 15, Loss: 0.00917466264218092 Updates: 0/1000, Avg Grad: 0.08362696319818497, Threshold: 0.10656889528036118\n",
            "Epoch: 15, Loss: 0.014922881498932838 Updates: 5/2000, Avg Grad: 0.06245722621679306, Threshold: 0.06870295107364655\n",
            "Epoch: 15, Loss: 0.012294450774788857 Updates: 14/3000, Avg Grad: 0.012552449479699135, Threshold: 0.013807694427669048\n",
            "Epoch: 15, Loss: 0.017856111750006676 Updates: 24/4000, Avg Grad: 0.014795061200857162, Threshold: 0.01627456769347191\n",
            "Epoch: 15, Loss: 0.01115212682634592 Updates: 31/5000, Avg Grad: 0.04394102096557617, Threshold: 0.06100074201822281\n",
            "Epoch: 15, Loss: 0.02110585942864418 Updates: 40/6000, Avg Grad: 0.04091344773769379, Threshold: 0.028221815824508667\n",
            "Epoch: 15, Loss: 0.010454714298248291 Updates: 48/7000, Avg Grad: 0.06396188586950302, Threshold: 0.035306401550769806\n",
            "Epoch: 15, Loss: 0.011268249712884426 Updates: 55/8000, Avg Grad: 0.023686155676841736, Threshold: 0.02607771009206772\n",
            "Epoch: 15, Loss: 0.010287143290042877 Updates: 60/9000, Avg Grad: 0.015545999631285667, Threshold: 0.01710060052573681\n",
            "Epoch 14 iteration 0 Loss: 0.170 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.565 | Acc: 68.317% (69/101)\n",
            "Epoch 14 iteration 200 Loss: 0.619 | Acc: 63.682% (128/201)\n",
            "Epoch 14 iteration 300 Loss: 0.609 | Acc: 65.116% (196/301)\n",
            "Epoch 14 iteration 400 Loss: 0.610 | Acc: 64.589% (259/401)\n",
            "Test accuracy: 0.6458852887153625\n",
            "Epoch: 15, Loss: 0.01564454659819603 Updates: 70/10000, Avg Grad: 0.018955573439598083, Threshold: 0.02085113152861595\n",
            "Epoch: 15, Loss: 0.0062269242480397224 Updates: 79/11000, Avg Grad: 0.018360115587711334, Threshold: 0.020196126773953438\n",
            "Epoch: 15, Loss: 0.02400245890021324 Updates: 88/12000, Avg Grad: 0.03835446760058403, Threshold: 0.02795487269759178\n",
            "Epoch: 15, Loss: 0.0023552579805254936 Updates: 95/13000, Avg Grad: 0.029104940593242645, Threshold: 0.05973803997039795\n",
            "Epoch: 15, Loss: 0.023038873448967934 Updates: 104/14000, Avg Grad: 0.020946351811289787, Threshold: 0.015763714909553528\n",
            "Epoch: 15, Loss: 0.014253919944167137 Updates: 113/15000, Avg Grad: 0.015426414087414742, Threshold: 0.024608712643384933\n",
            "Epoch: 15, Loss: 0.013120501302182674 Updates: 122/16000, Avg Grad: 0.012056194245815277, Threshold: 0.012852655723690987\n",
            "Epoch: 15, Loss: 0.0022885154467076063 Updates: 130/17000, Avg Grad: 0.02934940904378891, Threshold: 0.042037490755319595\n",
            "Epoch: 15, Loss: 0.037688959389925 Updates: 137/18000, Avg Grad: 0.023612868040800095, Threshold: 0.01814195327460766\n",
            "Epoch: 15, Loss: 0.007177744060754776 Updates: 144/19000, Avg Grad: 0.03472248092293739, Threshold: 0.03609851002693176\n",
            "Epoch 14 iteration 0 Loss: 0.321 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.559 | Acc: 72.277% (73/101)\n",
            "Epoch 14 iteration 200 Loss: 0.543 | Acc: 72.139% (145/201)\n",
            "Epoch 14 iteration 300 Loss: 0.565 | Acc: 69.767% (210/301)\n",
            "Epoch 14 iteration 400 Loss: 0.572 | Acc: 69.576% (279/401)\n",
            "Test accuracy: 0.6957606077194214\n",
            "Epoch: 15, Loss: 0.018580172210931778 Updates: 150/20000, Avg Grad: 0.02164902724325657, Threshold: 0.022372588515281677\n",
            "Epoch: 15, Loss: 0.010122324340045452 Updates: 160/21000, Avg Grad: 0.09844361245632172, Threshold: 0.07964633405208588\n",
            "Epoch: 15, Loss: 0.01660883240401745 Updates: 169/22000, Avg Grad: 0.04344871640205383, Threshold: 0.025424983352422714\n",
            "Epoch: 15, Loss: 0.022759176790714264 Updates: 178/23000, Avg Grad: 0.043639689683914185, Threshold: 0.04451463744044304\n",
            "Epoch: 15, Loss: 0.03341853618621826 Updates: 188/24000, Avg Grad: 0.020450787618756294, Threshold: 0.015772707760334015\n",
            "Epoch: 15, Loss: 0.008455977775156498 Updates: 197/25000, Avg Grad: 0.03623319789767265, Threshold: 0.01680999994277954\n",
            "Epoch: 15, Loss: 0.008177393116056919 Updates: 206/26000, Avg Grad: 0.022478051483631134, Threshold: 0.022748397663235664\n",
            "Epoch: 15, Loss: 0.006309222429990768 Updates: 216/27000, Avg Grad: 0.026962468400597572, Threshold: 0.023489978164434433\n",
            "Epoch: 15, Loss: 0.020749099552631378 Updates: 225/28000, Avg Grad: 0.013382685370743275, Threshold: 0.013497170060873032\n",
            "Epoch: 15, Loss: 0.008070169948041439 Updates: 233/29000, Avg Grad: 0.027354156598448753, Threshold: 0.029707368463277817\n",
            "Epoch 14 iteration 0 Loss: 0.706 | Acc: 0.000% (0/1)\n",
            "Epoch 14 iteration 100 Loss: 0.591 | Acc: 69.307% (70/101)\n",
            "Epoch 14 iteration 200 Loss: 0.600 | Acc: 69.154% (139/201)\n",
            "Epoch 14 iteration 300 Loss: 0.575 | Acc: 70.432% (212/301)\n",
            "Epoch 14 iteration 400 Loss: 0.571 | Acc: 70.574% (283/401)\n",
            "Test accuracy: 0.7057356834411621\n",
            "Epoch: 15, Loss: 0.014826884493231773 Updates: 241/30000, Avg Grad: 0.028385134413838387, Threshold: 0.04740314558148384\n",
            "Epoch: 15, Loss: 0.009301171638071537 Updates: 250/31000, Avg Grad: 0.030995342880487442, Threshold: 0.01934640482068062\n",
            "Epoch: 15, Loss: 0.010140464641153812 Updates: 258/32000, Avg Grad: 0.015262952074408531, Threshold: 0.015329010784626007\n",
            "Epoch 15 iteration 0 Loss: 1.669 | Acc: 0.000% (0/1)\n",
            "Epoch 15 iteration 100 Loss: 0.584 | Acc: 71.287% (72/101)\n",
            "Epoch 15 iteration 200 Loss: 0.597 | Acc: 70.647% (142/201)\n",
            "Epoch 15 iteration 300 Loss: 0.600 | Acc: 68.771% (207/301)\n",
            "Epoch 15 iteration 400 Loss: 0.615 | Acc: 68.828% (276/401)\n",
            "Test accuracy: 0.6882793307304382\n",
            "Epoch: 16, Loss: 0.3989936411380768 Updates: 0/0, Avg Grad: 0.1338079422712326, Threshold: 0.14718873798847198\n",
            "Epoch: 16, Loss: 0.004302171990275383 Updates: 4/1000, Avg Grad: 0.021344434469938278, Threshold: 0.023478878661990166\n",
            "Epoch: 16, Loss: 0.010664615780115128 Updates: 12/2000, Avg Grad: 0.04192948713898659, Threshold: 0.046122435480356216\n",
            "Epoch: 16, Loss: 0.008445651270449162 Updates: 22/3000, Avg Grad: 0.029980313032865524, Threshold: 0.03297834470868111\n",
            "Epoch: 16, Loss: 0.040852442383766174 Updates: 32/4000, Avg Grad: 0.03161930665373802, Threshold: 0.015161638148128986\n",
            "Epoch: 16, Loss: 0.007097050081938505 Updates: 40/5000, Avg Grad: 0.024756675586104393, Threshold: 0.028444532305002213\n",
            "Epoch: 16, Loss: 0.004077024292200804 Updates: 48/6000, Avg Grad: 0.03790193051099777, Threshold: 0.03675511106848717\n",
            "Epoch: 16, Loss: 0.012830632738769054 Updates: 56/7000, Avg Grad: 0.04191295802593231, Threshold: 0.05368397384881973\n",
            "Epoch: 16, Loss: 0.022578658536076546 Updates: 64/8000, Avg Grad: 0.018037790432572365, Threshold: 0.017512895166873932\n",
            "Epoch: 16, Loss: 0.015434415079653263 Updates: 72/9000, Avg Grad: 0.053765058517456055, Threshold: 0.05914156511425972\n",
            "Epoch 15 iteration 0 Loss: 0.434 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.582 | Acc: 70.297% (71/101)\n",
            "Epoch 15 iteration 200 Loss: 0.600 | Acc: 67.164% (135/201)\n",
            "Epoch 15 iteration 300 Loss: 0.577 | Acc: 70.100% (211/301)\n",
            "Epoch 15 iteration 400 Loss: 0.589 | Acc: 68.080% (273/401)\n",
            "Test accuracy: 0.6807979941368103\n",
            "Epoch: 16, Loss: 0.01592075638473034 Updates: 81/10000, Avg Grad: 0.014795546419918537, Threshold: 0.015336119569838047\n",
            "Epoch: 16, Loss: 0.00922356266528368 Updates: 90/11000, Avg Grad: 0.07795210182666779, Threshold: 0.05239150673151016\n",
            "Epoch: 16, Loss: 0.00758258206769824 Updates: 100/12000, Avg Grad: 0.05528463050723076, Threshold: 0.015247473493218422\n",
            "Epoch: 16, Loss: 0.004207184538245201 Updates: 108/13000, Avg Grad: 0.018872445449233055, Threshold: 0.020759690552949905\n",
            "Epoch: 16, Loss: 0.009664284996688366 Updates: 117/14000, Avg Grad: 0.052882734686136246, Threshold: 0.04237049072980881\n",
            "Epoch: 16, Loss: 0.006177414208650589 Updates: 124/15000, Avg Grad: 0.02882026880979538, Threshold: 0.018919682130217552\n",
            "Epoch: 16, Loss: 0.024923164397478104 Updates: 133/16000, Avg Grad: 0.015092294663190842, Threshold: 0.01608932949602604\n",
            "Epoch: 16, Loss: 0.011543924920260906 Updates: 142/17000, Avg Grad: 0.02248440310359001, Threshold: 0.0121311591938138\n",
            "Epoch: 16, Loss: 0.014585550874471664 Updates: 151/18000, Avg Grad: 0.025532949715852737, Threshold: 0.018643036484718323\n",
            "Epoch: 16, Loss: 0.008621481247246265 Updates: 160/19000, Avg Grad: 0.02922031842172146, Threshold: 0.015329546295106411\n",
            "Epoch 15 iteration 0 Loss: 0.903 | Acc: 0.000% (0/1)\n",
            "Epoch 15 iteration 100 Loss: 0.595 | Acc: 68.317% (69/101)\n",
            "Epoch 15 iteration 200 Loss: 0.576 | Acc: 71.144% (143/201)\n",
            "Epoch 15 iteration 300 Loss: 0.587 | Acc: 69.103% (208/301)\n",
            "Epoch 15 iteration 400 Loss: 0.575 | Acc: 70.574% (283/401)\n",
            "Test accuracy: 0.7057356834411621\n",
            "Epoch: 16, Loss: 0.0093704704195261 Updates: 168/20000, Avg Grad: 0.061119284480810165, Threshold: 0.024872662499547005\n",
            "Epoch: 16, Loss: 0.03678543493151665 Updates: 176/21000, Avg Grad: 0.0273375753313303, Threshold: 0.01757124438881874\n",
            "Epoch: 16, Loss: 0.022359007969498634 Updates: 185/22000, Avg Grad: 0.014989431016147137, Threshold: 0.012317311950027943\n",
            "Epoch: 16, Loss: 0.00791873224079609 Updates: 194/23000, Avg Grad: 0.02393421344459057, Threshold: 0.021384164690971375\n",
            "Epoch: 16, Loss: 0.006620519794523716 Updates: 202/24000, Avg Grad: 0.026418650522828102, Threshold: 0.026865363121032715\n",
            "Epoch: 16, Loss: 0.00676497258245945 Updates: 210/25000, Avg Grad: 0.018701668828725815, Threshold: 0.018968366086483\n",
            "Epoch: 16, Loss: 0.002611103467643261 Updates: 218/26000, Avg Grad: 0.03383822739124298, Threshold: 0.05737745389342308\n",
            "Epoch: 16, Loss: 0.01984487846493721 Updates: 226/27000, Avg Grad: 0.03810597211122513, Threshold: 0.014589149504899979\n",
            "Epoch: 16, Loss: 0.003046073717996478 Updates: 236/28000, Avg Grad: 0.03047179989516735, Threshold: 0.02505485899746418\n",
            "Epoch: 16, Loss: 0.011462781578302383 Updates: 243/29000, Avg Grad: 0.049580108374357224, Threshold: 0.04993781819939613\n",
            "Epoch 15 iteration 0 Loss: 0.448 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.632 | Acc: 62.376% (63/101)\n",
            "Epoch 15 iteration 200 Loss: 0.590 | Acc: 67.164% (135/201)\n",
            "Epoch 15 iteration 300 Loss: 0.592 | Acc: 67.110% (202/301)\n",
            "Epoch 15 iteration 400 Loss: 0.622 | Acc: 64.339% (258/401)\n",
            "Test accuracy: 0.6433915495872498\n",
            "Epoch: 16, Loss: 0.009145227260887623 Updates: 252/30000, Avg Grad: 0.04414162039756775, Threshold: 0.04441021382808685\n",
            "Epoch: 16, Loss: 0.00725072156637907 Updates: 260/31000, Avg Grad: 0.032074470072984695, Threshold: 0.04808227717876434\n",
            "Epoch: 16, Loss: 0.024042243137955666 Updates: 270/32000, Avg Grad: 0.017952507361769676, Threshold: 0.018030205741524696\n",
            "Epoch 16 iteration 0 Loss: 0.379 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.707 | Acc: 58.416% (59/101)\n",
            "Epoch 16 iteration 200 Loss: 0.660 | Acc: 65.174% (131/201)\n",
            "Epoch 16 iteration 300 Loss: 0.627 | Acc: 67.110% (202/301)\n",
            "Epoch 16 iteration 400 Loss: 0.613 | Acc: 69.077% (277/401)\n",
            "Test accuracy: 0.690773069858551\n",
            "Epoch: 17, Loss: 1.662611961364746 Updates: 0/0, Avg Grad: 0.3205588161945343, Threshold: 0.35261470079421997\n",
            "Epoch: 17, Loss: 0.01483960822224617 Updates: 0/1000, Avg Grad: 0.23564140498638153, Threshold: 0.35261470079421997\n",
            "Epoch: 17, Loss: 0.021543962880969048 Updates: 6/2000, Avg Grad: 0.03467437997460365, Threshold: 0.022428885102272034\n",
            "Epoch: 17, Loss: 0.022072389721870422 Updates: 15/3000, Avg Grad: 0.020239725708961487, Threshold: 0.016123894602060318\n",
            "Epoch: 17, Loss: 0.005036564543843269 Updates: 24/4000, Avg Grad: 0.03964405134320259, Threshold: 0.04360845685005188\n",
            "Epoch: 17, Loss: 0.014743007719516754 Updates: 33/5000, Avg Grad: 0.07366642355918884, Threshold: 0.05577738210558891\n",
            "Epoch: 17, Loss: 0.005554764997214079 Updates: 41/6000, Avg Grad: 0.058908235281705856, Threshold: 0.06479906290769577\n",
            "Epoch: 17, Loss: 0.01414456870406866 Updates: 51/7000, Avg Grad: 0.014037501066923141, Threshold: 0.015441251918673515\n",
            "Epoch: 17, Loss: 0.009130083955824375 Updates: 58/8000, Avg Grad: 0.023386545479297638, Threshold: 0.02572520077228546\n",
            "Epoch: 17, Loss: 0.023903368040919304 Updates: 67/9000, Avg Grad: 0.01755133830010891, Threshold: 0.019306473433971405\n",
            "Epoch 16 iteration 0 Loss: 0.532 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.595 | Acc: 73.267% (74/101)\n",
            "Epoch 16 iteration 200 Loss: 0.623 | Acc: 67.164% (135/201)\n",
            "Epoch 16 iteration 300 Loss: 0.617 | Acc: 68.106% (205/301)\n",
            "Epoch 16 iteration 400 Loss: 0.609 | Acc: 68.080% (273/401)\n",
            "Test accuracy: 0.6807979941368103\n",
            "Epoch: 17, Loss: 0.01495794765651226 Updates: 77/10000, Avg Grad: 0.04865552857518196, Threshold: 0.02262863703072071\n",
            "Epoch: 17, Loss: 0.013600991107523441 Updates: 86/11000, Avg Grad: 0.019737087190151215, Threshold: 0.016300581395626068\n",
            "Epoch: 17, Loss: 0.014014081098139286 Updates: 89/12000, Avg Grad: 0.02201143279671669, Threshold: 0.03525671735405922\n",
            "Epoch: 17, Loss: 0.013106557540595531 Updates: 98/13000, Avg Grad: 0.018464000895619392, Threshold: 0.024633044376969337\n",
            "Epoch: 17, Loss: 0.009521285071969032 Updates: 107/14000, Avg Grad: 0.031068896874785423, Threshold: 0.03395451605319977\n",
            "Epoch: 17, Loss: 0.01891930028796196 Updates: 115/15000, Avg Grad: 0.01357354037463665, Threshold: 0.014636770822107792\n",
            "Epoch: 17, Loss: 0.0018144170753657818 Updates: 122/16000, Avg Grad: 0.016280081123113632, Threshold: 0.017355583608150482\n",
            "Epoch: 17, Loss: 0.028021538630127907 Updates: 131/17000, Avg Grad: 0.021604273468255997, Threshold: 0.019016366451978683\n",
            "Epoch: 17, Loss: 0.010056081227958202 Updates: 140/18000, Avg Grad: 0.01875096559524536, Threshold: 0.01963205449283123\n",
            "Epoch: 17, Loss: 0.021974287927150726 Updates: 149/19000, Avg Grad: 0.021642448380589485, Threshold: 0.02250012382864952\n",
            "Epoch 16 iteration 0 Loss: 0.521 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.636 | Acc: 69.307% (70/101)\n",
            "Epoch 16 iteration 200 Loss: 0.646 | Acc: 64.677% (130/201)\n",
            "Epoch 16 iteration 300 Loss: 0.635 | Acc: 65.449% (197/301)\n",
            "Epoch 16 iteration 400 Loss: 0.633 | Acc: 65.087% (261/401)\n",
            "Test accuracy: 0.6508728265762329\n",
            "Epoch: 17, Loss: 0.030675604939460754 Updates: 159/20000, Avg Grad: 0.03905625268816948, Threshold: 0.026245303452014923\n",
            "Epoch: 17, Loss: 0.01652073673903942 Updates: 168/21000, Avg Grad: 0.014982947148382664, Threshold: 0.015405279584228992\n",
            "Epoch: 17, Loss: 0.014463325962424278 Updates: 175/22000, Avg Grad: 0.029123961925506592, Threshold: 0.031025506556034088\n",
            "Epoch: 17, Loss: 0.016322070732712746 Updates: 184/23000, Avg Grad: 0.01958361454308033, Threshold: 0.020652752369642258\n",
            "Epoch: 17, Loss: 0.007998797111213207 Updates: 193/24000, Avg Grad: 0.03361571952700615, Threshold: 0.03418412804603577\n",
            "Epoch: 17, Loss: 0.018034448847174644 Updates: 202/25000, Avg Grad: 0.020195191726088524, Threshold: 0.018166804686188698\n",
            "Epoch: 17, Loss: 0.015856368467211723 Updates: 211/26000, Avg Grad: 0.021135380491614342, Threshold: 0.02138957940042019\n",
            "Epoch: 17, Loss: 0.017720293253660202 Updates: 219/27000, Avg Grad: 0.013407442718744278, Threshold: 0.013543440029025078\n",
            "Epoch: 17, Loss: 0.004009888507425785 Updates: 227/28000, Avg Grad: 0.0295180082321167, Threshold: 0.04471368342638016\n",
            "Epoch: 17, Loss: 0.006559671834111214 Updates: 236/29000, Avg Grad: 0.01345896814018488, Threshold: 0.013556071557104588\n",
            "Epoch 16 iteration 0 Loss: 0.683 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.592 | Acc: 69.307% (70/101)\n",
            "Epoch 16 iteration 200 Loss: 0.633 | Acc: 66.667% (134/201)\n",
            "Epoch 16 iteration 300 Loss: 0.623 | Acc: 67.110% (202/301)\n",
            "Epoch 16 iteration 400 Loss: 0.631 | Acc: 66.334% (266/401)\n",
            "Test accuracy: 0.6633416414260864\n",
            "Epoch: 17, Loss: 0.001873097149655223 Updates: 246/30000, Avg Grad: 0.06642188876867294, Threshold: 0.048709504306316376\n",
            "Epoch: 17, Loss: 0.01908612810075283 Updates: 255/31000, Avg Grad: 0.021532025188207626, Threshold: 0.01654718443751335\n",
            "Epoch: 17, Loss: 0.019797181710600853 Updates: 263/32000, Avg Grad: 0.026425471529364586, Threshold: 0.04210520163178444\n",
            "Epoch 17 iteration 0 Loss: 0.598 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.562 | Acc: 77.228% (78/101)\n",
            "Epoch 17 iteration 200 Loss: 0.594 | Acc: 69.652% (140/201)\n",
            "Epoch 17 iteration 300 Loss: 0.594 | Acc: 69.103% (208/301)\n",
            "Epoch 17 iteration 400 Loss: 0.599 | Acc: 68.828% (276/401)\n",
            "Test accuracy: 0.6882793307304382\n",
            "Epoch: 18, Loss: 0.8613116145133972 Updates: 0/0, Avg Grad: 0.2386000007390976, Threshold: 0.26245999336242676\n",
            "Epoch: 18, Loss: 0.006277144420892 Updates: 0/1000, Avg Grad: 0.20290495455265045, Threshold: 0.26245999336242676\n",
            "Epoch: 18, Loss: 0.0109828170388937 Updates: 7/2000, Avg Grad: 0.02773061953485012, Threshold: 0.030503682792186737\n",
            "Epoch: 18, Loss: 0.022552164271473885 Updates: 17/3000, Avg Grad: 0.08901772648096085, Threshold: 0.08055130392313004\n",
            "Epoch: 18, Loss: 0.02147781103849411 Updates: 26/4000, Avg Grad: 0.07056345045566559, Threshold: 0.04333578795194626\n",
            "Epoch: 18, Loss: 0.00431270943954587 Updates: 34/5000, Avg Grad: 0.023629138246178627, Threshold: 0.025992052629590034\n",
            "Epoch: 18, Loss: 0.007068435661494732 Updates: 42/6000, Avg Grad: 0.06277214735746384, Threshold: 0.06839156150817871\n",
            "Epoch: 18, Loss: 0.028690502047538757 Updates: 51/7000, Avg Grad: 0.02042567729949951, Threshold: 0.020394327118992805\n",
            "Epoch: 18, Loss: 0.0024698786437511444 Updates: 60/8000, Avg Grad: 0.018236644566059113, Threshold: 0.020060310140252113\n",
            "Epoch: 18, Loss: 0.024935388937592506 Updates: 67/9000, Avg Grad: 0.02790549211204052, Threshold: 0.03036593832075596\n",
            "Epoch 17 iteration 0 Loss: 0.826 | Acc: 0.000% (0/1)\n",
            "Epoch 17 iteration 100 Loss: 0.585 | Acc: 72.277% (73/101)\n",
            "Epoch 17 iteration 200 Loss: 0.595 | Acc: 71.144% (143/201)\n",
            "Epoch 17 iteration 300 Loss: 0.626 | Acc: 67.774% (204/301)\n",
            "Epoch 17 iteration 400 Loss: 0.653 | Acc: 64.339% (258/401)\n",
            "Test accuracy: 0.6433915495872498\n",
            "Epoch: 18, Loss: 0.004421763587743044 Updates: 76/10000, Avg Grad: 0.03398016840219498, Threshold: 0.037378184497356415\n",
            "Epoch: 18, Loss: 0.015872523188591003 Updates: 86/11000, Avg Grad: 0.021281864494085312, Threshold: 0.023410052061080933\n",
            "Epoch: 18, Loss: 0.017887629568576813 Updates: 95/12000, Avg Grad: 0.029373683035373688, Threshold: 0.020392263308167458\n",
            "Epoch: 18, Loss: 0.009804108180105686 Updates: 103/13000, Avg Grad: 0.013760688714683056, Threshold: 0.015136757865548134\n",
            "Epoch: 18, Loss: 0.015335309319198132 Updates: 113/14000, Avg Grad: 0.03223209083080292, Threshold: 0.012772345915436745\n",
            "Epoch: 18, Loss: 0.01856182888150215 Updates: 120/15000, Avg Grad: 0.021635405719280243, Threshold: 0.02333013154566288\n",
            "Epoch: 18, Loss: 0.02780407853424549 Updates: 127/16000, Avg Grad: 0.0422479547560215, Threshold: 0.04503895714879036\n",
            "Epoch: 18, Loss: 0.013156378641724586 Updates: 136/17000, Avg Grad: 0.015024778433144093, Threshold: 0.015861891210079193\n",
            "Epoch: 18, Loss: 0.004343404900282621 Updates: 144/18000, Avg Grad: 0.030310168862342834, Threshold: 0.0317344106733799\n",
            "Epoch: 18, Loss: 0.006354299373924732 Updates: 153/19000, Avg Grad: 0.011978751048445702, Threshold: 0.012453461065888405\n",
            "Epoch 17 iteration 0 Loss: 0.147 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.541 | Acc: 75.248% (76/101)\n",
            "Epoch 17 iteration 200 Loss: 0.592 | Acc: 70.149% (141/201)\n",
            "Epoch 17 iteration 300 Loss: 0.606 | Acc: 69.767% (210/301)\n",
            "Epoch 17 iteration 400 Loss: 0.622 | Acc: 67.830% (272/401)\n",
            "Test accuracy: 0.6783042550086975\n",
            "Epoch: 18, Loss: 0.02013298124074936 Updates: 163/20000, Avg Grad: 0.02772340551018715, Threshold: 0.01623830385506153\n",
            "Epoch: 18, Loss: 0.0026108836755156517 Updates: 173/21000, Avg Grad: 0.049863189458847046, Threshold: 0.03856727108359337\n",
            "Epoch: 18, Loss: 0.0030897415708750486 Updates: 180/22000, Avg Grad: 0.016832862049341202, Threshold: 0.017233025282621384\n",
            "Epoch: 18, Loss: 0.013643380254507065 Updates: 188/23000, Avg Grad: 0.03377344831824303, Threshold: 0.018903344869613647\n",
            "Epoch: 18, Loss: 0.00916926097124815 Updates: 197/24000, Avg Grad: 0.01710289716720581, Threshold: 0.016683291643857956\n",
            "Epoch: 18, Loss: 0.005634046625345945 Updates: 206/25000, Avg Grad: 0.06236209347844124, Threshold: 0.06325142085552216\n",
            "Epoch: 18, Loss: 0.0033487556502223015 Updates: 215/26000, Avg Grad: 0.02273564413189888, Threshold: 0.023013578727841377\n",
            "Epoch: 18, Loss: 0.022144179791212082 Updates: 225/27000, Avg Grad: 0.04575236141681671, Threshold: 0.042280036956071854\n",
            "Epoch: 18, Loss: 0.009625349193811417 Updates: 232/28000, Avg Grad: 0.02771781198680401, Threshold: 0.02795492857694626\n",
            "Epoch: 18, Loss: 0.027990717440843582 Updates: 242/29000, Avg Grad: 0.05872936174273491, Threshold: 0.017943771556019783\n",
            "Epoch 17 iteration 0 Loss: 0.750 | Acc: 0.000% (0/1)\n",
            "Epoch 17 iteration 100 Loss: 0.631 | Acc: 63.366% (64/101)\n",
            "Epoch 17 iteration 200 Loss: 0.646 | Acc: 63.682% (128/201)\n",
            "Epoch 17 iteration 300 Loss: 0.643 | Acc: 64.452% (194/301)\n",
            "Epoch 17 iteration 400 Loss: 0.632 | Acc: 65.087% (261/401)\n",
            "Test accuracy: 0.6508728265762329\n",
            "Epoch: 18, Loss: 0.021946225315332413 Updates: 252/30000, Avg Grad: 0.02321760170161724, Threshold: 0.020357182249426842\n",
            "Epoch: 18, Loss: 0.005003008060157299 Updates: 260/31000, Avg Grad: 0.04299403354525566, Threshold: 0.031021704897284508\n",
            "Epoch: 18, Loss: 0.01895480416715145 Updates: 269/32000, Avg Grad: 0.014041163958609104, Threshold: 0.014101934619247913\n",
            "Epoch 18 iteration 0 Loss: 0.200 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.664 | Acc: 66.337% (67/101)\n",
            "Epoch 18 iteration 200 Loss: 0.652 | Acc: 66.169% (133/201)\n",
            "Epoch 18 iteration 300 Loss: 0.650 | Acc: 67.110% (202/301)\n",
            "Epoch 18 iteration 400 Loss: 0.645 | Acc: 66.584% (267/401)\n",
            "Test accuracy: 0.665835440158844\n",
            "Epoch: 19, Loss: 0.1520235687494278 Updates: 0/0, Avg Grad: 0.05562211573123932, Threshold: 0.06118432804942131\n",
            "Epoch: 19, Loss: 0.008143874816596508 Updates: 3/1000, Avg Grad: 0.05265212059020996, Threshold: 0.057917334139347076\n",
            "Epoch: 19, Loss: 0.020378246903419495 Updates: 13/2000, Avg Grad: 0.02369157038629055, Threshold: 0.02606072835624218\n",
            "Epoch: 19, Loss: 0.012556961737573147 Updates: 22/3000, Avg Grad: 0.03191733360290527, Threshold: 0.03510906919836998\n",
            "Epoch: 19, Loss: 0.010445510037243366 Updates: 29/4000, Avg Grad: 0.05839523300528526, Threshold: 0.037704527378082275\n",
            "Epoch: 19, Loss: 0.02608472853899002 Updates: 37/5000, Avg Grad: 0.02526228502392769, Threshold: 0.032006435096263885\n",
            "Epoch: 19, Loss: 0.007678105030208826 Updates: 45/6000, Avg Grad: 0.034420810639858246, Threshold: 0.03516697511076927\n",
            "Epoch: 19, Loss: 0.010443546809256077 Updates: 53/7000, Avg Grad: 0.024039680138230324, Threshold: 0.02315690368413925\n",
            "Epoch: 19, Loss: 0.015250112861394882 Updates: 62/8000, Avg Grad: 0.010347901843488216, Threshold: 0.011382692493498325\n",
            "Epoch: 19, Loss: 0.002471408573910594 Updates: 71/9000, Avg Grad: 0.022416189312934875, Threshold: 0.039173673838377\n",
            "Epoch 18 iteration 0 Loss: 0.540 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.573 | Acc: 73.267% (74/101)\n",
            "Epoch 18 iteration 200 Loss: 0.593 | Acc: 70.647% (142/201)\n",
            "Epoch 18 iteration 300 Loss: 0.583 | Acc: 71.761% (216/301)\n",
            "Epoch 18 iteration 400 Loss: 0.580 | Acc: 71.820% (288/401)\n",
            "Test accuracy: 0.7182044982910156\n",
            "Epoch: 19, Loss: 0.00867047905921936 Updates: 81/10000, Avg Grad: 0.029413511976599693, Threshold: 0.03235486522316933\n",
            "Epoch: 19, Loss: 0.0015486303018406034 Updates: 90/11000, Avg Grad: 0.01582346297800541, Threshold: 0.017405809834599495\n",
            "Epoch: 19, Loss: 0.012255660258233547 Updates: 100/12000, Avg Grad: 0.016518743708729744, Threshold: 0.018170619383454323\n",
            "Epoch: 19, Loss: 0.030051756650209427 Updates: 108/13000, Avg Grad: 0.031675904989242554, Threshold: 0.03338414058089256\n",
            "Epoch: 19, Loss: 0.016458632424473763 Updates: 118/14000, Avg Grad: 0.017824379727244377, Threshold: 0.019479874521493912\n",
            "Epoch: 19, Loss: 0.01081901416182518 Updates: 127/15000, Avg Grad: 0.09067658334970474, Threshold: 0.04761528596282005\n",
            "Epoch: 19, Loss: 0.0027394432108849287 Updates: 136/16000, Avg Grad: 0.051878247410058975, Threshold: 0.015397395938634872\n",
            "Epoch: 19, Loss: 0.011404411867260933 Updates: 145/17000, Avg Grad: 0.048457998782396317, Threshold: 0.046184126287698746\n",
            "Epoch: 19, Loss: 0.004890168085694313 Updates: 154/18000, Avg Grad: 0.01848915033042431, Threshold: 0.019357936456799507\n",
            "Epoch: 19, Loss: 0.021830879151821136 Updates: 163/19000, Avg Grad: 0.04810316488146782, Threshold: 0.0221935473382473\n",
            "Epoch 18 iteration 0 Loss: 0.253 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.605 | Acc: 65.347% (66/101)\n",
            "Epoch 18 iteration 200 Loss: 0.614 | Acc: 64.179% (129/201)\n",
            "Epoch 18 iteration 300 Loss: 0.628 | Acc: 65.781% (198/301)\n",
            "Epoch 18 iteration 400 Loss: 0.600 | Acc: 68.329% (274/401)\n",
            "Test accuracy: 0.6832917928695679\n",
            "Epoch: 19, Loss: 0.008784363977611065 Updates: 172/20000, Avg Grad: 0.018531277775764465, Threshold: 0.019150637090206146\n",
            "Epoch: 19, Loss: 0.006791940424591303 Updates: 181/21000, Avg Grad: 0.03498596325516701, Threshold: 0.03597212955355644\n",
            "Epoch: 19, Loss: 0.027061261236667633 Updates: 189/22000, Avg Grad: 0.031214065849781036, Threshold: 0.013244475238025188\n",
            "Epoch: 19, Loss: 0.011331913061439991 Updates: 197/23000, Avg Grad: 0.023848485201597214, Threshold: 0.02623789757490158\n",
            "Epoch: 19, Loss: 0.007149808574467897 Updates: 207/24000, Avg Grad: 0.0566214993596077, Threshold: 0.023551661521196365\n",
            "Epoch: 19, Loss: 0.004970097914338112 Updates: 213/25000, Avg Grad: 0.01499792467802763, Threshold: 0.015211804769933224\n",
            "Epoch: 19, Loss: 0.013581396080553532 Updates: 220/26000, Avg Grad: 0.033664166927337646, Threshold: 0.03391892462968826\n",
            "Epoch: 19, Loss: 0.02142070233821869 Updates: 229/27000, Avg Grad: 0.022502779960632324, Threshold: 0.022731034085154533\n",
            "Epoch: 19, Loss: 0.002110506873577833 Updates: 238/28000, Avg Grad: 0.027482032775878906, Threshold: 0.017119690775871277\n",
            "Epoch: 19, Loss: 0.00443892739713192 Updates: 246/29000, Avg Grad: 0.014978421851992607, Threshold: 0.015086487866938114\n",
            "Epoch 18 iteration 0 Loss: 1.207 | Acc: 0.000% (0/1)\n",
            "Epoch 18 iteration 100 Loss: 0.611 | Acc: 64.356% (65/101)\n",
            "Epoch 18 iteration 200 Loss: 0.592 | Acc: 67.662% (136/201)\n",
            "Epoch 18 iteration 300 Loss: 0.581 | Acc: 70.100% (211/301)\n",
            "Epoch 18 iteration 400 Loss: 0.592 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 19, Loss: 0.008621569722890854 Updates: 255/30000, Avg Grad: 0.03500153869390488, Threshold: 0.03521451726555824\n",
            "Epoch: 19, Loss: 0.004508115816861391 Updates: 265/31000, Avg Grad: 0.03491261973977089, Threshold: 0.021740972995758057\n",
            "Epoch: 19, Loss: 0.011537804268300533 Updates: 274/32000, Avg Grad: 0.024406254291534424, Threshold: 0.019221989437937737\n",
            "Epoch 19 iteration 0 Loss: 1.923 | Acc: 0.000% (0/1)\n",
            "Epoch 19 iteration 100 Loss: 0.588 | Acc: 69.307% (70/101)\n",
            "Epoch 19 iteration 200 Loss: 0.618 | Acc: 69.154% (139/201)\n",
            "Epoch 19 iteration 300 Loss: 0.619 | Acc: 68.106% (205/301)\n",
            "Epoch 19 iteration 400 Loss: 0.620 | Acc: 67.082% (269/401)\n",
            "Test accuracy: 0.6708229184150696\n",
            "Epoch: 20, Loss: 0.821487307548523 Updates: 0/0, Avg Grad: 0.2319081723690033, Threshold: 0.25509899854660034\n",
            "Epoch: 20, Loss: 0.00474395789206028 Updates: 0/1000, Avg Grad: 0.15057364106178284, Threshold: 0.25509899854660034\n",
            "Epoch: 20, Loss: 0.003985628020018339 Updates: 6/2000, Avg Grad: 0.042055707424879074, Threshold: 0.02592439204454422\n",
            "Epoch: 20, Loss: 0.006595324724912643 Updates: 13/3000, Avg Grad: 0.01780754141509533, Threshold: 0.019588295370340347\n",
            "Epoch: 20, Loss: 0.0026755202561616898 Updates: 20/4000, Avg Grad: 0.026678811758756638, Threshold: 0.02885335311293602\n",
            "Epoch: 20, Loss: 0.01690145768225193 Updates: 28/5000, Avg Grad: 0.028632087633013725, Threshold: 0.016722334548830986\n",
            "Epoch: 20, Loss: 0.01704541966319084 Updates: 37/6000, Avg Grad: 0.035131487995386124, Threshold: 0.025786999613046646\n",
            "Epoch: 20, Loss: 0.0106299864128232 Updates: 44/7000, Avg Grad: 0.055519185960292816, Threshold: 0.04278881475329399\n",
            "Epoch: 20, Loss: 0.007950023747980595 Updates: 51/8000, Avg Grad: 0.03750196844339371, Threshold: 0.03359101340174675\n",
            "Epoch: 20, Loss: 0.005217531695961952 Updates: 60/9000, Avg Grad: 0.031174853444099426, Threshold: 0.027871496975421906\n",
            "Epoch 19 iteration 0 Loss: 0.190 | Acc: 100.000% (1/1)\n",
            "Epoch 19 iteration 100 Loss: 0.544 | Acc: 73.267% (74/101)\n",
            "Epoch 19 iteration 200 Loss: 0.603 | Acc: 68.657% (138/201)\n",
            "Epoch 19 iteration 300 Loss: 0.594 | Acc: 69.103% (208/301)\n",
            "Epoch 19 iteration 400 Loss: 0.591 | Acc: 69.576% (279/401)\n",
            "Test accuracy: 0.6957606077194214\n",
            "Epoch: 20, Loss: 0.0058584935031831264 Updates: 69/10000, Avg Grad: 0.03357158973813057, Threshold: 0.036928750574588776\n",
            "Epoch: 20, Loss: 0.005702587775886059 Updates: 78/11000, Avg Grad: 0.04195699095726013, Threshold: 0.02988554909825325\n",
            "Epoch: 20, Loss: 0.008665746077895164 Updates: 87/12000, Avg Grad: 0.019951771944761276, Threshold: 0.014895723201334476\n",
            "Epoch: 20, Loss: 0.004595100414007902 Updates: 94/13000, Avg Grad: 0.021762948483228683, Threshold: 0.02393924444913864\n",
            "Epoch: 20, Loss: 0.02339869551360607 Updates: 102/14000, Avg Grad: 0.02088862843811512, Threshold: 0.014468318782746792\n",
            "Epoch: 20, Loss: 0.005186836700886488 Updates: 111/15000, Avg Grad: 0.07499580085277557, Threshold: 0.07555608451366425\n",
            "Epoch: 20, Loss: 0.005451344419270754 Updates: 120/16000, Avg Grad: 0.021980710327625275, Threshold: 0.03808409720659256\n",
            "Epoch: 20, Loss: 0.008423322811722755 Updates: 128/17000, Avg Grad: 0.03989886865019798, Threshold: 0.042121849954128265\n",
            "Epoch: 20, Loss: 0.010518641211092472 Updates: 132/18000, Avg Grad: 0.04323295131325722, Threshold: 0.012473405338823795\n",
            "Epoch: 20, Loss: 0.010233969427645206 Updates: 141/19000, Avg Grad: 0.04829634353518486, Threshold: 0.021336259320378304\n",
            "Epoch 19 iteration 0 Loss: 0.716 | Acc: 0.000% (0/1)\n",
            "Epoch 19 iteration 100 Loss: 0.612 | Acc: 65.347% (66/101)\n",
            "Epoch 19 iteration 200 Loss: 0.599 | Acc: 65.672% (132/201)\n",
            "Epoch 19 iteration 300 Loss: 0.601 | Acc: 67.442% (203/301)\n",
            "Epoch 19 iteration 400 Loss: 0.607 | Acc: 67.082% (269/401)\n",
            "Test accuracy: 0.6708229184150696\n",
            "Epoch: 20, Loss: 0.015967801213264465 Updates: 150/20000, Avg Grad: 0.023641467094421387, Threshold: 0.022194474935531616\n",
            "Epoch: 20, Loss: 0.004880193620920181 Updates: 160/21000, Avg Grad: 0.0513799749314785, Threshold: 0.0123861413449049\n",
            "Epoch: 20, Loss: 0.005111494567245245 Updates: 169/22000, Avg Grad: 0.047279104590415955, Threshold: 0.0320512019097805\n",
            "Epoch: 20, Loss: 0.017434509471058846 Updates: 176/23000, Avg Grad: 0.029918184503912926, Threshold: 0.030518023297190666\n",
            "Epoch: 20, Loss: 0.005265126936137676 Updates: 185/24000, Avg Grad: 0.02504548244178295, Threshold: 0.017704106867313385\n",
            "Epoch: 20, Loss: 0.00786464661359787 Updates: 193/25000, Avg Grad: 0.02477237768471241, Threshold: 0.025125646963715553\n",
            "Epoch: 20, Loss: 0.01724519394338131 Updates: 201/26000, Avg Grad: 0.015636876225471497, Threshold: 0.015824943780899048\n",
            "Epoch: 20, Loss: 0.016938582062721252 Updates: 210/27000, Avg Grad: 0.01790672354400158, Threshold: 0.02010525017976761\n",
            "Epoch: 20, Loss: 0.0029534557834267616 Updates: 219/28000, Avg Grad: 0.021484525874257088, Threshold: 0.021668318659067154\n",
            "Epoch: 20, Loss: 0.011360961012542248 Updates: 227/29000, Avg Grad: 0.01653904654085636, Threshold: 0.022400975227355957\n",
            "Epoch 19 iteration 0 Loss: 1.111 | Acc: 0.000% (0/1)\n",
            "Epoch 19 iteration 100 Loss: 0.665 | Acc: 59.406% (60/101)\n",
            "Epoch 19 iteration 200 Loss: 0.612 | Acc: 65.672% (132/201)\n",
            "Epoch 19 iteration 300 Loss: 0.608 | Acc: 67.774% (204/301)\n",
            "Epoch 19 iteration 400 Loss: 0.604 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 20, Loss: 0.008135300129652023 Updates: 234/30000, Avg Grad: 0.02296648919582367, Threshold: 0.02310623601078987\n",
            "Epoch: 20, Loss: 0.010385923087596893 Updates: 244/31000, Avg Grad: 0.05568866804242134, Threshold: 0.044906023889780045\n",
            "Epoch: 20, Loss: 0.004408259876072407 Updates: 252/32000, Avg Grad: 0.013129260390996933, Threshold: 0.013186084106564522\n",
            "Epoch 20 iteration 0 Loss: 0.738 | Acc: 0.000% (0/1)\n",
            "Epoch 20 iteration 100 Loss: 0.643 | Acc: 64.356% (65/101)\n",
            "Epoch 20 iteration 200 Loss: 0.647 | Acc: 65.672% (132/201)\n",
            "Epoch 20 iteration 300 Loss: 0.635 | Acc: 65.449% (197/301)\n",
            "Epoch 20 iteration 400 Loss: 0.648 | Acc: 64.339% (258/401)\n",
            "Test accuracy: 0.6433915495872498\n",
            "Epoch: 21, Loss: 1.33413565158844 Updates: 0/0, Avg Grad: 0.2938549816608429, Threshold: 0.3232404887676239\n",
            "Epoch: 21, Loss: 0.003420398337766528 Updates: 4/1000, Avg Grad: 0.4052562713623047, Threshold: 0.3232404887676239\n",
            "Epoch: 21, Loss: 0.005629164166748524 Updates: 13/2000, Avg Grad: 0.08425968140363693, Threshold: 0.09268565475940704\n",
            "Epoch: 21, Loss: 0.00962691381573677 Updates: 22/3000, Avg Grad: 0.02914186753332615, Threshold: 0.0320560559630394\n",
            "Epoch: 21, Loss: 0.006134297698736191 Updates: 30/4000, Avg Grad: 0.021383874118328094, Threshold: 0.023522261530160904\n",
            "Epoch: 21, Loss: 0.03894300386309624 Updates: 38/5000, Avg Grad: 0.04749924689531326, Threshold: 0.05224917456507683\n",
            "Epoch: 21, Loss: 0.013140980154275894 Updates: 47/6000, Avg Grad: 0.029956495389342308, Threshold: 0.032952144742012024\n",
            "Epoch: 21, Loss: 0.010278747417032719 Updates: 56/7000, Avg Grad: 0.023581327870488167, Threshold: 0.018847236409783363\n",
            "Epoch: 21, Loss: 0.008906982839107513 Updates: 62/8000, Avg Grad: 0.025324180722236633, Threshold: 0.024199897423386574\n",
            "Epoch: 21, Loss: 0.018046366050839424 Updates: 70/9000, Avg Grad: 0.027004336938261986, Threshold: 0.019646568223834038\n",
            "Epoch 20 iteration 0 Loss: 0.369 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.627 | Acc: 69.307% (70/101)\n",
            "Epoch 20 iteration 200 Loss: 0.603 | Acc: 72.637% (146/201)\n",
            "Epoch 20 iteration 300 Loss: 0.586 | Acc: 73.422% (221/301)\n",
            "Epoch 20 iteration 400 Loss: 0.587 | Acc: 72.070% (289/401)\n",
            "Test accuracy: 0.7206982374191284\n",
            "Epoch: 21, Loss: 0.003950003534555435 Updates: 78/10000, Avg Grad: 0.016440236940979958, Threshold: 0.02312728576362133\n",
            "Epoch: 21, Loss: 0.018755536526441574 Updates: 88/11000, Avg Grad: 0.01441957801580429, Threshold: 0.01586153544485569\n",
            "Epoch: 21, Loss: 0.0077803111635148525 Updates: 94/12000, Avg Grad: 0.0180308036506176, Threshold: 0.014445899985730648\n",
            "Epoch: 21, Loss: 0.015248030424118042 Updates: 100/13000, Avg Grad: 0.05684085935354233, Threshold: 0.06699718534946442\n",
            "Epoch: 21, Loss: 0.003784029744565487 Updates: 108/14000, Avg Grad: 0.01684022881090641, Threshold: 0.018404318019747734\n",
            "Epoch: 21, Loss: 0.0081282714381814 Updates: 118/15000, Avg Grad: 0.0201458390802145, Threshold: 0.021723885089159012\n",
            "Epoch: 21, Loss: 0.006338436622172594 Updates: 126/16000, Avg Grad: 0.07182540744543076, Threshold: 0.07657036930322647\n",
            "Epoch: 21, Loss: 0.01498330570757389 Updates: 134/17000, Avg Grad: 0.0337996631860733, Threshold: 0.03568282723426819\n",
            "Epoch: 21, Loss: 0.014771556481719017 Updates: 142/18000, Avg Grad: 0.014379856176674366, Threshold: 0.015055550262331963\n",
            "Epoch: 21, Loss: 0.015670359134674072 Updates: 152/19000, Avg Grad: 0.036603424698114395, Threshold: 0.03007766604423523\n",
            "Epoch 20 iteration 0 Loss: 0.351 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.649 | Acc: 66.337% (67/101)\n",
            "Epoch 20 iteration 200 Loss: 0.601 | Acc: 67.662% (136/201)\n",
            "Epoch 20 iteration 300 Loss: 0.612 | Acc: 67.110% (202/301)\n",
            "Epoch 20 iteration 400 Loss: 0.625 | Acc: 67.082% (269/401)\n",
            "Test accuracy: 0.6708229184150696\n",
            "Epoch: 21, Loss: 0.008992536924779415 Updates: 159/20000, Avg Grad: 0.02208026498556137, Threshold: 0.03149912878870964\n",
            "Epoch: 21, Loss: 0.006362241692841053 Updates: 168/21000, Avg Grad: 0.02054622955620289, Threshold: 0.021125376224517822\n",
            "Epoch: 21, Loss: 0.005575945600867271 Updates: 177/22000, Avg Grad: 0.017854727804660797, Threshold: 0.01827918365597725\n",
            "Epoch: 21, Loss: 0.005875030532479286 Updates: 186/23000, Avg Grad: 0.010619005188345909, Threshold: 0.010831909254193306\n",
            "Epoch: 21, Loss: 0.007791957817971706 Updates: 196/24000, Avg Grad: 0.057487163692712784, Threshold: 0.027775414288043976\n",
            "Epoch: 21, Loss: 0.015152467414736748 Updates: 204/25000, Avg Grad: 0.034355055540800095, Threshold: 0.034844979643821716\n",
            "Epoch: 21, Loss: 0.01221664808690548 Updates: 214/26000, Avg Grad: 0.07729679346084595, Threshold: 0.03101780079305172\n",
            "Epoch: 21, Loss: 0.004473511129617691 Updates: 223/27000, Avg Grad: 0.03314422443509102, Threshold: 0.01477575022727251\n",
            "Epoch: 21, Loss: 0.004602875094860792 Updates: 232/28000, Avg Grad: 0.04915314167737961, Threshold: 0.02083056978881359\n",
            "Epoch: 21, Loss: 0.020060008391737938 Updates: 241/29000, Avg Grad: 0.013191130943596363, Threshold: 0.013286301866173744\n",
            "Epoch 20 iteration 0 Loss: 0.285 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.554 | Acc: 70.297% (71/101)\n",
            "Epoch 20 iteration 200 Loss: 0.541 | Acc: 74.627% (150/201)\n",
            "Epoch 20 iteration 300 Loss: 0.557 | Acc: 72.757% (219/301)\n",
            "Epoch 20 iteration 400 Loss: 0.567 | Acc: 71.322% (286/401)\n",
            "Test accuracy: 0.7132169604301453\n",
            "Epoch: 21, Loss: 0.0036932560615241528 Updates: 251/30000, Avg Grad: 0.04292761906981468, Threshold: 0.0162599328905344\n",
            "Epoch: 21, Loss: 0.02379772439599037 Updates: 259/31000, Avg Grad: 0.025005357339978218, Threshold: 0.03077518381178379\n",
            "Epoch: 21, Loss: 0.013826319947838783 Updates: 267/32000, Avg Grad: 0.06096463277935982, Threshold: 0.018485555425286293\n",
            "Epoch 21 iteration 0 Loss: 0.481 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.622 | Acc: 66.337% (67/101)\n",
            "Epoch 21 iteration 200 Loss: 0.601 | Acc: 68.657% (138/201)\n",
            "Epoch 21 iteration 300 Loss: 0.615 | Acc: 68.106% (205/301)\n",
            "Epoch 21 iteration 400 Loss: 0.608 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 22, Loss: 1.9316465854644775 Updates: 0/0, Avg Grad: 0.3014533817768097, Threshold: 0.3315987288951874\n",
            "Epoch: 22, Loss: 0.05634785071015358 Updates: 7/1000, Avg Grad: 0.024925146251916885, Threshold: 0.03169093281030655\n",
            "Epoch: 22, Loss: 0.017838135361671448 Updates: 16/2000, Avg Grad: 0.029983656480908394, Threshold: 0.02703118324279785\n",
            "Epoch: 22, Loss: 0.012432501651346684 Updates: 25/3000, Avg Grad: 0.025579113513231277, Threshold: 0.028137026354670525\n",
            "Epoch: 22, Loss: 0.019570451229810715 Updates: 35/4000, Avg Grad: 0.05296528711915016, Threshold: 0.03571765869855881\n",
            "Epoch: 22, Loss: 0.006252046674489975 Updates: 44/5000, Avg Grad: 0.03190358355641365, Threshold: 0.03466808423399925\n",
            "Epoch: 22, Loss: 0.0025933184660971165 Updates: 54/6000, Avg Grad: 0.06660404056310654, Threshold: 0.07326444983482361\n",
            "Epoch: 22, Loss: 0.008845645934343338 Updates: 63/7000, Avg Grad: 0.04199513420462608, Threshold: 0.0461946502327919\n",
            "Epoch: 22, Loss: 0.021400783210992813 Updates: 70/8000, Avg Grad: 0.012085966765880585, Threshold: 0.013294563628733158\n",
            "Epoch: 22, Loss: 0.022271733731031418 Updates: 80/9000, Avg Grad: 0.046172093600034714, Threshold: 0.014873070642352104\n",
            "Epoch 21 iteration 0 Loss: 0.262 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.606 | Acc: 70.297% (71/101)\n",
            "Epoch 21 iteration 200 Loss: 0.608 | Acc: 69.154% (139/201)\n",
            "Epoch 21 iteration 300 Loss: 0.607 | Acc: 69.103% (208/301)\n",
            "Epoch 21 iteration 400 Loss: 0.609 | Acc: 68.329% (274/401)\n",
            "Test accuracy: 0.6832917928695679\n",
            "Epoch: 22, Loss: 0.003595333080738783 Updates: 89/10000, Avg Grad: 0.018762484192848206, Threshold: 0.020638732239603996\n",
            "Epoch: 22, Loss: 0.014133925549685955 Updates: 98/11000, Avg Grad: 0.047924209386110306, Threshold: 0.044802818447351456\n",
            "Epoch: 22, Loss: 0.026612479239702225 Updates: 106/12000, Avg Grad: 0.07425562292337418, Threshold: 0.05557934194803238\n",
            "Epoch: 22, Loss: 0.01862967200577259 Updates: 115/13000, Avg Grad: 0.016765601933002472, Threshold: 0.01844216324388981\n",
            "Epoch: 22, Loss: 0.022572407498955727 Updates: 125/14000, Avg Grad: 0.03853246569633484, Threshold: 0.030588051304221153\n",
            "Epoch: 22, Loss: 0.02148601971566677 Updates: 134/15000, Avg Grad: 0.024729004129767418, Threshold: 0.02079806849360466\n",
            "Epoch: 22, Loss: 0.007322028279304504 Updates: 143/16000, Avg Grad: 0.07298819720745087, Threshold: 0.042363546788692474\n",
            "Epoch: 22, Loss: 0.037317998707294464 Updates: 150/17000, Avg Grad: 0.02671371027827263, Threshold: 0.028202077373862267\n",
            "Epoch: 22, Loss: 0.004191664047539234 Updates: 158/18000, Avg Grad: 0.011474791914224625, Threshold: 0.012013980187475681\n",
            "Epoch: 22, Loss: 0.0124158700928092 Updates: 168/19000, Avg Grad: 0.024562528356909752, Threshold: 0.024027075618505478\n",
            "Epoch 21 iteration 0 Loss: 0.395 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.618 | Acc: 67.327% (68/101)\n",
            "Epoch 21 iteration 200 Loss: 0.597 | Acc: 70.149% (141/201)\n",
            "Epoch 21 iteration 300 Loss: 0.596 | Acc: 68.771% (207/301)\n",
            "Epoch 21 iteration 400 Loss: 0.586 | Acc: 69.576% (279/401)\n",
            "Test accuracy: 0.6957606077194214\n",
            "Epoch: 22, Loss: 0.011327862739562988 Updates: 176/20000, Avg Grad: 0.02407069131731987, Threshold: 0.04338131099939346\n",
            "Epoch: 22, Loss: 0.009829055517911911 Updates: 186/21000, Avg Grad: 0.04006998986005783, Threshold: 0.012012348510324955\n",
            "Epoch: 22, Loss: 0.007372862659394741 Updates: 195/22000, Avg Grad: 0.06836964935064316, Threshold: 0.026303770020604134\n",
            "Epoch: 22, Loss: 0.027844395488500595 Updates: 204/23000, Avg Grad: 0.06648992747068405, Threshold: 0.02002906985580921\n",
            "Epoch: 22, Loss: 0.007356538902968168 Updates: 213/24000, Avg Grad: 0.01273017656058073, Threshold: 0.012945431284606457\n",
            "Epoch: 22, Loss: 0.01072651892900467 Updates: 223/25000, Avg Grad: 0.03409203886985779, Threshold: 0.03159843012690544\n",
            "Epoch: 22, Loss: 0.019374532625079155 Updates: 232/26000, Avg Grad: 0.012240375392138958, Threshold: 0.01238759234547615\n",
            "Epoch: 22, Loss: 0.008879081346094608 Updates: 239/27000, Avg Grad: 0.08252499252557755, Threshold: 0.0471675731241703\n",
            "Epoch: 22, Loss: 0.01011649239808321 Updates: 247/28000, Avg Grad: 0.03570453077554703, Threshold: 0.02451176568865776\n",
            "Epoch: 22, Loss: 0.035233207046985626 Updates: 255/29000, Avg Grad: 0.034105125814676285, Threshold: 0.03435118868947029\n",
            "Epoch 21 iteration 0 Loss: 0.198 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.601 | Acc: 69.307% (70/101)\n",
            "Epoch 21 iteration 200 Loss: 0.604 | Acc: 67.662% (136/201)\n",
            "Epoch 21 iteration 300 Loss: 0.603 | Acc: 69.103% (208/301)\n",
            "Epoch 21 iteration 400 Loss: 0.590 | Acc: 70.075% (281/401)\n",
            "Test accuracy: 0.7007481455802917\n",
            "Epoch: 22, Loss: 0.014861012808978558 Updates: 264/30000, Avg Grad: 0.01290740817785263, Threshold: 0.012985947541892529\n",
            "Epoch: 22, Loss: 0.04992891475558281 Updates: 273/31000, Avg Grad: 0.017457107082009315, Threshold: 0.017546692863106728\n",
            "Epoch: 22, Loss: 0.0024314632173627615 Updates: 282/32000, Avg Grad: 0.02555815875530243, Threshold: 0.025668775662779808\n",
            "Epoch 22 iteration 0 Loss: 0.567 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.670 | Acc: 65.347% (66/101)\n",
            "Epoch 22 iteration 200 Loss: 0.637 | Acc: 66.169% (133/201)\n",
            "Epoch 22 iteration 300 Loss: 0.612 | Acc: 68.771% (207/301)\n",
            "Epoch 22 iteration 400 Loss: 0.608 | Acc: 68.329% (274/401)\n",
            "Test accuracy: 0.6832917928695679\n",
            "Epoch: 23, Loss: 0.5259525179862976 Updates: 0/0, Avg Grad: 0.15922367572784424, Threshold: 0.17514604330062866\n",
            "Epoch: 23, Loss: 0.003993735183030367 Updates: 6/1000, Avg Grad: 0.016472214832901955, Threshold: 0.012352771125733852\n",
            "Epoch: 23, Loss: 0.0027518882416188717 Updates: 14/2000, Avg Grad: 0.06109218671917915, Threshold: 0.023982642218470573\n",
            "Epoch: 23, Loss: 0.003947439603507519 Updates: 24/3000, Avg Grad: 0.07593746483325958, Threshold: 0.0534517839550972\n",
            "Epoch: 23, Loss: 0.014921200461685658 Updates: 32/4000, Avg Grad: 0.039225321263074875, Threshold: 0.04152815043926239\n",
            "Epoch: 23, Loss: 0.011826791800558567 Updates: 40/5000, Avg Grad: 0.019104499369859695, Threshold: 0.024233121424913406\n",
            "Epoch: 23, Loss: 0.021625032648444176 Updates: 48/6000, Avg Grad: 0.01851005293428898, Threshold: 0.02036105841398239\n",
            "Epoch: 23, Loss: 0.0042831432074308395 Updates: 56/7000, Avg Grad: 0.02188880741596222, Threshold: 0.02407768927514553\n",
            "Epoch: 23, Loss: 0.021089237183332443 Updates: 64/8000, Avg Grad: 0.03186063468456268, Threshold: 0.03704143688082695\n",
            "Epoch: 23, Loss: 0.007570283953100443 Updates: 73/9000, Avg Grad: 0.04916497692465782, Threshold: 0.05408147722482681\n",
            "Epoch 22 iteration 0 Loss: 0.965 | Acc: 0.000% (0/1)\n",
            "Epoch 22 iteration 100 Loss: 0.578 | Acc: 69.307% (70/101)\n",
            "Epoch 22 iteration 200 Loss: 0.607 | Acc: 64.179% (129/201)\n",
            "Epoch 22 iteration 300 Loss: 0.572 | Acc: 68.771% (207/301)\n",
            "Epoch 22 iteration 400 Loss: 0.572 | Acc: 70.075% (281/401)\n",
            "Test accuracy: 0.7007481455802917\n",
            "Epoch: 23, Loss: 0.038713470101356506 Updates: 80/10000, Avg Grad: 0.03181169927120209, Threshold: 0.049173854291439056\n",
            "Epoch: 23, Loss: 0.007181322202086449 Updates: 86/11000, Avg Grad: 0.035389043390750885, Threshold: 0.048472415655851364\n",
            "Epoch: 23, Loss: 0.005191703327000141 Updates: 94/12000, Avg Grad: 0.03592819720506668, Threshold: 0.03952101618051529\n",
            "Epoch: 23, Loss: 0.002138052135705948 Updates: 104/13000, Avg Grad: 0.03701309114694595, Threshold: 0.027319401502609253\n",
            "Epoch: 23, Loss: 0.03130938857793808 Updates: 112/14000, Avg Grad: 0.0317055881023407, Threshold: 0.03465034440159798\n",
            "Epoch: 23, Loss: 0.031094996258616447 Updates: 121/15000, Avg Grad: 0.05041051283478737, Threshold: 0.05435922369360924\n",
            "Epoch: 23, Loss: 0.0025149285793304443 Updates: 131/16000, Avg Grad: 0.010809353552758694, Threshold: 0.011523446068167686\n",
            "Epoch: 23, Loss: 0.008462732657790184 Updates: 139/17000, Avg Grad: 0.021818356588482857, Threshold: 0.016036754474043846\n",
            "Epoch: 23, Loss: 0.006440396420657635 Updates: 147/18000, Avg Grad: 0.0632118508219719, Threshold: 0.041726093739271164\n",
            "Epoch: 23, Loss: 0.01613527722656727 Updates: 154/19000, Avg Grad: 0.04342766851186752, Threshold: 0.04514867812395096\n",
            "Epoch 22 iteration 0 Loss: 0.729 | Acc: 0.000% (0/1)\n",
            "Epoch 22 iteration 100 Loss: 0.649 | Acc: 64.356% (65/101)\n",
            "Epoch 22 iteration 200 Loss: 0.605 | Acc: 68.159% (137/201)\n",
            "Epoch 22 iteration 300 Loss: 0.611 | Acc: 67.774% (204/301)\n",
            "Epoch 22 iteration 400 Loss: 0.608 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 23, Loss: 0.005571873392909765 Updates: 162/20000, Avg Grad: 0.029098087921738625, Threshold: 0.03582659736275673\n",
            "Epoch: 23, Loss: 0.021020997315645218 Updates: 172/21000, Avg Grad: 0.02754371240735054, Threshold: 0.018124429509043694\n",
            "Epoch: 23, Loss: 0.010562220588326454 Updates: 180/22000, Avg Grad: 0.02682509459555149, Threshold: 0.027462799102067947\n",
            "Epoch: 23, Loss: 0.0039451224729418755 Updates: 190/23000, Avg Grad: 0.04895763844251633, Threshold: 0.02091638743877411\n",
            "Epoch: 23, Loss: 0.01676253043115139 Updates: 199/24000, Avg Grad: 0.08064358681440353, Threshold: 0.04328007623553276\n",
            "Epoch: 23, Loss: 0.01540638878941536 Updates: 206/25000, Avg Grad: 0.04225366190075874, Threshold: 0.015146749094128609\n",
            "Epoch: 23, Loss: 0.034216832369565964 Updates: 215/26000, Avg Grad: 0.04491075500845909, Threshold: 0.045450903475284576\n",
            "Epoch: 23, Loss: 0.005678641144186258 Updates: 224/27000, Avg Grad: 0.02314719185233116, Threshold: 0.014705436304211617\n",
            "Epoch: 23, Loss: 0.02406453900039196 Updates: 229/28000, Avg Grad: 0.014842672273516655, Threshold: 0.01496964693069458\n",
            "Epoch: 23, Loss: 0.005575633142143488 Updates: 236/29000, Avg Grad: 0.015008781105279922, Threshold: 0.015117065981030464\n",
            "Epoch 22 iteration 0 Loss: 0.166 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.529 | Acc: 77.228% (78/101)\n",
            "Epoch 22 iteration 200 Loss: 0.570 | Acc: 71.642% (144/201)\n",
            "Epoch 22 iteration 300 Loss: 0.576 | Acc: 71.096% (214/301)\n",
            "Epoch 22 iteration 400 Loss: 0.579 | Acc: 70.823% (284/401)\n",
            "Test accuracy: 0.7082294225692749\n",
            "Epoch: 23, Loss: 0.008733216673135757 Updates: 244/30000, Avg Grad: 0.021232858300209045, Threshold: 0.021349867805838585\n",
            "Epoch: 23, Loss: 0.037313319742679596 Updates: 253/31000, Avg Grad: 0.022828329354524612, Threshold: 0.022945478558540344\n",
            "Epoch: 23, Loss: 0.0073563046753406525 Updates: 262/32000, Avg Grad: 0.024606779217720032, Threshold: 0.02471327781677246\n",
            "Epoch 23 iteration 0 Loss: 0.393 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.588 | Acc: 74.257% (75/101)\n",
            "Epoch 23 iteration 200 Loss: 0.556 | Acc: 73.632% (148/201)\n",
            "Epoch 23 iteration 300 Loss: 0.561 | Acc: 73.422% (221/301)\n",
            "Epoch 23 iteration 400 Loss: 0.580 | Acc: 71.571% (287/401)\n",
            "Test accuracy: 0.7157106995582581\n",
            "Epoch: 24, Loss: 1.0751937627792358 Updates: 0/0, Avg Grad: 0.2446773201227188, Threshold: 0.26914507150650024\n",
            "Epoch: 24, Loss: 0.00339933461509645 Updates: 9/1000, Avg Grad: 0.03422712907195091, Threshold: 0.03764984384179115\n",
            "Epoch: 24, Loss: 0.008651385083794594 Updates: 17/2000, Avg Grad: 0.02658560313284397, Threshold: 0.029244164004921913\n",
            "Epoch: 24, Loss: 0.012132246047258377 Updates: 26/3000, Avg Grad: 0.025877797976136208, Threshold: 0.020643213763833046\n",
            "Epoch: 24, Loss: 0.018505774438381195 Updates: 33/4000, Avg Grad: 0.04078223556280136, Threshold: 0.04244530200958252\n",
            "Epoch: 24, Loss: 0.02034398540854454 Updates: 42/5000, Avg Grad: 0.02339019812643528, Threshold: 0.02055443823337555\n",
            "Epoch: 24, Loss: 0.01669560745358467 Updates: 51/6000, Avg Grad: 0.04650058597326279, Threshold: 0.03211451321840286\n",
            "Epoch: 24, Loss: 0.009278904646635056 Updates: 59/7000, Avg Grad: 0.017980411648750305, Threshold: 0.012808415107429028\n",
            "Epoch: 24, Loss: 0.012620317749679089 Updates: 69/8000, Avg Grad: 0.03482140228152275, Threshold: 0.031681448221206665\n",
            "Epoch: 24, Loss: 0.010422147810459137 Updates: 78/9000, Avg Grad: 0.015971088781952858, Threshold: 0.017568198963999748\n",
            "Epoch 23 iteration 0 Loss: 0.062 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.620 | Acc: 67.327% (68/101)\n",
            "Epoch 23 iteration 200 Loss: 0.619 | Acc: 67.662% (136/201)\n",
            "Epoch 23 iteration 300 Loss: 0.618 | Acc: 66.777% (201/301)\n",
            "Epoch 23 iteration 400 Loss: 0.609 | Acc: 67.332% (270/401)\n",
            "Test accuracy: 0.6733167171478271\n",
            "Epoch: 24, Loss: 0.008319348096847534 Updates: 88/10000, Avg Grad: 0.04280703514814377, Threshold: 0.04168026149272919\n",
            "Epoch: 24, Loss: 0.01706290990114212 Updates: 95/11000, Avg Grad: 0.015926532447338104, Threshold: 0.017519185319542885\n",
            "Epoch: 24, Loss: 0.01945352554321289 Updates: 103/12000, Avg Grad: 0.040578294545412064, Threshold: 0.02627982385456562\n",
            "Epoch: 24, Loss: 0.0159497931599617 Updates: 111/13000, Avg Grad: 0.07369298487901688, Threshold: 0.03933541849255562\n",
            "Epoch: 24, Loss: 0.02845514379441738 Updates: 118/14000, Avg Grad: 0.025298243388533592, Threshold: 0.02683267369866371\n",
            "Epoch: 24, Loss: 0.003865781705826521 Updates: 127/15000, Avg Grad: 0.012132172472774982, Threshold: 0.03515373542904854\n",
            "Epoch: 24, Loss: 0.01416008360683918 Updates: 134/16000, Avg Grad: 0.04544749855995178, Threshold: 0.03523397073149681\n",
            "Epoch: 24, Loss: 0.007811340037733316 Updates: 141/17000, Avg Grad: 0.013456350192427635, Threshold: 0.014206076972186565\n",
            "Epoch: 24, Loss: 0.0015886059263721108 Updates: 150/18000, Avg Grad: 0.03984939679503441, Threshold: 0.031234897673130035\n",
            "Epoch: 24, Loss: 0.017887547612190247 Updates: 157/19000, Avg Grad: 0.014819729141891003, Threshold: 0.015407024882733822\n",
            "Epoch 23 iteration 0 Loss: 0.228 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.583 | Acc: 64.356% (65/101)\n",
            "Epoch 23 iteration 200 Loss: 0.633 | Acc: 60.697% (122/201)\n",
            "Epoch 23 iteration 300 Loss: 0.636 | Acc: 62.126% (187/301)\n",
            "Epoch 23 iteration 400 Loss: 0.606 | Acc: 65.835% (264/401)\n",
            "Test accuracy: 0.6583541035652161\n",
            "Epoch: 24, Loss: 0.005068610422313213 Updates: 166/20000, Avg Grad: 0.02469724416732788, Threshold: 0.02358548529446125\n",
            "Epoch: 24, Loss: 0.005146550480276346 Updates: 172/21000, Avg Grad: 0.04041853919625282, Threshold: 0.0454571470618248\n",
            "Epoch: 24, Loss: 0.009049791842699051 Updates: 178/22000, Avg Grad: 0.049570485949516296, Threshold: 0.05074891075491905\n",
            "Epoch: 24, Loss: 0.031121520325541496 Updates: 188/23000, Avg Grad: 0.024026086553931236, Threshold: 0.02450779266655445\n",
            "Epoch: 24, Loss: 0.0048211426474153996 Updates: 197/24000, Avg Grad: 0.025188429281115532, Threshold: 0.023795217275619507\n",
            "Epoch: 24, Loss: 0.004493567161262035 Updates: 206/25000, Avg Grad: 0.026181790977716446, Threshold: 0.02655516006052494\n",
            "Epoch: 24, Loss: 0.021732812747359276 Updates: 215/26000, Avg Grad: 0.01602192036807537, Threshold: 0.01115894503891468\n",
            "Epoch: 24, Loss: 0.011407873593270779 Updates: 225/27000, Avg Grad: 0.025185540318489075, Threshold: 0.020618297159671783\n",
            "Epoch: 24, Loss: 0.0033636828884482384 Updates: 234/28000, Avg Grad: 0.02298903465270996, Threshold: 0.019173789769411087\n",
            "Epoch: 24, Loss: 0.01015663705766201 Updates: 243/29000, Avg Grad: 0.025864509865641594, Threshold: 0.026051117107272148\n",
            "Epoch 23 iteration 0 Loss: 0.288 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.691 | Acc: 58.416% (59/101)\n",
            "Epoch 23 iteration 200 Loss: 0.629 | Acc: 63.682% (128/201)\n",
            "Epoch 23 iteration 300 Loss: 0.601 | Acc: 68.106% (205/301)\n",
            "Epoch 23 iteration 400 Loss: 0.603 | Acc: 67.830% (272/401)\n",
            "Test accuracy: 0.6783042550086975\n",
            "Epoch: 24, Loss: 0.012998884543776512 Updates: 253/30000, Avg Grad: 0.023722486570477486, Threshold: 0.02386683225631714\n",
            "Epoch: 24, Loss: 0.04194468632340431 Updates: 263/31000, Avg Grad: 0.015366251580417156, Threshold: 0.015445106662809849\n",
            "Epoch: 24, Loss: 0.011345703154802322 Updates: 273/32000, Avg Grad: 0.02714354917407036, Threshold: 0.027261026203632355\n",
            "Epoch 24 iteration 0 Loss: 1.984 | Acc: 0.000% (0/1)\n",
            "Epoch 24 iteration 100 Loss: 0.669 | Acc: 63.366% (64/101)\n",
            "Epoch 24 iteration 200 Loss: 0.623 | Acc: 65.672% (132/201)\n",
            "Epoch 24 iteration 300 Loss: 0.612 | Acc: 68.439% (206/301)\n",
            "Epoch 24 iteration 400 Loss: 0.618 | Acc: 68.579% (275/401)\n",
            "Test accuracy: 0.6857855319976807\n",
            "Epoch: 25, Loss: 0.7236747145652771 Updates: 0/0, Avg Grad: 0.19777099788188934, Threshold: 0.21754810214042664\n",
            "Epoch: 25, Loss: 0.00380015023984015 Updates: 9/1000, Avg Grad: 0.12912987172603607, Threshold: 0.08273354917764664\n",
            "Epoch: 25, Loss: 0.01177882682532072 Updates: 18/2000, Avg Grad: 0.05353543162345886, Threshold: 0.05888897553086281\n",
            "Epoch: 25, Loss: 0.01138229575008154 Updates: 27/3000, Avg Grad: 0.03831590339541435, Threshold: 0.04214749485254288\n",
            "Epoch: 25, Loss: 0.011522709392011166 Updates: 36/4000, Avg Grad: 0.018507806584239006, Threshold: 0.01616401970386505\n",
            "Epoch: 25, Loss: 0.004450992215424776 Updates: 45/5000, Avg Grad: 0.03668021038174629, Threshold: 0.04034823179244995\n",
            "Epoch: 25, Loss: 0.009268608875572681 Updates: 54/6000, Avg Grad: 0.04333791136741638, Threshold: 0.04767170175909996\n",
            "Epoch: 25, Loss: 0.01035501342266798 Updates: 63/7000, Avg Grad: 0.029711155220866203, Threshold: 0.023641401901841164\n",
            "Epoch: 25, Loss: 0.007251144852489233 Updates: 71/8000, Avg Grad: 0.028020313009619713, Threshold: 0.03082234412431717\n",
            "Epoch: 25, Loss: 0.01820940338075161 Updates: 79/9000, Avg Grad: 0.049065642058849335, Threshold: 0.05397220700979233\n",
            "Epoch 24 iteration 0 Loss: 0.511 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.557 | Acc: 69.307% (70/101)\n",
            "Epoch 24 iteration 200 Loss: 0.592 | Acc: 67.164% (135/201)\n",
            "Epoch 24 iteration 300 Loss: 0.607 | Acc: 67.774% (204/301)\n",
            "Epoch 24 iteration 400 Loss: 0.591 | Acc: 69.077% (277/401)\n",
            "Test accuracy: 0.690773069858551\n",
            "Epoch: 25, Loss: 0.009492773562669754 Updates: 87/10000, Avg Grad: 0.05852491781115532, Threshold: 0.02853233553469181\n",
            "Epoch: 25, Loss: 0.019322525709867477 Updates: 93/11000, Avg Grad: 0.035522960126399994, Threshold: 0.039075255393981934\n",
            "Epoch: 25, Loss: 0.025935906916856766 Updates: 100/12000, Avg Grad: 0.030461713671684265, Threshold: 0.03350788727402687\n",
            "Epoch: 25, Loss: 0.026251716539263725 Updates: 109/13000, Avg Grad: 0.06353148072957993, Threshold: 0.032758455723524094\n",
            "Epoch: 25, Loss: 0.007560116704553366 Updates: 115/14000, Avg Grad: 0.030157525092363358, Threshold: 0.0329585000872612\n",
            "Epoch: 25, Loss: 0.008031103760004044 Updates: 123/15000, Avg Grad: 0.04416989907622337, Threshold: 0.04140070080757141\n",
            "Epoch: 25, Loss: 0.005345460958778858 Updates: 132/16000, Avg Grad: 0.020290782675147057, Threshold: 0.019611582159996033\n",
            "Epoch: 25, Loss: 0.0022568409331142902 Updates: 141/17000, Avg Grad: 0.02978835254907608, Threshold: 0.03144802525639534\n",
            "Epoch: 25, Loss: 0.006984637584537268 Updates: 149/18000, Avg Grad: 0.029396889731287956, Threshold: 0.015820827335119247\n",
            "Epoch: 25, Loss: 0.018743200227618217 Updates: 159/19000, Avg Grad: 0.04197342321276665, Threshold: 0.009863157756626606\n",
            "Epoch 24 iteration 0 Loss: 0.150 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.575 | Acc: 73.267% (74/101)\n",
            "Epoch 24 iteration 200 Loss: 0.574 | Acc: 73.632% (148/201)\n",
            "Epoch 24 iteration 300 Loss: 0.577 | Acc: 72.757% (219/301)\n",
            "Epoch 24 iteration 400 Loss: 0.577 | Acc: 71.820% (288/401)\n",
            "Test accuracy: 0.7182044982910156\n",
            "Epoch: 25, Loss: 0.005756493657827377 Updates: 167/20000, Avg Grad: 0.02764010988175869, Threshold: 0.01367911510169506\n",
            "Epoch: 25, Loss: 0.027903318405151367 Updates: 176/21000, Avg Grad: 0.022259093821048737, Threshold: 0.022886522114276886\n",
            "Epoch: 25, Loss: 0.0033624477218836546 Updates: 186/22000, Avg Grad: 0.04316263273358345, Threshold: 0.033366139978170395\n",
            "Epoch: 25, Loss: 0.006371356081217527 Updates: 192/23000, Avg Grad: 0.043661851435899734, Threshold: 0.03974040597677231\n",
            "Epoch: 25, Loss: 0.00503157963976264 Updates: 199/24000, Avg Grad: 0.017441170290112495, Threshold: 0.017736082896590233\n",
            "Epoch: 25, Loss: 0.009749740362167358 Updates: 208/25000, Avg Grad: 0.03665801137685776, Threshold: 0.018617309629917145\n",
            "Epoch: 25, Loss: 0.0025817090645432472 Updates: 216/26000, Avg Grad: 0.04247023165225983, Threshold: 0.04298102855682373\n",
            "Epoch: 25, Loss: 0.014277901500463486 Updates: 225/27000, Avg Grad: 0.05275198817253113, Threshold: 0.0292233694344759\n",
            "Epoch: 25, Loss: 0.0327882394194603 Updates: 233/28000, Avg Grad: 0.021976975724101067, Threshold: 0.022164981812238693\n",
            "Epoch: 25, Loss: 0.0035220503341406584 Updates: 243/29000, Avg Grad: 0.031213749200105667, Threshold: 0.018038569018244743\n",
            "Epoch 24 iteration 0 Loss: 0.614 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.595 | Acc: 69.307% (70/101)\n",
            "Epoch 24 iteration 200 Loss: 0.596 | Acc: 69.154% (139/201)\n",
            "Epoch 24 iteration 300 Loss: 0.613 | Acc: 68.439% (206/301)\n",
            "Epoch 24 iteration 400 Loss: 0.627 | Acc: 67.581% (271/401)\n",
            "Test accuracy: 0.6758104562759399\n",
            "Epoch: 25, Loss: 0.009852731600403786 Updates: 248/30000, Avg Grad: 0.024857308715581894, Threshold: 0.025008561089634895\n",
            "Epoch: 25, Loss: 0.005958764348179102 Updates: 257/31000, Avg Grad: 0.030919084325432777, Threshold: 0.01701602339744568\n",
            "Epoch: 25, Loss: 0.00634092977270484 Updates: 267/32000, Avg Grad: 0.041767071932554245, Threshold: 0.013703816570341587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(network_tester(net, testloader, 400, 0, True).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRrMkk_ykaUa",
        "outputId": "f11423db-05e0-4ddf-f50e-9713c0595f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 0.297 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 0.598 | Acc: 69.307% (70/101)\n",
            "Epoch 0 iteration 200 Loss: 0.610 | Acc: 67.164% (135/201)\n",
            "Epoch 0 iteration 300 Loss: 0.610 | Acc: 67.442% (203/301)\n",
            "Epoch 0 iteration 400 Loss: 0.612 | Acc: 66.085% (265/401)\n",
            "0.6608479022979736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wire_R = [(i+1)*2 for i in range(40)]\n",
        "p_stuck = [0.01*(i) for i in range(40)]\n",
        "R_std = [1e2*i for i in range(1, 101, 4)]\n",
        "\n",
        "\n",
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b\n",
        "\n",
        "err_org = network_tester(net, testloader, 400, 0, False).item()\n",
        "print(err_org)\n",
        "\n",
        "result = []\n",
        "\n",
        "for std in R_std:\n",
        "    result_wire = []\n",
        "    for var in p_stuck:\n",
        "        device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": std,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": std*10,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": var,\n",
        "                 \"p_stuck_off\": var}\n",
        "        crb_new = crossbar(device_params)\n",
        "\n",
        "        net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "        net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "        net.fc1.cb = crb_new\n",
        "        net.fc1.remap()\n",
        "\n",
        "        err = network_tester(net, testloader, 400, 0, False).item()\n",
        "        result_wire.append(err/err_org)\n",
        "\n",
        "    result.append(result_wire)\n",
        "\n",
        "print(result)\n",
        "with open(\"result_non_ideal_RNN2.txt\", 'w') as writefile:\n",
        "    writefile.write(str(result))\n",
        "\n",
        "ax = sns.heatmap(result, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "DWS7xo2dX4Gr",
        "outputId": "969e6354-e033-4af0-bb97-ed796b4c17e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5112219452857971\n",
            "[[1.063414660877305, 1.0048779970247925, 1.029268331926256, 1.0, 1.0243902183089633, 1.0878048791862682, 1.0048779970247925, 1.0195122212841707, 1.0, 1.0439024395931342, 0.9902438893579146, 1.0243902183089633, 1.0243902183089633, 1.0731707715193903, 1.1073171004704392, 1.0829268821614757, 1.0682926579020975, 1.0731707715193903, 1.092682876211061, 1.1121950974952317, 0.9512195050858232, 1.0341463289510489, 1.014634107666878, 1.1073171004704392, 1.1024389868531463, 1.0243902183089633, 1.063414660877305, 1.092682876211061, 1.1121950974952317, 1.0878048791862682, 1.0975609898283536, 1.063414660877305, 0.9951220029752075, 1.0, 1.0536585502352196, 1.029268331926256, 1.0439024395931342, 0.9609756157279086, 1.0975609898283536, 1.0439024395931342], [1.0195122212841707, 1.0243902183089633, 1.0585365472600121, 1.0585365472600121, 0.9756097816910366, 1.0780487685441829, 1.0975609898283536, 0.9951220029752075, 1.0097561106420854, 0.9463414497647804, 0.9756097816910366, 0.9951220029752075, 0.9804877787158293, 1.0048779970247925, 1.0585365472600121, 1.0097561106420854, 1.0585365472600121, 1.029268331926256, 0.9609756157279086, 1.0731707715193903, 1.0878048791862682, 1.0829268821614757, 1.0390244425683415, 0.9170731761347743, 1.0439024395931342, 0.9512195050858232, 1.092682876211061, 1.0, 1.0341463289510489, 1.0731707715193903, 1.0390244425683415, 1.0487804366179267, 1.0682926579020975, 1.0829268821614757, 1.0243902183089633, 0.9902438893579146, 1.092682876211061, 1.029268331926256, 1.0097561106420854, 1.0487804366179267], [1.063414660877305, 1.0878048791862682, 1.063414660877305, 1.1073171004704392, 1.0048779970247925, 0.985365892333122, 1.0975609898283536, 1.1512195400635732, 1.0780487685441829, 1.0195122212841707, 1.0048779970247925, 1.0536585502352196, 1.0048779970247925, 1.092682876211061, 1.0195122212841707, 1.0536585502352196, 1.092682876211061, 1.1073171004704392, 1.0780487685441829, 1.029268331926256, 1.0878048791862682, 1.0975609898283536, 1.0341463289510489, 1.0243902183089633, 0.936585339122695, 1.1121950974952317, 1.0439024395931342, 1.1512195400635732, 1.092682876211061, 1.0487804366179267, 0.9951220029752075, 1.12682932175461, 1.0487804366179267, 1.092682876211061, 1.1073171004704392, 0.9219512314558171, 1.0, 1.0439024395931342, 1.0878048791862682, 1.0097561106420854], [0.9121951208137317, 0.9170731761347743, 1.0536585502352196, 0.9609756157279086, 1.1804877553973292, 0.8829268471837256, 0.936585339122695, 1.1024389868531463, 0.9707317263699939, 1.0195122212841707, 0.9804877787158293, 0.9170731761347743, 0.907317065492689, 0.9268292867768598, 0.8536585152574695, 0.9463414497647804, 0.936585339122695, 0.8975609548506036, 0.9951220029752075, 1.0682926579020975, 1.0097561106420854, 0.9560975604068659, 0.9804877787158293, 1.0341463289510489, 1.0, 0.9317073420979025, 0.8243902416274634, 0.936585339122695, 0.985365892333122, 0.8634146258995549, 1.0682926579020975, 0.9756097816910366, 0.9804877787158293, 0.9219512314558171, 1.029268331926256, 0.9219512314558171, 0.9170731761347743, 1.029268331926256, 0.985365892333122, 0.8439024629116342], [0.9463414497647804, 0.8487804599364268, 0.8829268471837256, 0.8878049025047683, 0.8390244075905915, 0.9219512314558171, 0.9268292867768598, 0.9170731761347743, 0.9512195050858232, 0.8829268471837256, 0.8487804599364268, 0.9414633944437377, 0.8975609548506036, 0.8975609548506036, 0.907317065492689, 0.9902438893579146, 0.9512195050858232, 1.0341463289510489, 0.9658536710489513, 0.8731707365416402, 0.936585339122695, 0.8829268471837256, 0.8878049025047683, 0.8975609548506036, 0.9560975604068659, 0.8243902416274634, 0.9170731761347743, 0.8878049025047683, 0.9804877787158293, 0.9414633944437377, 0.8146341309853781, 0.8682926812205976, 0.936585339122695, 0.9609756157279086, 0.8536585152574695, 1.0, 0.9804877787158293, 0.9024390101716463, 0.8146341309853781, 0.8926828995295609], [0.8878049025047683, 0.9317073420979025, 0.8926828995295609, 0.8536585152574695, 0.9512195050858232, 0.936585339122695, 0.8926828995295609, 0.8731707365416402, 0.8829268471837256, 0.9609756157279086, 0.8536585152574695, 0.9609756157279086, 0.9560975604068659, 0.8926828995295609, 0.9756097816910366, 0.9317073420979025, 0.9463414497647804, 0.9219512314558171, 0.9121951208137317, 0.9024390101716463, 0.8634146258995549, 0.9268292867768598, 0.9268292867768598, 0.8829268471837256, 0.8682926812205976, 0.9024390101716463, 1.029268331926256, 0.9268292867768598, 0.9463414497647804, 0.8243902416274634, 0.8731707365416402, 0.8585365705785122, 0.9024390101716463, 0.9121951208137317, 0.8731707365416402, 0.8878049025047683, 1.014634107666878, 0.9121951208137317, 0.9317073420979025, 0.8878049025047683], [0.9512195050858232, 0.9804877787158293, 0.9512195050858232, 0.8878049025047683, 0.9268292867768598, 0.9951220029752075, 0.8634146258995549, 0.9609756157279086, 1.0, 0.936585339122695, 0.8926828995295609, 0.907317065492689, 0.9317073420979025, 0.9121951208137317, 0.9512195050858232, 0.9609756157279086, 0.8390244075905915, 0.8926828995295609, 0.9268292867768598, 0.9902438893579146, 0.9121951208137317, 0.9268292867768598, 0.936585339122695, 0.8829268471837256, 0.9951220029752075, 0.985365892333122, 0.8975609548506036, 0.9414633944437377, 0.8975609548506036, 0.9170731761347743, 0.8195121863064208, 0.8878049025047683, 0.936585339122695, 0.907317065492689, 0.9268292867768598, 0.8634146258995549, 0.8487804599364268, 0.9560975604068659, 0.8829268471837256, 0.9268292867768598], [0.8048780203432926, 0.9219512314558171, 0.8682926812205976, 0.8243902416274634, 0.8731707365416402, 0.9707317263699939, 0.9268292867768598, 0.9512195050858232, 0.8634146258995549, 0.8780487918626829, 0.8243902416274634, 0.8731707365416402, 0.8878049025047683, 0.9317073420979025, 0.9414633944437377, 0.8000000233185001, 0.8536585152574695, 0.9121951208137317, 0.8195121863064208, 0.9268292867768598, 0.8878049025047683, 0.9317073420979025, 0.8829268471837256, 0.9121951208137317, 0.8536585152574695, 0.8634146258995549, 0.9219512314558171, 0.9024390101716463, 0.9463414497647804, 0.9170731761347743, 0.8926828995295609, 0.9121951208137317, 1.014634107666878, 0.9804877787158293, 0.9219512314558171, 0.936585339122695, 0.9707317263699939, 0.8487804599364268, 0.9512195050858232, 0.8780487918626829], [0.8536585152574695, 0.8878049025047683, 0.8682926812205976, 0.8487804599364268, 0.9658536710489513, 0.9414633944437377, 0.8634146258995549, 0.9219512314558171, 0.9317073420979025, 1.0097561106420854, 0.9512195050858232, 0.9902438893579146, 0.8536585152574695, 0.9219512314558171, 0.8585365705785122, 0.8536585152574695, 0.9317073420979025, 0.9414633944437377, 0.9414633944437377, 0.907317065492689, 0.9658536710489513, 0.8390244075905915, 0.9414633944437377, 0.9170731761347743, 0.936585339122695, 0.8585365705785122, 0.9024390101716463, 0.9219512314558171, 0.9512195050858232, 0.8682926812205976, 0.907317065492689, 0.8585365705785122, 0.8487804599364268, 0.9951220029752075, 0.9560975604068659, 0.9658536710489513, 0.8780487918626829, 0.8536585152574695, 0.9658536710489513, 0.8975609548506036], [1.014634107666878, 0.9024390101716463, 0.936585339122695, 0.9512195050858232, 0.9024390101716463, 0.8341463522695488, 0.9902438893579146, 0.8926828995295609, 0.907317065492689, 0.8731707365416402, 0.9268292867768598, 0.8731707365416402, 0.8780487918626829, 0.9560975604068659, 0.9512195050858232, 0.785365857355372, 0.9268292867768598, 0.8829268471837256, 0.9658536710489513, 0.9317073420979025, 0.9219512314558171, 0.8292682969485061, 0.9658536710489513, 0.9658536710489513, 0.9512195050858232, 0.8829268471837256, 0.8829268471837256, 0.936585339122695, 0.9414633944437377, 0.9414633944437377, 0.9463414497647804, 0.9463414497647804, 0.9268292867768598, 0.8390244075905915, 0.9024390101716463, 0.9609756157279086, 0.8487804599364268, 0.8975609548506036, 0.9024390101716463, 0.9219512314558171], [0.9170731761347743, 0.8097560756643353, 0.8975609548506036, 0.9268292867768598, 0.9512195050858232, 0.9121951208137317, 0.9121951208137317, 0.8975609548506036, 0.9219512314558171, 0.9268292867768598, 0.8390244075905915, 0.936585339122695, 0.9170731761347743, 0.9463414497647804, 0.8878049025047683, 0.8829268471837256, 0.8780487918626829, 0.9463414497647804, 0.8829268471837256, 0.9024390101716463, 0.8878049025047683, 0.9170731761347743, 0.907317065492689, 0.8536585152574695, 0.9268292867768598, 0.9024390101716463, 0.8926828995295609, 0.8829268471837256, 0.9609756157279086, 0.8975609548506036, 0.8878049025047683, 0.907317065492689, 0.8682926812205976, 0.985365892333122, 0.8536585152574695, 0.9414633944437377, 0.9121951208137317, 1.0, 0.8975609548506036, 0.8780487918626829], [0.9658536710489513, 0.9121951208137317, 0.9756097816910366, 0.9024390101716463, 0.9170731761347743, 0.9414633944437377, 0.9414633944437377, 0.8731707365416402, 0.8439024629116342, 0.9219512314558171, 0.9219512314558171, 0.8975609548506036, 0.9463414497647804, 0.9121951208137317, 0.936585339122695, 0.8243902416274634, 0.8682926812205976, 0.8780487918626829, 0.9707317263699939, 0.8731707365416402, 0.907317065492689, 0.9756097816910366, 0.985365892333122, 0.907317065492689, 0.8585365705785122, 0.9560975604068659, 0.9414633944437377, 0.9317073420979025, 0.9170731761347743, 0.9658536710489513, 0.8829268471837256, 0.8439024629116342, 0.8926828995295609, 0.8829268471837256, 0.9609756157279086, 0.8926828995295609, 0.8926828995295609, 0.9804877787158293, 0.8536585152574695, 0.907317065492689], [0.9707317263699939, 0.9414633944437377, 0.8243902416274634, 0.8926828995295609, 0.8634146258995549, 0.9512195050858232, 0.8536585152574695, 0.8780487918626829, 0.9219512314558171, 0.8536585152574695, 0.9024390101716463, 0.9170731761347743, 0.8975609548506036, 0.936585339122695, 0.9707317263699939, 0.9317073420979025, 0.8780487918626829, 0.9024390101716463, 0.8682926812205976, 0.9268292867768598, 0.9560975604068659, 0.9902438893579146, 0.8878049025047683, 0.9463414497647804, 0.8390244075905915, 0.8829268471837256, 0.8439024629116342, 0.9658536710489513, 0.936585339122695, 0.9268292867768598, 0.9804877787158293, 0.9512195050858232, 0.8585365705785122, 0.9268292867768598, 0.9219512314558171, 0.9268292867768598, 0.9219512314558171, 0.9317073420979025, 0.9024390101716463, 0.8926828995295609], [1.0439024395931342, 0.9268292867768598, 0.9560975604068659, 0.8195121863064208, 0.8634146258995549, 0.9219512314558171, 0.8536585152574695, 0.936585339122695, 0.8926828995295609, 0.8926828995295609, 0.8439024629116342, 0.8829268471837256, 0.9268292867768598, 0.9414633944437377, 0.907317065492689, 0.9219512314558171, 1.0, 0.9463414497647804, 0.9609756157279086, 0.9268292867768598, 0.9121951208137317, 0.9609756157279086, 0.907317065492689, 0.9414633944437377, 0.9756097816910366, 0.7902439126764147, 1.0195122212841707, 0.9024390101716463, 0.8975609548506036, 0.9609756157279086, 0.9463414497647804, 0.8829268471837256, 0.8829268471837256, 0.8975609548506036, 0.8634146258995549, 0.9121951208137317, 0.9317073420979025, 0.9317073420979025, 0.8829268471837256, 0.8341463522695488], [0.9268292867768598, 0.9268292867768598, 0.9121951208137317, 0.936585339122695, 0.8780487918626829, 0.8439024629116342, 1.0, 0.8878049025047683, 0.9560975604068659, 0.8975609548506036, 0.9707317263699939, 0.8097560756643353, 0.9121951208137317, 0.9560975604068659, 0.9268292867768598, 0.9756097816910366, 0.907317065492689, 0.8829268471837256, 0.9560975604068659, 0.9317073420979025, 0.8097560756643353, 0.8829268471837256, 0.9024390101716463, 0.9024390101716463, 0.9317073420979025, 0.8731707365416402, 0.9317073420979025, 0.9170731761347743, 0.9317073420979025, 0.9317073420979025, 0.9609756157279086, 0.8634146258995549, 0.9170731761347743, 1.014634107666878, 0.9512195050858232, 0.9268292867768598, 0.9707317263699939, 0.8634146258995549, 0.8780487918626829, 0.9024390101716463], [0.7756097467132865, 0.8829268471837256, 0.8585365705785122, 0.9609756157279086, 0.9268292867768598, 0.9121951208137317, 0.9219512314558171, 0.8585365705785122, 0.907317065492689, 0.8536585152574695, 0.9707317263699939, 0.9024390101716463, 0.9707317263699939, 0.8682926812205976, 0.9268292867768598, 0.8536585152574695, 0.9463414497647804, 0.9024390101716463, 0.8731707365416402, 0.8878049025047683, 0.8926828995295609, 0.8682926812205976, 0.8292682969485061, 0.9170731761347743, 0.9902438893579146, 0.9268292867768598, 0.9317073420979025, 0.9414633944437377, 0.9024390101716463, 0.8829268471837256, 0.9170731761347743, 0.9170731761347743, 0.8682926812205976, 0.8878049025047683, 0.9658536710489513, 0.8243902416274634, 0.9463414497647804, 0.9414633944437377, 0.8195121863064208, 0.9121951208137317], [0.9463414497647804, 0.9268292867768598, 0.9756097816910366, 0.985365892333122, 0.9512195050858232, 0.9121951208137317, 0.8975609548506036, 0.8829268471837256, 0.8146341309853781, 0.8878049025047683, 0.9804877787158293, 0.9024390101716463, 0.9560975604068659, 1.029268331926256, 0.9902438893579146, 0.8975609548506036, 0.907317065492689, 0.936585339122695, 0.9219512314558171, 0.8243902416274634, 0.8926828995295609, 0.9902438893579146, 0.8682926812205976, 0.9170731761347743, 0.8926828995295609, 0.9560975604068659, 0.9609756157279086, 0.9756097816910366, 0.9268292867768598, 0.9024390101716463, 0.9219512314558171, 0.8682926812205976, 0.9756097816910366, 0.8975609548506036, 0.8487804599364268, 0.9219512314558171, 0.8926828995295609, 0.8292682969485061, 0.9951220029752075, 0.8243902416274634], [0.936585339122695, 0.9219512314558171, 0.8682926812205976, 1.0, 0.9414633944437377, 0.907317065492689, 0.9951220029752075, 0.9121951208137317, 0.907317065492689, 0.9024390101716463, 0.8878049025047683, 0.9609756157279086, 0.8439024629116342, 1.0243902183089633, 0.936585339122695, 0.8878049025047683, 0.9804877787158293, 0.8878049025047683, 0.9024390101716463, 0.8878049025047683, 0.8439024629116342, 0.9024390101716463, 0.9121951208137317, 0.9609756157279086, 0.8731707365416402, 1.0, 0.9756097816910366, 0.8878049025047683, 0.9219512314558171, 0.8829268471837256, 0.9463414497647804, 0.8731707365416402, 0.9609756157279086, 0.8682926812205976, 0.8634146258995549, 0.8731707365416402, 0.985365892333122, 0.9414633944437377, 0.9219512314558171, 0.8536585152574695], [0.8682926812205976, 0.936585339122695, 0.9414633944437377, 0.9024390101716463, 0.8585365705785122, 0.8878049025047683, 0.936585339122695, 0.9609756157279086, 0.9268292867768598, 0.9902438893579146, 0.9658536710489513, 0.8829268471837256, 0.9024390101716463, 0.9024390101716463, 0.8634146258995549, 0.9414633944437377, 0.8439024629116342, 0.9121951208137317, 0.8487804599364268, 0.8926828995295609, 1.0, 0.9512195050858232, 0.8682926812205976, 0.9414633944437377, 0.9756097816910366, 0.9756097816910366, 0.9951220029752075, 0.936585339122695, 0.8634146258995549, 0.9170731761347743, 0.9463414497647804, 0.9707317263699939, 0.9317073420979025, 0.907317065492689, 0.8439024629116342, 0.936585339122695, 0.8878049025047683, 0.8536585152574695, 0.936585339122695, 0.8390244075905915], [0.8390244075905915, 0.8878049025047683, 0.8878049025047683, 0.8585365705785122, 0.9219512314558171, 0.8390244075905915, 0.8878049025047683, 0.9414633944437377, 0.9219512314558171, 0.8780487918626829, 0.9219512314558171, 0.9804877787158293, 0.9170731761347743, 0.8731707365416402, 1.0097561106420854, 0.8731707365416402, 0.9609756157279086, 0.8780487918626829, 0.936585339122695, 0.9024390101716463, 0.8195121863064208, 0.9463414497647804, 0.9951220029752075, 0.8634146258995549, 0.9463414497647804, 0.9170731761347743, 0.9024390101716463, 0.8780487918626829, 0.9121951208137317, 0.9317073420979025, 0.8780487918626829, 0.9609756157279086, 0.8829268471837256, 0.8731707365416402, 0.936585339122695, 0.9219512314558171, 0.9658536710489513, 0.9609756157279086, 0.9170731761347743, 0.9317073420979025], [0.8780487918626829, 0.8975609548506036, 0.8195121863064208, 0.7609755807501585, 0.9268292867768598, 0.8731707365416402, 0.9414633944437377, 0.9170731761347743, 0.8390244075905915, 0.9170731761347743, 0.9512195050858232, 0.9560975604068659, 0.9121951208137317, 0.9024390101716463, 0.8878049025047683, 0.9121951208137317, 0.7804878020343292, 0.9707317263699939, 0.9317073420979025, 0.907317065492689, 0.8682926812205976, 0.9707317263699939, 0.9756097816910366, 0.8682926812205976, 0.9414633944437377, 0.8975609548506036, 0.9512195050858232, 0.8829268471837256, 0.8829268471837256, 0.9463414497647804, 0.8926828995295609, 0.9707317263699939, 0.9268292867768598, 0.8682926812205976, 0.9512195050858232, 0.907317065492689, 0.8878049025047683, 0.9804877787158293, 1.014634107666878, 0.8292682969485061], [0.8975609548506036, 0.9121951208137317, 0.8146341309853781, 0.9902438893579146, 0.9317073420979025, 1.014634107666878, 0.8878049025047683, 0.936585339122695, 0.8780487918626829, 0.9756097816910366, 0.9902438893579146, 0.9463414497647804, 0.9804877787158293, 0.8292682969485061, 0.8878049025047683, 0.8926828995295609, 0.8439024629116342, 0.8780487918626829, 1.0, 0.8487804599364268, 0.8439024629116342, 0.8487804599364268, 0.9170731761347743, 0.8829268471837256, 0.8439024629116342, 0.936585339122695, 0.9317073420979025, 0.8780487918626829, 0.9024390101716463, 0.9170731761347743, 0.936585339122695, 0.8975609548506036, 0.9219512314558171, 0.9512195050858232, 0.8926828995295609, 0.8878049025047683, 0.9512195050858232, 1.0, 0.9219512314558171, 0.9317073420979025], [0.8731707365416402, 0.9121951208137317, 0.8780487918626829, 0.9268292867768598, 0.8390244075905915, 0.936585339122695, 0.8682926812205976, 0.9170731761347743, 0.9560975604068659, 0.8878049025047683, 0.8878049025047683, 0.9024390101716463, 0.936585339122695, 0.9024390101716463, 0.9463414497647804, 0.8829268471837256, 0.9268292867768598, 0.907317065492689, 0.936585339122695, 0.8878049025047683, 0.9024390101716463, 0.8926828995295609, 0.8926828995295609, 0.8731707365416402, 0.8585365705785122, 0.9512195050858232, 0.9317073420979025, 0.9317073420979025, 0.8975609548506036, 0.9317073420979025, 0.9707317263699939, 0.9512195050858232, 0.9560975604068659, 0.8585365705785122, 0.9560975604068659, 0.936585339122695, 0.8975609548506036, 0.936585339122695, 0.8878049025047683, 0.8634146258995549], [0.9317073420979025, 0.8536585152574695, 0.936585339122695, 0.9219512314558171, 0.8536585152574695, 0.907317065492689, 0.9121951208137317, 0.936585339122695, 0.9658536710489513, 0.8634146258995549, 0.907317065492689, 0.9317073420979025, 0.8439024629116342, 0.9219512314558171, 0.9219512314558171, 0.9219512314558171, 0.8243902416274634, 0.8292682969485061, 0.9756097816910366, 0.8926828995295609, 0.8975609548506036, 0.9317073420979025, 0.8585365705785122, 0.9414633944437377, 0.9609756157279086, 0.8829268471837256, 0.8780487918626829, 0.8731707365416402, 0.936585339122695, 0.936585339122695, 0.8097560756643353, 1.0, 0.9024390101716463, 0.9560975604068659, 0.9024390101716463, 0.9170731761347743, 0.8926828995295609, 0.9609756157279086, 0.8780487918626829, 0.9463414497647804], [0.8341463522695488, 0.9463414497647804, 0.9121951208137317, 0.8292682969485061, 0.8634146258995549, 0.8195121863064208, 0.8780487918626829, 0.9414633944437377, 0.8926828995295609, 0.907317065492689, 0.8829268471837256, 0.9170731761347743, 0.9170731761347743, 0.9414633944437377, 0.907317065492689, 0.8682926812205976, 0.9804877787158293, 0.8878049025047683, 0.936585339122695, 0.9268292867768598, 0.8439024629116342, 0.9170731761347743, 0.9658536710489513, 0.936585339122695, 0.9463414497647804, 0.8585365705785122, 0.9609756157279086, 0.9463414497647804, 0.9268292867768598, 0.9219512314558171, 0.9219512314558171, 0.9707317263699939, 0.8975609548506036, 0.8195121863064208, 0.9414633944437377, 0.8585365705785122, 0.8634146258995549, 1.0536585502352196, 0.8634146258995549, 0.9024390101716463]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = [ele.numpy().tolist() for ele in c3f1_thr_per_batch2]\n",
        "with open(\"result_output_threshold.txt\", 'w') as writefile:\n",
        "    writefile.write(str(res))"
      ],
      "metadata": {
        "id": "GzV7MHw3no50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trad_outputs = []\n",
        "for i in range(1000):\n",
        "    trad_outputs.append(net2(next(iter(testloader))[0]))\n",
        "trad_plottable = []\n",
        "for i in trad_outputs:\n",
        "    for j in i[0]:\n",
        "        trad_plottable.append(float(j.detach().numpy()))\n",
        "\n",
        "with open(\"result_output_distribution_trad5.txt\", 'w') as writefile:\n",
        "    writefile.write(str(trad_plottable))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1uapADciAV7",
        "outputId": "f6e8a2d4-4751-495f-d30e-68cede01f02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresh_outputs = []\n",
        "for i in range(1000):\n",
        "    thresh_outputs.append(net(next(iter(testloader))[0]))\n",
        "thresh_plottable = []\n",
        "for i in thresh_outputs:\n",
        "    for j in i[0]:\n",
        "        thresh_plottable.append(float(j.detach().numpy()))\n",
        "\n",
        "with open(\"result_output_distribution_codex6.txt\", 'w') as writefile:\n",
        "    writefile.write(str(thresh_plottable))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej7l-ycqhkNH",
        "outputId": "3998d254-78a8-43b7-f1c3-9ad762dbf4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresh_outputs, thresh_loss = [], []\n",
        "for i in range(1000):\n",
        "    item = next(iter(testloader))\n",
        "    res = net3(item[0])\n",
        "    thresh_outputs.append(res[0])\n",
        "    loss_curr =  F.nll_loss(res, item[1])\n",
        "    thresh_loss.append(loss_curr.item())\n",
        "\n",
        "with open(\"err_codex_manh.txt\", 'w') as writefile:\n",
        "    writefile.write(str(thresh_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbEw3X5McqSh",
        "outputId": "4d5aade9-3a25-41ee-82e9-2f0ecc560950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "err2 = np.cumsum(thresh_loss)\n",
        "print(thresh_loss[:3])\n",
        "print(err2[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqVODPvDncWv",
        "outputId": "230844c9-4cbb-4311-aaad-9bced9d35338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.8103506565093994, 0.758525550365448, 0.2682112753391266]\n",
            "[1.81035066 2.56887621 2.83708748]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"err_codex_manh_cumsum.txt\", 'w') as writefile:\n",
        "    writefile.write(str(err2))"
      ],
      "metadata": {
        "id": "YSQaaakEnskl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manh_outputs = []\n",
        "for i in range(500):\n",
        "    manh_outputs.append(net3(next(iter(testloader))[0]))\n",
        "manh_plottable = []\n",
        "for i in manh_outputs:\n",
        "    for j in i[0]:\n",
        "        manh_plottable.append(float(j.detach().numpy()))\n",
        "\n",
        "with open(\"result_output_distribution_manh.txt\", 'w') as writefile:\n",
        "    writefile.write(str(manh_plottable))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGqb_PfqkeMm",
        "outputId": "092d2c03-2731-4bab-e081-a7e5c2663272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(thresh_plottable, label = 'with CODEX')\n",
        "sns.distplot(trad_plottable, label = 'traditional')\n",
        "#sns.distplot(manh_plottable, label = 'Manhattan')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel=(\"Output\")\n",
        "plt.ylabel=(\"Frequency\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "Zmoh19YUiHNM",
        "outputId": "a2d6579d-f8d5-4b90-cb79-5b6adc0b4cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhb5Znw/++t3fvuxImzk4UESEJMSICkrC1bgWHoCy3TFKb9MUDpzEA3aDtvaX/Tlim8A2VooYGWpVNoXyi0QGFoKFugJMQJDkt2sjpxvO+2rO15/ziS4ziOLcuSZUv357p0WdY5Ouc5VnJuPdv9iDEGpZRS6cuW7AIopZRKLg0ESimV5jQQKKVUmtNAoJRSaU4DgVJKpTlHsgswXMXFxWb69OnJLoZSSo0rGzdubDDGlAy0bdwFgunTp1NZWZnsYiil1LgiIvuOt02bhpRSKs1pIFBKqTSngUAppdLcuOsjGIjf76e6uhqv15vsoqQdj8dDeXk5Tqcz2UVRSsUoJQJBdXU1OTk5TJ8+HRFJdnHShjGGxsZGqqurmTFjRrKLo5SKUUo0DXm9XoqKijQIjDIRoaioSGtiSo1zKREIAA0CSaJ/d6XGv5QJBEoppWKTsD4CEfEAbwHu8HmeMcZ8v98+buAJYAnQCFxtjNk70nM/uX7/SA9xlC+cPnXEx7j44ot58sknAXjyySe5+eabAXjjjTe45557ePHFF4c8xj333MMjjzyCx+PB6XTyta99jVWrVuHz+fjWt77Fiy++iIgwf/58fv7zn1NeXg6A3W7n5JNPxu/343A4WLVqFbfeeis2m4033niDyy+//Kg2/nvuuYe5c+eycuVKNm7cSGFhIc3NzZx66qm8/vrr6MxupVJLIjuLe4BzjTEdIuIE3haRl40x6/rs82Wg2RhzgohcA/wHcHUCy5Q0L730EgB79+7lF7/4RW8giNZDDz3EmjVreO+998jNzaWtrY3nnnsOgO985zu0t7ezfft27HY7jz76KFdeeSXr169HRMjIyKCqqgqAuro6vvCFL9DW1sYPfvADAFasWDFgILrpppu4/fbbWb16Nbfffjs33HCDBgEVs4G+oMXjS5YauYQ1DRlLR/hXZ/jRfzm0y4HHw8+fAc6TcdjofPfdd3P//fcDcOutt3LuuecC8Nprr3HttdcCVmqMhoYGbr/9dj755BMWLVrEN7/5TQA6Ojq46qqrmDdvHtdeey0DrRr34x//mAcffJDc3FwAcnNz+dKXvkRXVxePPvoo9957L3a7HYDrr78et9vNa6+9dsxxSktLWb16NQ888MCA5+nr1ltvZd26ddx33328/fbbfOMb34jxL6SUGssS2kcgInYRqQLqgDXGmPX9dpkMHAAwxgSAVqAokWVKhBUrVrB27VoAKisr6ejowO/3s3btWlauXHnUvnfddRezZs2iqqqKu+++G4D333+f++67jy1btrB7927eeeedo97T1tZGe3s7M2fOPObcu3btYurUqb0BIqKiooKPP/54wPLOnDmTYDBIXV0dAGvXrmXRokW9j08++QQAp9PJ3Xffza233sp9992ncwWUSlEJDQTGmKAxZhFQDiwVkZNiOY6I3CAilSJSWV9fH99CxsGSJUvYuHEjbW1tuN1uli9fTmVlJWvXrmXFihVDvn/p0qWUl5djs9lYtGgRe/fuTXyh+1ixYgVVVVW9j1mzZvVue/nllykrK+Ojjz4a1TIppUbPqIwaMsa0AK8DF/bbdBCYAiAiDiAPq9O4//tXG2MqjDEVJSUDZlFNKqfTyYwZM3jsscc444wzWLFiBa+//jq7du3ixBNPHPL9bre797ndbicQCBy1PTc3l+zsbHbv3n3Me2fNmsX+/ftpb28/6vWNGzeyYMGCAc+3e/du7HY7paWlg5arqqqKNWvWsG7dOu69915qamqGvBal1PiTsEAgIiUikh9+ngFcAGzrt9vzwJfCz68CXjNDNVyPUStWrOCee+5h5cqVrFixgoceeojFixcfM84+JyfnmJt2NO644w6++tWv0tbWBlj9Ck888QRZWVl86Utf4rbbbiMYDALwxBNP0NXV1dtX0Vd9fT033ngjt9xyy6BzAIwx3HTTTdx3331MnTqVb37zm9pHoFSKSuSooTLgcRGxYwWc/2uMeVFEfghUGmOeB34F/EZEdgFNwDXxOHEyRiKsWLGCH/3oRyxfvpysrCw8Hs+AzUJFRUWceeaZnHTSSVx00UVccsklUR3/pptuoqOjg9NOOw2n04nT6eTrX/86AD/5yU/4xje+wZw5c7DZbMybN4/nnnuu90bf3d3NokWLeoePfvGLX+S2227rPXakjyDie9/7Hk1NTUydOpULLrgAgJtvvplHH32UN998k0996lMx/52UUmOPjLcv4BUVFab/wjRbt26NqglGJYb+/VU0dPhoconIRmNMxUDbdGaxUkqlOQ0ESimV5jQQKKVUmtNAoJRSaU4DgVJKpTkNBEopleZSYqnKY1Q+Gt/jVVw/6OaWlpajUkvH4rHHHqOyspIHHniAhx56iMzMTFatWsVjjz3Gpz/9aSZNmgTAV77yFW677Tbmz58f87kGO7dSKv1ojSAOWlpa+MUvfnHM6/1TRUTrxhtvZNWqVYB1kz506FDvtkceeSSuQUAppTQQxEHf1NKnnXYaK1as4LLLLuu9YV9xxRUsWbKEBQsWsHr16t73Pfroo8yZM4elS5celXH0zjvv5J577uGZZ56hsrKSa6+9lkWLFtHd3c3ZZ59NZELdU089xcknn8xJJ53Et7/97d73Z2dn893vfpeFCxeybNkyamtrAXjhhRc4/fTTWbx4Meeff37v60qp9KaBIA76p5betGkTP/vZz9ixYwcAv/71r9m4cSOVlZXcf//9NDY2UlNTw/e//33eeecd3n77bbZs2XLMca+66ioqKir47W9/S1VVFRkZGb3bDh06xLe//W1ee+01qqqq2LBhA3/84x8B6OzsZNmyZWzevJmVK1fy8MMPA3DWWWexbt063n//fa655hp++tOfjsJfRyk11qVmH0GSLV269KilH++///7e1cQOHDjAzp07OXz4MGeffTaRbKpXX311b+CIxoYNG456/7XXXstbb73FFVdcgcvl4tJLLwWsFNlr1qwBoLq6mquvvpqamhp8Pt9RZVRKpS+tESRAVlZW7/M33niDV199lXfffZfNmzezePFivF5vQs/vdDp7E871TWv9ta99jVtuuYUPP/yQX/7ylwkvh1JqfNBAEAeDpZZubW2loKCAzMxMtm3bxrp11pLNp59+Om+++SaNjY34/X6efvrpYR176dKlvPnmmzQ0NBAMBnnqqaeGzAra2trK5MmTAXj88ccH3VcplT5Ss2loiOGe8dY3tXRGRgYTJkzo3XbhhRfy0EMPceKJJzJ37lyWLVsGQFlZGXfeeSfLly8nPz//qDTQfV133XXceOONZGRk8O677/a+XlZWxl133cU555yDMYZLLrmEyy+/fNBy3nnnnXzuc5+joKCAc889lz179sTh6pVS452moVYjpn9/FQ1NQ51cmoZaKaXUcWkgUEqpNJcygWC8NXGlCv27KzX+pUQg8Hg8NDY26k1plBljaGxsxOPxJLsoSqkRSIlRQ+Xl5VRXV1NfX5/soqQdj8dDeXl5souhlBqBlAgETqdTZ8kqpVSMUqJpSCmlVOw0ECilRsXBlm6aO33JLoYagAYCpVTC1bZ5Wf3WJ/zqnT34g6FkF0f1o4FAKZVw97yynWDI0NTp462dOqhjrElYZ7GITAGeACYABlhtjPlZv33OBv4ERJLePGuM+WGiyqSUGj2RlBK+QIg/bKpm6Ywimjp72LSvmfPmTRji3Wo0JXLUUAD4ujFmk4jkABtFZI0xpv8KLGuNMZcmsBxKpb1k5vmpae0mZOCEkmwaMpzsqO2goydAtjslBi2mhIQ1DRljaowxm8LP24GtwOREnU8pNTYdbOkGYHJBBuUF1ip7B5u7k1kk1c+o9BGIyHRgMbB+gM3LRWSziLwsIgtGozxKqdFzqMVLlttBrsfB5PwMBKhu6Up2sVQfCa+biUg28AfgX40xbf02bwKmGWM6RORi4I/A7AGOcQNwA8DUqZq2Vqnx5FBLN5PzPYgIbqed4hy31gjGmITWCETEiRUEfmuMebb/dmNMmzGmI/z8JcApIsUD7LfaGFNhjKmIrNGrlBr7/MEQde1eJudn9L5Wnp+hgWCMSVggEGvR3F8BW40x/3mcfSaG90NElobL05ioMimlRldDRw8hAxNyjyQmLM1x094ToMcfTGLJVF+JbBo6E/gi8KGIVIVf+w4wFcAY8xBwFXCTiASAbuAaoylElUoZzZ1+AAqzXL2vFYSfN3XpLOOxImGBwBjzNiBD7PMA8ECiyqCUSq7Izb4w80ggKMpyW9s03cSYoTOLlVIJ09zpw+2wkeGy974WqR1oIBg7NBAopRKmqdNHYZaLcFcgABkuOxlOuwaCMUQDgVIqYZq7fBT0aRaKKMxyaSAYQzQQKKUSwhgTDgTOY7ZpIBhbNBAopRKioyeAP2iOGjEUUZjlornLR0BTUo8JGgiUUgkRWYSmYIBAUJDpImSgtr1ntIulBqCBQCmVEC3d1hyCvIxjm4Yirx1u9Y5qmdTANBAopRKizRsAIM+jgWCs00CglEqI9m4/DpscNYcgIjfDmsta06o5h8YCDQRKqYRo9frJzXAeNYcgIsNpx2kXrRGMERoIlFIJ0dYdINczcBYbESHX4+RwmwaCsUADgVIqIdq8fnIG6B+IyMtwao1gjNBAoJSKO2MM7V7/gCOGIvIynNRoIBgTNBAopeKurduaTHa8piGA3AwntW1eQiHNPJ9sGgiUUnEXafvPHaJGEAgZGjp1UlmyaSBQSsVdJBAM1keQ69G5BGOFBgKl0sz+xk7++P5Bun2JWyqyNhwIBusjiMwlqGvTGkGyJXKpSqXUGNPU6eOJdfvo8gW58b838pkFE4/a/oXTp8blPLWtkRrB8W8x2e5wINB8Q0mnNQKl0sgb2+sIhAxzJ+Swdmd9wlJBH27zkumy47Qf/xaT7YkEAm0aSjYNBEqlCWMMu+o6mF2azSWnlBEysLOuPSHnqm3z9vYBHI/DZqMwy6U1gjFAA4FSaaKp00dLt59ZJdkUZbnI8TjY09CZkHPVtvX09gEMpjTHrX0EY4AGAqXSxK76DgBOKMlGRJhRnMXehk6Mif84/sNR1AgASnLc1GvTUNJpIFAqTeyu7yQvw0lRtrVQzPSiLNq8AZq7/HE9jz8YoqGjZ9A5BBGlOR5tGhoDNBAolSYOt3qZnJ/Rmw10enEWAPsa49s8VN/egzFEVSMozXVT396js4uTTAOBUmkgEAzR2NnDhFx372sl2W7sItTGuY2+tndWcXR9BIGQtci9Sp6EBQIRmSIir4vIFhH5WET+ZYB9RETuF5FdIvKBiJyaqPIolc7qO3oIGSjN9fS+ZrcJxTmuuA/f7A0E0dQIcqzyaPNQciWyRhAAvm6MmQ8sA74qIvP77XMRMDv8uAF4MIHlUSptRUbmTMjxHPV6ItroIykjouojCNdQNBAkV8ICgTGmxhizKfy8HdgKTO632+XAE8ayDsgXkbJElUmpdFXb5sUmUJzjOur10hw3zZ0+/MFQ3M51uK0Hp13IHGCJyv5Kc8KBQBeoSapR6SMQkenAYmB9v02TgQN9fq/m2GChlBqh2vYeirLdOGxH/5cvzfVgsDp446WuzUtpjgfbAEtU9qdNQ2NDwgOBiGQDfwD+1RjTFuMxbhCRShGprK+vj28BlUoD9e3e3m/fffV+I4/jjfhwm/eoTunBZLjs5LgdcQ1EavgSGghExIkVBH5rjHl2gF0OAlP6/F4efu0oxpjVxpgKY0xFSUlJYgqrVIoKhQzNXX6KslzHbCvKdmGT+Ob7OdzmZWKeZ+gdw0py3ZpvKMkSOWpIgF8BW40x/3mc3Z4HVoVHDy0DWo0xNYkqk1LpqLbdSzBkKBggEDhsNvIzXXFNPlfb6mVCbvSBQNNMJF8i01CfCXwR+FBEqsKvfQeYCmCMeQh4CbgY2AV0AdcnsDxKpaX9jV0AFGYeGwgAirJcNHbEJxB09ATo9AWZOKxA4KHqQEtczq9ik7BAYIx5Gxi0t8hYSU6+mqgyKKXgQHM3wIA1AoDCLBfVza1xOVdk6OjEPA+dPUMvfPPk+v00dfqoae3mt+v2ISJxWxNBRU9nFiuV4vY3dSFA/nHG9Rdluej2B+nyBUZ8rshkstKc6GsEOR4H/qChJxC/IaxqeDQQKJXiqpu6yM1w4jjOIjGFWdYIn3g0D/WtEUQrsopZmze+ye9U9DQQKJXiDjR3UXCc/gGgNxtpPDqMa8Ojf4bTRxBZ4L7dO/IaiYqNBgKlUtz+pi4Kj9M/APRua+wc+cid2lYvuR4HGVHMKo7ICa9drIEgeTQQKJXCvP4gtW09FGQdP++P024j1+OIT9NQ2/CGjkLfGoE2DSWLBgKlUtjBFmvE0PGGjkYUZbvj0jR0sKWbyQUZw3qPx2nDYROtESRRVIFARJ4VkUtERAOHUuPI/qbwHIJBmoYgPJcgHoGguZvJ+cMLBCJCjsehNYIkivbG/gvgC8BOEblLROYmsExKqTipDgeCwTqLwQoUHT0BOnti/1be5bOWvRxujQCs5iGtESRPVIHAGPOqMeZa4FRgL/CqiPxNRK4P5xNSSo1B+5u6cDlsZHsGnztalG0NId0XnoUci4PhiWvDrREA4RqBBoJkibqpR0SKgOuArwDvAz/DCgxrElIypdSIHWjqZkpBxpApoSMJ6UayfnF1uD+iPNYaQY82DSVLVCkmROQ5YC7wG+CzfRLD/V5EKhNVOKXUyBxo7mJKYeaQ+0X6EPY1jbxGMCmGGkGux4HXH4rrAjkqetHmGnrYGPNS3xdExG2M6THGVCSgXEqpONjf1MWpUwuG3M/jtJPlso+oRnCwpRuHTYaVXiIiMrtYm4eSI9qmoX8f4LV341kQpVR8tXb5afcGmBpFjQCsfoK9DSOrEZTle7Dbhl6ZrD+dS5Bcg9YIRGQi1tKRGSKymCPZRHOB6P51KaWS4kCzdVOfUphBU+fQN9jCLFfvcNNYHGwZ/tDRiCP5hrRGkAxDNQ19BquDuBzou7hMO9baAkqpMSpyU59SmElT59BppouyXGyubqEnEMTtiD5FRMS+xk7Omzdh2O8DrREk26CBwBjzOPC4iPy9MeYPo1QmpVQcHOgTCDYfiCIQZLswxhppdEJp9rDO1drtp6HDx4ySrJjKmumyYxPtI0iWoZqG/sEY89/AdBG5rf/2QZagVEol2YHmLvIynOR6opvqE0lHva+xc9iBYE+D1ck8szi2QGATIdutcwmSZaimocinOrx/FUqppNvf1B11RzEcmUuwN4ZJZXsaOgCYGWONACKzi7VpKBmGahr6ZfjnD0anOEqpeKlu6mJeWU7U+2e67OR4HOyPYQjpnvpObAJTC0cSCBy0dGkgSIZok879VERyRcQpIn8VkXoR+YdEF04pFZtQyFDd3M2UguhrBCLCtKLMmGoEuxs6mVKYicsRe15KrREkT7Sf2qeNMW3ApVi5hk4AvpmoQimlRqa23YsvGIpqVnFf04qyYppUtru+kxkx9g9E5HgcdPqCOrs4CaINBJEmpEuAp40xQw9BUEolzf7GIyOGhmNWcRb7m7rw+oNRvycYMuxp6GRm8ci6EiNzCRo6Rr5SmhqeaAPBiyKyDVgC/FVESgBv4oqllBqJSM6gacMMBHMn5hIy8El9R9Tv2V3fQbc/yIJJucM6V3+R0U11bRoIRlu0aahvB84AKowxfqATuDyRBVNKxW5/Yxd2mwx7bYC5E61v9Ttq26N+z0eHrAaCk8vzhnWu/iI1grp2DQSjLdqkcwDzsOYT9H3PE3Euj1IqDvY2djI5PwOnfXidt9OKsnDZbWw/HH2N4MPqNjxOW8xzCCIis4vr2rWxYbRFm4b6N8AsoAqINB4aBgkEIvJrrM7lOmPMSQNsPxv4E7An/NKzxpgfRl1ypdRx7W/qYlrR8NOBOe02ZpZkDa9GcLCV+WW5OIYZdPrLdjsQtGkoGaKtEVQA840xZhjHfgx4gMFrDWuNMZcO45hKqSjsa+ziswvLYnrv3Ik5VO5tjmrfUMjw8aFW/n5JeUzn6stuEzJddm0aSoJoQ/hHwMThHNgY8xbQNOwSKaVGpKXLR2u3n2kxTu6aMyGHgy3dtHYPPaZ/S00bnb4gC8vzYzpXfzkeJ/XaNDTqog0ExcAWEXlFRJ6PPOJw/uUisllEXhaRBXE4nlJpL7LucCxNQwCLp1g39aoDLUPu+9bOegBWzC6O6Vz95XgcWiNIgmibhu5MwLk3AdOMMR0icjHwR2D2QDuKyA3ADQBTp05NQFGUSh17wxPCphXFViNYOCUfm8DGfc18ak7JoPu+taOeE8tyKc0d/qpkA8nxODkUXvtYjZ5oh4++iTWj2Bl+vgHrRh4zY0ybMaYj/PwlwCkiA36tMMasNsZUGGMqSkoG/4epVLqLTCYbTsK5vrLcDk4sy2XTvsH7CTp7Amzc18zKOfGpDYBVI2jo6CEUGk53pBqpaHMN/X/AM8Avwy9NxvoGHzMRmSgiEn6+NFyWxpEcUyllTSabkOsmwzX8xWUilkwr4P39zQQHuSGv2VKLP2g4Z25pzOfpL8fjIBAyNHX54nZMNbRo+wi+CpwJtAEYY3YCg376IvIU1rrGc0WkWkS+LCI3isiN4V2uAj4Skc3A/cA1wxyVpJQawL7Gzpg7iiOWTCug0xfkw4PHzybz5Pr9TCvKZOn0whGdq68cnV2cFNH2EfQYY3zhL/CEJ5UNetM2xnx+iO0PYA0vVUrF0bbD7cwpzeHJ9ftjPsan5pTgsAkvf1jDoinHjgjaWdvOe3ubuP2iedhiWKz+eHJ7Zxd7mc/IUlao6EVbI3hTRL6DtYj9BcDTwAuJK5ZSKhZdvgDt3gCF2a4RHSc/08WK2cW8+EENA1XUf/rKdrJcdj4Xh/kDfR2ZXaw1gtEUbSC4HagHPgT+CXgJ+F6iCqWUik1kwfrIamMjcckpkzjY0s2GfpPLXttWy5ottdxy7myKst0jPk9fkXxD9RoIRlVUTUPGmJCI/BH4ozGmPsFlUkrFKDKHoDAOgeDCkybyH/+zjR/9eQvP3nwmdpuwq66df/1dFXMn5PCPZ03v3XckzVCz9j991O85zjnU7XofctYevWPF9TGfQw1uqMXrBfg+cAvh2oOIBIH/0rxASo09kUVlirJG/k092+3g3Lml/L7yAFf+4h3K8jJ4Y0cd2W4Hj3ypArcj9lFJgyn1hKjzDtBYUfno0G/WYBGToWoEt2KNFjrNGLMHQERmAg+KyK3GmHsTXUClVPT2NHSS6bKPaOhoX6eU51Hb7mXtjgY2V7cytTCT/1UxhbU7G+Jy/IEcNxCohBkqEHwRuMAY0/upG2N2h9cr/guggUCpMWRXXQclOfFrtxcRPj1/IstnFgFHOnMTqdQTYlNT4s+jjhgq7Dr7BoGIcD+BflJKjTG76joojWMgiMjxOEclCABMzAhS221DJxePnqFqBINN79Opf0qNIU2dPpq7/JTkxJ73ZySdvvEyKTOELyQ09gglHo0Go2GoQLBQRNoGeF2A+GSZUkrFxa46a1WxkjgP6RxNEgqy2Psuv3R+ROa6BsjMhPKlUHYKiPYbJMqggcAYk5hhAUqpuIssOJ+IpqHRkNl9iJkHXyCrp5ZiWyHdUkpW+2HY9BgUzIDTvgKukaXOUAPTEKtUithV14HHaSMvc/x13xW0bWXBnkdxBrtoX/hlzuy5n+cn3Qrnfg8Wfh5a98O7D4Av+rWUVfQ0ECiVInbVdTCzOBubxC/3z2goavmA2QeeptMzkQ9n/RPZ5Sfjsgk1XXarOWjK6XDaDdBZB5t/B5qbMu40ECiVIj6p7+CE0uxkF2NYJtW9yayDf6Itazrbpq8i4MhEBCZlBjnU3ef2VDIX5n0Waj+C/X9LXoFTlAYCpVJAty/IwZZuZpWMn0BQ0LqVs97/Bp2eMnZMuZqQ7UiTVllGiENd/booZ6yE4rmw9QVtIoozDQRKjXNPrt/Pz1/fhTFwuG18LPzu6Wlg5aav0ePKZ/u0awjZj+7gnpQZpKa73+1JbLDg7yDQAzv/MoqlTX0aCJRKAZFsnfGcVZwoEgpwZtW3cPtaeevU+wk4jq3FTMoMUdttwx/qtyFnotVnsPcd6NIFDeNFA4FSKaCuvQcBiuOQdTTRTt71IBOaNrBhwfdozjtxwH3KMoIYhNr+tQKAORdaM5k+eT2xBU0jGgiUSgH1HT0UZrlw2Mf2f+my+ndY8MnDfDL5CvaUX378/TKtqkBN9wBTmTLyYdISqH4PfJ2JKmpaGdv/apRSUalr8475ZqGM7sMs33wHrdmzqFzwnUH3nZwZBOBQ13FuUTPPhqBPRxDFiQYCpca5QChEQ0cPE3LHbtYXCQU4c/O3sYe8vL34/xC0Zwy6f1mGVSM4ZuRQRO4kawTRnrchFIx3cdOOBgKlxrmGdh8hAxPHcCA4Zed/Udq8ifdO+j5t2TOH3D/bachxho4dOdTX9DOhpxXqt8WxpOkpqqUqlVJjV2TI6FirEUSWoMxv38Hc/b+jtuBUHIGuY5amPJ5JA80l6Kt0AbhzYP+7MGFBPIqctrRGoNQ4V9vmxSZQnDP2Rgy5fK3MPPgnOj0T2TfxwmG995jZxf3Z7FZm0rot4G0dYUnTmwYCpca52nBHscM2tv47SyjA7OqnsZkgO8uvwtiG1wBRlhmy8g0NZsrpYEJQXTmCkqqx9S9HKTVsh9u8Y65ZCGB6zctkdx/ik8lX0OMuHPb7J2UEafbZ6A4MslN2KeRPhUObYi+o0kCg1HjW7vXT0uUfcx3Fsw48Q2nL+xwsPovm3HkxHSMyl+DQQHMJ+pq0BNoOQvvhmM6jEhgIROTXIlInIh8dZ7uIyP0isktEPhCRUxNVFqVS1Y5aK/naWKoRFLV8QMXHP6YlexbVpWfHfJwp4bkE1Z1DBYJFgGitYAQSWSN4DBisd+giYJ+7km0AABrJSURBVHb4cQPwYALLolRK2n64HRg7gSCr6wArN36Nbk8puyZfOaLlJadmW4Fg/1CBwJMHxbPh4CZdqyBGCQsExpi3gKZBdrkceMJY1gH5IlKWqPIolYp21LbjctjIHwOrkrl7mjhnw03YTJDXKx4k6Bh80thQSj0hXDbDgc4oblOTToWuBq0VxCiZfQSTgQN9fq8Ov3YMEblBRCpFpLK+vn5UCqfUeLDtcBsTctxJX5XMHujiUxtvIdN7mDeXPEB79oyYj7V+TxPr9zSxYW8TxU4fVXX9U5AOYOIp1nDSD/8Q83nT2bjoLDbGrDbGVBhjKkpKSpJdHKXGBGMM2w+3J71ZyB70suL92yhs/Zi/LfoPGgoWxe3YpW4/dT1R1HZcmVByInz0B005EYNkBoKDwJQ+v5eHX1NKRaGuvYfmLj8T85IXCBzhmkBZw99476Q7qZ5wXlyPP8Htp7bHGV3T/+Ql0HEY9r0T1zKkg2QGgueBVeHRQ8uAVmNMTRLLo9S48vEhazbtpLyRtcXHKsNbx/nrr6O0cQPrTvl3dk/5u7ifo9Ttoztkp9UfRdPXhAXgzIIPn4l7OVJdIoePPgW8C8wVkWoR+bKI3CgiN4Z3eQnYDewCHgZuTlRZlEpFHx1sQwTKklAjmNC4ngv/djU5nXt5a8l/sWfyZYk5j9sPwP6OIUYOAdhdMPci2Po8BP0JKU+qSljSOWPM54fYboCvJur8SqW6jw62MqMoC7cziptknDj97Zyy87+Ys+8pvK5itk5fRYa3NupEcsNV6goHgk47pxQONsU47KQr4aNnYPebMPv8hJQpFWn2UaVGW+Wj0e1Xcf2gmz8+1Map0wriUKChuXwtnHDgGebteRy3v5XawtM4UHoeIXtiE92Vuo8EgqiccD64c+HjZzUQDIMGAqXGqkECRlOPcLClhFVT6oHlcT+1LeQnt2M3xS2bmVS/lrL6t7GbAIdKzmLz7FsobN0S93MOJMMeIscR4EC0gcDhhnmXwtYX4dJ7rd/VkDQQKDUOfdxi/dc9Kd/PvmjfZEJ4ehrJ8taQ2V1DprcOt78Zj68Jt68ZT08T2V37cQa6cIS8vW/rceZRV7CE+oJFdHsmjFoQiJjg8kdfIwCreWjzk/DJa1afgRqSBgKlxqpQEFr2Q8s+K99+sAdsTnC4cTTl8L/shSzyBelu9GDCqRxsJmjd1H1NeHoayPTWktUdufEfxm6ObmcPiZ0eZz49rgK8rkK6PBPxO7II2DPxuovo8JTR4yqEJE5YK3X72d85jCaomWdDRgF89KwGgihpIFBqrAn0WN9m978LPW3Wa+EAQNAPQR/LMSx3ApvhfH494GFC2Oj2lNKZUUaPK5/2rKn0OPPwOfPocebhd+QQsGck9SYfjQluH+tbcgiEwBHNOEe7E078rBUI/N3gTM7w2vFEA4FSY0n9Ntj8O/C2QOl8awWuwpngyT2yjzFc9D9ZLMpp5Scn1/Jqz4kI1owrg9DjysfrKsLnyseI1aSSqFE9o6HU7SdohJpuG1Oyokg3AbDgStj0BOz8C8y/PLEFTAEaCJQaK/autb7F5kyAJf8CBQPn62kL2Njamc0l04E8J3XBpaNazNFW2mcuQdSBYPoKyCy2/p4aCIY0LnINKZXy9rxp5cmZMB/OvPW4QQBgS7ijeEF+FOPqU8AElw+AfcPpMLY7rACw4xXo6UhQyVKH1giUSrZDm+Dj52DCybDkOiuL5iA2N1lJ2E4usL4pj+dmn2gUuQK4bIa90cwu7uukK6HyV7Djf+DkqxJTuBShNQKlkqn9sNUnUDADTl01ZBAA2NjoZHp2gGJPeizCYhOYlh1kd/swv7dOXQ7ZE60gqwalgUCpZAn6YONjVo6cJddZo12GYAxsanRyalF65dKZmR1gd/swawQ2Oyz4O9i5BrpbElOwFKGBQKlk2f6ylTZ58Ret5RajcKDTRkOPjSXpFghyguzvtOOPsq+41ymfs+ZfaK1gUBoIlEqG5r2w+w2YegaUzI36bRsbrVrDqYXpFwgCRqJPNREx6VQomQdVTyamYClCA4FSo82ErAyZnjw4cXjpm99rcJHjDDEnL71W4ZqZY42QGnbzkAgs+gJUvwcNuxJQstSggUCp0XbofWittpKjOYe3lsDf6pycXuzHPrYnA8fdrBwr8A07EACccjWIzco/pAakgUCp0RTogW1/htzJMPnUYb31YJeNfZ0Ozij1JahwY1e+y1DkDrG7I4YR7zkTYdZ51ugsXc94QBoIlBpNGx6B7iarSUiG99/v3Tor8drykvQLBOv3NFHi9FJVb1i/p2n4B1j0BWg7aE3cU8fQQKDUaOlugbfuhuK5w+ogjvhbnZNCV4i5adY/EFHm9lHjjXEhnLkXW30yVU/Ft1ApQgOBUqPlnfugu9nKjDlMQQNvHHazcqIPW5r1D0RM8vhoDTjoDMRw23J64KSrrPWMva3xL9w4pykmlBoNrdWw7kGr4zKvfNhv39zkoMlnY6q9kfV72hNQwLFvksdqEjvUM0itYLBlQD35EPDCC/8Cn3ssvoUb57RGoNRoeP0n1rDRc74b29tr3NjFsDC3M84FGz96A0GszUP5UyB/Gux9x5qirXppIFAq0Wo/toYuLr0BCqbFdIg1NW6WFPnJdgx3am3qKHX7sGNi7ycAmH4WdNZZk/lULw0ESiXaq3eCKwdWfD2mt+9qs7Ot1cHF5T3xLdc44xArGBz0jmBB+rJF4MqyRm+pXhoIlEqkPWutVbJW3AqZhTEd4sVqN4LhosnpHQgAyjN8VHePoEZgd8KUZbD9JWg5EL+CjXMaCJRKFGNgzf+2Jo+dfmPMh3jhgIelxX4mZKRvs1DEtIweanpcdI9kTZ5pZ1o/Nw7SsZxmEhoIRORCEdkuIrtE5PYBtl8nIvUiUhV+fCWR5VFqVH38nLXozDnfjWkB9fV7mnjio24+aXewMKsxtolUKWZqhheDsLNtBAMeMwthzkWw8XFrprdKXCAQETvwc+AiYD7weRGZP8CuvzfGLAo/tOFOpYaAD/76AyhdAAuvifkwf63PJ8MW5MyCtjgWbvyammHduLe1jnDk++k3QFcDfPD7OJRq/EtkjWApsMsYs9sY4wN+B+gq0io9bHjYSjV9/p1RrTo2kI6AjXebczirqA2PXYc7Akxw+3HbQmwdaSCY8SkoWwjv/EzzD5HYQDAZ6NsbUx1+rb+/F5EPROQZEZky0IFE5AYRqRSRyvr6+kSUVan46aiHN+6yEp3NviDmw6xtysNvbJxfrKtrRdgEpnh6Rl4jEIEz/xUad1lJANNcsjuLXwCmG2NOAdYAjw+0kzFmtTGmwhhTUVJSMqoFVGrYXvsh+LvgwrusG04MjIFX6/OZldnN9Extx+5raqYVCEY8J2z+5dZa0W/fm/YTzBIZCA4Cfb/hl4df62WMaTTGRP6VPwIsSWB5lEq8Q+/Dpt9Yo4RK5sR8mPUNTqq9bs4v0dpAf1Mzemj22ajzjvD2ZbPDmf9sdejveSs+hRunEplraAMwW0RmYAWAa4Av9N1BRMqMMTXhXy8DtiawPEolljHw9PXWhKXcyYPnvRnCIzsyyXEEOKtQO4n7m5bhBWBrq4MJGSNMyb3wC1b6j3fug5mfikPpxqeEBQJjTEBEbgFeAezAr40xH4vID4FKY8zzwD+LyGVAAGgCrktUeZRKuKonoXkPLPx8TMNFI3a12Xm1xs1VZfW4bOndZDGQyMih7a0Ozp4YYyDoG6SnLIVtL8KrP4D8qUfvV3F9jKUcXxKafdQY8xLwUr/X/nef53cAdySyDEqNirZD8D93QOFMKD9tRId6ZGcmbpvhM6XaLDSQbEeIsozgyDuMI6adBZ+8BttfhtP/KT7HHGeS3Vms1PhnDDz/zxD0WbWBYa481led18az+zx8bno3uQ4d1ng88/ICbG2NbVjuMZwemHUu1G+Fpj3xOeY4o4FAqZH623/BrjXw6f8fsmIb1bZ+TxPr9zRxVyX4Q1CRUTP0m9LYvLwAn7Q56IlXrJy+AlzZVg6iNKSBQKmR2PeulV30xM/CaSPLkOINCn+pL6Aiv4Myjz8+5UtRpxQE8BsZ+cSyCIcbTrgAGndCw474HHMc0UCgVKyadsPvr7XWGLjsgZjnDES80ZhHR9DOZRMa41TA1LWo0AqUVU3O+B102hnWusbbX067eQUaCJSKRfth+O+rrFXHrn0GMvJHdLiggT/XFjInq4s52d44FTJ1TcwIUeoJsjmegcDuhNmftkZ+1W2J33HHAQ0ESg1X+2F47FLr5+d/B0WzRnzI95pzqPO5uGyiZhiNhggsLAxQ1RTngY9TlkFWKWz5U1rlINJAoNRw1H4Mj5xvDRf9h2dg6rIRH9IYeL62kDJ3D0vyOuJQyPSwqMDPng4HLb6RNckdxWa3Uk901sG+t+N33DFOA4FS0dr8e/jVpyEUgOtfstqU42BdvZPdXRlcOqEJWxzvaanu1CKrn6CyIY7NQwCl86FkLux4BbrSo4aW0AllSqWE9sPw5NVQU2VNGFu8ysopdOj9uBz+lzsyyXMEWFmk6SSGY3GRH5fNsL7exfmTRphqoi8ROPEKeOunVhbZi38av2OPUVojUOp4fF2w9v/AA6dB7Ucw92JY9tURdwz3tb3VzhuH3XymtFnTSQyTx26NHloX7xoBQG6ZVePb8AjUbYv/8ccYrREo1V9HvbWwzHsPQ3eTFQDKFkN2/FOgr96RSYbd8OmS5rgfOx0sK/HzwNZM2vxCrjPOgXTORdbooRf+Ga7/H7Cl7vfm1L0ypYYjFITdb8BzN8K9C+DNn1odwf/4Cnz+qYQEgcPdNp7f7+HqGd3kOHRh+lgsK/ERQlhfn4BagTvbWlPiwHqrZpDCtEag0lvtFvjgd/DB09B+CNy5sPhaWHYzFM9O6Kkf3ZlB0MCXZ3dxqC6hp0pZS4r8ZDlCvFbj5oJ49hNEnHI1fPB/rfWn5154bHbSFKGBQKWXUBAOvAfb/2zNIG3cBWKHE86Hz/wI5l40ohTS0Wr1CU/uzuDi8h6mZIU4lPAzpia3HVZO8PFajQtjRjy5+1gi8Nn74OfL4MVbrcmDcT9J8mkgUKmvs8Fq9tn1V9j5F+hqAJsTpp9lLWBetgjcOeBthc2/G5UiPbIjk/aAjZvmdY3K+VLZeWU+Xj7o4aMWBycXBOJ/gvypcP6d8PI34f3/hlO/GP9zJJkGApV6Aj2wfx288zOo3w5t1dbrzkxrfPjci6H0xFH55j+Qeq/wq50ZXFruZUF+Am5caebcsh7sYvhztTsxgQCshIJbn4eXvwVTTh/RMqRjkQYCNf71dED1e7Dvb1Y20IOVEPBa6wIUzLBu/CXzIK98RGsFxMvPt2XRExJuW9CZ7KKkhEK34ZyJPp7d5+EbCzpxJOIjttngyofhoTPh6evgK2usJUlThAYCNb6EQtD0CRyqsiZ07X8XajaDCVo3+bKFUPFlmLHSSh7m8CS7xEep7rTx5O4MPlXUSn1DPfUNyS7R+LR+z9Ezfhdm+Hi1ppy1tS7OKUtApzFYcwuufBh+exX88Sb43OMp01+ggUCNXaGQ1ZlbU2Xd+GuqoOYD8LVb2x0emLwEzrrVmvwzZanV1h8xgsXjE8EYuLMqBwH+vkwjQDydmtdBsTvIr3dmJi4QAJxwHlzwQ/jL9+C1f4fz/i1x5xpFGgjU2BAKWVP6Ww5A6wForbba9gPWQuXYnJA7yfrGnz8F8qZA9gQrSRhAy37rMYb9udrNqzVu7ji5g2KX9g3Ek8MGN8zp4scf5lDZ4KSiOIEL+yy/BRp2wtp7ILMIlt+cuHONEg0EavQZY33TP7jpyLf9wx+AL5x5M3LTn3zawDf9cWhvh507NuawsMDPl2d3sXFfskuUev5hVjerd2Tx7x9k88zZzYnpKwCrOejSe6G7GV65w1qT4oxbEnSy0aGBQCVewAev/8ha0atpDzTvBl+4o9TmhLzJMGmx1ZmbAjf9/hq8wj++k4fdBj9f1pq4G1Say3TAvy1s51/ey+MX2zL55/lxGJo7WPPijJVWLfQv34WOWmuI6Tj9d6uBQMWftxUObLA6cvevOzKKByCzGEoXQOEMyJ8O2aXj9j9PNA502vjyO/kc6rLzmxUtlGdpKolEunxqD68f9vKfW7Ip8oS4dmYCV3uzOeDUVVC7BP52v7VWxRUPQs6ExJ0zQTQQqJExxvqmf+A9KyfLgffCy/wZa8ZuZBSPv8tK4dy3MzeFhQz8cb+HH27OJmTg12e2cFoi261Vr/9Y0karT/juplw2NTq5bUEnkzMTFIDFBpf+J0w8GV7+NvximTVD/ZRrxlWSOjHjbJHmiooKU1lZmexipKegHxo/sW70dVut1Mx73jrStu/wQMF061E4E/KngcOdzBJHpf9QRIDTZxTGdKyuALxY7eGh7ZnsbndwaqGfVWX7KfNoEEi0vp+ZPwQ/25LFg9syMcAZhW2cU9TKiTldnDEzts/2uCqut37Wb4c/fRWqN8DEU2DFbTDvs2AfG9+3RWSjMaZioG0JLaGIXAj8DLADjxhj7uq33Q08ASwBGoGrjTF7E1mmtBAMWDdnX4c12crXaX0jD/qs1bWCvvDDH374jvwM9X3NZ63Q1FptLc3YvNfaDtY3ocJZ1gzdghnWI2fCmJiwNRwdfmF3p5uDXjdNfge+kBAywod+N8WeEKWeEJMzQ0zMCOIeoAWrwy9sb7WzudnJO3Uu3jrsxG9sTM/w8i8zallW0K6rjiWB0wbfOKmTE501vFRbyF8b8ni7KY9cR4BLmv2cX9ZDRbGfPNfAX4Rj+nJQMhf+8S/w0R/gjR9bE8+yJ8JJV8LMc6xstp7cOFxd/CWsRiAidmAHcAFQDWwAPm+M2dJnn5uBU4wxN4rINcDfGWOuHuy4CakRGGP1/B/vEQqGb6B+6+dRz/3Wjbf3uT+8v3+A/cPbTKSaKkcmpPROTIm8Fv4Z8Fk3cX83+DvDP7uPvsn7OqCn/cjzwMjbRYPYCIqDgLsAX+ZEgjmTCOVNo6tgLp15c2jPmYkfF8Htr2Aif0LAIeC0GZw266fDBi6bOeb1vj/tw7xRRs5lsJpg+p7/yDYhZKArILT4hBafjXqvjX2ddg502tnXYWd3u51a77F3d8FgOLZQxe4QxR7rs/OHoN5ro81/JPCVZwY5ObuVpfntnJjdnSpzjVJCT0ioas3m3eYcNrdl0xW0IRhOyA0yOTPIBE8IEetz7fALB9pCtAdsdATtdAdtOASynFDiCTE5M8ikTOtnWWaIolM+Q3G2i8IsNx6nDafdhoMQsvMVqHrSym8V9FlfkiaebM1yL5hh1Zwzi8CTZwUIT56VBsXmsPrNbA6reTVOTUyD1QgSGQiWA3caYz4T/v0OAGPMT/rs80p4n3dFxAEcBkrMIIWKORBsfQGe/ad+N/m+N+VxwOYEuwvsTnC4wO6xml76PuweHt6dT10gkzaTQZdx00EGXlz4jAM/kYfd+mkchMRGpsuOMdARctEWdNJjRq86K1gBwSGm9wYe+QdgDIQ4+gY/UnmOAKVuP2VuH5M8kUcPxa4AblsIAU4sL6Khx0Ztt42DXXY2HOqh0eek1W9HBGwY8p1BCl1+yj0+ZmZ6KdS5AePCwqmFbGp08l6Dk4+anRzutlHrtSFYNYksh8Ee8pHtCJJtD5JpDxEwQm6Wh7puG4e67RzstNEVHPwG7bQLuR4nG799ptVctPdtqx+taY81V4Zo771yJDCc8TU493sxXXeymoYmAwf6/F4NnH68fYwxARFpBYqAo6ZdisgNwA3hXztEZHscylfc/zwpKB2uEdLjOtPhGiEFr1P+94Avx3id/xZ+xGTa8TaMjV6MIRhjVgOr43lMEak8XnRMFelwjZAe15kO1wh6ncmSyJ69g8CUPr+Xh18bcJ9w01AeVqexUkqpUZLIQLABmC0iM0TEBVwDPN9vn+eBL4WfXwW8Nlj/gFJKqfhLWNNQuM3/FuAVrOGjvzbGfCwiPwQqjTHPA78CfiMiu4AmrGAxWuLa1DRGpcM1QnpcZzpcI+h1JsW4m1CmlFIqvsbX7B+llFJxp4FAKaXSXNoEAhG5W0S2icgHIvKciOQfZ7+9IvKhiFSJyLhKajSMa7xQRLaLyC4RuX20yzlSIvI5EflYREIictwheOP8s4z2Gsf7Z1koImtEZGf4Z8Fx9guGP8cqEek/6GRMGuqzERG3iPw+vH29iEwf/VJa0iYQAGuAk4wxp2ClvrhjkH3PMcYsGkvjfKM05DWGU3/8HLgImA98XkTmj2opR+4j4ErgrSj2Ha+f5ZDXmCKf5e3AX40xs4G/hn8fSHf4c1xkjLls9IoXmyg/my8DzcaYE4B7gf8Y3VIekTaBwBjzF2NMJAfAOqx5DSklymtcCuwyxuw2xviA3wGXj1YZ48EYs9UYE4/Z5WNWlNc47j9LrPI+Hn7+OHBFEssST9F8Nn2v/RngPJHkZKhKm0DQzz8CLx9nmwH+IiIbw6ktxqvjXeNAqT8mj0qJRl+qfJbHkwqf5QRjTE34+WHgeKu6eESkUkTWich4CBbRfDZHpdgBIil2Rt24SDERLRF5FZg4wKbvGmP+FN7nu0AA+O1xDnOWMeagiJQCa0RkmzEmmiaIURGnaxzzornOKIz7zzIVDHadfX8xxhgROd549mnhz3Im8JqIfGiM+STeZU1XKRUIjDHnD7ZdRK4DLgXOO94MZmPMwfDPOhF5DquKN2ZuHnG4xmhSfyTdUNcZ5THG9WcZhXH/WYpIrYiUGWNqRKQMqDvOMSKf5W4ReQNYDIzlQDCcFDvVyU6xkzZNQ+FFcr4FXGaMGXBVaxHJEpGcyHPg01idduNCNNdIdKk/xr3x/llGKRU+y75pZr4EHFMTEpECsRaxQkSKgTOBLf33G2PGV4odY0xaPIBdWO1xVeHHQ+HXJwEvhZ/PBDaHHx9jVdGTXvZ4XmP494uxRhV9Mt6uMVz+v8Nqc+0BaoFXUvCzHPIaU+SzLMIaLbQTeBUoDL9egbWqIcAZwIfhz/JD4MvJLneU13bMZwP8EOuLGoAHeDr8//Y9YGayyqopJpRSKs2lTdOQUkqpgWkgUEqpNKeBQCml0pwGAqWUSnMaCJRSKs1pIFBKqTSngUAppdLc/wOdOMZsk4qOYAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(net.fc1.cb.W.detach().numpy()[:50,:4], cmap=\"YlGnBu\")\n",
        "plt.show()\n",
        "\n",
        "with open(\"heatmap_thresh_RNN.txt\", 'w') as writefile:\n",
        "    writefile.write(str(net.fc1.cb.W.detach().numpy()[:50,:4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "pXsa4LOKfSFc",
        "outputId": "85b09c62-5f68-4a15-f403-1ca3cd9888c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(net.fc1.cb.W.detach().numpy()[:50,:4], cmap=\"YlGnBu\")\n",
        "plt.show()\n",
        "\n",
        "with open(\"heatmap_thresh_RNN001.txt\", 'w') as writefile:\n",
        "    writefile.write(str(net.fc1.cb.W.detach().numpy()[:50,:4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "zUqivWqigCbd",
        "outputId": "15f4da2d-438d-49aa-e0e0-5a2c37d9c73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.heatmap(net.fc1.cb.W.detach().numpy()[:50,:4], cmap=\"YlGnBu\")\n",
        "plt.show()\n",
        "\n",
        "with open(\"heatmap_thresh_RNN.txt\", 'w') as writefile:\n",
        "    writefile.write(str(net.fc1.cb.W.detach().numpy()[:50,:4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "4FwiQz3ggXgg",
        "outputId": "f9c19175-1b66-49d5-a28e-bde5c131addb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD6CAYAAAC1W2xyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5xdVX338c93ZpJQrrHcTdAECGKoCBJC+yAXoUi0lqjFGqwFLTR9WvBa5aK+qCJesAUerYAOBkH6SICoMAJKUQLeIAlykwSCQ0IlAVSSEEQgMJlf/9jrJMdxzuwzM2dm7+z5vn3tl2fW3nvt3+xXWGfN2mv9tiICMzOrjraiAzAzs9Zyw25mVjFu2M3MKsYNu5lZxbhhNzOrGDfsZmYV44bdzKwBSbMkLZfULenMfvZPkHR12r9I0pS6fWel8uWSjs2rU9JpqSwk7VRX/neS7pf0C0k/k/TavLg7hvNLN+OKX97sifLJ2d8e8du9xXj0jMlFh1Aae37goaJDKI2VX5yt4dbxJ684oek25/lfXdXwepLagYuAY4BVwBJJXRGxrO6wk4F1EbG3pDnAecA7JU0H5gD7AS8HfiBpn3ROozp/CtwA3NYnlJXAERGxTtKbgE7gkIF+L/fYzcz6NxPojogVEfEiMB+Y3eeY2cAV6fMC4GhJSuXzI2JDRKwEulN9DeuMiHsi4tG+QUTEzyJiXfrxTiC3V5TbhZS0b7rwpFS0GuiKiAfzzjUzG21S8/1VSXOBuXVFnRHRmT5PAh6r27eKP+4pbzomInokrQd2TOV39jm31obm1TmQk4Hv5R004B2QdAbZN4qAxWkTcFV/4011582VdJeku26bf9MgYjYzG542dTS9RURnRMyo2zrzr1AMSW8ga9jPyDs2r8d+MrBfRLzU5wIXAEuBz/d3Uro5nQA9vfd5jD355Pybiw6hNK46/qX8g8aIbQ7806JDqJTB9NhzrAb2qPt5cirr75hVkjqAHYA1Oefm1flHJO0PfA14U0SsyTs+7w70kg3897V72mdmViqSmt5yLAGmSZoqaTzZw9CuPsd0ASelz8cDt0aWWbELmJNmzUwFppGNeDRTZ9/f5xXAt4G/j4iHm7kHeT32DwI/lPRLNo8LvQLYGzitmQuYmY2u1vTY05j5acDNQDtwWUQslXQOcFdEdAHzgCsldQNryRpq0nHXAMuAHuDUiNgI2bTGvnWm8vcDpwO7AfdLuikiTgHOJhu3vzh9GfVExIyBYlde2l5lf9fM5A8fni6pBZnHQzGbTTvIQzE1n1mwf9EhlMZnf7xN0SGUxgPvOWzY0x132Gtu023O+kc6h329MsqdFRMRvfzh010zs9Jq4Rj7FmvEV8y0t40f6UtsMTRzr6JDKI137bVn0SGUxscv9gKlVmqTFwL6DphZpbjH7obdzCrGDbsbdjOrGFHJ56GDMuINe/bs1QDaf/540SGURm/sW3QIpaFbm5qabE1yj909djOrmLY2N2vNJAGbCURELEmpKGcBD0WEk8CYWQm5x56XBOzfgC8Bl0j6HPBlYBvgTEkfH+C8TUnAOjuvbWnAZmYDkdqa3qoqr8d+PHAAMAF4EpgcEc9I+g9gEfCZ/k6qTwLWG8u88jTZeFB/aXfGpqc2PFJ0CKUx6VN/XnQIlVLlBrtZeQ17T0od8JykRyLiGYCIeF6Sn4qaWenIQzG5DfuLkraOiOeAg2qFknbA2R3NrITcY89v2A+PiA2wKWdMzTg2p6o0MyuNtrb2okMo3IANe61R76f8KeCpEYnIzGwYPBQzGguUPGKziZ7p93tyTHrmRSeHq3ls8e+KDqE8jht+FR6K8QIlM6sYN+xu2M2sYjwU44bdzCpGTikw8g17m/yEuub5XzxYdAilsed2JxQdQmm0/WZZ0SFUShMvqa48f7WZWaV4KCY/V8whkrZPn/9E0qckfVfSeWmRkplZqThXTH4atMuA59LnLwI7AOelsq83OukPk4Bd05JAzcyaIjW/VVTeUExbRPSkzzMi4nXp808k3dvopPokYP+9+qa4ZfXK4UdaAa+94NiiQygNj4Nu9tQPvl90CCVy/PCrqG5HvGl5t+ABSe9Nn++TNANA0j7ASyMamZnZULS1Nb9VVN5vdgpwhKRHgOnAHZJWAJemfWZm5dI2iK2i8nLFrAfekx6gTk3Hr4qIX49GcGZmgxUe5mtuumPKw37fCMdiZjZ8btdHfh77r5/3AqWaSVv35B80RvSGH9HU7Pz6Y4oOoVra3LJ7gZKZVYuHYtywm1nFtLthr/BzYTMbk1q4QEnSLEnLJXVLOrOf/RMkXZ32L5I0pW7fWal8uaRj8+qUdFoqC0k71ZVL0pfSvvsl1dYTNTTiPfZ2xUhfYovx3z93T6Lm+UPWFh1CafTuvk3RIVRLi/4zk9QOXAQcA6wClkjqioj6rG0nA+siYm9Jc8hW5r9T0nRgDrAf8HLgB2n9DwPU+VPgBuC2PqG8CZiWtkOAS9L/N+Qeu5lVS5ua3wY2E+iOiBUR8SIwH5jd55jZwBXp8wLgaGXLqmcD8yNiQ0SsBLpTfQ3rjIh7IuLRfuKYDXwjMncCEyXtPlDguT12SXsCbwf2ADYCDwPfTFMgzczKZRA9dklzgbl1RZ0pJQrAJOCxun2r+OOe8qZjIqJH0npgx1R+Z59zJ6XPeXX21V8ck4AnGp2Ql93x/cBXgK2Ag4EJZA38nZKOHOC8TUnAbp1/U07MZmatE+1tzW8RnRExo27rzL9C+eX12P8ROCAiNkq6ALgpIo6U9FXgeuDA/k6qTwJ27j0/iBXu2wPwloP9Yu+aix/0nP6ad//tVkWHUC2te5S1mqwjWzM5lfV3zCpJHWQZcNfknJtX51Di+APNjLHXGv8JwLYAEfErYFwT55qZja7WzYpZAkyTNFXSeLKHoV19jukCTkqfjwdujYhI5XPSrJmpZA8+FzdZZ19dwIlpdsyfA+sjouEwDOT32L9G9tR2EXAY2RNfJO0MeFqDmZVPi1aepjHz04CbgXbgsohYKukc4K6I6ALmAVdK6iZrE+ekc5dKugZYBvQAp0bERsimNfatM5W/Hzgd2A24X9JNEXEKcBPwZrIHsM8BtYy7DSn7chngAGk/4NXAAxHx0CDuC5ANxQz2nKp63OkVNpmyrYdian73kqfB1nz6oL8c9s3Y+23faLrN6f7OiZW8+bmzYtK3ydJRiMXMbPicUmDkFyh97IABp1uOKfsc/IOiQyiNBxcfUXQIpTH96CVFh1Aan17YgkqcUsC5YsysYtxjd8NuZhXjdt0Nu5lVSzgf+8g37MHGkb7EFiNetUvRIZRGm9yn2GQrz5ZqKQ/F5KYU2F7S5yRdKeldffZdPLKhmZkNgQaxVVTeytOvk/363yJbRfUtSRPSvj8f0cjMzIaiva35raLyfrO9IuLMiLguIo4D7gZulbTjQCfVJwG7tHNBy4I1M8vlHnvuGPsESW0R0QsQEZ+RtBr4ESlvTH/qk4D1xlKvPE0e/3FeSoix49x7dy46hNJ4+amvKjqEavHD09we+3eBo+oLIuJy4F+BF0coJjOzoWvdiza2WAP22CPi9Abl35f02ZEJycxs6KK67XXThvP04FMti8LMrFX88HTgHruk+xvtAnZtfThmZsNU4SGWZuU9PN0VOBZY16dcwM+au4Rvcs0ekw4vOoTSOGnv54sOoTQun/9C0SGUx1taUEd1O+JNy2vYbwC2jYh7++6QdNuIRGRmNhxeeZr78PTkAfa9q9E+M7PCeCjGScDMrFrCPfbRaNi9Pqmm56Ddig6hNCZtM7noEEqj42eLiw6hWjrcsOclAZtV93kHSfMk3S/pm5I8K8bMykdqfquovOfH9YuQzgeeAP4aWAJ8daSCMjMbMq88HdTEoBkR8YmI+J+IuBCY0ujA+iRgnZ3XDjtIM7OmOQlY7hj7LpI+THYLtpekiKgNmjf8UqhPArYxHojwODsAer6n6BBKo13jiw6hPLbyHIZW8huU8hv2S4Ht0ucrgJ2A30raDfijue1mZoVzw547j73ffDAR8aSkhSMTkpnZMLS7YXcSMDOrFs+KcRIwM6sYD8WMfBIwOSPPJm0Pry06hNI47Lq+/6TGrl3+xW9Qaik37E4CZmbV4pQCOWPsEXFyRPykwT4nATOz8mlX81sOSbMkLZfULenMfvZPkHR12r9I0pS6fWel8uWSjs2rU9LUVEd3qnN8Kn+FpIWS7kkr/9+cF7fHScysWlq08lRSO3AR8CZgOnCCpOl9DjsZWBcRewMXAuelc6cDc4D9gFnAxZLac+o8D7gw1bUu1Q3wCeCaiDgw1Xlx3i0Y8ZUR8p9Fmzz9m4eKDqE07vvrvv99jF0HHLm06BDK422vH34drRtjnwl0R8QKAEnzgdnAsrpjZgOfTJ8XAF9W1ujNBuZHxAZgpaTuVB/91SnpQeAooDYSckWq9xKyTIrbp/IdgMfzAs9LAjYj/QnwX5L2kHSLpPWSlkg6MK9yM7NRN4iUAvXpT9I2t66mScBjdT+vSmX0d0xE9ADrgR0HOLdR+Y7A06mOvtf6JPBuSauAm4D35d2CvB77xcC/ARPJZsF8KCKOkXR02vcXeRcwMxtNg0kpUJ/+pMROAC6PiPMl/QVwpaQ/i4jeRifkjbGPi4jvRcRVQETEArIPPwS2anTSHyYBu2YIv4eZ2RC1boHSamCPup8np7J+j5HUQTZUsmaAcxuVrwEmpjr6Xutk4BqAiLiDrO3daaDA83rsL0h6Ywo2JL01Iq6TdASwsdFJ9d+Cazd8N9Zt6M65zNiw3RGHFh1CaXzr0eeKDqE0jvz8vkWHUC2tSymwBJgmaSpZIzuHzWPgNV3AScAdwPHArRERkrqAb0q6AHg5MA1YTDYI9Ed1pnMWpjrmpzqvT9f4FXA0cLmkV5M17L8dKPC8hv3/Al8AeskWKv2zpMtTQP+Yc66Z2ahra9Fcv4jokXQacDPQDlwWEUslnQPcFRFdwDyyoZFuYC1ZQ0067hqyB609wKkRsRGgvzrTJc8A5ks6F7gn1Q3wr8Clkj5E9iD1PXVZdvulnP2NT5TeGxFfzztu7YbvOmdvMuN9G4oOoTQ+dfp2+QeNET9+ckLRIZRG5+uPHHZ3e+pFtzfd5qw89YhKTttzEjAzqxTnAHMSMDOrGK+dGYUkYBPH7zmEsKrpie+fW3QIpfHuzrOLDqE0zv7Ph4sOoTxasT7J6+mdBMzMqkVu2HPfoHTyAPucBMzMSscjMaOQK8bMbDQ5HfswGnZJ34uIN+Udd9dTjw71EpWz1zn/UnQIpdEbLxUdQml0LB1wrYkNknvs+bNiXtdoF3BA68MxMxseN+z5PfYlwO1kDXlfE1sfjpnZ8LS1LqXAFivv+fGDwD9FxBv6bsBTjU6qTwL2nW98v6UBm5kNxAuU8nvsn6Rx498wJ3B9ErDFv73RKQXMbNRUucFuVt50xwWS9k351xdFxLN1u19o5gIzdpoyjPCq5YXrf150CKURJ7626BDKo8MTr1vJDXv+G5TeT5Y68n3AA5Jm1+3+7EgGZmY2FC165ekWLW8o5h+BgyLi2fT27QWSpkTEF+n/gaqZWaHcY89v2Ntqwy8R8aikI8ka91fiht3MSsizYvIb9l9LOqCWKyb13N8CXAa8ppkLfGTxr4cZYnVM+2cnRKtxBr7NtLapx1XWJP/Typ/ueCLwZH1BRPRExInA4SMWlZnZEHm6Y/6smFUD7Ptp68MxMxueKjfYzRp0rhhJu0TEb0YiGDOz4arybJdm5eWK+dO+RcBiSQeSvS91bd4FvnDwjsMIr1r2Pe6eokMojRWH7FR0CKWx81n7Fx1CpbS1Fx1B8fJ67E8B/9OnbBJwN9nbsv000MxKxUMx+Q37R4FjgI9GxC8AJK2MiKkjHpmZ2RB4xlXOrJiIOB84BThb0gWStiPrqQ+oPgnYpZ0LWhSqmVk+z4pp4uFpmhnzDknHAbcAWzdxzqYkYD299zkJmJmNmio32M3Kbdgl7Us2rn4rWcO+VyqfFRHOyTsIG1d7MlHNlO38npaa337hjqJDKI+3vX7YVbhhH2QSMOCNEfFA2u0kYGZWOh1tzW9V5SRgZlYpbfLor5OAmVmleIHSKCQBa9OgF7dWVvxVU7dsTPh9z+NFh1Aa4+buV3QIlVLhEZam5bW6JwI99QUR0QOcKOmrIxaVmdkQeSgmfx77qoh4ssE+JwEzs9Jp5RuUJM2StFxSt6Qz+9k/QdLVaf+i9Cyytu+sVL5c0rF5dUqamuroTnWOr9v3t5KWSVoq6Zu59yD/VzMz23J0qPltIJLagYuANwHTgRMkTe9z2MnAuojYG7gQOC+dOx2YA+wHzAIultSeU+d5wIWprnWpbiRNA84CDo2I/YAP5t6DnF/sbuDbwFUR8UheZf15rscv2qjp3SV3bdeY8exLzxQdQmk8d++6okOoFLVuKGYm0B0RK7J6NR+YDSyrO2Y28Mn0eQHwZWU5DWYD8yNiA7BSUneqj/7qlPQgcBTwrnTMFaneS8hmJ14UEesAmsmum9djfxkwEVgoabGkD0l6eV6lZmZFGcxQTH36k7TNratqEvBY3c+rUhn9HZOeP64Hdhzg3EblOwJPpzr6XmsfYB9JP5V0p6RZefcg7+Hpuoj4CPARSYcBJwB3p2+Xq1LqADOz0hjM+HJ9+pMS6wCmAUcCk4EfSXpNRDzd6ISm70FE/Dgi/oXsW+Q84C8aHVv/LXjZ125o9hJmZsPWpmh6y7Ea2KPu58mprN9jJHUAOwBrBji3UfkaYGKqo++1VgFdEfFSRKwEHiZr6Bvfg5xf7OG+BRGxMSK+HxHvbXRSRHRGxIyImPEPp7wl5xJmZq3TqoenwBJgWpqtMp7sYWhXn2O6gJPS5+OBWyMiUvmcNGtmKllDvLhRnemchakOUp3Xp8/XkfXWkbQT2dDMigHvwUA7I2JOXRKwRbVVqOkCTgI2SB0P575waszYusOvuanZuOfEokOolFatPI2IHkmnATcD7cBlEbFU0jnAXRHRBcwDrkwPR9eSNdSk464he9DaA5waERsB+qszXfIMYL6kc4F7Ut2kY98oaRmwkez9GGsGij1vVsz7gNOAB4F5kj4QEbVvkc8CbtjNrFRauUApIm4CbupTdnbd5xeAdzQ49zPAZ5qpM5WvYPPMmfryAD6ctqbkPTydi5OAmdkWxLlinATMzCrGqy5HIQnY1h27DjPE6tCafrMzjEnbjtun6BBKo321/120knPFOAmYmVVMlV+g0ay8WTGrBtjnJGBmVjpu15t456mZ2ZbEQzH50x07yDKMvQ2o5YhZTTZxfl5EvJR3gZ54YbgxVsfW44qOoDTa8L2oaVvzfNEhVIpnxeT32K8EnibLMlYblplMtirqv4B3jlhkZmZD4KGY/Ib9oIjoO31hFXCnpD9KN2BmVjT32PO/3NZKeoekTcdJapP0TrJE8P2qTwL2tc5vtypWM7Nc7W3R9FZVeT32OWSZHC+SVEsROZEsWc2cRifVp8J8sffn1b17ZlY6HorJn+74qKQLgPOBR4B9ydL1LkvpI3Nd+cunhh1kVRx20i5Fh1AabfKErJq2lQ3TatsQeFZM/qyYfyN7N18HcAtZgprbgDMlHZiS3JiZlYbH2POHYo4HDgAmAE8CkyPiGUn/ASyin8xlZmZFcsOe37D3pBzCz0l6JCKeAYiI5yX1jnx4ZmaDM85DMbkN+4uSto6I54CDaoWSdgCaath/+4IfZdRM2ron/6Axom6i1ZjXO9Uv2mgl99jzG/bDI2IDQETUN+Tj2Pw6KDOz0nDDnj8rZkOD8qcAT3cxs9Jpd8PuJGBmVi3usedPd9ya7J2nAfwn2aKktwMPAefUv9y6kY+8xnO3a6Z96NGiQyiNE/Z6pOgQSmP6nN2KDqFSPI89f5HW5cCuwFTgRmAG8O9kr8W7ZEQjMzMbgnFqfquqvKGYfSLibyUJeAL4y4gIST8B7hv58MzMBsdDMU2mVYiIAG5K/1/7ueHfO/VJwC7tXNCaSM3MmtCmaHqrqrwe+12Sto2IZyPiH2qFkvYCftfopPokYD2991X37plZ6XhWTP50x1MkzZQUEbFE0nRgFrAcOKyZCzjZ02a9u25ddAilMXkbL9aqWXbl40WHUB7HDL8KD8UMIgmYpFuAQ8hS9p5BlkPGuWLMrFQ6vKjZScDMrFraKzx23iwnATOzSnGHfRSSgN23tqn3cYwJHQ/9vugQSmOH8QcUHUJptD/4g6JDqBSPsed/uR2eGnUnATOzLUKbmt/ySJolabmkbkln9rN/gqSr0/5FkqbU7TsrlS+XdGxenZKmpjq6U53j+1zrbySFpBm592CgnQMlAYuIX+RVbmY22toVTW8DkdQOXEQ2gWQ6cEKaGVjvZGBdROwNXEj2jmjScXOA/chmEl4sqT2nzvOAC1Nd61LdtVi2Az5A9mwzl4ejzKxSOtqa33LMBLojYkVEvAjMB2b3OWY2cEX6vAA4Oq3Unw3Mj4gN6f3Q3am+futM5xyV6iDV+da663yarOF/oal7MNBOSael4J6StDdwGbA/2Tz2U5rptW/jaeybbJy0bdEhlMa1K/zspabndU4C1kqDGWOXNBeYW1fUmRZYAkwCHqvbt4psyne9TcdERI+k9cCOqfzOPudOSp/7q3NH4OmI6Ol7vKTXAXtExI2SPtrM75XX7P5zRHw5ff4i2Z8J35F0JPAV4NBmLmJmNloGs/K0fpV8GSl71dgFwHsGc15ew16/f5eI+A5ARNyWxnzMzEqlhTlgVgN71P08OZX1d8wqSR3ADsCanHP7K18DTJTUkXrttfLtgD8DbstGa9gN6JJ0XETc1SjwvFGmBZIul7Qn8B1JH5T0SknvBX7V6KT6JGBXX/79nEuYmbVO2yC2HEuAaWm2yniyh6FdfY7pYvMMweOBW1OSxC5gTpo1MxWYBixuVGc6Z2Gqg1Tn9RGxPiJ2iogpETGFbHhnwEYd8nPFfFzSe4CrgL3IVqDOBa4D/m6A8zb9efPw+hu8DMzMRk2r5rGnMfPTgJuBduCyiFgq6RzgrojoAuYBV0rqBtaSNdSk464BlgE9wKlpsSf91ZkueQYwX9K5wD2p7iFRysTb+ABpZhZnLJFUm7rzYETc1MwFNsYDbtiTfU5amn/QGPHQ5fsUHUJp7POvDf/4HXNWXjh72M3yj5+8sek257Dd/qqSy5kGmwRsJnAbcKakAyPCuWLMrFS88tRJwMysYtywOwmYmVWMV12OQhIw4a/PTcb5n1xNuyYUHUJptHevKzqESpGbnNyG/fBavhgnATOzLYGHYvKnOzZMAgY8NSIRmZkNg/8uzu+xm5ltUeQ3KOVOd2wjy1HwN2RLXDcCDwNfiYjbRjq4qvn9HXcXHUJpXPHL7YsOoTTe/LFXFh1CpXgkJr/HPg/4H+BzZFMfnwF+DHxC0msi4j9HOD4zs0Hxw9P8hv2giHhv+vwTSXdGxNmSfgTcC7hhN7NScbue/5zhJUl7waacwC/CpoeqDQey6pOAdXZe27JgzczytKv5raryeuwfBRZK2pCOnQMgaWfghkYn1ScB642lfpJhZqPGQzH50x1vlfROshWoSyRNl/Rh4KGIOL2ZCwQbWxFnJWy7b9/XJY5dJ07bvegQSuPs468rOoTS+NL9bxh2HW7XnQTMzCrGDbuTgJlZxXjlqZOAmVnFuF0fhSRgeS/yGEviZVsVHYKV0PgDX1V0CJXSwneebrGcBMzMKsWzYpwEzMwqxknAnATMzCrGPfacLzdJ7ZL+SdKnJR3aZ98nRjY0M7PB0yC2qsrrsX8V2BpYDHxJ0u0R8eG07+3AuXkXOO2OtcOLsEJ69n5Z0SGUxopnflV0CKXRu/PWRYdQKZ7umD8cNTMi3hUR/w84BNhW0rclTaDaX3hmtoVqU/NbVeU17ONrHyKiJyLmAvcBtwLbNjqpPgnYsusbppQxM2s5D8XkN+x3SZpVXxARnwK+DkxpdFJEdEbEjIiYMX32W4YfpZlZk6RoequqvOmO7+5bJukbEXEi8LVmLnDJoS8fYmjVs/f7v1d0CKWx15nHFB1CabStvq/oECqlyj3xZuUlAevqWwS8QdJEgIg4bqQCMzMbCk93zJ8VswewlKx3HmQN+wzg/BGOy8xsSNqLDqAE8sbYDwJ+DnwcWJ9eYP18RNweEbePdHBmZoMlNb9VVd4Yey9woaRr0///Ou+cvjZsXDeM8Cpmfz9vqFny20eLDqE8xruP2VoVbrGb1FRahYhYFRHvAL4H/NfIhmRmNnQaxP9y65JmSVouqVvSmf3snyDp6rR/kaQpdfvOSuXLJR2bV6ekqamO7lTn+FT+YUnLJN0v6YeSXpkX96Dy5UTEjRHxscGcY2Y2mqS2preB61E7cBHZW+SmAydI6vt+y5OBdRGxN3AhcF46dzrZO6L3A2YBF6cULQPVeR5wYaprXaob4B5gRkTsDywAvpB3D5wIzcwqpmVLlGYC3RGxIiJeBOYDs/scMxu4In1eABwtSal8fkRsiIiVQHeqr9860zlHpTpIdb4VICIWpndiANwJTM4LPC8J2P51n8dJ+oSkLkmfleQEF2ZWOqKt+a1ulXza5tZVNQl4rO7nVamM/o6JiB5gPbDjAOc2Kt8ReDrV0ehakPXicxfE5D0IvRx4Xfr8+XTx88m+Sb4CnJh3gdMXP593yJhx2Ht3LTqE0nj1y14sOoTy6PAfzq2UN8RSLyI6gc6Ri6Z1JL2bbLr5EXnH5jXs9X+rHA0cHBEvSfoRWc4YM7OSadmsmNVka3lqJqey/o5ZJakD2AFYk3Nuf+VrgImSOlKv/Q+uJekvyaadH9HoBUj18r7adpD0Nkl/A0yIiJcAInuRacNEC/V/3jxwnZOAmdnoaeGsmCXAtDRbZTzZw9C+q/G72Pya0OOBW1P72AXMSbNmpgLTyNKf91tnOmdhqoNU5/UAkg4kS6F+XET8ppl7kNdj/xFQSxtwp6RdI+LXknZjgFfj1f958/47FlY3046ZlU4z0xibERE9kk4DbiZb0HpZRCyVdA5wV0R0AfOAKyV1A2vJGmrScdcAy4Ae4NSI2AjQX53pkgDrtAkAAAQ0SURBVGcA8yWdSzYTZl4q/3eybLrXZs9Y+VVeOhdlXxTNq0sC1pSXeu9xw55MfW3fL/uxa+V9fScXjF2vfvv9RYdQGt3XnTjsVvn3Pbc33eZs03FEJVczDTYJGMBRTgJmZuVVybZ6UIaSBOxgnATMzEqqVUMxWzInATOzimkbxFZNI54E7PYnnhhGeNUy9TNHFx1CaXxr5ZNFh1Aa/+dDuak/bBDcY2+ykY6IVcA7JP0V8MzIhmRmNnSqcj7eJg2q9x0RNwI3jlAsZmbDJr9qY3ANu5lZ+bnHnpcEbE9Jl0k6V9K2ki6V9ICka+vzDpuZlYWkpreqaiYJ2FVk+Q/uBL4OnAO8EbiMLM3kgO5eM254EVbI6hv8wLDm0KMnFB1CaZz11Z78g8aK3PRWzahug92svPk+20XEJRHxeWD7iDg/Ih6LiHnAy0YhPjOzQRlM2t6qyvvNeiXtI+lgYGtJMwAk7c0ALwOvTwK2aIGftZrZaGrZiza2WHlDMacD3wV6yXKwn5VevrEDMLfRSfVJwL5w/y3OFWNmo6ZtEPnYqypvgdIPgVfVFf1E0g1k6SN7m7nA26e8MIzwquWSl21VdAilcevjfvZSs/3+fhlZa7lhH0oSsCOB6yQ5CZiZlY5XnjoJmJlVjht2JwEzs0rxPPZRSAJmZjaanFJgkG9QSknADo2Ij41cSCND0tw0W2fM873YzPdiM9+L6hj0q/G2VJLuiogZRcdRBr4Xm/lebOZ7UR2eF2RmVjFu2M3MKmYsNeweO9zM92Iz34vNfC8qYsyMsZuZjRVjqcduZjYmuGE3M6uYyjfskmZJWi6pW9KZRcdTpPQ2rN9IeqDoWIokaQ9JCyUtk7RU0geKjqkokraStFjSfelefKromGz4Kj3GLqkdeBg4BlgFLAFOiIhlhQZWEEmHA88C34iIPys6nqJI2h3YPSLulrQdWdqMt47FfxfK1tVvExHPShoH/AT4QETcWXBoNgxV77HPBLojYkVEvAjMB2YXHFNhIuJHwNqi4yhaRDwREXenz78DHgQmFRtVMSLzbPpxXNqq29sbI6resE8CHqv7eRVj9D9g6196KfuBwKJiIymOpHZJ9wK/AW6JiDF7L6qi6g27WUOStgW+BXwwIp4pOp6iRMTGiDgAmAzMlDRmh+mqouoN+2qynPI1k1OZjXFpPPlbwP+PiG8XHU8ZRMTTwEJgVtGx2PBUvWFfAkyTNFXSeGAO0N9boWwMSQ8M5wEPRsQFRcdTJEk7S5qYPv8J2USDh4qNyoar0g17RPQApwE3kz0guyYilhYbVXEkXQXcAbxK0ipJJxcdU0EOBf4eOErSvWl7c9FBFWR3YKGk+8k6QrdExA0Fx2TDVOnpjmZmY1Gle+xmZmORG3Yzs4pxw25mVjFu2M3MKsYNu5lZxbhhNzOrGDfsZmYV87+tZMh/qTY5hAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = {\n",
        "    \"num_epochs\" : 45,\n",
        "    \"batch_size\" : 100,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 1,\n",
        "    'learning_rate' : 0.0002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.1,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "\n",
        "with open('log_baseline_train3.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test3.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "with open('record3.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"update\", \"grad\", \"threshold\"])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "net = TweetRNN(50, 50, 2)\n",
        "trainloader = TweetBatcher(train, batch_size=1, drop_last=True)\n",
        "testloader = TweetBatcher(valid, batch_size=1, drop_last=False)\n",
        "c3f1_loss_per_epoch2, c3f1_update_per_epoch2, c3f1_every_loss2, c3f1_thr_per_batch2 = net_trainer(net, trainloader, testloader, train_config)\n",
        "\n",
        "# model = TweetRNN(50, 50, 2)\n",
        "\n",
        "# train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=2e-4)\n",
        "# get_accuracy(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpw0ILm7xwQ0",
        "outputId": "e3b41b48-7952-440e-99f1-b43fc8e5070b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 2.434 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.947 | Acc: 50.495% (51/101)\n",
            "Epoch 0 iteration 200 Loss: 0.865 | Acc: 55.721% (112/201)\n",
            "Test accuracy: 0.5572139024734497\n",
            "Epoch: 1, Loss: 1.461568832397461 Updates: 0/0, Avg Grad: 0.23934757709503174, Threshold: 0.26328232884407043\n",
            "Epoch: 1, Loss: 0.024587588384747505 Updates: 2/1000, Avg Grad: 0.14794102311134338, Threshold: 0.35156378149986267\n",
            "Epoch: 1, Loss: 0.01572323404252529 Updates: 4/2000, Avg Grad: 0.03635737672448158, Threshold: 0.3700334131717682\n",
            "Epoch: 1, Loss: 0.002937908982858062 Updates: 5/3000, Avg Grad: 0.11159443855285645, Threshold: 0.43685922026634216\n",
            "Epoch: 1, Loss: 0.005956822074949741 Updates: 5/4000, Avg Grad: 0.4231644570827484, Threshold: 0.43685922026634216\n",
            "Epoch: 1, Loss: 0.0015758919762447476 Updates: 6/5000, Avg Grad: 0.21581962704658508, Threshold: 0.5055859684944153\n",
            "Epoch: 1, Loss: 0.005920419469475746 Updates: 6/6000, Avg Grad: 0.4165143072605133, Threshold: 0.5055859684944153\n",
            "Epoch: 1, Loss: 0.00781682413071394 Updates: 7/7000, Avg Grad: 0.07187090069055557, Threshold: 0.5576127171516418\n",
            "Epoch: 1, Loss: 0.00685855932533741 Updates: 7/8000, Avg Grad: 0.14151234924793243, Threshold: 0.5576127171516418\n",
            "Epoch: 1, Loss: 0.00670706806704402 Updates: 7/9000, Avg Grad: 0.1943684220314026, Threshold: 0.5576127171516418\n",
            "Epoch 0 iteration 0 Loss: 0.439 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 0.797 | Acc: 36.634% (37/101)\n",
            "Epoch 0 iteration 200 Loss: 0.790 | Acc: 42.289% (85/201)\n",
            "Test accuracy: 0.42288556694984436\n",
            "Epoch: 1, Loss: 0.005328252445906401 Updates: 7/10000, Avg Grad: 0.2860327959060669, Threshold: 0.5576127171516418\n",
            "Epoch: 1, Loss: 0.0059183198027312756 Updates: 7/11000, Avg Grad: 0.3869549036026001, Threshold: 0.5576127171516418\n",
            "Epoch: 1, Loss: 0.008902031928300858 Updates: 7/12000, Avg Grad: 0.468772292137146, Threshold: 0.5576127171516418\n",
            "Epoch: 1, Loss: 0.002808381337672472 Updates: 8/13000, Avg Grad: 0.016242124140262604, Threshold: 0.6358308792114258\n",
            "Epoch: 1, Loss: 0.0050749103538692 Updates: 8/14000, Avg Grad: 0.12380728870630264, Threshold: 0.6358308792114258\n",
            "Epoch: 1, Loss: 0.014846522361040115 Updates: 8/15000, Avg Grad: 0.2629111409187317, Threshold: 0.6358308792114258\n",
            "Epoch: 1, Loss: 0.005848925560712814 Updates: 8/16000, Avg Grad: 0.3458588719367981, Threshold: 0.6358308792114258\n",
            "Epoch: 1, Loss: 0.011984947137534618 Updates: 8/17000, Avg Grad: 0.48427069187164307, Threshold: 0.6358308792114258\n",
            "Epoch: 1, Loss: 0.0070024942979216576 Updates: 9/18000, Avg Grad: 0.03377542272210121, Threshold: 0.6728329062461853\n",
            "Epoch: 1, Loss: 0.009703686460852623 Updates: 9/19000, Avg Grad: 0.2508145272731781, Threshold: 0.6728329062461853\n",
            "Epoch 0 iteration 0 Loss: 1.170 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.768 | Acc: 50.495% (51/101)\n",
            "Epoch 0 iteration 200 Loss: 0.799 | Acc: 46.766% (94/201)\n",
            "Test accuracy: 0.46766167879104614\n",
            "Epoch: 1, Loss: 0.004869490396231413 Updates: 9/20000, Avg Grad: 0.5097342729568481, Threshold: 0.6728329062461853\n",
            "Epoch: 1, Loss: 0.005764448083937168 Updates: 10/21000, Avg Grad: 0.10696949064731598, Threshold: 0.7030554413795471\n",
            "Epoch: 1, Loss: 0.010287913493812084 Updates: 10/22000, Avg Grad: 0.40961718559265137, Threshold: 0.7030554413795471\n",
            "Epoch: 1, Loss: 0.011249891482293606 Updates: 11/23000, Avg Grad: 0.041269492357969284, Threshold: 0.7544666528701782\n",
            "Epoch: 1, Loss: 0.006140267942100763 Updates: 11/24000, Avg Grad: 0.28565895557403564, Threshold: 0.7544666528701782\n",
            "Epoch: 1, Loss: 0.012640879489481449 Updates: 11/25000, Avg Grad: 0.5441728830337524, Threshold: 0.7544666528701782\n",
            "Epoch: 1, Loss: 0.010397050529718399 Updates: 11/26000, Avg Grad: 0.7437662482261658, Threshold: 0.7544666528701782\n",
            "Epoch: 1, Loss: 0.008413347415626049 Updates: 12/27000, Avg Grad: 0.22521822154521942, Threshold: 0.784666895866394\n",
            "Epoch: 1, Loss: 0.012081422843039036 Updates: 12/28000, Avg Grad: 0.45629242062568665, Threshold: 0.784666895866394\n",
            "Epoch: 1, Loss: 0.008936038240790367 Updates: 12/29000, Avg Grad: 0.7390962243080139, Threshold: 0.784666895866394\n",
            "Epoch 0 iteration 0 Loss: 0.288 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 0.748 | Acc: 50.495% (51/101)\n",
            "Epoch 0 iteration 200 Loss: 0.780 | Acc: 46.269% (93/201)\n",
            "Test accuracy: 0.46268656849861145\n",
            "Epoch: 1, Loss: 0.009952441789209843 Updates: 13/30000, Avg Grad: 0.16622671484947205, Threshold: 0.8152126669883728\n",
            "Epoch: 1, Loss: 0.005529841873794794 Updates: 13/31000, Avg Grad: 0.2710505425930023, Threshold: 0.8152126669883728\n",
            "Epoch: 1, Loss: 0.009056064300239086 Updates: 13/32000, Avg Grad: 0.3434741199016571, Threshold: 0.8152126669883728\n",
            "Epoch 1 iteration 0 Loss: 0.665 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.818 | Acc: 41.584% (42/101)\n",
            "Epoch 1 iteration 200 Loss: 0.768 | Acc: 47.264% (95/201)\n",
            "Test accuracy: 0.4726368188858032\n",
            "Epoch: 2, Loss: 0.459336519241333 Updates: 0/0, Avg Grad: 0.10687702149152756, Threshold: 0.11756472289562225\n",
            "Epoch: 2, Loss: 0.004682194907218218 Updates: 1/1000, Avg Grad: 0.03778631240129471, Threshold: 0.13067573308944702\n",
            "Epoch: 2, Loss: 0.004502445925027132 Updates: 1/2000, Avg Grad: 0.07170091569423676, Threshold: 0.13067573308944702\n",
            "Epoch: 2, Loss: 0.008339595049619675 Updates: 2/3000, Avg Grad: 0.13195642828941345, Threshold: 0.14515207707881927\n",
            "Epoch: 2, Loss: 0.015609580092132092 Updates: 2/4000, Avg Grad: 0.06475319713354111, Threshold: 0.14515207707881927\n",
            "Epoch: 2, Loss: 0.01611477881669998 Updates: 3/5000, Avg Grad: 0.02430027909576893, Threshold: 0.16931113600730896\n",
            "Epoch: 2, Loss: 0.007939819246530533 Updates: 4/6000, Avg Grad: 0.084139384329319, Threshold: 0.1995275914669037\n",
            "Epoch: 2, Loss: 0.012074798345565796 Updates: 5/7000, Avg Grad: 0.033424265682697296, Threshold: 0.2296241670846939\n",
            "Epoch: 2, Loss: 0.004913985263556242 Updates: 5/8000, Avg Grad: 0.18895934522151947, Threshold: 0.2296241670846939\n",
            "Epoch: 2, Loss: 0.008710375055670738 Updates: 6/9000, Avg Grad: 0.11541472375392914, Threshold: 0.25672271847724915\n",
            "Epoch 1 iteration 0 Loss: 0.496 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.761 | Acc: 53.465% (54/101)\n",
            "Epoch 1 iteration 200 Loss: 0.768 | Acc: 50.746% (102/201)\n",
            "Test accuracy: 0.5074626803398132\n",
            "Epoch: 2, Loss: 0.004565664101392031 Updates: 7/10000, Avg Grad: 0.06803862750530243, Threshold: 0.3182521164417267\n",
            "Epoch: 2, Loss: 0.005453822202980518 Updates: 7/11000, Avg Grad: 0.2648334205150604, Threshold: 0.3182521164417267\n",
            "Epoch: 2, Loss: 0.005043847020715475 Updates: 8/12000, Avg Grad: 0.052754390984773636, Threshold: 0.37610796093940735\n",
            "Epoch: 2, Loss: 0.005360924638807774 Updates: 8/13000, Avg Grad: 0.1415555626153946, Threshold: 0.37610796093940735\n",
            "Epoch: 2, Loss: 0.006313303019851446 Updates: 8/14000, Avg Grad: 0.3136335611343384, Threshold: 0.37610796093940735\n",
            "Epoch: 2, Loss: 0.008831152692437172 Updates: 9/15000, Avg Grad: 0.0370887815952301, Threshold: 0.4374462962150574\n",
            "Epoch: 2, Loss: 0.00645570270717144 Updates: 9/16000, Avg Grad: 0.08658185601234436, Threshold: 0.4374462962150574\n",
            "Epoch: 2, Loss: 0.008329413831233978 Updates: 9/17000, Avg Grad: 0.12913888692855835, Threshold: 0.4374462962150574\n",
            "Epoch: 2, Loss: 0.005240785423666239 Updates: 9/18000, Avg Grad: 0.20448794960975647, Threshold: 0.4374462962150574\n",
            "Epoch: 2, Loss: 0.01205330342054367 Updates: 9/19000, Avg Grad: 0.24664859473705292, Threshold: 0.4374462962150574\n",
            "Epoch 1 iteration 0 Loss: 0.767 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 0.755 | Acc: 50.495% (51/101)\n",
            "Epoch 1 iteration 200 Loss: 0.756 | Acc: 46.766% (94/201)\n",
            "Test accuracy: 0.46766167879104614\n",
            "Epoch: 2, Loss: 0.0072571937926113605 Updates: 9/20000, Avg Grad: 0.31499114632606506, Threshold: 0.4374462962150574\n",
            "Epoch: 2, Loss: 0.007177309598773718 Updates: 9/21000, Avg Grad: 0.38508522510528564, Threshold: 0.4374462962150574\n",
            "Epoch: 2, Loss: 0.01678643189370632 Updates: 10/22000, Avg Grad: 0.007334710564464331, Threshold: 0.4667123556137085\n",
            "Epoch: 2, Loss: 0.013997180387377739 Updates: 10/23000, Avg Grad: 0.06475662440061569, Threshold: 0.4667123556137085\n",
            "Epoch: 2, Loss: 0.008257974870502949 Updates: 10/24000, Avg Grad: 0.14407654106616974, Threshold: 0.4667123556137085\n",
            "Epoch: 2, Loss: 0.008868767879903316 Updates: 10/25000, Avg Grad: 0.22641222178936005, Threshold: 0.4667123556137085\n",
            "Epoch: 2, Loss: 0.005417967215180397 Updates: 10/26000, Avg Grad: 0.31473198533058167, Threshold: 0.4667123556137085\n",
            "Epoch: 2, Loss: 0.004566672258079052 Updates: 10/27000, Avg Grad: 0.37478378415107727, Threshold: 0.4667123556137085\n",
            "Epoch: 2, Loss: 0.009354058653116226 Updates: 11/28000, Avg Grad: 0.040244437754154205, Threshold: 0.485354483127594\n",
            "Epoch: 2, Loss: 0.005008467473089695 Updates: 11/29000, Avg Grad: 0.15598121285438538, Threshold: 0.485354483127594\n",
            "Epoch 1 iteration 0 Loss: 0.907 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 0.730 | Acc: 54.455% (55/101)\n",
            "Epoch 1 iteration 200 Loss: 0.727 | Acc: 51.244% (103/201)\n",
            "Test accuracy: 0.5124378204345703\n",
            "Epoch: 2, Loss: 0.008163059130311012 Updates: 11/30000, Avg Grad: 0.16756652295589447, Threshold: 0.485354483127594\n",
            "Epoch: 2, Loss: 0.008278170600533485 Updates: 11/31000, Avg Grad: 0.27114227414131165, Threshold: 0.485354483127594\n",
            "Epoch: 2, Loss: 0.009977337904274464 Updates: 11/32000, Avg Grad: 0.3808051645755768, Threshold: 0.485354483127594\n",
            "Epoch 2 iteration 0 Loss: 0.301 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 100 Loss: 0.852 | Acc: 40.594% (41/101)\n",
            "Epoch 2 iteration 200 Loss: 0.801 | Acc: 43.284% (87/201)\n",
            "Test accuracy: 0.43283581733703613\n",
            "Epoch: 3, Loss: 0.41659730672836304 Updates: 0/0, Avg Grad: 0.10422924160957336, Threshold: 0.11465217173099518\n",
            "Epoch: 3, Loss: 0.01014932431280613 Updates: 1/1000, Avg Grad: 0.07873360812664032, Threshold: 0.14407651126384735\n",
            "Epoch: 3, Loss: 0.003033947665244341 Updates: 3/2000, Avg Grad: 0.06167374551296234, Threshold: 0.2387489527463913\n",
            "Epoch: 3, Loss: 0.006701233796775341 Updates: 4/3000, Avg Grad: 0.17968246340751648, Threshold: 0.27947181463241577\n",
            "Epoch: 3, Loss: 0.012624496594071388 Updates: 5/4000, Avg Grad: 0.33198031783103943, Threshold: 0.3348592221736908\n",
            "Epoch: 3, Loss: 0.0152768325060606 Updates: 6/5000, Avg Grad: 0.33227407932281494, Threshold: 0.41726142168045044\n",
            "Epoch: 3, Loss: 0.009642431512475014 Updates: 7/6000, Avg Grad: 0.1755830943584442, Threshold: 0.4831417202949524\n",
            "Epoch: 3, Loss: 0.004812463652342558 Updates: 8/7000, Avg Grad: 0.05593772977590561, Threshold: 0.5317868590354919\n",
            "Epoch: 3, Loss: 0.010694864206016064 Updates: 8/8000, Avg Grad: 0.4286832809448242, Threshold: 0.5317868590354919\n",
            "Epoch: 3, Loss: 0.003418461186811328 Updates: 9/9000, Avg Grad: 0.10923774540424347, Threshold: 0.5897665619850159\n",
            "Epoch 2 iteration 0 Loss: 0.632 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 100 Loss: 0.719 | Acc: 52.475% (53/101)\n",
            "Epoch 2 iteration 200 Loss: 0.728 | Acc: 54.229% (109/201)\n",
            "Test accuracy: 0.5422885417938232\n",
            "Epoch: 3, Loss: 0.006074977107346058 Updates: 9/10000, Avg Grad: 0.23938167095184326, Threshold: 0.5897665619850159\n",
            "Epoch: 3, Loss: 0.0010207031155005097 Updates: 9/11000, Avg Grad: 0.38044002652168274, Threshold: 0.5897665619850159\n",
            "Epoch: 3, Loss: 0.010056554339826107 Updates: 10/12000, Avg Grad: 0.5943263173103333, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.005430934485048056 Updates: 10/13000, Avg Grad: 0.0498792938888073, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.006843496114015579 Updates: 10/14000, Avg Grad: 0.13092301785945892, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.01068565621972084 Updates: 10/15000, Avg Grad: 0.15448884665966034, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.0034052154514938593 Updates: 10/16000, Avg Grad: 0.23355117440223694, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.005451889708638191 Updates: 10/17000, Avg Grad: 0.2960885763168335, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.007695319596678019 Updates: 10/18000, Avg Grad: 0.37876656651496887, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.005941743031144142 Updates: 10/19000, Avg Grad: 0.3731325566768646, Threshold: 0.6537589430809021\n",
            "Epoch 2 iteration 0 Loss: 0.960 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.730 | Acc: 48.515% (49/101)\n",
            "Epoch 2 iteration 200 Loss: 0.726 | Acc: 48.756% (98/201)\n",
            "Test accuracy: 0.4875621795654297\n",
            "Epoch: 3, Loss: 0.0028877975419163704 Updates: 10/20000, Avg Grad: 0.42671290040016174, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.0048054964281618595 Updates: 10/21000, Avg Grad: 0.4767945110797882, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.008050406351685524 Updates: 10/22000, Avg Grad: 0.5509837865829468, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.004970638081431389 Updates: 10/23000, Avg Grad: 0.5941117405891418, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.007269880268722773 Updates: 10/24000, Avg Grad: 0.609480082988739, Threshold: 0.6537589430809021\n",
            "Epoch: 3, Loss: 0.006432895548641682 Updates: 11/25000, Avg Grad: 0.04697919264435768, Threshold: 0.6709457635879517\n",
            "Epoch: 3, Loss: 0.004995904862880707 Updates: 11/26000, Avg Grad: 0.1552673727273941, Threshold: 0.6709457635879517\n",
            "Epoch: 3, Loss: 0.010570150800049305 Updates: 11/27000, Avg Grad: 0.37246882915496826, Threshold: 0.6709457635879517\n",
            "Epoch: 3, Loss: 0.0032545134890824556 Updates: 11/28000, Avg Grad: 0.49939659237861633, Threshold: 0.6709457635879517\n",
            "Epoch: 3, Loss: 0.00647976528853178 Updates: 11/29000, Avg Grad: 0.6383826732635498, Threshold: 0.6709457635879517\n",
            "Epoch 2 iteration 0 Loss: 1.270 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.747 | Acc: 49.505% (50/101)\n",
            "Epoch 2 iteration 200 Loss: 0.726 | Acc: 51.741% (104/201)\n",
            "Test accuracy: 0.5174129605293274\n",
            "Epoch: 3, Loss: 0.004117321223020554 Updates: 12/30000, Avg Grad: 0.186418354511261, Threshold: 0.6936737298965454\n",
            "Epoch: 3, Loss: 0.006118301302194595 Updates: 12/31000, Avg Grad: 0.37490230798721313, Threshold: 0.6936737298965454\n",
            "Epoch: 3, Loss: 0.008127188310027122 Updates: 12/32000, Avg Grad: 0.600893497467041, Threshold: 0.6936737298965454\n",
            "Epoch 3 iteration 0 Loss: 0.716 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.706 | Acc: 56.436% (57/101)\n",
            "Epoch 3 iteration 200 Loss: 0.721 | Acc: 54.229% (109/201)\n",
            "Test accuracy: 0.5422885417938232\n",
            "Epoch: 4, Loss: 0.4230152368545532 Updates: 0/0, Avg Grad: 0.131914421916008, Threshold: 0.14510586857795715\n",
            "Epoch: 4, Loss: 0.01242288202047348 Updates: 0/1000, Avg Grad: 0.08860602229833603, Threshold: 0.14510586857795715\n",
            "Epoch: 4, Loss: 0.005471381358802319 Updates: 1/2000, Avg Grad: 0.0543222613632679, Threshold: 0.1607659012079239\n",
            "Epoch: 4, Loss: 0.01036601047962904 Updates: 1/3000, Avg Grad: 0.13211184740066528, Threshold: 0.1607659012079239\n",
            "Epoch: 4, Loss: 0.004412881564348936 Updates: 2/4000, Avg Grad: 0.1567317247390747, Threshold: 0.1783818155527115\n",
            "Epoch: 4, Loss: 0.008277973160147667 Updates: 3/5000, Avg Grad: 0.1686890423297882, Threshold: 0.21812692284584045\n",
            "Epoch: 4, Loss: 0.00915485993027687 Updates: 4/6000, Avg Grad: 0.2114623636007309, Threshold: 0.24148789048194885\n",
            "Epoch: 4, Loss: 0.007296067662537098 Updates: 5/7000, Avg Grad: 0.21562440693378448, Threshold: 0.2684282958507538\n",
            "Epoch: 4, Loss: 0.009992827661335468 Updates: 6/8000, Avg Grad: 0.12246784567832947, Threshold: 0.3052051365375519\n",
            "Epoch: 4, Loss: 0.0023589557968080044 Updates: 6/9000, Avg Grad: 0.23860909044742584, Threshold: 0.3052051365375519\n",
            "Epoch 3 iteration 0 Loss: 0.666 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 100 Loss: 0.727 | Acc: 53.465% (54/101)\n",
            "Epoch 3 iteration 200 Loss: 0.728 | Acc: 53.731% (108/201)\n",
            "Test accuracy: 0.5373134613037109\n",
            "Epoch: 4, Loss: 0.004365656524896622 Updates: 7/10000, Avg Grad: 0.024936849251389503, Threshold: 0.3443867564201355\n",
            "Epoch: 4, Loss: 0.00867269467562437 Updates: 7/11000, Avg Grad: 0.11482756584882736, Threshold: 0.3443867564201355\n",
            "Epoch: 4, Loss: 0.007064817938953638 Updates: 7/12000, Avg Grad: 0.11620135605335236, Threshold: 0.3443867564201355\n",
            "Epoch: 4, Loss: 0.007835924625396729 Updates: 7/13000, Avg Grad: 0.1368696242570877, Threshold: 0.3443867564201355\n",
            "Epoch: 4, Loss: 0.007471634540706873 Updates: 7/14000, Avg Grad: 0.18695059418678284, Threshold: 0.3443867564201355\n",
            "Epoch: 4, Loss: 0.009709013625979424 Updates: 7/15000, Avg Grad: 0.28109821677207947, Threshold: 0.3443867564201355\n",
            "Epoch: 4, Loss: 0.006097840145230293 Updates: 8/16000, Avg Grad: 0.014516297727823257, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.008875240571796894 Updates: 8/17000, Avg Grad: 0.054126378148794174, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.004054991528391838 Updates: 8/18000, Avg Grad: 0.07201321423053741, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.0070617105811834335 Updates: 8/19000, Avg Grad: 0.08334266394376755, Threshold: 0.37448006868362427\n",
            "Epoch 3 iteration 0 Loss: 1.097 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.744 | Acc: 49.505% (50/101)\n",
            "Epoch 3 iteration 200 Loss: 0.723 | Acc: 50.746% (102/201)\n",
            "Test accuracy: 0.5074626803398132\n",
            "Epoch: 4, Loss: 0.006843525916337967 Updates: 8/20000, Avg Grad: 0.1073487251996994, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.00798792578279972 Updates: 8/21000, Avg Grad: 0.13487650454044342, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.003538754303008318 Updates: 8/22000, Avg Grad: 0.14200693368911743, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.005227416753768921 Updates: 8/23000, Avg Grad: 0.1643654853105545, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.00937704462558031 Updates: 8/24000, Avg Grad: 0.19717422127723694, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.005656912922859192 Updates: 8/25000, Avg Grad: 0.2187282145023346, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.009150007739663124 Updates: 8/26000, Avg Grad: 0.2449520230293274, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.011294415220618248 Updates: 8/27000, Avg Grad: 0.2624402344226837, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.009223387576639652 Updates: 8/28000, Avg Grad: 0.30214980244636536, Threshold: 0.37448006868362427\n",
            "Epoch: 4, Loss: 0.008801303803920746 Updates: 8/29000, Avg Grad: 0.3446155786514282, Threshold: 0.37448006868362427\n",
            "Epoch 3 iteration 0 Loss: 0.711 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.719 | Acc: 50.495% (51/101)\n",
            "Epoch 3 iteration 200 Loss: 0.711 | Acc: 49.751% (100/201)\n",
            "Test accuracy: 0.49751242995262146\n",
            "Epoch: 4, Loss: 0.004800549708306789 Updates: 9/30000, Avg Grad: 0.04671977087855339, Threshold: 0.3823280930519104\n",
            "Epoch: 4, Loss: 0.012192697264254093 Updates: 9/31000, Avg Grad: 0.08575920760631561, Threshold: 0.3823280930519104\n",
            "Epoch: 4, Loss: 0.005087874364107847 Updates: 9/32000, Avg Grad: 0.07913728803396225, Threshold: 0.3823280930519104\n",
            "Epoch 4 iteration 0 Loss: 0.683 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 100 Loss: 0.703 | Acc: 55.446% (56/101)\n",
            "Epoch 4 iteration 200 Loss: 0.732 | Acc: 46.766% (94/201)\n",
            "Test accuracy: 0.46766167879104614\n",
            "Epoch: 5, Loss: 1.4061228036880493 Updates: 0/0, Avg Grad: 0.25826019048690796, Threshold: 0.2840862274169922\n",
            "Epoch: 5, Loss: 0.008038172498345375 Updates: 1/1000, Avg Grad: 0.26865488290786743, Threshold: 0.31683841347694397\n",
            "Epoch: 5, Loss: 0.009629554115235806 Updates: 2/2000, Avg Grad: 0.21883003413677216, Threshold: 0.35922548174858093\n",
            "Epoch: 5, Loss: 0.004840959794819355 Updates: 3/3000, Avg Grad: 0.09558197855949402, Threshold: 0.3965996503829956\n",
            "Epoch: 5, Loss: 0.010073835030198097 Updates: 3/4000, Avg Grad: 0.23904453217983246, Threshold: 0.3965996503829956\n",
            "Epoch: 5, Loss: 0.005684778559952974 Updates: 4/5000, Avg Grad: 0.012774400413036346, Threshold: 0.4559646248817444\n",
            "Epoch: 5, Loss: 0.006266465410590172 Updates: 4/6000, Avg Grad: 0.17102110385894775, Threshold: 0.4559646248817444\n",
            "Epoch: 5, Loss: 0.0035839290358126163 Updates: 4/7000, Avg Grad: 0.24915823340415955, Threshold: 0.4559646248817444\n",
            "Epoch: 5, Loss: 0.004459822084754705 Updates: 4/8000, Avg Grad: 0.3868878185749054, Threshold: 0.4559646248817444\n",
            "Epoch: 5, Loss: 0.009642503224313259 Updates: 4/9000, Avg Grad: 0.44031646847724915, Threshold: 0.4559646248817444\n",
            "Epoch 4 iteration 0 Loss: 0.801 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.657 | Acc: 61.386% (62/101)\n",
            "Epoch 4 iteration 200 Loss: 0.691 | Acc: 52.736% (106/201)\n",
            "Test accuracy: 0.5273631811141968\n",
            "Epoch: 5, Loss: 0.0012509339721873403 Updates: 5/10000, Avg Grad: 0.03889596089720726, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.004575591534376144 Updates: 5/11000, Avg Grad: 0.09564802795648575, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.009235560894012451 Updates: 5/12000, Avg Grad: 0.0999215692281723, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.008669569157063961 Updates: 5/13000, Avg Grad: 0.12367822974920273, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.005061147268861532 Updates: 5/14000, Avg Grad: 0.1682962030172348, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.003143169917166233 Updates: 5/15000, Avg Grad: 0.20407581329345703, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.004110559355467558 Updates: 5/16000, Avg Grad: 0.23684588074684143, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.007092553190886974 Updates: 5/17000, Avg Grad: 0.24235951900482178, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.01019325852394104 Updates: 5/18000, Avg Grad: 0.31571823358535767, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.00726362457498908 Updates: 5/19000, Avg Grad: 0.3491683900356293, Threshold: 0.5162550210952759\n",
            "Epoch 4 iteration 0 Loss: 0.513 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 100 Loss: 0.721 | Acc: 51.485% (52/101)\n",
            "Epoch 4 iteration 200 Loss: 0.730 | Acc: 51.244% (103/201)\n",
            "Test accuracy: 0.5124378204345703\n",
            "Epoch: 5, Loss: 0.005281028337776661 Updates: 5/20000, Avg Grad: 0.37916865944862366, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.006740256678313017 Updates: 5/21000, Avg Grad: 0.42128899693489075, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.007104240823537111 Updates: 5/22000, Avg Grad: 0.43955233693122864, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.007325038779526949 Updates: 5/23000, Avg Grad: 0.4475044310092926, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.007649627514183521 Updates: 5/24000, Avg Grad: 0.46949267387390137, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.007599701173603535 Updates: 5/25000, Avg Grad: 0.49001234769821167, Threshold: 0.5162550210952759\n",
            "Epoch: 5, Loss: 0.006301539018750191 Updates: 6/26000, Avg Grad: 0.041941188275814056, Threshold: 0.5247986912727356\n",
            "Epoch: 5, Loss: 0.004594963975250721 Updates: 6/27000, Avg Grad: 0.1727042943239212, Threshold: 0.5247986912727356\n",
            "Epoch: 5, Loss: 0.00655619939789176 Updates: 6/28000, Avg Grad: 0.25679072737693787, Threshold: 0.5247986912727356\n",
            "Epoch: 5, Loss: 0.00994572788476944 Updates: 6/29000, Avg Grad: 0.37286895513534546, Threshold: 0.5247986912727356\n",
            "Epoch 4 iteration 0 Loss: 1.041 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.715 | Acc: 50.495% (51/101)\n",
            "Epoch 4 iteration 200 Loss: 0.702 | Acc: 55.721% (112/201)\n",
            "Test accuracy: 0.5572139024734497\n",
            "Epoch: 5, Loss: 0.014369082637131214 Updates: 6/30000, Avg Grad: 0.37534648180007935, Threshold: 0.5247986912727356\n",
            "Epoch: 5, Loss: 0.002255895873531699 Updates: 6/31000, Avg Grad: 0.4306699335575104, Threshold: 0.5247986912727356\n",
            "Epoch: 5, Loss: 0.00664976192638278 Updates: 6/32000, Avg Grad: 0.520341157913208, Threshold: 0.5247986912727356\n",
            "Epoch 5 iteration 0 Loss: 0.700 | Acc: 0.000% (0/1)\n",
            "Epoch 5 iteration 100 Loss: 0.697 | Acc: 53.465% (54/101)\n",
            "Epoch 5 iteration 200 Loss: 0.681 | Acc: 53.731% (108/201)\n",
            "Test accuracy: 0.5373134613037109\n",
            "Epoch: 6, Loss: 0.578641951084137 Updates: 0/0, Avg Grad: 0.16165421903133392, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.005142400041222572 Updates: 0/1000, Avg Grad: 0.11816191673278809, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.007187051232904196 Updates: 0/2000, Avg Grad: 0.10809606313705444, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.004913810174912214 Updates: 0/3000, Avg Grad: 0.1064566820859909, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.009543852880597115 Updates: 0/4000, Avg Grad: 0.12317213416099548, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.008347594179213047 Updates: 0/5000, Avg Grad: 0.15235787630081177, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.0058534638956189156 Updates: 0/6000, Avg Grad: 0.17773902416229248, Threshold: 0.1778196394443512\n",
            "Epoch: 6, Loss: 0.0073367017321288586 Updates: 1/7000, Avg Grad: 0.05406360328197479, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.003268561093136668 Updates: 1/8000, Avg Grad: 0.061832938343286514, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.0039191474206745625 Updates: 1/9000, Avg Grad: 0.07766804844141006, Threshold: 0.21167975664138794\n",
            "Epoch 5 iteration 0 Loss: 0.560 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.677 | Acc: 55.446% (56/101)\n",
            "Epoch 5 iteration 200 Loss: 0.698 | Acc: 55.224% (111/201)\n",
            "Test accuracy: 0.5522388219833374\n",
            "Epoch: 6, Loss: 0.008652585558593273 Updates: 1/10000, Avg Grad: 0.10195079445838928, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.007772617507725954 Updates: 1/11000, Avg Grad: 0.12453839927911758, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.004988848697394133 Updates: 1/12000, Avg Grad: 0.14102385938167572, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.007412031292915344 Updates: 1/13000, Avg Grad: 0.16085056960582733, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.0060647474601864815 Updates: 1/14000, Avg Grad: 0.1731346845626831, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.006314462050795555 Updates: 1/15000, Avg Grad: 0.20355713367462158, Threshold: 0.21167975664138794\n",
            "Epoch: 6, Loss: 0.0044556776992976665 Updates: 2/16000, Avg Grad: 0.06929203122854233, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.00796970259398222 Updates: 2/17000, Avg Grad: 0.0542711578309536, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.013564709573984146 Updates: 2/18000, Avg Grad: 0.08084811270236969, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.0060196989215910435 Updates: 2/19000, Avg Grad: 0.09948083013296127, Threshold: 0.22946013510227203\n",
            "Epoch 5 iteration 0 Loss: 0.538 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.664 | Acc: 60.396% (61/101)\n",
            "Epoch 5 iteration 200 Loss: 0.692 | Acc: 56.219% (113/201)\n",
            "Test accuracy: 0.5621890425682068\n",
            "Epoch: 6, Loss: 0.0038332107942551374 Updates: 2/20000, Avg Grad: 0.11960168182849884, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.0038233110681176186 Updates: 2/21000, Avg Grad: 0.1391967236995697, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.007192475721240044 Updates: 2/22000, Avg Grad: 0.160354882478714, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.0068234773352742195 Updates: 2/23000, Avg Grad: 0.1663636565208435, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.007597482297569513 Updates: 2/24000, Avg Grad: 0.19509214162826538, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.009320679120719433 Updates: 2/25000, Avg Grad: 0.19848360121250153, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.00559942377731204 Updates: 2/26000, Avg Grad: 0.21735933423042297, Threshold: 0.22946013510227203\n",
            "Epoch: 6, Loss: 0.006628541275858879 Updates: 3/27000, Avg Grad: 0.02784370444715023, Threshold: 0.23424005508422852\n",
            "Epoch: 6, Loss: 0.0063514988869428635 Updates: 3/28000, Avg Grad: 0.08820656687021255, Threshold: 0.23424005508422852\n",
            "Epoch: 6, Loss: 0.009767435491085052 Updates: 3/29000, Avg Grad: 0.1513688862323761, Threshold: 0.23424005508422852\n",
            "Epoch 5 iteration 0 Loss: 0.512 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.643 | Acc: 63.366% (64/101)\n",
            "Epoch 5 iteration 200 Loss: 0.660 | Acc: 60.697% (122/201)\n",
            "Test accuracy: 0.606965184211731\n",
            "Epoch: 6, Loss: 0.008040654473006725 Updates: 3/30000, Avg Grad: 0.19406211376190186, Threshold: 0.23424005508422852\n",
            "Epoch: 6, Loss: 0.0049736034125089645 Updates: 3/31000, Avg Grad: 0.2055247277021408, Threshold: 0.23424005508422852\n",
            "Epoch: 6, Loss: 0.007384732365608215 Updates: 3/32000, Avg Grad: 0.20542016625404358, Threshold: 0.23424005508422852\n",
            "Epoch 6 iteration 0 Loss: 0.844 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.663 | Acc: 53.465% (54/101)\n",
            "Epoch 6 iteration 200 Loss: 0.673 | Acc: 55.224% (111/201)\n",
            "Test accuracy: 0.5522388219833374\n",
            "Epoch: 7, Loss: 0.8713396787643433 Updates: 0/0, Avg Grad: 0.19259631633758545, Threshold: 0.211855947971344\n",
            "Epoch: 7, Loss: 0.005309910047799349 Updates: 1/1000, Avg Grad: 0.07792946696281433, Threshold: 0.2600468099117279\n",
            "Epoch: 7, Loss: 0.00685410900041461 Updates: 1/2000, Avg Grad: 0.12210574746131897, Threshold: 0.2600468099117279\n",
            "Epoch: 7, Loss: 0.003863125341013074 Updates: 1/3000, Avg Grad: 0.20823363959789276, Threshold: 0.2600468099117279\n",
            "Epoch: 7, Loss: 0.007511880248785019 Updates: 2/4000, Avg Grad: 0.04286852478981018, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.006062377244234085 Updates: 2/5000, Avg Grad: 0.05417978763580322, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.004231245722621679 Updates: 2/6000, Avg Grad: 0.09000664949417114, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.006082264706492424 Updates: 2/7000, Avg Grad: 0.09371371567249298, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.013615529984235764 Updates: 2/8000, Avg Grad: 0.11934705823659897, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.008459430187940598 Updates: 2/9000, Avg Grad: 0.1291457712650299, Threshold: 0.29102128744125366\n",
            "Epoch 6 iteration 0 Loss: 0.532 | Acc: 100.000% (1/1)\n",
            "Epoch 6 iteration 100 Loss: 0.651 | Acc: 61.386% (62/101)\n",
            "Epoch 6 iteration 200 Loss: 0.663 | Acc: 58.706% (118/201)\n",
            "Test accuracy: 0.5870646834373474\n",
            "Epoch: 7, Loss: 0.007776929065585136 Updates: 2/10000, Avg Grad: 0.18036386370658875, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.010515358299016953 Updates: 2/11000, Avg Grad: 0.19377849996089935, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.010679339990019798 Updates: 2/12000, Avg Grad: 0.20943568646907806, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.005632826127111912 Updates: 2/13000, Avg Grad: 0.22958852350711823, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.008319242857396603 Updates: 2/14000, Avg Grad: 0.24860362708568573, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.005876426585018635 Updates: 2/15000, Avg Grad: 0.2570549249649048, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.005859594792127609 Updates: 2/16000, Avg Grad: 0.27785950899124146, Threshold: 0.29102128744125366\n",
            "Epoch: 7, Loss: 0.01099195796996355 Updates: 3/17000, Avg Grad: 0.039326272904872894, Threshold: 0.30844637751579285\n",
            "Epoch: 7, Loss: 0.0067604947835206985 Updates: 3/18000, Avg Grad: 0.11068111658096313, Threshold: 0.30844637751579285\n",
            "Epoch: 7, Loss: 0.003527524881064892 Updates: 3/19000, Avg Grad: 0.1786365509033203, Threshold: 0.30844637751579285\n",
            "Epoch 6 iteration 0 Loss: 0.799 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.714 | Acc: 55.446% (56/101)\n",
            "Epoch 6 iteration 200 Loss: 0.711 | Acc: 56.716% (114/201)\n",
            "Test accuracy: 0.5671641826629639\n",
            "Epoch: 7, Loss: 0.003043799428269267 Updates: 3/20000, Avg Grad: 0.27146098017692566, Threshold: 0.30844637751579285\n",
            "Epoch: 7, Loss: 0.006564402021467686 Updates: 4/21000, Avg Grad: 0.0932454988360405, Threshold: 0.32818880677223206\n",
            "Epoch: 7, Loss: 0.004095252137631178 Updates: 4/22000, Avg Grad: 0.18373340368270874, Threshold: 0.32818880677223206\n",
            "Epoch: 7, Loss: 0.0063965278677642345 Updates: 4/23000, Avg Grad: 0.18133457005023956, Threshold: 0.32818880677223206\n",
            "Epoch: 7, Loss: 0.006583783775568008 Updates: 4/24000, Avg Grad: 0.25017285346984863, Threshold: 0.32818880677223206\n",
            "Epoch: 7, Loss: 0.005069017875939608 Updates: 5/25000, Avg Grad: 0.0452730692923069, Threshold: 0.3519289791584015\n",
            "Epoch: 7, Loss: 0.004703889135271311 Updates: 5/26000, Avg Grad: 0.14322176575660706, Threshold: 0.3519289791584015\n",
            "Epoch: 7, Loss: 0.006183273624628782 Updates: 5/27000, Avg Grad: 0.16872255504131317, Threshold: 0.3519289791584015\n",
            "Epoch: 7, Loss: 0.007025419268757105 Updates: 5/28000, Avg Grad: 0.28358277678489685, Threshold: 0.3519289791584015\n",
            "Epoch: 7, Loss: 0.00399281969293952 Updates: 5/29000, Avg Grad: 0.29452675580978394, Threshold: 0.3519289791584015\n",
            "Epoch 6 iteration 0 Loss: 0.755 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.688 | Acc: 59.406% (60/101)\n",
            "Epoch 6 iteration 200 Loss: 0.688 | Acc: 59.204% (119/201)\n",
            "Test accuracy: 0.5920398235321045\n",
            "Epoch: 7, Loss: 0.004182292614132166 Updates: 6/30000, Avg Grad: 0.019077947363257408, Threshold: 0.36788201332092285\n",
            "Epoch: 7, Loss: 0.009029357694089413 Updates: 6/31000, Avg Grad: 0.1109187975525856, Threshold: 0.36788201332092285\n",
            "Epoch: 7, Loss: 0.003906254656612873 Updates: 6/32000, Avg Grad: 0.2367236316204071, Threshold: 0.36788201332092285\n",
            "Epoch 7 iteration 0 Loss: 0.564 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.665 | Acc: 63.366% (64/101)\n",
            "Epoch 7 iteration 200 Loss: 0.659 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 8, Loss: 0.6384187936782837 Updates: 0/0, Avg Grad: 0.16214542090892792, Threshold: 0.1783599704504013\n",
            "Epoch: 8, Loss: 0.006557825021445751 Updates: 1/1000, Avg Grad: 0.11932297796010971, Threshold: 0.20049646496772766\n",
            "Epoch: 8, Loss: 0.0037425763439387083 Updates: 2/2000, Avg Grad: 0.2029649168252945, Threshold: 0.22873841226100922\n",
            "Epoch: 8, Loss: 0.005898399278521538 Updates: 3/3000, Avg Grad: 0.16779825091362, Threshold: 0.2700251340866089\n",
            "Epoch: 8, Loss: 0.006775303278118372 Updates: 3/4000, Avg Grad: 0.23255901038646698, Threshold: 0.2700251340866089\n",
            "Epoch: 8, Loss: 0.005788462236523628 Updates: 4/5000, Avg Grad: 0.05538056045770645, Threshold: 0.31396546959877014\n",
            "Epoch: 8, Loss: 0.00531671941280365 Updates: 4/6000, Avg Grad: 0.11251190304756165, Threshold: 0.31396546959877014\n",
            "Epoch: 8, Loss: 0.011766364797949791 Updates: 4/7000, Avg Grad: 0.10330034047365189, Threshold: 0.31396546959877014\n",
            "Epoch: 8, Loss: 0.009592710994184017 Updates: 4/8000, Avg Grad: 0.10085203498601913, Threshold: 0.31396546959877014\n",
            "Epoch: 8, Loss: 0.004487906116992235 Updates: 4/9000, Avg Grad: 0.13712067902088165, Threshold: 0.31396546959877014\n",
            "Epoch 7 iteration 0 Loss: 0.364 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.678 | Acc: 57.426% (58/101)\n",
            "Epoch 7 iteration 200 Loss: 0.676 | Acc: 58.209% (117/201)\n",
            "Test accuracy: 0.5820895433425903\n",
            "Epoch: 8, Loss: 0.006980894133448601 Updates: 4/10000, Avg Grad: 0.21290665864944458, Threshold: 0.31396546959877014\n",
            "Epoch: 8, Loss: 0.00772161316126585 Updates: 4/11000, Avg Grad: 0.27754876017570496, Threshold: 0.31396546959877014\n",
            "Epoch: 8, Loss: 0.006142330355942249 Updates: 5/12000, Avg Grad: 0.06685584783554077, Threshold: 0.3508043587207794\n",
            "Epoch: 8, Loss: 0.00801067240536213 Updates: 5/13000, Avg Grad: 0.13787013292312622, Threshold: 0.3508043587207794\n",
            "Epoch: 8, Loss: 0.005917706992477179 Updates: 5/14000, Avg Grad: 0.22756309807300568, Threshold: 0.3508043587207794\n",
            "Epoch: 8, Loss: 0.003645386779680848 Updates: 5/15000, Avg Grad: 0.22609849274158478, Threshold: 0.3508043587207794\n",
            "Epoch: 8, Loss: 0.007222181651741266 Updates: 5/16000, Avg Grad: 0.2810496389865875, Threshold: 0.3508043587207794\n",
            "Epoch: 8, Loss: 0.006709801033139229 Updates: 5/17000, Avg Grad: 0.28381550312042236, Threshold: 0.3508043587207794\n",
            "Epoch: 8, Loss: 0.001457414822652936 Updates: 6/18000, Avg Grad: 0.053334638476371765, Threshold: 0.39748623967170715\n",
            "Epoch: 8, Loss: 0.006159912329167128 Updates: 6/19000, Avg Grad: 0.09673012048006058, Threshold: 0.39748623967170715\n",
            "Epoch 7 iteration 0 Loss: 0.353 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.683 | Acc: 60.396% (61/101)\n",
            "Epoch 7 iteration 200 Loss: 0.681 | Acc: 62.687% (126/201)\n",
            "Test accuracy: 0.6268656849861145\n",
            "Epoch: 8, Loss: 0.008389687165617943 Updates: 6/20000, Avg Grad: 0.24857468903064728, Threshold: 0.39748623967170715\n",
            "Epoch: 8, Loss: 0.004433722700923681 Updates: 7/21000, Avg Grad: 0.020106827840209007, Threshold: 0.41828519105911255\n",
            "Epoch: 8, Loss: 0.008790782652795315 Updates: 7/22000, Avg Grad: 0.10099030286073685, Threshold: 0.41828519105911255\n",
            "Epoch: 8, Loss: 0.005125345662236214 Updates: 7/23000, Avg Grad: 0.15785422921180725, Threshold: 0.41828519105911255\n",
            "Epoch: 8, Loss: 0.006221999414265156 Updates: 7/24000, Avg Grad: 0.21627773344516754, Threshold: 0.41828519105911255\n",
            "Epoch: 8, Loss: 0.008247866295278072 Updates: 7/25000, Avg Grad: 0.32791149616241455, Threshold: 0.41828519105911255\n",
            "Epoch: 8, Loss: 0.0017862734384834766 Updates: 8/26000, Avg Grad: 0.016485340893268585, Threshold: 0.4277818500995636\n",
            "Epoch: 8, Loss: 0.011343520134687424 Updates: 8/27000, Avg Grad: 0.1381429135799408, Threshold: 0.4277818500995636\n",
            "Epoch: 8, Loss: 0.007535506971180439 Updates: 8/28000, Avg Grad: 0.20813490450382233, Threshold: 0.4277818500995636\n",
            "Epoch: 8, Loss: 0.010134364478290081 Updates: 8/29000, Avg Grad: 0.3229833245277405, Threshold: 0.4277818500995636\n",
            "Epoch 7 iteration 0 Loss: 0.581 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.689 | Acc: 53.465% (54/101)\n",
            "Epoch 7 iteration 200 Loss: 0.683 | Acc: 56.716% (114/201)\n",
            "Test accuracy: 0.5671641826629639\n",
            "Epoch: 8, Loss: 0.008475777693092823 Updates: 9/30000, Avg Grad: 0.07275624573230743, Threshold: 0.432813823223114\n",
            "Epoch: 8, Loss: 0.007351962849497795 Updates: 9/31000, Avg Grad: 0.08491344749927521, Threshold: 0.432813823223114\n",
            "Epoch: 8, Loss: 0.002348263980820775 Updates: 9/32000, Avg Grad: 0.17719212174415588, Threshold: 0.432813823223114\n",
            "Epoch 8 iteration 0 Loss: 0.548 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.667 | Acc: 60.396% (61/101)\n",
            "Epoch 8 iteration 200 Loss: 0.692 | Acc: 55.721% (112/201)\n",
            "Test accuracy: 0.5572139024734497\n",
            "Epoch: 9, Loss: 0.25825417041778564 Updates: 0/0, Avg Grad: 0.08770359307527542, Threshold: 0.09647395461797714\n",
            "Epoch: 9, Loss: 0.006403899751603603 Updates: 1/1000, Avg Grad: 0.05720215663313866, Threshold: 0.1134381964802742\n",
            "Epoch: 9, Loss: 0.008122285827994347 Updates: 2/2000, Avg Grad: 0.10010188072919846, Threshold: 0.15062975883483887\n",
            "Epoch: 9, Loss: 0.007637702394276857 Updates: 3/3000, Avg Grad: 0.10523829609155655, Threshold: 0.17224110662937164\n",
            "Epoch: 9, Loss: 0.007015883922576904 Updates: 4/4000, Avg Grad: 0.05290459096431732, Threshold: 0.21063441038131714\n",
            "Epoch: 9, Loss: 0.006245297845453024 Updates: 4/5000, Avg Grad: 0.14598144590854645, Threshold: 0.21063441038131714\n",
            "Epoch: 9, Loss: 0.0063865608535707 Updates: 5/6000, Avg Grad: 0.06493368744850159, Threshold: 0.23831899464130402\n",
            "Epoch: 9, Loss: 0.0033836900256574154 Updates: 5/7000, Avg Grad: 0.15869316458702087, Threshold: 0.23831899464130402\n",
            "Epoch: 9, Loss: 0.00805858988314867 Updates: 5/8000, Avg Grad: 0.16530443727970123, Threshold: 0.23831899464130402\n",
            "Epoch: 9, Loss: 0.006899888627231121 Updates: 5/9000, Avg Grad: 0.21749073266983032, Threshold: 0.23831899464130402\n",
            "Epoch 8 iteration 0 Loss: 0.689 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.684 | Acc: 52.475% (53/101)\n",
            "Epoch 8 iteration 200 Loss: 0.687 | Acc: 55.224% (111/201)\n",
            "Test accuracy: 0.5522388219833374\n",
            "Epoch: 9, Loss: 0.00554104195907712 Updates: 6/10000, Avg Grad: 0.0225312989205122, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.007044968660920858 Updates: 6/11000, Avg Grad: 0.04558388143777847, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.008231591433286667 Updates: 6/12000, Avg Grad: 0.06438932567834854, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.00759688438847661 Updates: 6/13000, Avg Grad: 0.0832923948764801, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.009676839224994183 Updates: 6/14000, Avg Grad: 0.1673157811164856, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.0028341528959572315 Updates: 6/15000, Avg Grad: 0.20597288012504578, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.0026809456758201122 Updates: 6/16000, Avg Grad: 0.17183364927768707, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.009443676099181175 Updates: 6/17000, Avg Grad: 0.23966586589813232, Threshold: 0.2782587707042694\n",
            "Epoch: 9, Loss: 0.01307631004601717 Updates: 7/18000, Avg Grad: 0.08980460464954376, Threshold: 0.29353609681129456\n",
            "Epoch: 9, Loss: 0.008049114607274532 Updates: 7/19000, Avg Grad: 0.17011500895023346, Threshold: 0.29353609681129456\n",
            "Epoch 8 iteration 0 Loss: 0.814 | Acc: 0.000% (0/1)\n",
            "Epoch 8 iteration 100 Loss: 0.667 | Acc: 58.416% (59/101)\n",
            "Epoch 8 iteration 200 Loss: 0.668 | Acc: 60.697% (122/201)\n",
            "Test accuracy: 0.606965184211731\n",
            "Epoch: 9, Loss: 0.005820196121931076 Updates: 7/20000, Avg Grad: 0.22766298055648804, Threshold: 0.29353609681129456\n",
            "Epoch: 9, Loss: 0.005455160513520241 Updates: 8/21000, Avg Grad: 0.059182800352573395, Threshold: 0.31560018658638\n",
            "Epoch: 9, Loss: 0.012113851495087147 Updates: 8/22000, Avg Grad: 0.0740576684474945, Threshold: 0.31560018658638\n",
            "Epoch: 9, Loss: 0.015750205144286156 Updates: 8/23000, Avg Grad: 0.1848054826259613, Threshold: 0.31560018658638\n",
            "Epoch: 9, Loss: 0.007490336894989014 Updates: 8/24000, Avg Grad: 0.29778069257736206, Threshold: 0.31560018658638\n",
            "Epoch: 9, Loss: 0.005524304695427418 Updates: 9/25000, Avg Grad: 0.08643157035112381, Threshold: 0.3296649754047394\n",
            "Epoch: 9, Loss: 0.0037311844062060118 Updates: 9/26000, Avg Grad: 0.2232455462217331, Threshold: 0.3296649754047394\n",
            "Epoch: 9, Loss: 0.005972944665700197 Updates: 10/27000, Avg Grad: 0.35294094681739807, Threshold: 0.35655155777931213\n",
            "Epoch: 9, Loss: 0.010683972388505936 Updates: 10/28000, Avg Grad: 0.10583711415529251, Threshold: 0.35655155777931213\n",
            "Epoch: 9, Loss: 0.0037721111439168453 Updates: 10/29000, Avg Grad: 0.1745016872882843, Threshold: 0.35655155777931213\n",
            "Epoch 8 iteration 0 Loss: 0.058 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.618 | Acc: 67.327% (68/101)\n",
            "Epoch 8 iteration 200 Loss: 0.637 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 9, Loss: 0.003217412857338786 Updates: 10/30000, Avg Grad: 0.2624894082546234, Threshold: 0.35655155777931213\n",
            "Epoch: 9, Loss: 0.015349115245044231 Updates: 11/31000, Avg Grad: 0.02333810180425644, Threshold: 0.3653337061405182\n",
            "Epoch: 9, Loss: 0.006014541257172823 Updates: 11/32000, Avg Grad: 0.10206761956214905, Threshold: 0.3653337061405182\n",
            "Epoch 9 iteration 0 Loss: 0.729 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 100 Loss: 0.714 | Acc: 60.396% (61/101)\n",
            "Epoch 9 iteration 200 Loss: 0.699 | Acc: 57.214% (115/201)\n",
            "Test accuracy: 0.572139322757721\n",
            "Epoch: 10, Loss: 0.6584811806678772 Updates: 0/0, Avg Grad: 0.16166822612285614, Threshold: 0.17783504724502563\n",
            "Epoch: 10, Loss: 0.003867172636091709 Updates: 0/1000, Avg Grad: 0.14844398200511932, Threshold: 0.17783504724502563\n",
            "Epoch: 10, Loss: 0.010974595323204994 Updates: 1/2000, Avg Grad: 0.036826517432928085, Threshold: 0.2078515589237213\n",
            "Epoch: 10, Loss: 0.01691410131752491 Updates: 1/3000, Avg Grad: 0.09701916575431824, Threshold: 0.2078515589237213\n",
            "Epoch: 10, Loss: 0.007291452959179878 Updates: 1/4000, Avg Grad: 0.11953115463256836, Threshold: 0.2078515589237213\n",
            "Epoch: 10, Loss: 0.004573869053274393 Updates: 1/5000, Avg Grad: 0.14984861016273499, Threshold: 0.2078515589237213\n",
            "Epoch: 10, Loss: 0.006024837493896484 Updates: 2/6000, Avg Grad: 0.2089877724647522, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.004953667055815458 Updates: 2/7000, Avg Grad: 0.06554912030696869, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.006536928936839104 Updates: 2/8000, Avg Grad: 0.06283610314130783, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.005305089522153139 Updates: 2/9000, Avg Grad: 0.06942994147539139, Threshold: 0.22988656163215637\n",
            "Epoch 9 iteration 0 Loss: 1.262 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 100 Loss: 0.687 | Acc: 56.436% (57/101)\n",
            "Epoch 9 iteration 200 Loss: 0.680 | Acc: 57.214% (115/201)\n",
            "Test accuracy: 0.572139322757721\n",
            "Epoch: 10, Loss: 0.007262913044542074 Updates: 2/10000, Avg Grad: 0.08660799264907837, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.008000845089554787 Updates: 2/11000, Avg Grad: 0.14736433327198029, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.007321865763515234 Updates: 2/12000, Avg Grad: 0.12657056748867035, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.006374645978212357 Updates: 2/13000, Avg Grad: 0.10580246150493622, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.0038346245419234037 Updates: 2/14000, Avg Grad: 0.15704864263534546, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.004154107999056578 Updates: 2/15000, Avg Grad: 0.14769388735294342, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.004855942912399769 Updates: 2/16000, Avg Grad: 0.1688137799501419, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.007396145258098841 Updates: 2/17000, Avg Grad: 0.14698657393455505, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.008288080804049969 Updates: 2/18000, Avg Grad: 0.1440001130104065, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.007219789084047079 Updates: 2/19000, Avg Grad: 0.1566779464483261, Threshold: 0.22988656163215637\n",
            "Epoch 9 iteration 0 Loss: 2.172 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 100 Loss: 0.618 | Acc: 66.337% (67/101)\n",
            "Epoch 9 iteration 200 Loss: 0.651 | Acc: 62.189% (125/201)\n",
            "Test accuracy: 0.6218905448913574\n",
            "Epoch: 10, Loss: 0.0049418238922953606 Updates: 2/20000, Avg Grad: 0.17156103253364563, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.003022960154339671 Updates: 2/21000, Avg Grad: 0.1944250911474228, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.005937297362834215 Updates: 2/22000, Avg Grad: 0.21207790076732635, Threshold: 0.22988656163215637\n",
            "Epoch: 10, Loss: 0.011603946797549725 Updates: 3/23000, Avg Grad: 0.012138073332607746, Threshold: 0.2411344200372696\n",
            "Epoch: 10, Loss: 0.0035012117587029934 Updates: 3/24000, Avg Grad: 0.050185102969408035, Threshold: 0.2411344200372696\n",
            "Epoch: 10, Loss: 0.0027478113770484924 Updates: 3/25000, Avg Grad: 0.04806730896234512, Threshold: 0.2411344200372696\n",
            "Epoch: 10, Loss: 0.008163959719240665 Updates: 3/26000, Avg Grad: 0.13257817924022675, Threshold: 0.2411344200372696\n",
            "Epoch: 10, Loss: 0.007972560822963715 Updates: 3/27000, Avg Grad: 0.15421274304389954, Threshold: 0.2411344200372696\n",
            "Epoch: 10, Loss: 0.00748889334499836 Updates: 3/28000, Avg Grad: 0.21700184047222137, Threshold: 0.2411344200372696\n",
            "Epoch: 10, Loss: 0.0029846234247088432 Updates: 4/29000, Avg Grad: 0.03134027123451233, Threshold: 0.25262701511383057\n",
            "Epoch 9 iteration 0 Loss: 0.678 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 100 Loss: 0.681 | Acc: 54.455% (55/101)\n",
            "Epoch 9 iteration 200 Loss: 0.697 | Acc: 55.721% (112/201)\n",
            "Test accuracy: 0.5572139024734497\n",
            "Epoch: 10, Loss: 0.006346856243908405 Updates: 4/30000, Avg Grad: 0.04667459800839424, Threshold: 0.25262701511383057\n",
            "Epoch: 10, Loss: 0.008523284457623959 Updates: 4/31000, Avg Grad: 0.07981552183628082, Threshold: 0.25262701511383057\n",
            "Epoch: 10, Loss: 0.006992529146373272 Updates: 4/32000, Avg Grad: 0.1600155085325241, Threshold: 0.25262701511383057\n",
            "Epoch 10 iteration 0 Loss: 0.743 | Acc: 0.000% (0/1)\n",
            "Epoch 10 iteration 100 Loss: 0.679 | Acc: 58.416% (59/101)\n",
            "Epoch 10 iteration 200 Loss: 0.670 | Acc: 60.199% (121/201)\n",
            "Test accuracy: 0.6019900441169739\n",
            "Epoch: 11, Loss: 0.4533466398715973 Updates: 0/0, Avg Grad: 0.12385369092226028, Threshold: 0.13623906672000885\n",
            "Epoch: 11, Loss: 0.002793134655803442 Updates: 0/1000, Avg Grad: 0.1273215413093567, Threshold: 0.13623906672000885\n",
            "Epoch: 11, Loss: 0.005608027800917625 Updates: 1/2000, Avg Grad: 0.0438491627573967, Threshold: 0.15056756138801575\n",
            "Epoch: 11, Loss: 0.007056263275444508 Updates: 1/3000, Avg Grad: 0.04300415888428688, Threshold: 0.15056756138801575\n",
            "Epoch: 11, Loss: 0.0023661141749471426 Updates: 1/4000, Avg Grad: 0.0507468618452549, Threshold: 0.15056756138801575\n",
            "Epoch: 11, Loss: 0.003935480490326881 Updates: 1/5000, Avg Grad: 0.12883038818836212, Threshold: 0.15056756138801575\n",
            "Epoch: 11, Loss: 0.002129583153873682 Updates: 2/6000, Avg Grad: 0.02821841649711132, Threshold: 0.1831217110157013\n",
            "Epoch: 11, Loss: 0.008962655439972878 Updates: 2/7000, Avg Grad: 0.09190744161605835, Threshold: 0.1831217110157013\n",
            "Epoch: 11, Loss: 0.0035850161220878363 Updates: 2/8000, Avg Grad: 0.09312012046575546, Threshold: 0.1831217110157013\n",
            "Epoch: 11, Loss: 0.01375962421298027 Updates: 3/9000, Avg Grad: 0.1888100951910019, Threshold: 0.20769110321998596\n",
            "Epoch 10 iteration 0 Loss: 0.851 | Acc: 0.000% (0/1)\n",
            "Epoch 10 iteration 100 Loss: 0.682 | Acc: 59.406% (60/101)\n",
            "Epoch 10 iteration 200 Loss: 0.683 | Acc: 58.706% (118/201)\n",
            "Test accuracy: 0.5870646834373474\n",
            "Epoch: 11, Loss: 0.00970648042857647 Updates: 3/10000, Avg Grad: 0.05962277948856354, Threshold: 0.20769110321998596\n",
            "Epoch: 11, Loss: 0.005851557943969965 Updates: 3/11000, Avg Grad: 0.05688062310218811, Threshold: 0.20769110321998596\n",
            "Epoch: 11, Loss: 0.006629511248320341 Updates: 3/12000, Avg Grad: 0.07773619145154953, Threshold: 0.20769110321998596\n",
            "Epoch: 11, Loss: 0.01215441431850195 Updates: 3/13000, Avg Grad: 0.08187855780124664, Threshold: 0.20769110321998596\n",
            "Epoch: 11, Loss: 0.008348939009010792 Updates: 3/14000, Avg Grad: 0.10828164219856262, Threshold: 0.20769110321998596\n",
            "Epoch: 11, Loss: 0.007199195679277182 Updates: 3/15000, Avg Grad: 0.13951848447322845, Threshold: 0.20769110321998596\n",
            "Epoch: 11, Loss: 0.011624222621321678 Updates: 4/16000, Avg Grad: 0.029848391190171242, Threshold: 0.22672878205776215\n",
            "Epoch: 11, Loss: 0.009142829105257988 Updates: 4/17000, Avg Grad: 0.0674341693520546, Threshold: 0.22672878205776215\n",
            "Epoch: 11, Loss: 0.007383433636277914 Updates: 4/18000, Avg Grad: 0.18295235931873322, Threshold: 0.22672878205776215\n",
            "Epoch: 11, Loss: 0.004652714356780052 Updates: 4/19000, Avg Grad: 0.22425277531147003, Threshold: 0.22672878205776215\n",
            "Epoch 10 iteration 0 Loss: 0.658 | Acc: 100.000% (1/1)\n",
            "Epoch 10 iteration 100 Loss: 0.684 | Acc: 59.406% (60/101)\n",
            "Epoch 10 iteration 200 Loss: 0.656 | Acc: 62.687% (126/201)\n",
            "Test accuracy: 0.6268656849861145\n",
            "Epoch: 11, Loss: 0.0038468430284410715 Updates: 5/20000, Avg Grad: 0.04306839779019356, Threshold: 0.2590569257736206\n",
            "Epoch: 11, Loss: 0.005652537569403648 Updates: 5/21000, Avg Grad: 0.1346714049577713, Threshold: 0.2590569257736206\n",
            "Epoch: 11, Loss: 0.005081344395875931 Updates: 5/22000, Avg Grad: 0.23335182666778564, Threshold: 0.2590569257736206\n",
            "Epoch: 11, Loss: 0.005595020484179258 Updates: 6/23000, Avg Grad: 0.04866007715463638, Threshold: 0.27297472953796387\n",
            "Epoch: 11, Loss: 0.009620649740099907 Updates: 6/24000, Avg Grad: 0.1589973419904709, Threshold: 0.27297472953796387\n",
            "Epoch: 11, Loss: 0.00649365596473217 Updates: 6/25000, Avg Grad: 0.24269156157970428, Threshold: 0.27297472953796387\n",
            "Epoch: 11, Loss: 0.005536286160349846 Updates: 7/26000, Avg Grad: 0.03987877443432808, Threshold: 0.28448039293289185\n",
            "Epoch: 11, Loss: 0.00422268733382225 Updates: 7/27000, Avg Grad: 0.07157320529222488, Threshold: 0.28448039293289185\n",
            "Epoch: 11, Loss: 0.0042418790981173515 Updates: 7/28000, Avg Grad: 0.1483762115240097, Threshold: 0.28448039293289185\n",
            "Epoch: 11, Loss: 0.005456653423607349 Updates: 7/29000, Avg Grad: 0.10969584435224533, Threshold: 0.28448039293289185\n",
            "Epoch 10 iteration 0 Loss: 1.201 | Acc: 0.000% (0/1)\n",
            "Epoch 10 iteration 100 Loss: 0.677 | Acc: 60.396% (61/101)\n",
            "Epoch 10 iteration 200 Loss: 0.672 | Acc: 60.697% (122/201)\n",
            "Test accuracy: 0.606965184211731\n",
            "Epoch: 11, Loss: 0.007185178808867931 Updates: 7/30000, Avg Grad: 0.08788163214921951, Threshold: 0.28448039293289185\n",
            "Epoch: 11, Loss: 0.006728000473231077 Updates: 7/31000, Avg Grad: 0.1034577339887619, Threshold: 0.28448039293289185\n",
            "Epoch: 11, Loss: 0.006476367823779583 Updates: 7/32000, Avg Grad: 0.14434665441513062, Threshold: 0.28448039293289185\n",
            "Epoch 11 iteration 0 Loss: 1.002 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.696 | Acc: 50.495% (51/101)\n",
            "Epoch 11 iteration 200 Loss: 0.685 | Acc: 53.234% (107/201)\n",
            "Test accuracy: 0.5323383212089539\n",
            "Epoch: 12, Loss: 0.8202028870582581 Updates: 0/0, Avg Grad: 0.1934962421655655, Threshold: 0.21284587681293488\n",
            "Epoch: 12, Loss: 0.00787312351167202 Updates: 1/1000, Avg Grad: 0.09945955127477646, Threshold: 0.25867411494255066\n",
            "Epoch: 12, Loss: 0.0026787591632455587 Updates: 1/2000, Avg Grad: 0.21113155782222748, Threshold: 0.25867411494255066\n",
            "Epoch: 12, Loss: 0.006554692517966032 Updates: 2/3000, Avg Grad: 0.05894659087061882, Threshold: 0.2873727083206177\n",
            "Epoch: 12, Loss: 0.010353594087064266 Updates: 2/4000, Avg Grad: 0.11711114645004272, Threshold: 0.2873727083206177\n",
            "Epoch: 12, Loss: 0.007639515213668346 Updates: 2/5000, Avg Grad: 0.22211849689483643, Threshold: 0.2873727083206177\n",
            "Epoch: 12, Loss: 0.004035888705402613 Updates: 2/6000, Avg Grad: 0.19431515038013458, Threshold: 0.2873727083206177\n",
            "Epoch: 12, Loss: 0.0070990691892802715 Updates: 3/7000, Avg Grad: 0.023620041087269783, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.006653743330389261 Updates: 3/8000, Avg Grad: 0.047646619379520416, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.007085958495736122 Updates: 3/9000, Avg Grad: 0.09505727142095566, Threshold: 0.3311581611633301\n",
            "Epoch 11 iteration 0 Loss: 0.727 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.673 | Acc: 63.366% (64/101)\n",
            "Epoch 11 iteration 200 Loss: 0.683 | Acc: 59.204% (119/201)\n",
            "Test accuracy: 0.5920398235321045\n",
            "Epoch: 12, Loss: 0.004935034085065126 Updates: 3/10000, Avg Grad: 0.1413043588399887, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.005322657059878111 Updates: 3/11000, Avg Grad: 0.1428048461675644, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.004685923922806978 Updates: 3/12000, Avg Grad: 0.19876161217689514, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.017929518595337868 Updates: 3/13000, Avg Grad: 0.21642959117889404, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.009982091374695301 Updates: 3/14000, Avg Grad: 0.21769024431705475, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.0055760713294148445 Updates: 3/15000, Avg Grad: 0.1801173985004425, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.007962845265865326 Updates: 3/16000, Avg Grad: 0.18885557353496552, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.011425125412642956 Updates: 3/17000, Avg Grad: 0.17617346346378326, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.007348616607487202 Updates: 3/18000, Avg Grad: 0.19071045517921448, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.0033221892081201077 Updates: 3/19000, Avg Grad: 0.20198142528533936, Threshold: 0.3311581611633301\n",
            "Epoch 11 iteration 0 Loss: 1.392 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.649 | Acc: 64.356% (65/101)\n",
            "Epoch 11 iteration 200 Loss: 0.648 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 12, Loss: 0.003959814552217722 Updates: 3/20000, Avg Grad: 0.23424476385116577, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.0060242898762226105 Updates: 3/21000, Avg Grad: 0.2569839358329773, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.013205042108893394 Updates: 3/22000, Avg Grad: 0.22636017203330994, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.0064903986640274525 Updates: 3/23000, Avg Grad: 0.2126932293176651, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.00465777050703764 Updates: 3/24000, Avg Grad: 0.23240312933921814, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.005361969582736492 Updates: 3/25000, Avg Grad: 0.242207333445549, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.009139175526797771 Updates: 3/26000, Avg Grad: 0.3159390389919281, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.005592369008809328 Updates: 3/27000, Avg Grad: 0.32911351323127747, Threshold: 0.3311581611633301\n",
            "Epoch: 12, Loss: 0.005092179402709007 Updates: 4/28000, Avg Grad: 0.08436813205480576, Threshold: 0.3472754657268524\n",
            "Epoch: 12, Loss: 0.00575184216722846 Updates: 4/29000, Avg Grad: 0.12973619997501373, Threshold: 0.3472754657268524\n",
            "Epoch 11 iteration 0 Loss: 0.420 | Acc: 100.000% (1/1)\n",
            "Epoch 11 iteration 100 Loss: 0.655 | Acc: 61.386% (62/101)\n",
            "Epoch 11 iteration 200 Loss: 0.680 | Acc: 57.214% (115/201)\n",
            "Test accuracy: 0.572139322757721\n",
            "Epoch: 12, Loss: 0.00503312423825264 Updates: 4/30000, Avg Grad: 0.24271990358829498, Threshold: 0.3472754657268524\n",
            "Epoch: 12, Loss: 0.004776521120220423 Updates: 4/31000, Avg Grad: 0.32318875193595886, Threshold: 0.3472754657268524\n",
            "Epoch: 12, Loss: 0.0023180535063147545 Updates: 5/32000, Avg Grad: 0.06871282309293747, Threshold: 0.3493407070636749\n",
            "Epoch 12 iteration 0 Loss: 0.936 | Acc: 0.000% (0/1)\n",
            "Epoch 12 iteration 100 Loss: 0.646 | Acc: 66.337% (67/101)\n",
            "Epoch 12 iteration 200 Loss: 0.665 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 13, Loss: 0.34313008189201355 Updates: 0/0, Avg Grad: 0.11644110828638077, Threshold: 0.12808522582054138\n",
            "Epoch: 13, Loss: 0.005657250992953777 Updates: 1/1000, Avg Grad: 0.038854971528053284, Threshold: 0.1704692244529724\n",
            "Epoch: 13, Loss: 0.006556531880050898 Updates: 1/2000, Avg Grad: 0.07645717263221741, Threshold: 0.1704692244529724\n",
            "Epoch: 13, Loss: 0.008485011756420135 Updates: 1/3000, Avg Grad: 0.08053487539291382, Threshold: 0.1704692244529724\n",
            "Epoch: 13, Loss: 0.0008048599120229483 Updates: 1/4000, Avg Grad: 0.07883909344673157, Threshold: 0.1704692244529724\n",
            "Epoch: 13, Loss: 0.007037419825792313 Updates: 1/5000, Avg Grad: 0.10606162995100021, Threshold: 0.1704692244529724\n",
            "Epoch: 13, Loss: 0.010375771671533585 Updates: 1/6000, Avg Grad: 0.16264113783836365, Threshold: 0.1704692244529724\n",
            "Epoch: 13, Loss: 0.008871226571500301 Updates: 2/7000, Avg Grad: 0.08616529405117035, Threshold: 0.19084809720516205\n",
            "Epoch: 13, Loss: 0.00395582802593708 Updates: 2/8000, Avg Grad: 0.1666087955236435, Threshold: 0.19084809720516205\n",
            "Epoch: 13, Loss: 0.002440398558974266 Updates: 3/9000, Avg Grad: 0.01644839718937874, Threshold: 0.23489910364151\n",
            "Epoch 12 iteration 0 Loss: 0.555 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.659 | Acc: 64.356% (65/101)\n",
            "Epoch 12 iteration 200 Loss: 0.685 | Acc: 60.199% (121/201)\n",
            "Test accuracy: 0.6019900441169739\n",
            "Epoch: 13, Loss: 0.0059101758524775505 Updates: 3/10000, Avg Grad: 0.11639206856489182, Threshold: 0.23489910364151\n",
            "Epoch: 13, Loss: 0.017357343807816505 Updates: 4/11000, Avg Grad: 0.011577694676816463, Threshold: 0.265201210975647\n",
            "Epoch: 13, Loss: 0.0070292167365550995 Updates: 4/12000, Avg Grad: 0.07451359182596207, Threshold: 0.265201210975647\n",
            "Epoch: 13, Loss: 0.008700844831764698 Updates: 4/13000, Avg Grad: 0.1542191058397293, Threshold: 0.265201210975647\n",
            "Epoch: 13, Loss: 0.006500065326690674 Updates: 4/14000, Avg Grad: 0.21793510019779205, Threshold: 0.265201210975647\n",
            "Epoch: 13, Loss: 0.011023785918951035 Updates: 5/15000, Avg Grad: 0.06528691947460175, Threshold: 0.30823221802711487\n",
            "Epoch: 13, Loss: 0.004324200563132763 Updates: 5/16000, Avg Grad: 0.15736709535121918, Threshold: 0.30823221802711487\n",
            "Epoch: 13, Loss: 0.013469667173922062 Updates: 5/17000, Avg Grad: 0.19350957870483398, Threshold: 0.30823221802711487\n",
            "Epoch: 13, Loss: 0.005106391850858927 Updates: 5/18000, Avg Grad: 0.21481814980506897, Threshold: 0.30823221802711487\n",
            "Epoch: 13, Loss: 0.005248618312180042 Updates: 5/19000, Avg Grad: 0.2263602614402771, Threshold: 0.30823221802711487\n",
            "Epoch 12 iteration 0 Loss: 0.225 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.644 | Acc: 64.356% (65/101)\n",
            "Epoch 12 iteration 200 Loss: 0.668 | Acc: 58.209% (117/201)\n",
            "Test accuracy: 0.5820895433425903\n",
            "Epoch: 13, Loss: 0.007002303376793861 Updates: 5/20000, Avg Grad: 0.21740186214447021, Threshold: 0.30823221802711487\n",
            "Epoch: 13, Loss: 0.005664001684635878 Updates: 5/21000, Avg Grad: 0.2920917272567749, Threshold: 0.30823221802711487\n",
            "Epoch: 13, Loss: 0.004272324964404106 Updates: 6/22000, Avg Grad: 0.05562467873096466, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.003324412042275071 Updates: 6/23000, Avg Grad: 0.13616304099559784, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.013268555514514446 Updates: 6/24000, Avg Grad: 0.1368960738182068, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.005725005175918341 Updates: 6/25000, Avg Grad: 0.16500310599803925, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.0038676641415804625 Updates: 6/26000, Avg Grad: 0.17915397882461548, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.006518763955682516 Updates: 6/27000, Avg Grad: 0.197167307138443, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.00926925614476204 Updates: 6/28000, Avg Grad: 0.2487371861934662, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.003995944745838642 Updates: 6/29000, Avg Grad: 0.21436311304569244, Threshold: 0.3231535255908966\n",
            "Epoch 12 iteration 0 Loss: 0.577 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.672 | Acc: 61.386% (62/101)\n",
            "Epoch 12 iteration 200 Loss: 0.685 | Acc: 58.706% (118/201)\n",
            "Test accuracy: 0.5870646834373474\n",
            "Epoch: 13, Loss: 0.004273764323443174 Updates: 6/30000, Avg Grad: 0.3077792227268219, Threshold: 0.3231535255908966\n",
            "Epoch: 13, Loss: 0.0016876492882147431 Updates: 7/31000, Avg Grad: 0.03511007875204086, Threshold: 0.34321656823158264\n",
            "Epoch: 13, Loss: 0.006770358886569738 Updates: 7/32000, Avg Grad: 0.05851226672530174, Threshold: 0.34321656823158264\n",
            "Epoch 13 iteration 0 Loss: 0.387 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.689 | Acc: 52.475% (53/101)\n",
            "Epoch 13 iteration 200 Loss: 0.679 | Acc: 54.229% (109/201)\n",
            "Test accuracy: 0.5422885417938232\n",
            "Epoch: 14, Loss: 0.9223875999450684 Updates: 0/0, Avg Grad: 0.27750271558761597, Threshold: 0.3052529990673065\n",
            "Epoch: 14, Loss: 0.015264430083334446 Updates: 0/1000, Avg Grad: 0.20297031104564667, Threshold: 0.3052529990673065\n",
            "Epoch: 14, Loss: 0.00829868484288454 Updates: 1/2000, Avg Grad: 0.12383051216602325, Threshold: 0.34690728783607483\n",
            "Epoch: 14, Loss: 0.006452233996242285 Updates: 1/3000, Avg Grad: 0.3032705783843994, Threshold: 0.34690728783607483\n",
            "Epoch: 14, Loss: 0.004538153298199177 Updates: 2/4000, Avg Grad: 0.1962604969739914, Threshold: 0.3896488547325134\n",
            "Epoch: 14, Loss: 0.004747195169329643 Updates: 2/5000, Avg Grad: 0.3573446273803711, Threshold: 0.3896488547325134\n",
            "Epoch: 14, Loss: 0.005927280988544226 Updates: 3/6000, Avg Grad: 0.12418835610151291, Threshold: 0.44797712564468384\n",
            "Epoch: 14, Loss: 0.0028708153404295444 Updates: 3/7000, Avg Grad: 0.27790379524230957, Threshold: 0.44797712564468384\n",
            "Epoch: 14, Loss: 0.007282534148544073 Updates: 3/8000, Avg Grad: 0.41009002923965454, Threshold: 0.44797712564468384\n",
            "Epoch: 14, Loss: 0.00579018285498023 Updates: 4/9000, Avg Grad: 0.13529051840305328, Threshold: 0.49838390946388245\n",
            "Epoch 13 iteration 0 Loss: 0.606 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.701 | Acc: 52.475% (53/101)\n",
            "Epoch 13 iteration 200 Loss: 0.669 | Acc: 58.209% (117/201)\n",
            "Test accuracy: 0.5820895433425903\n",
            "Epoch: 14, Loss: 0.009212705306708813 Updates: 4/10000, Avg Grad: 0.25557708740234375, Threshold: 0.49838390946388245\n",
            "Epoch: 14, Loss: 0.0046099768951535225 Updates: 4/11000, Avg Grad: 0.40439242124557495, Threshold: 0.49838390946388245\n",
            "Epoch: 14, Loss: 0.007246752269566059 Updates: 5/12000, Avg Grad: 0.014161188155412674, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.006320553831756115 Updates: 5/13000, Avg Grad: 0.03416388854384422, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.005121160298585892 Updates: 5/14000, Avg Grad: 0.06076544150710106, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.005431686528027058 Updates: 5/15000, Avg Grad: 0.07634228467941284, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.005923000629991293 Updates: 5/16000, Avg Grad: 0.08479226380586624, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.0027622547931969166 Updates: 5/17000, Avg Grad: 0.09425650537014008, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.0087541239336133 Updates: 5/18000, Avg Grad: 0.10411830246448517, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.005437568295747042 Updates: 5/19000, Avg Grad: 0.14919088780879974, Threshold: 0.552225649356842\n",
            "Epoch 13 iteration 0 Loss: 0.830 | Acc: 0.000% (0/1)\n",
            "Epoch 13 iteration 100 Loss: 0.631 | Acc: 63.366% (64/101)\n",
            "Epoch 13 iteration 200 Loss: 0.621 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 14, Loss: 0.004587965086102486 Updates: 5/20000, Avg Grad: 0.1882862150669098, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.004186232574284077 Updates: 5/21000, Avg Grad: 0.16493456065654755, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.006029244512319565 Updates: 5/22000, Avg Grad: 0.18562816083431244, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.0051253545098006725 Updates: 5/23000, Avg Grad: 0.21635091304779053, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.011916046030819416 Updates: 5/24000, Avg Grad: 0.16250447928905487, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.0073824296705424786 Updates: 5/25000, Avg Grad: 0.18618187308311462, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.0036992975510656834 Updates: 5/26000, Avg Grad: 0.18763084709644318, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.006945087108761072 Updates: 5/27000, Avg Grad: 0.19373182952404022, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.004395586438477039 Updates: 5/28000, Avg Grad: 0.2310529202222824, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.003665711497887969 Updates: 5/29000, Avg Grad: 0.25719937682151794, Threshold: 0.552225649356842\n",
            "Epoch 13 iteration 0 Loss: 0.304 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.657 | Acc: 62.376% (63/101)\n",
            "Epoch 13 iteration 200 Loss: 0.637 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 14, Loss: 0.010137918405234814 Updates: 5/30000, Avg Grad: 0.24926219880580902, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.0015274264151230454 Updates: 5/31000, Avg Grad: 0.2545340061187744, Threshold: 0.552225649356842\n",
            "Epoch: 14, Loss: 0.006903793662786484 Updates: 5/32000, Avg Grad: 0.30888834595680237, Threshold: 0.552225649356842\n",
            "Epoch 14 iteration 0 Loss: 1.108 | Acc: 0.000% (0/1)\n",
            "Epoch 14 iteration 100 Loss: 0.614 | Acc: 65.347% (66/101)\n",
            "Epoch 14 iteration 200 Loss: 0.651 | Acc: 62.189% (125/201)\n",
            "Test accuracy: 0.6218905448913574\n",
            "Epoch: 15, Loss: 1.0169111490249634 Updates: 0/0, Avg Grad: 0.22033201158046722, Threshold: 0.24236521124839783\n",
            "Epoch: 15, Loss: 0.0065051717683672905 Updates: 0/1000, Avg Grad: 0.2011348307132721, Threshold: 0.24236521124839783\n",
            "Epoch: 15, Loss: 0.013209480792284012 Updates: 0/2000, Avg Grad: 0.14539919793605804, Threshold: 0.24236521124839783\n",
            "Epoch: 15, Loss: 0.003623470664024353 Updates: 0/3000, Avg Grad: 0.13158255815505981, Threshold: 0.24236521124839783\n",
            "Epoch: 15, Loss: 0.013816223479807377 Updates: 0/4000, Avg Grad: 0.15624485909938812, Threshold: 0.24236521124839783\n",
            "Epoch: 15, Loss: 0.0035582103300839663 Updates: 1/5000, Avg Grad: 0.24243298172950745, Threshold: 0.26667627692222595\n",
            "Epoch: 15, Loss: 0.010362152941524982 Updates: 1/6000, Avg Grad: 0.24792243540287018, Threshold: 0.26667627692222595\n",
            "Epoch: 15, Loss: 0.010937504470348358 Updates: 2/7000, Avg Grad: 0.1788773387670517, Threshold: 0.3003740906715393\n",
            "Epoch: 15, Loss: 0.0020253772381693125 Updates: 3/8000, Avg Grad: 0.04556359723210335, Threshold: 0.36243829131126404\n",
            "Epoch: 15, Loss: 0.010748367756605148 Updates: 4/9000, Avg Grad: 0.36624205112457275, Threshold: 0.40286627411842346\n",
            "Epoch 14 iteration 0 Loss: 0.326 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.667 | Acc: 63.366% (64/101)\n",
            "Epoch 14 iteration 200 Loss: 0.701 | Acc: 59.701% (120/201)\n",
            "Test accuracy: 0.5970149040222168\n",
            "Epoch: 15, Loss: 0.0034668289590626955 Updates: 4/10000, Avg Grad: 0.21809226274490356, Threshold: 0.40286627411842346\n",
            "Epoch: 15, Loss: 0.014852046966552734 Updates: 5/11000, Avg Grad: 0.075627401471138, Threshold: 0.46821358799934387\n",
            "Epoch: 15, Loss: 0.010919737629592419 Updates: 5/12000, Avg Grad: 0.3212117850780487, Threshold: 0.46821358799934387\n",
            "Epoch: 15, Loss: 0.004140323493629694 Updates: 6/13000, Avg Grad: 0.032069191336631775, Threshold: 0.5265328288078308\n",
            "Epoch: 15, Loss: 0.011966025456786156 Updates: 6/14000, Avg Grad: 0.19325506687164307, Threshold: 0.5265328288078308\n",
            "Epoch: 15, Loss: 0.0036398903466761112 Updates: 6/15000, Avg Grad: 0.32102450728416443, Threshold: 0.5265328288078308\n",
            "Epoch: 15, Loss: 0.008114048279821873 Updates: 6/16000, Avg Grad: 0.45865848660469055, Threshold: 0.5265328288078308\n",
            "Epoch: 15, Loss: 0.0033643392380326986 Updates: 7/17000, Avg Grad: 0.048127248883247375, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.005239010322839022 Updates: 7/18000, Avg Grad: 0.09634634852409363, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.005100509617477655 Updates: 7/19000, Avg Grad: 0.12001189589500427, Threshold: 0.5638524293899536\n",
            "Epoch 14 iteration 0 Loss: 0.602 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.636 | Acc: 68.317% (69/101)\n",
            "Epoch 14 iteration 200 Loss: 0.654 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 15, Loss: 0.006852652411907911 Updates: 7/20000, Avg Grad: 0.14152571558952332, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.0031712064519524574 Updates: 7/21000, Avg Grad: 0.18362779915332794, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.0022070538252592087 Updates: 7/22000, Avg Grad: 0.23073773086071014, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.0046377163380384445 Updates: 7/23000, Avg Grad: 0.286084920167923, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.005326960235834122 Updates: 7/24000, Avg Grad: 0.2681344747543335, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.006017107982188463 Updates: 7/25000, Avg Grad: 0.3191937804222107, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.006215949542820454 Updates: 7/26000, Avg Grad: 0.3708377480506897, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.0028252480551600456 Updates: 7/27000, Avg Grad: 0.4900965392589569, Threshold: 0.5638524293899536\n",
            "Epoch: 15, Loss: 0.003068110439926386 Updates: 8/28000, Avg Grad: 0.06475725024938583, Threshold: 0.5915119051933289\n",
            "Epoch: 15, Loss: 0.003048891434445977 Updates: 8/29000, Avg Grad: 0.1592082381248474, Threshold: 0.5915119051933289\n",
            "Epoch 14 iteration 0 Loss: 0.364 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.658 | Acc: 57.426% (58/101)\n",
            "Epoch 14 iteration 200 Loss: 0.714 | Acc: 54.229% (109/201)\n",
            "Test accuracy: 0.5422885417938232\n",
            "Epoch: 15, Loss: 0.006232268176972866 Updates: 8/30000, Avg Grad: 0.31260645389556885, Threshold: 0.5915119051933289\n",
            "Epoch: 15, Loss: 0.0064939153380692005 Updates: 8/31000, Avg Grad: 0.45271748304367065, Threshold: 0.5915119051933289\n",
            "Epoch: 15, Loss: 0.004778883419930935 Updates: 9/32000, Avg Grad: 0.035561852157115936, Threshold: 0.5991204380989075\n",
            "Epoch 15 iteration 0 Loss: 1.095 | Acc: 0.000% (0/1)\n",
            "Epoch 15 iteration 100 Loss: 0.753 | Acc: 47.525% (48/101)\n",
            "Epoch 15 iteration 200 Loss: 0.719 | Acc: 53.234% (107/201)\n",
            "Test accuracy: 0.5323383212089539\n",
            "Epoch: 16, Loss: 1.3937307596206665 Updates: 0/0, Avg Grad: 0.2483816295862198, Threshold: 0.2732197940349579\n",
            "Epoch: 16, Loss: 0.006759416311979294 Updates: 1/1000, Avg Grad: 0.3151598572731018, Threshold: 0.3166899085044861\n",
            "Epoch: 16, Loss: 0.00552884628996253 Updates: 2/2000, Avg Grad: 0.3359605669975281, Threshold: 0.37081238627433777\n",
            "Epoch: 16, Loss: 0.003663779003545642 Updates: 3/3000, Avg Grad: 0.21940460801124573, Threshold: 0.44079750776290894\n",
            "Epoch: 16, Loss: 0.007058112416416407 Updates: 4/4000, Avg Grad: 0.1010991781949997, Threshold: 0.5231934189796448\n",
            "Epoch: 16, Loss: 0.0049691651947796345 Updates: 4/5000, Avg Grad: 0.36856260895729065, Threshold: 0.5231934189796448\n",
            "Epoch: 16, Loss: 0.0059358542785048485 Updates: 5/6000, Avg Grad: 0.03973253816366196, Threshold: 0.5772172808647156\n",
            "Epoch: 16, Loss: 0.002762560499832034 Updates: 5/7000, Avg Grad: 0.14006271958351135, Threshold: 0.5772172808647156\n",
            "Epoch: 16, Loss: 0.00481605576351285 Updates: 5/8000, Avg Grad: 0.22914791107177734, Threshold: 0.5772172808647156\n",
            "Epoch: 16, Loss: 0.003610289189964533 Updates: 5/9000, Avg Grad: 0.3111458718776703, Threshold: 0.5772172808647156\n",
            "Epoch 15 iteration 0 Loss: 0.595 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.682 | Acc: 57.426% (58/101)\n",
            "Epoch 15 iteration 200 Loss: 0.647 | Acc: 61.692% (124/201)\n",
            "Test accuracy: 0.6169154047966003\n",
            "Epoch: 16, Loss: 0.014426667243242264 Updates: 5/10000, Avg Grad: 0.4014093279838562, Threshold: 0.5772172808647156\n",
            "Epoch: 16, Loss: 0.009560740552842617 Updates: 5/11000, Avg Grad: 0.49810558557510376, Threshold: 0.5772172808647156\n",
            "Epoch: 16, Loss: 0.004498928319662809 Updates: 6/12000, Avg Grad: 0.09335987269878387, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.007985546253621578 Updates: 6/13000, Avg Grad: 0.15848539769649506, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.005698646884411573 Updates: 6/14000, Avg Grad: 0.22793856263160706, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.005969964899122715 Updates: 6/15000, Avg Grad: 0.29025325179100037, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.007586045656353235 Updates: 6/16000, Avg Grad: 0.4014255404472351, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.005562980193644762 Updates: 6/17000, Avg Grad: 0.5537698268890381, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.006335732992738485 Updates: 6/18000, Avg Grad: 0.5452166199684143, Threshold: 0.6456392407417297\n",
            "Epoch: 16, Loss: 0.00872565247118473 Updates: 6/19000, Avg Grad: 0.6066682934761047, Threshold: 0.6456392407417297\n",
            "Epoch 15 iteration 0 Loss: 0.376 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.596 | Acc: 68.317% (69/101)\n",
            "Epoch 15 iteration 200 Loss: 0.648 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 16, Loss: 0.01122482493519783 Updates: 7/20000, Avg Grad: 0.14321079850196838, Threshold: 0.6804820895195007\n",
            "Epoch: 16, Loss: 0.01179931964725256 Updates: 7/21000, Avg Grad: 0.3411475718021393, Threshold: 0.6804820895195007\n",
            "Epoch: 16, Loss: 0.0047265454195439816 Updates: 7/22000, Avg Grad: 0.5095896124839783, Threshold: 0.6804820895195007\n",
            "Epoch: 16, Loss: 0.009786704555153847 Updates: 8/23000, Avg Grad: 0.6812598705291748, Threshold: 0.695035457611084\n",
            "Epoch: 16, Loss: 0.005553818307816982 Updates: 8/24000, Avg Grad: 0.15289311110973358, Threshold: 0.695035457611084\n",
            "Epoch: 16, Loss: 0.004979470744729042 Updates: 8/25000, Avg Grad: 0.3988581597805023, Threshold: 0.695035457611084\n",
            "Epoch: 16, Loss: 0.004886855371296406 Updates: 8/26000, Avg Grad: 0.6421985030174255, Threshold: 0.695035457611084\n",
            "Epoch: 16, Loss: 0.00471012108027935 Updates: 9/27000, Avg Grad: 0.19861774146556854, Threshold: 0.706705629825592\n",
            "Epoch: 16, Loss: 0.007375362329185009 Updates: 9/28000, Avg Grad: 0.4249843657016754, Threshold: 0.706705629825592\n",
            "Epoch: 16, Loss: 0.009770240634679794 Updates: 9/29000, Avg Grad: 0.5865224599838257, Threshold: 0.706705629825592\n",
            "Epoch 15 iteration 0 Loss: 0.525 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.659 | Acc: 53.465% (54/101)\n",
            "Epoch 15 iteration 200 Loss: 0.676 | Acc: 55.224% (111/201)\n",
            "Test accuracy: 0.5522388219833374\n",
            "Epoch: 16, Loss: 0.005187397822737694 Updates: 10/30000, Avg Grad: 0.06973739713430405, Threshold: 0.7118244767189026\n",
            "Epoch: 16, Loss: 0.007882577367126942 Updates: 10/31000, Avg Grad: 0.1796637773513794, Threshold: 0.7118244767189026\n",
            "Epoch: 16, Loss: 0.0051643638871610165 Updates: 10/32000, Avg Grad: 0.2518254518508911, Threshold: 0.7118244767189026\n",
            "Epoch 16 iteration 0 Loss: 1.923 | Acc: 0.000% (0/1)\n",
            "Epoch 16 iteration 100 Loss: 0.736 | Acc: 51.485% (52/101)\n",
            "Epoch 16 iteration 200 Loss: 0.715 | Acc: 57.214% (115/201)\n",
            "Test accuracy: 0.572139322757721\n",
            "Epoch: 17, Loss: 0.40614771842956543 Updates: 0/0, Avg Grad: 0.11797929555177689, Threshold: 0.1297772228717804\n",
            "Epoch: 17, Loss: 0.006805845070630312 Updates: 0/1000, Avg Grad: 0.07915439456701279, Threshold: 0.1297772228717804\n",
            "Epoch: 17, Loss: 0.00456597376614809 Updates: 1/2000, Avg Grad: 0.03608996048569679, Threshold: 0.1548866629600525\n",
            "Epoch: 17, Loss: 0.004446165636181831 Updates: 2/3000, Avg Grad: 0.08304043859243393, Threshold: 0.17194871604442596\n",
            "Epoch: 17, Loss: 0.005052358377724886 Updates: 3/4000, Avg Grad: 0.1847837120294571, Threshold: 0.20313572883605957\n",
            "Epoch: 17, Loss: 0.012136279605329037 Updates: 5/5000, Avg Grad: 0.030477767810225487, Threshold: 0.28158819675445557\n",
            "Epoch: 17, Loss: 0.00568111427128315 Updates: 6/6000, Avg Grad: 0.02784673124551773, Threshold: 0.31117185950279236\n",
            "Epoch: 17, Loss: 0.007438974920660257 Updates: 7/7000, Avg Grad: 0.3342844247817993, Threshold: 0.3677128851413727\n",
            "Epoch: 17, Loss: 0.012667414732277393 Updates: 7/8000, Avg Grad: 0.20822687447071075, Threshold: 0.3677128851413727\n",
            "Epoch: 17, Loss: 0.0037402573507279158 Updates: 7/9000, Avg Grad: 0.30111807584762573, Threshold: 0.3677128851413727\n",
            "Epoch 16 iteration 0 Loss: 0.405 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.661 | Acc: 58.416% (59/101)\n",
            "Epoch 16 iteration 200 Loss: 0.666 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 17, Loss: 0.006264036521315575 Updates: 8/10000, Avg Grad: 0.011528033763170242, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.0029239398427307606 Updates: 8/11000, Avg Grad: 0.03500032424926758, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.0074568563140928745 Updates: 8/12000, Avg Grad: 0.05399952828884125, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.0059511978179216385 Updates: 8/13000, Avg Grad: 0.10743724554777145, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.010753283277153969 Updates: 8/14000, Avg Grad: 0.17318637669086456, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.006377628073096275 Updates: 8/15000, Avg Grad: 0.2135130763053894, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.019627854228019714 Updates: 8/16000, Avg Grad: 0.313044011592865, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.008011934347450733 Updates: 8/17000, Avg Grad: 0.35904431343078613, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.006798930931836367 Updates: 8/18000, Avg Grad: 0.3620748221874237, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.006646750029176474 Updates: 8/19000, Avg Grad: 0.38563644886016846, Threshold: 0.4131055772304535\n",
            "Epoch 16 iteration 0 Loss: 0.880 | Acc: 0.000% (0/1)\n",
            "Epoch 16 iteration 100 Loss: 0.655 | Acc: 67.327% (68/101)\n",
            "Epoch 16 iteration 200 Loss: 0.661 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 17, Loss: 0.006720751989632845 Updates: 8/20000, Avg Grad: 0.39282768964767456, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.0033232690766453743 Updates: 8/21000, Avg Grad: 0.36835944652557373, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.009331258945167065 Updates: 8/22000, Avg Grad: 0.3511931300163269, Threshold: 0.4131055772304535\n",
            "Epoch: 17, Loss: 0.010705215856432915 Updates: 9/23000, Avg Grad: 0.09226445853710175, Threshold: 0.42534616589546204\n",
            "Epoch: 17, Loss: 0.009973431937396526 Updates: 9/24000, Avg Grad: 0.2331845909357071, Threshold: 0.42534616589546204\n",
            "Epoch: 17, Loss: 0.008549797348678112 Updates: 9/25000, Avg Grad: 0.37710535526275635, Threshold: 0.42534616589546204\n",
            "Epoch: 17, Loss: 0.005309402476996183 Updates: 10/26000, Avg Grad: 0.15800873935222626, Threshold: 0.4350476861000061\n",
            "Epoch: 17, Loss: 0.0064010643400251865 Updates: 10/27000, Avg Grad: 0.313403844833374, Threshold: 0.4350476861000061\n",
            "Epoch: 17, Loss: 0.006037390325218439 Updates: 11/28000, Avg Grad: 0.09147998690605164, Threshold: 0.4478592574596405\n",
            "Epoch: 17, Loss: 0.009829546324908733 Updates: 11/29000, Avg Grad: 0.30447039008140564, Threshold: 0.4478592574596405\n",
            "Epoch 16 iteration 0 Loss: 0.759 | Acc: 0.000% (0/1)\n",
            "Epoch 16 iteration 100 Loss: 0.687 | Acc: 59.406% (60/101)\n",
            "Epoch 16 iteration 200 Loss: 0.694 | Acc: 58.706% (118/201)\n",
            "Test accuracy: 0.5870646834373474\n",
            "Epoch: 17, Loss: 0.005778392776846886 Updates: 12/30000, Avg Grad: 0.0769038125872612, Threshold: 0.47540080547332764\n",
            "Epoch: 17, Loss: 0.0014141113497316837 Updates: 12/31000, Avg Grad: 0.36795365810394287, Threshold: 0.47540080547332764\n",
            "Epoch: 17, Loss: 0.004776565358042717 Updates: 13/32000, Avg Grad: 0.10034322738647461, Threshold: 0.49621203541755676\n",
            "Epoch 17 iteration 0 Loss: 0.521 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.658 | Acc: 54.455% (55/101)\n",
            "Epoch 17 iteration 200 Loss: 0.631 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 18, Loss: 0.8784262537956238 Updates: 0/0, Avg Grad: 0.20362667739391327, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.003912398591637611 Updates: 0/1000, Avg Grad: 0.1899639070034027, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.003269087988883257 Updates: 0/2000, Avg Grad: 0.1684858351945877, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.00240213330835104 Updates: 0/3000, Avg Grad: 0.11562969535589218, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.004173945635557175 Updates: 0/4000, Avg Grad: 0.11143456399440765, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.008928471244871616 Updates: 0/5000, Avg Grad: 0.1234004944562912, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.0068913656286895275 Updates: 0/6000, Avg Grad: 0.1397540271282196, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.004174967296421528 Updates: 0/7000, Avg Grad: 0.16816487908363342, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.005440037231892347 Updates: 0/8000, Avg Grad: 0.19813191890716553, Threshold: 0.2239893525838852\n",
            "Epoch: 18, Loss: 0.0016619800589978695 Updates: 0/9000, Avg Grad: 0.21115922927856445, Threshold: 0.2239893525838852\n",
            "Epoch 17 iteration 0 Loss: 0.756 | Acc: 0.000% (0/1)\n",
            "Epoch 17 iteration 100 Loss: 0.640 | Acc: 62.376% (63/101)\n",
            "Epoch 17 iteration 200 Loss: 0.647 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 18, Loss: 0.012205068953335285 Updates: 1/10000, Avg Grad: 0.02499309927225113, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.0067878300324082375 Updates: 1/11000, Avg Grad: 0.050960760563611984, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.0031072134152054787 Updates: 1/12000, Avg Grad: 0.0526774637401104, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.0053371828980743885 Updates: 1/13000, Avg Grad: 0.06682375073432922, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.004903356544673443 Updates: 1/14000, Avg Grad: 0.06721864640712738, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.006448274943977594 Updates: 1/15000, Avg Grad: 0.07775609195232391, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.0038498439826071262 Updates: 1/16000, Avg Grad: 0.08234966546297073, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.004289583303034306 Updates: 1/17000, Avg Grad: 0.10086560994386673, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.007087301462888718 Updates: 1/18000, Avg Grad: 0.11048810184001923, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.008252954110503197 Updates: 1/19000, Avg Grad: 0.12453252822160721, Threshold: 0.25605714321136475\n",
            "Epoch 17 iteration 0 Loss: 0.390 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.719 | Acc: 52.475% (53/101)\n",
            "Epoch 17 iteration 200 Loss: 0.721 | Acc: 53.234% (107/201)\n",
            "Test accuracy: 0.5323383212089539\n",
            "Epoch: 18, Loss: 0.0035696597769856453 Updates: 1/20000, Avg Grad: 0.16773921251296997, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.005661753937602043 Updates: 1/21000, Avg Grad: 0.1682770550251007, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.004176342859864235 Updates: 1/22000, Avg Grad: 0.1930086463689804, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.0071680424734950066 Updates: 1/23000, Avg Grad: 0.19907960295677185, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.007466224953532219 Updates: 1/24000, Avg Grad: 0.20392072200775146, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.002840742701664567 Updates: 1/25000, Avg Grad: 0.20520471036434174, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.007917300797998905 Updates: 1/26000, Avg Grad: 0.21271556615829468, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.0026102839037775993 Updates: 1/27000, Avg Grad: 0.22406664490699768, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.009887272492051125 Updates: 1/28000, Avg Grad: 0.22338181734085083, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.007251062896102667 Updates: 1/29000, Avg Grad: 0.2270479053258896, Threshold: 0.25605714321136475\n",
            "Epoch 17 iteration 0 Loss: 0.286 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.613 | Acc: 67.327% (68/101)\n",
            "Epoch 17 iteration 200 Loss: 0.637 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 18, Loss: 0.011320489458739758 Updates: 1/30000, Avg Grad: 0.23332519829273224, Threshold: 0.25605714321136475\n",
            "Epoch: 18, Loss: 0.007242779713124037 Updates: 2/31000, Avg Grad: 0.05475659668445587, Threshold: 0.2589428126811981\n",
            "Epoch: 18, Loss: 0.010055570863187313 Updates: 2/32000, Avg Grad: 0.0571901835501194, Threshold: 0.2589428126811981\n",
            "Epoch 18 iteration 0 Loss: 0.354 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.642 | Acc: 60.396% (61/101)\n",
            "Epoch 18 iteration 200 Loss: 0.664 | Acc: 58.209% (117/201)\n",
            "Test accuracy: 0.5820895433425903\n",
            "Epoch: 19, Loss: 0.2755274176597595 Updates: 0/0, Avg Grad: 0.08940956741571426, Threshold: 0.09835052490234375\n",
            "Epoch: 19, Loss: 0.010163115337491035 Updates: 2/1000, Avg Grad: 0.04335765913128853, Threshold: 0.14401860535144806\n",
            "Epoch: 19, Loss: 0.002250768942758441 Updates: 3/2000, Avg Grad: 0.06569889932870865, Threshold: 0.1655188798904419\n",
            "Epoch: 19, Loss: 0.0058504752814769745 Updates: 4/3000, Avg Grad: 0.01569472998380661, Threshold: 0.19547297060489655\n",
            "Epoch: 19, Loss: 0.00665083946660161 Updates: 4/4000, Avg Grad: 0.08533591032028198, Threshold: 0.19547297060489655\n",
            "Epoch: 19, Loss: 0.014098762534558773 Updates: 4/5000, Avg Grad: 0.1764051765203476, Threshold: 0.19547297060489655\n",
            "Epoch: 19, Loss: 0.004623232409358025 Updates: 5/6000, Avg Grad: 0.0552547425031662, Threshold: 0.22335456311702728\n",
            "Epoch: 19, Loss: 0.004842498805373907 Updates: 5/7000, Avg Grad: 0.13984736800193787, Threshold: 0.22335456311702728\n",
            "Epoch: 19, Loss: 0.016910092905163765 Updates: 6/8000, Avg Grad: 0.016294201835989952, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.0036791693419218063 Updates: 6/9000, Avg Grad: 0.038516804575920105, Threshold: 0.26370447874069214\n",
            "Epoch 18 iteration 0 Loss: 0.331 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.658 | Acc: 62.376% (63/101)\n",
            "Epoch 18 iteration 200 Loss: 0.653 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 19, Loss: 0.010255076922476292 Updates: 6/10000, Avg Grad: 0.03916540369391441, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.008416383527219296 Updates: 6/11000, Avg Grad: 0.0569356344640255, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.012271422892808914 Updates: 6/12000, Avg Grad: 0.07537159323692322, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.015203752554953098 Updates: 6/13000, Avg Grad: 0.102803535759449, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.006491188425570726 Updates: 6/14000, Avg Grad: 0.11857353150844574, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.0053851185366511345 Updates: 6/15000, Avg Grad: 0.10571131110191345, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.0065033454447984695 Updates: 6/16000, Avg Grad: 0.1305006742477417, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.006585366558283567 Updates: 6/17000, Avg Grad: 0.1755850911140442, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.00828404352068901 Updates: 6/18000, Avg Grad: 0.13086041808128357, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.007862272672355175 Updates: 6/19000, Avg Grad: 0.11445307731628418, Threshold: 0.26370447874069214\n",
            "Epoch 18 iteration 0 Loss: 0.723 | Acc: 0.000% (0/1)\n",
            "Epoch 18 iteration 100 Loss: 0.619 | Acc: 68.317% (69/101)\n",
            "Epoch 18 iteration 200 Loss: 0.637 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 19, Loss: 0.0044455863535404205 Updates: 6/20000, Avg Grad: 0.15543591976165771, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.008402825333178043 Updates: 6/21000, Avg Grad: 0.16148056089878082, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.0048998091369867325 Updates: 6/22000, Avg Grad: 0.17776624858379364, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.011529658921062946 Updates: 6/23000, Avg Grad: 0.17205436527729034, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.005049548111855984 Updates: 6/24000, Avg Grad: 0.18702758848667145, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.003359883790835738 Updates: 6/25000, Avg Grad: 0.20647205412387848, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.007416610140353441 Updates: 6/26000, Avg Grad: 0.21222709119319916, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.006607796065509319 Updates: 6/27000, Avg Grad: 0.2181321084499359, Threshold: 0.26370447874069214\n",
            "Epoch: 19, Loss: 0.0027367088478058577 Updates: 7/28000, Avg Grad: 0.26903805136680603, Threshold: 0.27135926485061646\n",
            "Epoch: 19, Loss: 0.00818988960236311 Updates: 7/29000, Avg Grad: 0.07984574139118195, Threshold: 0.27135926485061646\n",
            "Epoch 18 iteration 0 Loss: 0.910 | Acc: 0.000% (0/1)\n",
            "Epoch 18 iteration 100 Loss: 0.646 | Acc: 61.386% (62/101)\n",
            "Epoch 18 iteration 200 Loss: 0.620 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 19, Loss: 0.011548733338713646 Updates: 7/30000, Avg Grad: 0.12383090704679489, Threshold: 0.27135926485061646\n",
            "Epoch: 19, Loss: 0.007310624234378338 Updates: 7/31000, Avg Grad: 0.1733776032924652, Threshold: 0.27135926485061646\n",
            "Epoch: 19, Loss: 0.003491828450933099 Updates: 8/32000, Avg Grad: 0.27716073393821716, Threshold: 0.27837052941322327\n",
            "Epoch 19 iteration 0 Loss: 0.684 | Acc: 100.000% (1/1)\n",
            "Epoch 19 iteration 100 Loss: 0.599 | Acc: 70.297% (71/101)\n",
            "Epoch 19 iteration 200 Loss: 0.613 | Acc: 68.657% (138/201)\n",
            "Test accuracy: 0.6865671873092651\n",
            "Epoch: 20, Loss: 0.8830341696739197 Updates: 0/0, Avg Grad: 0.19409912824630737, Threshold: 0.21350905299186707\n",
            "Epoch: 20, Loss: 0.0020219418220221996 Updates: 0/1000, Avg Grad: 0.20876802504062653, Threshold: 0.21350905299186707\n",
            "Epoch: 20, Loss: 0.003935853019356728 Updates: 0/2000, Avg Grad: 0.13457679748535156, Threshold: 0.21350905299186707\n",
            "Epoch: 20, Loss: 0.003900356823578477 Updates: 0/3000, Avg Grad: 0.14604443311691284, Threshold: 0.21350905299186707\n",
            "Epoch: 20, Loss: 0.0057300846092402935 Updates: 0/4000, Avg Grad: 0.12542526423931122, Threshold: 0.21350905299186707\n",
            "Epoch: 20, Loss: 0.009330397471785545 Updates: 0/5000, Avg Grad: 0.15324440598487854, Threshold: 0.21350905299186707\n",
            "Epoch: 20, Loss: 0.014073837548494339 Updates: 1/6000, Avg Grad: 0.03150667995214462, Threshold: 0.24185927212238312\n",
            "Epoch: 20, Loss: 0.005562438163906336 Updates: 1/7000, Avg Grad: 0.18831248581409454, Threshold: 0.24185927212238312\n",
            "Epoch: 20, Loss: 0.004295671358704567 Updates: 2/8000, Avg Grad: 0.03855673968791962, Threshold: 0.28423452377319336\n",
            "Epoch: 20, Loss: 0.002139164600521326 Updates: 2/9000, Avg Grad: 0.12298829853534698, Threshold: 0.28423452377319336\n",
            "Epoch 19 iteration 0 Loss: 0.546 | Acc: 100.000% (1/1)\n",
            "Epoch 19 iteration 100 Loss: 0.665 | Acc: 59.406% (60/101)\n",
            "Epoch 19 iteration 200 Loss: 0.665 | Acc: 60.199% (121/201)\n",
            "Test accuracy: 0.6019900441169739\n",
            "Epoch: 20, Loss: 0.006905955728143454 Updates: 2/10000, Avg Grad: 0.18251247704029083, Threshold: 0.28423452377319336\n",
            "Epoch: 20, Loss: 0.004490586929023266 Updates: 2/11000, Avg Grad: 0.23363487422466278, Threshold: 0.28423452377319336\n",
            "Epoch: 20, Loss: 0.00660743098706007 Updates: 3/12000, Avg Grad: 0.011384552344679832, Threshold: 0.3130590319633484\n",
            "Epoch: 20, Loss: 0.005577397998422384 Updates: 3/13000, Avg Grad: 0.06895074248313904, Threshold: 0.3130590319633484\n",
            "Epoch: 20, Loss: 0.012106073088943958 Updates: 3/14000, Avg Grad: 0.10132946074008942, Threshold: 0.3130590319633484\n",
            "Epoch: 20, Loss: 0.00433206744492054 Updates: 3/15000, Avg Grad: 0.14374971389770508, Threshold: 0.3130590319633484\n",
            "Epoch: 20, Loss: 0.0039839777164161205 Updates: 3/16000, Avg Grad: 0.18257877230644226, Threshold: 0.3130590319633484\n",
            "Epoch: 20, Loss: 0.004266377538442612 Updates: 3/17000, Avg Grad: 0.26908090710639954, Threshold: 0.3130590319633484\n",
            "Epoch: 20, Loss: 0.008549287915229797 Updates: 4/18000, Avg Grad: 0.016147108748555183, Threshold: 0.32897642254829407\n",
            "Epoch: 20, Loss: 0.003904617391526699 Updates: 4/19000, Avg Grad: 0.13538917899131775, Threshold: 0.32897642254829407\n",
            "Epoch 19 iteration 0 Loss: 1.522 | Acc: 0.000% (0/1)\n",
            "Epoch 19 iteration 100 Loss: 0.647 | Acc: 59.406% (60/101)\n",
            "Epoch 19 iteration 200 Loss: 0.659 | Acc: 59.204% (119/201)\n",
            "Test accuracy: 0.5920398235321045\n",
            "Epoch: 20, Loss: 0.01555964071303606 Updates: 4/20000, Avg Grad: 0.20216935873031616, Threshold: 0.32897642254829407\n",
            "Epoch: 20, Loss: 0.003969527315348387 Updates: 4/21000, Avg Grad: 0.2588208317756653, Threshold: 0.32897642254829407\n",
            "Epoch: 20, Loss: 0.001834183232858777 Updates: 4/22000, Avg Grad: 0.3110814690589905, Threshold: 0.32897642254829407\n",
            "Epoch: 20, Loss: 0.0037163840606808662 Updates: 5/23000, Avg Grad: 0.06579355895519257, Threshold: 0.34369248151779175\n",
            "Epoch: 20, Loss: 0.0060568032786250114 Updates: 5/24000, Avg Grad: 0.12677952647209167, Threshold: 0.34369248151779175\n",
            "Epoch: 20, Loss: 0.001475961646065116 Updates: 5/25000, Avg Grad: 0.1689530611038208, Threshold: 0.34369248151779175\n",
            "Epoch: 20, Loss: 0.006468874402344227 Updates: 5/26000, Avg Grad: 0.19461242854595184, Threshold: 0.34369248151779175\n",
            "Epoch: 20, Loss: 0.005520422011613846 Updates: 5/27000, Avg Grad: 0.25246620178222656, Threshold: 0.34369248151779175\n",
            "Epoch: 20, Loss: 0.005910569801926613 Updates: 5/28000, Avg Grad: 0.3124768137931824, Threshold: 0.34369248151779175\n",
            "Epoch: 20, Loss: 0.003738604485988617 Updates: 6/29000, Avg Grad: 0.030859263613820076, Threshold: 0.3611179292201996\n",
            "Epoch 19 iteration 0 Loss: 0.394 | Acc: 100.000% (1/1)\n",
            "Epoch 19 iteration 100 Loss: 0.598 | Acc: 73.267% (74/101)\n",
            "Epoch 19 iteration 200 Loss: 0.610 | Acc: 69.652% (140/201)\n",
            "Test accuracy: 0.6965174078941345\n",
            "Epoch: 20, Loss: 0.009857871569693089 Updates: 6/30000, Avg Grad: 0.06275519728660583, Threshold: 0.3611179292201996\n",
            "Epoch: 20, Loss: 0.004935157485306263 Updates: 6/31000, Avg Grad: 0.09033890068531036, Threshold: 0.3611179292201996\n",
            "Epoch: 20, Loss: 0.014824061654508114 Updates: 6/32000, Avg Grad: 0.14202114939689636, Threshold: 0.3611179292201996\n",
            "Epoch 20 iteration 0 Loss: 0.417 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.737 | Acc: 52.475% (53/101)\n",
            "Epoch 20 iteration 200 Loss: 0.690 | Acc: 59.204% (119/201)\n",
            "Test accuracy: 0.5920398235321045\n",
            "Epoch: 21, Loss: 0.6891160607337952 Updates: 0/0, Avg Grad: 0.18276210129261017, Threshold: 0.20103831589221954\n",
            "Epoch: 21, Loss: 0.005926619749516249 Updates: 1/1000, Avg Grad: 0.017483389005064964, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.006149450317025185 Updates: 1/2000, Avg Grad: 0.03875492140650749, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.0037529529072344303 Updates: 1/3000, Avg Grad: 0.06136395409703255, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.004566041752696037 Updates: 1/4000, Avg Grad: 0.08884450793266296, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.00752535043284297 Updates: 1/5000, Avg Grad: 0.0926954448223114, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.005269882269203663 Updates: 1/6000, Avg Grad: 0.10303719341754913, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.00486657302826643 Updates: 1/7000, Avg Grad: 0.13315989077091217, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.010110681876540184 Updates: 1/8000, Avg Grad: 0.14630016684532166, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.00561617873609066 Updates: 1/9000, Avg Grad: 0.12534861266613007, Threshold: 0.23703411221504211\n",
            "Epoch 20 iteration 0 Loss: 0.827 | Acc: 0.000% (0/1)\n",
            "Epoch 20 iteration 100 Loss: 0.638 | Acc: 60.396% (61/101)\n",
            "Epoch 20 iteration 200 Loss: 0.635 | Acc: 64.677% (130/201)\n",
            "Test accuracy: 0.646766185760498\n",
            "Epoch: 21, Loss: 0.0063333953730762005 Updates: 1/10000, Avg Grad: 0.10694851726293564, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.0034041190519928932 Updates: 1/11000, Avg Grad: 0.14137399196624756, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.007852373644709587 Updates: 1/12000, Avg Grad: 0.13914325833320618, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.006574999075382948 Updates: 1/13000, Avg Grad: 0.15943092107772827, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.008757085539400578 Updates: 1/14000, Avg Grad: 0.17085333168506622, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.00547003373503685 Updates: 1/15000, Avg Grad: 0.15410608053207397, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.003674028441309929 Updates: 1/16000, Avg Grad: 0.13641458749771118, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.0007729403441771865 Updates: 1/17000, Avg Grad: 0.14396192133426666, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.009968412108719349 Updates: 1/18000, Avg Grad: 0.14099153876304626, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.004114450421184301 Updates: 1/19000, Avg Grad: 0.14448027312755585, Threshold: 0.23703411221504211\n",
            "Epoch 20 iteration 0 Loss: 0.626 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.589 | Acc: 70.297% (71/101)\n",
            "Epoch 20 iteration 200 Loss: 0.614 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 21, Loss: 0.005938372574746609 Updates: 1/20000, Avg Grad: 0.16302390396595, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.005386891774833202 Updates: 1/21000, Avg Grad: 0.18319223821163177, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.002508241916075349 Updates: 1/22000, Avg Grad: 0.20885951817035675, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.0105208158493042 Updates: 1/23000, Avg Grad: 0.19978657364845276, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.010604766197502613 Updates: 1/24000, Avg Grad: 0.20188163220882416, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.005687343887984753 Updates: 1/25000, Avg Grad: 0.2025279998779297, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.005464527755975723 Updates: 1/26000, Avg Grad: 0.2002733200788498, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.0014759431360289454 Updates: 1/27000, Avg Grad: 0.231231227517128, Threshold: 0.23703411221504211\n",
            "Epoch: 21, Loss: 0.00944893155246973 Updates: 2/28000, Avg Grad: 0.04628925770521164, Threshold: 0.24524830281734467\n",
            "Epoch: 21, Loss: 0.0043588667176663876 Updates: 2/29000, Avg Grad: 0.11239160597324371, Threshold: 0.24524830281734467\n",
            "Epoch 20 iteration 0 Loss: 0.432 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.624 | Acc: 62.376% (63/101)\n",
            "Epoch 20 iteration 200 Loss: 0.645 | Acc: 61.692% (124/201)\n",
            "Test accuracy: 0.6169154047966003\n",
            "Epoch: 21, Loss: 0.009868549183011055 Updates: 2/30000, Avg Grad: 0.2002943754196167, Threshold: 0.24524830281734467\n",
            "Epoch: 21, Loss: 0.0027082276064902544 Updates: 3/31000, Avg Grad: 0.013144909404218197, Threshold: 0.24804921448230743\n",
            "Epoch: 21, Loss: 0.004269266035407782 Updates: 3/32000, Avg Grad: 0.10132505744695663, Threshold: 0.24804921448230743\n",
            "Epoch 21 iteration 0 Loss: 0.801 | Acc: 0.000% (0/1)\n",
            "Epoch 21 iteration 100 Loss: 0.626 | Acc: 66.337% (67/101)\n",
            "Epoch 21 iteration 200 Loss: 0.637 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 22, Loss: 0.42135515809059143 Updates: 0/0, Avg Grad: 0.12552793323993683, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.005132643040269613 Updates: 0/1000, Avg Grad: 0.05329114943742752, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.004980955272912979 Updates: 0/2000, Avg Grad: 0.0639706626534462, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.0024110341910272837 Updates: 0/3000, Avg Grad: 0.085199736058712, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.008396737277507782 Updates: 0/4000, Avg Grad: 0.08506043255329132, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.00596170499920845 Updates: 0/5000, Avg Grad: 0.09281420707702637, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.00649902643635869 Updates: 0/6000, Avg Grad: 0.11283361166715622, Threshold: 0.13808073103427887\n",
            "Epoch: 22, Loss: 0.010085822083055973 Updates: 1/7000, Avg Grad: 0.06647035479545593, Threshold: 0.15521900355815887\n",
            "Epoch: 22, Loss: 0.006371338851749897 Updates: 1/8000, Avg Grad: 0.036467477679252625, Threshold: 0.15521900355815887\n",
            "Epoch: 22, Loss: 0.008480963297188282 Updates: 1/9000, Avg Grad: 0.053059231489896774, Threshold: 0.15521900355815887\n",
            "Epoch 21 iteration 0 Loss: 0.529 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.615 | Acc: 60.396% (61/101)\n",
            "Epoch 21 iteration 200 Loss: 0.630 | Acc: 63.184% (127/201)\n",
            "Test accuracy: 0.6318408250808716\n",
            "Epoch: 22, Loss: 0.0031092253047972918 Updates: 1/10000, Avg Grad: 0.10443195700645447, Threshold: 0.15521900355815887\n",
            "Epoch: 22, Loss: 0.0032335729338228703 Updates: 2/11000, Avg Grad: 0.018634118139743805, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.007419407833367586 Updates: 2/12000, Avg Grad: 0.04482872411608696, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.008647401817142963 Updates: 2/13000, Avg Grad: 0.053920503705739975, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.007607275620102882 Updates: 2/14000, Avg Grad: 0.07973243296146393, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.006129339803010225 Updates: 2/15000, Avg Grad: 0.12914018332958221, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.016744235530495644 Updates: 2/16000, Avg Grad: 0.10979404300451279, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.0020033393520861864 Updates: 2/17000, Avg Grad: 0.10310813039541245, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.0035183336585760117 Updates: 2/18000, Avg Grad: 0.10320670902729034, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.010630247183144093 Updates: 2/19000, Avg Grad: 0.1097012460231781, Threshold: 0.17843486368656158\n",
            "Epoch 21 iteration 0 Loss: 0.474 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.717 | Acc: 58.416% (59/101)\n",
            "Epoch 21 iteration 200 Loss: 0.675 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 22, Loss: 0.010647672228515148 Updates: 2/20000, Avg Grad: 0.15436802804470062, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.013713760301470757 Updates: 2/21000, Avg Grad: 0.16236478090286255, Threshold: 0.17843486368656158\n",
            "Epoch: 22, Loss: 0.010229069739580154 Updates: 3/22000, Avg Grad: 0.03827964887022972, Threshold: 0.19026876986026764\n",
            "Epoch: 22, Loss: 0.005393211729824543 Updates: 3/23000, Avg Grad: 0.092095747590065, Threshold: 0.19026876986026764\n",
            "Epoch: 22, Loss: 0.0020852796733379364 Updates: 3/24000, Avg Grad: 0.07558520883321762, Threshold: 0.19026876986026764\n",
            "Epoch: 22, Loss: 0.0011380041250959039 Updates: 3/25000, Avg Grad: 0.1281394064426422, Threshold: 0.19026876986026764\n",
            "Epoch: 22, Loss: 0.004647174384444952 Updates: 3/26000, Avg Grad: 0.174077108502388, Threshold: 0.19026876986026764\n",
            "Epoch: 22, Loss: 0.003948142286390066 Updates: 4/27000, Avg Grad: 0.03787032514810562, Threshold: 0.21039405465126038\n",
            "Epoch: 22, Loss: 0.00641363812610507 Updates: 4/28000, Avg Grad: 0.05473269522190094, Threshold: 0.21039405465126038\n",
            "Epoch: 22, Loss: 0.010667319409549236 Updates: 4/29000, Avg Grad: 0.16769786179065704, Threshold: 0.21039405465126038\n",
            "Epoch 21 iteration 0 Loss: 0.317 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.662 | Acc: 59.406% (60/101)\n",
            "Epoch 21 iteration 200 Loss: 0.666 | Acc: 58.209% (117/201)\n",
            "Test accuracy: 0.5820895433425903\n",
            "Epoch: 22, Loss: 0.005106346216052771 Updates: 5/30000, Avg Grad: 0.008617964573204517, Threshold: 0.21626490354537964\n",
            "Epoch: 22, Loss: 0.0052234516479074955 Updates: 5/31000, Avg Grad: 0.03144865110516548, Threshold: 0.21626490354537964\n",
            "Epoch: 22, Loss: 0.012815206311643124 Updates: 5/32000, Avg Grad: 0.05917714163661003, Threshold: 0.21626490354537964\n",
            "Epoch 22 iteration 0 Loss: 0.612 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.632 | Acc: 67.327% (68/101)\n",
            "Epoch 22 iteration 200 Loss: 0.629 | Acc: 67.164% (135/201)\n",
            "Test accuracy: 0.6716417670249939\n",
            "Epoch: 23, Loss: 0.5883060693740845 Updates: 0/0, Avg Grad: 0.16790145635604858, Threshold: 0.18469160795211792\n",
            "Epoch: 23, Loss: 0.009672284126281738 Updates: 0/1000, Avg Grad: 0.18358422815799713, Threshold: 0.18469160795211792\n",
            "Epoch: 23, Loss: 0.010299945250153542 Updates: 1/2000, Avg Grad: 0.04510333389043808, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.007791651878505945 Updates: 1/3000, Avg Grad: 0.05082510784268379, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.005467605777084827 Updates: 1/4000, Avg Grad: 0.06679806113243103, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.005643568001687527 Updates: 1/5000, Avg Grad: 0.15928500890731812, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.005509295500814915 Updates: 1/6000, Avg Grad: 0.1796705275774002, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.005680567119270563 Updates: 1/7000, Avg Grad: 0.20032869279384613, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.0029438985511660576 Updates: 1/8000, Avg Grad: 0.18201100826263428, Threshold: 0.21888528764247894\n",
            "Epoch: 23, Loss: 0.005210803356021643 Updates: 2/9000, Avg Grad: 0.016498848795890808, Threshold: 0.24136045575141907\n",
            "Epoch 22 iteration 0 Loss: 0.339 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.635 | Acc: 60.396% (61/101)\n",
            "Epoch 22 iteration 200 Loss: 0.658 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 23, Loss: 0.006972606759518385 Updates: 2/10000, Avg Grad: 0.11008439213037491, Threshold: 0.24136045575141907\n",
            "Epoch: 23, Loss: 0.0015419631963595748 Updates: 2/11000, Avg Grad: 0.19999553263187408, Threshold: 0.24136045575141907\n",
            "Epoch: 23, Loss: 0.0035541111137717962 Updates: 3/12000, Avg Grad: 0.03916556388139725, Threshold: 0.27746912837028503\n",
            "Epoch: 23, Loss: 0.011603374034166336 Updates: 3/13000, Avg Grad: 0.07255157828330994, Threshold: 0.27746912837028503\n",
            "Epoch: 23, Loss: 0.010974776931107044 Updates: 3/14000, Avg Grad: 0.1540660262107849, Threshold: 0.27746912837028503\n",
            "Epoch: 23, Loss: 0.006050094962120056 Updates: 3/15000, Avg Grad: 0.2576996088027954, Threshold: 0.27746912837028503\n",
            "Epoch: 23, Loss: 0.003514980897307396 Updates: 4/16000, Avg Grad: 0.037413593381643295, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.0036143071483820677 Updates: 4/17000, Avg Grad: 0.06638798117637634, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.01505571324378252 Updates: 4/18000, Avg Grad: 0.07555561512708664, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.0044089555740356445 Updates: 4/19000, Avg Grad: 0.17173229157924652, Threshold: 0.3158610761165619\n",
            "Epoch 22 iteration 0 Loss: 0.622 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.617 | Acc: 69.307% (70/101)\n",
            "Epoch 22 iteration 200 Loss: 0.623 | Acc: 65.174% (131/201)\n",
            "Test accuracy: 0.6517412662506104\n",
            "Epoch: 23, Loss: 0.005204701330512762 Updates: 4/20000, Avg Grad: 0.12814083695411682, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.003868786385282874 Updates: 4/21000, Avg Grad: 0.19356974959373474, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.0039924131706357 Updates: 4/22000, Avg Grad: 0.25619322061538696, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.0024200084153562784 Updates: 4/23000, Avg Grad: 0.27453523874282837, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.0055979466997087 Updates: 4/24000, Avg Grad: 0.23687323927879333, Threshold: 0.3158610761165619\n",
            "Epoch: 23, Loss: 0.002838082844391465 Updates: 5/25000, Avg Grad: 0.01775037869811058, Threshold: 0.32587894797325134\n",
            "Epoch: 23, Loss: 0.003520709928125143 Updates: 5/26000, Avg Grad: 0.03347083926200867, Threshold: 0.32587894797325134\n",
            "Epoch: 23, Loss: 0.0051387036219239235 Updates: 5/27000, Avg Grad: 0.05138124153017998, Threshold: 0.32587894797325134\n",
            "Epoch: 23, Loss: 0.002428946318104863 Updates: 5/28000, Avg Grad: 0.10502906143665314, Threshold: 0.32587894797325134\n",
            "Epoch: 23, Loss: 0.00620940700173378 Updates: 5/29000, Avg Grad: 0.1753382533788681, Threshold: 0.32587894797325134\n",
            "Epoch 22 iteration 0 Loss: 0.347 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.662 | Acc: 61.386% (62/101)\n",
            "Epoch 22 iteration 200 Loss: 0.645 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 23, Loss: 0.005353274289518595 Updates: 5/30000, Avg Grad: 0.20358651876449585, Threshold: 0.32587894797325134\n",
            "Epoch: 23, Loss: 0.010509870015084743 Updates: 5/31000, Avg Grad: 0.23966798186302185, Threshold: 0.32587894797325134\n",
            "Epoch: 23, Loss: 0.004657192621380091 Updates: 5/32000, Avg Grad: 0.2745150327682495, Threshold: 0.32587894797325134\n",
            "Epoch 23 iteration 0 Loss: 0.154 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.634 | Acc: 61.386% (62/101)\n",
            "Epoch 23 iteration 200 Loss: 0.668 | Acc: 60.199% (121/201)\n",
            "Test accuracy: 0.6019900441169739\n",
            "Epoch: 24, Loss: 0.21204069256782532 Updates: 0/0, Avg Grad: 0.06575001776218414, Threshold: 0.07232502102851868\n",
            "Epoch: 24, Loss: 0.010131368413567543 Updates: 0/1000, Avg Grad: 0.05193097144365311, Threshold: 0.07232502102851868\n",
            "Epoch: 24, Loss: 0.007773399353027344 Updates: 2/2000, Avg Grad: 0.10487614572048187, Threshold: 0.11536376178264618\n",
            "Epoch: 24, Loss: 0.0036665734369307756 Updates: 2/3000, Avg Grad: 0.10790814459323883, Threshold: 0.11536376178264618\n",
            "Epoch: 24, Loss: 0.015493347309529781 Updates: 3/4000, Avg Grad: 0.036361709237098694, Threshold: 0.13906428217887878\n",
            "Epoch: 24, Loss: 0.005590598098933697 Updates: 3/5000, Avg Grad: 0.07783195376396179, Threshold: 0.13906428217887878\n",
            "Epoch: 24, Loss: 0.0028887202497571707 Updates: 4/6000, Avg Grad: 0.17860761284828186, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.008714265190064907 Updates: 4/7000, Avg Grad: 0.0378008633852005, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.004041899461299181 Updates: 4/8000, Avg Grad: 0.05646843835711479, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.010509558022022247 Updates: 4/9000, Avg Grad: 0.08548884093761444, Threshold: 0.19646838307380676\n",
            "Epoch 23 iteration 0 Loss: 0.567 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.627 | Acc: 66.337% (67/101)\n",
            "Epoch 23 iteration 200 Loss: 0.625 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 24, Loss: 0.0037129553966224194 Updates: 4/10000, Avg Grad: 0.0979001522064209, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.010158058255910873 Updates: 4/11000, Avg Grad: 0.08334358036518097, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.008022686466574669 Updates: 4/12000, Avg Grad: 0.0880645290017128, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.0018422871362417936 Updates: 4/13000, Avg Grad: 0.09735418111085892, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.008006430231034756 Updates: 4/14000, Avg Grad: 0.10420650243759155, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.01433394942432642 Updates: 4/15000, Avg Grad: 0.11304403096437454, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.00797720905393362 Updates: 4/16000, Avg Grad: 0.13176900148391724, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.007796240970492363 Updates: 4/17000, Avg Grad: 0.12265138328075409, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.0051507907919585705 Updates: 4/18000, Avg Grad: 0.14204952120780945, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.002830201992765069 Updates: 4/19000, Avg Grad: 0.1484208106994629, Threshold: 0.19646838307380676\n",
            "Epoch 23 iteration 0 Loss: 0.671 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.651 | Acc: 65.347% (66/101)\n",
            "Epoch 23 iteration 200 Loss: 0.628 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 24, Loss: 0.003259108169004321 Updates: 4/20000, Avg Grad: 0.1703144609928131, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.006696956232190132 Updates: 4/21000, Avg Grad: 0.14415429532527924, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.006975619588047266 Updates: 4/22000, Avg Grad: 0.14047478139400482, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.0031055095605552197 Updates: 4/23000, Avg Grad: 0.1757446527481079, Threshold: 0.19646838307380676\n",
            "Epoch: 24, Loss: 0.008254088461399078 Updates: 5/24000, Avg Grad: 0.0664653554558754, Threshold: 0.21114572882652283\n",
            "Epoch: 24, Loss: 0.006993018556386232 Updates: 5/25000, Avg Grad: 0.08697432279586792, Threshold: 0.21114572882652283\n",
            "Epoch: 24, Loss: 0.00731921661645174 Updates: 5/26000, Avg Grad: 0.1632256954908371, Threshold: 0.21114572882652283\n",
            "Epoch: 24, Loss: 0.006527355406433344 Updates: 5/27000, Avg Grad: 0.18215219676494598, Threshold: 0.21114572882652283\n",
            "Epoch: 24, Loss: 0.005029820371419191 Updates: 6/28000, Avg Grad: 0.020446879789233208, Threshold: 0.22380495071411133\n",
            "Epoch: 24, Loss: 0.01804046332836151 Updates: 6/29000, Avg Grad: 0.04336995631456375, Threshold: 0.22380495071411133\n",
            "Epoch 23 iteration 0 Loss: 0.558 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.650 | Acc: 58.416% (59/101)\n",
            "Epoch 23 iteration 200 Loss: 0.646 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 24, Loss: 0.011342168785631657 Updates: 6/30000, Avg Grad: 0.06739654392004013, Threshold: 0.22380495071411133\n",
            "Epoch: 24, Loss: 0.003577488474547863 Updates: 6/31000, Avg Grad: 0.0778217613697052, Threshold: 0.22380495071411133\n",
            "Epoch: 24, Loss: 0.009014297276735306 Updates: 6/32000, Avg Grad: 0.09529722481966019, Threshold: 0.22380495071411133\n",
            "Epoch 24 iteration 0 Loss: 0.473 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.638 | Acc: 65.347% (66/101)\n",
            "Epoch 24 iteration 200 Loss: 0.654 | Acc: 64.677% (130/201)\n",
            "Test accuracy: 0.646766185760498\n",
            "Epoch: 25, Loss: 0.5676289796829224 Updates: 0/0, Avg Grad: 0.1478830873966217, Threshold: 0.16267140209674835\n",
            "Epoch: 25, Loss: 0.008105244487524033 Updates: 0/1000, Avg Grad: 0.09485051780939102, Threshold: 0.16267140209674835\n",
            "Epoch: 25, Loss: 0.003320545656606555 Updates: 1/2000, Avg Grad: 0.011167090386152267, Threshold: 0.1813841015100479\n",
            "Epoch: 25, Loss: 0.014448211528360844 Updates: 1/3000, Avg Grad: 0.15387608110904694, Threshold: 0.1813841015100479\n",
            "Epoch: 25, Loss: 0.007090866565704346 Updates: 2/4000, Avg Grad: 0.09498871117830276, Threshold: 0.20995894074440002\n",
            "Epoch: 25, Loss: 0.004314384888857603 Updates: 3/5000, Avg Grad: 0.013871848583221436, Threshold: 0.23431634902954102\n",
            "Epoch: 25, Loss: 0.004671717528253794 Updates: 3/6000, Avg Grad: 0.05869903415441513, Threshold: 0.23431634902954102\n",
            "Epoch: 25, Loss: 0.0065943049266934395 Updates: 3/7000, Avg Grad: 0.09052994102239609, Threshold: 0.23431634902954102\n",
            "Epoch: 25, Loss: 0.011531131342053413 Updates: 3/8000, Avg Grad: 0.1261785626411438, Threshold: 0.23431634902954102\n",
            "Epoch: 25, Loss: 0.004665242042392492 Updates: 3/9000, Avg Grad: 0.1988793909549713, Threshold: 0.23431634902954102\n",
            "Epoch 24 iteration 0 Loss: 0.586 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.652 | Acc: 62.376% (63/101)\n",
            "Epoch 24 iteration 200 Loss: 0.636 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 25, Loss: 0.005543659441173077 Updates: 4/10000, Avg Grad: 0.04953585937619209, Threshold: 0.2771494686603546\n",
            "Epoch: 25, Loss: 0.0183255597949028 Updates: 4/11000, Avg Grad: 0.10008265823125839, Threshold: 0.2771494686603546\n",
            "Epoch: 25, Loss: 0.006534836255013943 Updates: 4/12000, Avg Grad: 0.18420927226543427, Threshold: 0.2771494686603546\n",
            "Epoch: 25, Loss: 0.004831769969314337 Updates: 5/13000, Avg Grad: 0.02303279936313629, Threshold: 0.3233475983142853\n",
            "Epoch: 25, Loss: 0.009370498359203339 Updates: 5/14000, Avg Grad: 0.1618301421403885, Threshold: 0.3233475983142853\n",
            "Epoch: 25, Loss: 0.0027437033131718636 Updates: 5/15000, Avg Grad: 0.2527511715888977, Threshold: 0.3233475983142853\n",
            "Epoch: 25, Loss: 0.003229436930269003 Updates: 5/16000, Avg Grad: 0.2987454831600189, Threshold: 0.3233475983142853\n",
            "Epoch: 25, Loss: 0.002724096179008484 Updates: 6/17000, Avg Grad: 0.04745146259665489, Threshold: 0.3511289060115814\n",
            "Epoch: 25, Loss: 0.00495117949321866 Updates: 6/18000, Avg Grad: 0.10902240872383118, Threshold: 0.3511289060115814\n",
            "Epoch: 25, Loss: 0.008818767964839935 Updates: 6/19000, Avg Grad: 0.1734103262424469, Threshold: 0.3511289060115814\n",
            "Epoch 24 iteration 0 Loss: 0.166 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.622 | Acc: 63.366% (64/101)\n",
            "Epoch 24 iteration 200 Loss: 0.611 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 25, Loss: 0.0076520382426679134 Updates: 6/20000, Avg Grad: 0.25906702876091003, Threshold: 0.3511289060115814\n",
            "Epoch: 25, Loss: 0.005087013356387615 Updates: 6/21000, Avg Grad: 0.2733474373817444, Threshold: 0.3511289060115814\n",
            "Epoch: 25, Loss: 0.010056572034955025 Updates: 7/22000, Avg Grad: 0.028699874877929688, Threshold: 0.36198264360427856\n",
            "Epoch: 25, Loss: 0.0007312290836125612 Updates: 7/23000, Avg Grad: 0.07745880633592606, Threshold: 0.36198264360427856\n",
            "Epoch: 25, Loss: 0.002661227947100997 Updates: 7/24000, Avg Grad: 0.2418515831232071, Threshold: 0.36198264360427856\n",
            "Epoch: 25, Loss: 0.009750883094966412 Updates: 7/25000, Avg Grad: 0.3048010468482971, Threshold: 0.36198264360427856\n",
            "Epoch: 25, Loss: 0.004416302777826786 Updates: 8/26000, Avg Grad: 0.04775490611791611, Threshold: 0.37503236532211304\n",
            "Epoch: 25, Loss: 0.02148905210196972 Updates: 8/27000, Avg Grad: 0.23349393904209137, Threshold: 0.37503236532211304\n",
            "Epoch: 25, Loss: 0.012533653527498245 Updates: 8/28000, Avg Grad: 0.304156631231308, Threshold: 0.37503236532211304\n",
            "Epoch: 25, Loss: 0.0021205178927630186 Updates: 9/29000, Avg Grad: 0.04038546606898308, Threshold: 0.39682620763778687\n",
            "Epoch 24 iteration 0 Loss: 0.749 | Acc: 0.000% (0/1)\n",
            "Epoch 24 iteration 100 Loss: 0.617 | Acc: 59.406% (60/101)\n",
            "Epoch 24 iteration 200 Loss: 0.640 | Acc: 62.189% (125/201)\n",
            "Test accuracy: 0.6218905448913574\n",
            "Epoch: 25, Loss: 0.015727777034044266 Updates: 9/30000, Avg Grad: 0.16850657761096954, Threshold: 0.39682620763778687\n",
            "Epoch: 25, Loss: 0.007026117295026779 Updates: 9/31000, Avg Grad: 0.16917866468429565, Threshold: 0.39682620763778687\n",
            "Epoch: 25, Loss: 0.0034239375963807106 Updates: 9/32000, Avg Grad: 0.22154271602630615, Threshold: 0.39682620763778687\n",
            "Epoch 25 iteration 0 Loss: 1.404 | Acc: 0.000% (0/1)\n",
            "Epoch 25 iteration 100 Loss: 0.611 | Acc: 65.347% (66/101)\n",
            "Epoch 25 iteration 200 Loss: 0.637 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 26, Loss: 1.2981752157211304 Updates: 0/0, Avg Grad: 0.2716110944747925, Threshold: 0.2987722158432007\n",
            "Epoch: 26, Loss: 0.009143790230154991 Updates: 1/1000, Avg Grad: 0.10059390962123871, Threshold: 0.3352709114551544\n",
            "Epoch: 26, Loss: 0.010118195787072182 Updates: 1/2000, Avg Grad: 0.19926059246063232, Threshold: 0.3352709114551544\n",
            "Epoch: 26, Loss: 0.004459058865904808 Updates: 1/3000, Avg Grad: 0.26687392592430115, Threshold: 0.3352709114551544\n",
            "Epoch: 26, Loss: 0.00694233225658536 Updates: 2/4000, Avg Grad: 0.02218223735690117, Threshold: 0.37180352210998535\n",
            "Epoch: 26, Loss: 0.004329040180891752 Updates: 2/5000, Avg Grad: 0.17310722172260284, Threshold: 0.37180352210998535\n",
            "Epoch: 26, Loss: 0.002682680729776621 Updates: 2/6000, Avg Grad: 0.31131711602211, Threshold: 0.37180352210998535\n",
            "Epoch: 26, Loss: 0.008014348335564137 Updates: 2/7000, Avg Grad: 0.36666661500930786, Threshold: 0.37180352210998535\n",
            "Epoch: 26, Loss: 0.006303753238171339 Updates: 3/8000, Avg Grad: 0.03785651549696922, Threshold: 0.44650453329086304\n",
            "Epoch: 26, Loss: 0.0018563696648925543 Updates: 3/9000, Avg Grad: 0.11446839570999146, Threshold: 0.44650453329086304\n",
            "Epoch 25 iteration 0 Loss: 0.935 | Acc: 0.000% (0/1)\n",
            "Epoch 25 iteration 100 Loss: 0.648 | Acc: 61.386% (62/101)\n",
            "Epoch 25 iteration 200 Loss: 0.637 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 26, Loss: 0.004374520853161812 Updates: 3/10000, Avg Grad: 0.17806637287139893, Threshold: 0.44650453329086304\n",
            "Epoch: 26, Loss: 0.0032024479005485773 Updates: 3/11000, Avg Grad: 0.25330469012260437, Threshold: 0.44650453329086304\n",
            "Epoch: 26, Loss: 0.00610206788405776 Updates: 3/12000, Avg Grad: 0.2984617054462433, Threshold: 0.44650453329086304\n",
            "Epoch: 26, Loss: 0.007842430844902992 Updates: 3/13000, Avg Grad: 0.39417344331741333, Threshold: 0.44650453329086304\n",
            "Epoch: 26, Loss: 0.01129634864628315 Updates: 4/14000, Avg Grad: 0.011700212955474854, Threshold: 0.4997519850730896\n",
            "Epoch: 26, Loss: 0.00524001382291317 Updates: 4/15000, Avg Grad: 0.1239740401506424, Threshold: 0.4997519850730896\n",
            "Epoch: 26, Loss: 0.0032579016406089067 Updates: 4/16000, Avg Grad: 0.2538922429084778, Threshold: 0.4997519850730896\n",
            "Epoch: 26, Loss: 0.007177033927291632 Updates: 4/17000, Avg Grad: 0.36705493927001953, Threshold: 0.4997519850730896\n",
            "Epoch: 26, Loss: 0.0013160656671971083 Updates: 4/18000, Avg Grad: 0.4803370535373688, Threshold: 0.4997519850730896\n",
            "Epoch: 26, Loss: 0.007958421483635902 Updates: 5/19000, Avg Grad: 0.0808696448802948, Threshold: 0.5318611860275269\n",
            "Epoch 25 iteration 0 Loss: 0.677 | Acc: 100.000% (1/1)\n",
            "Epoch 25 iteration 100 Loss: 0.671 | Acc: 65.347% (66/101)\n",
            "Epoch 25 iteration 200 Loss: 0.674 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 26, Loss: 0.0050621903501451015 Updates: 5/20000, Avg Grad: 0.16798630356788635, Threshold: 0.5318611860275269\n",
            "Epoch: 26, Loss: 0.004532491788268089 Updates: 5/21000, Avg Grad: 0.24720075726509094, Threshold: 0.5318611860275269\n",
            "Epoch: 26, Loss: 0.00755623634904623 Updates: 5/22000, Avg Grad: 0.3963991105556488, Threshold: 0.5318611860275269\n",
            "Epoch: 26, Loss: 0.014802400022745132 Updates: 6/23000, Avg Grad: 0.015947438776493073, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.004642751067876816 Updates: 6/24000, Avg Grad: 0.03056035004556179, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.006007419433444738 Updates: 6/25000, Avg Grad: 0.04810820892453194, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.00838015042245388 Updates: 6/26000, Avg Grad: 0.05751706287264824, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.011740312911570072 Updates: 6/27000, Avg Grad: 0.059998806565999985, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.0023273902479559183 Updates: 6/28000, Avg Grad: 0.057285696268081665, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.004706193692982197 Updates: 6/29000, Avg Grad: 0.0897330492734909, Threshold: 0.5534886121749878\n",
            "Epoch 25 iteration 0 Loss: 0.278 | Acc: 100.000% (1/1)\n",
            "Epoch 25 iteration 100 Loss: 0.624 | Acc: 64.356% (65/101)\n",
            "Epoch 25 iteration 200 Loss: 0.616 | Acc: 67.662% (136/201)\n",
            "Test accuracy: 0.676616907119751\n",
            "Epoch: 26, Loss: 0.0037274586502462626 Updates: 6/30000, Avg Grad: 0.07656639814376831, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.0034843632020056248 Updates: 6/31000, Avg Grad: 0.08128330856561661, Threshold: 0.5534886121749878\n",
            "Epoch: 26, Loss: 0.014212297275662422 Updates: 6/32000, Avg Grad: 0.08393707871437073, Threshold: 0.5534886121749878\n",
            "Epoch 26 iteration 0 Loss: 0.753 | Acc: 0.000% (0/1)\n",
            "Epoch 26 iteration 100 Loss: 0.627 | Acc: 65.347% (66/101)\n",
            "Epoch 26 iteration 200 Loss: 0.642 | Acc: 64.677% (130/201)\n",
            "Test accuracy: 0.646766185760498\n",
            "Epoch: 27, Loss: 0.46808767318725586 Updates: 0/0, Avg Grad: 0.1239696592092514, Threshold: 0.13636662065982819\n",
            "Epoch: 27, Loss: 0.003419534070417285 Updates: 0/1000, Avg Grad: 0.0894986093044281, Threshold: 0.13636662065982819\n",
            "Epoch: 27, Loss: 0.0007775240228511393 Updates: 1/2000, Avg Grad: 0.10251843184232712, Threshold: 0.16839273273944855\n",
            "Epoch: 27, Loss: 0.0013636511284857988 Updates: 3/3000, Avg Grad: 0.026470361277461052, Threshold: 0.21595193445682526\n",
            "Epoch: 27, Loss: 0.0039162649773061275 Updates: 4/4000, Avg Grad: 0.07457706332206726, Threshold: 0.27326709032058716\n",
            "Epoch: 27, Loss: 0.006111925933510065 Updates: 5/5000, Avg Grad: 0.014113458804786205, Threshold: 0.3080163598060608\n",
            "Epoch: 27, Loss: 0.0029528134036809206 Updates: 5/6000, Avg Grad: 0.131998673081398, Threshold: 0.3080163598060608\n",
            "Epoch: 27, Loss: 0.008413667790591717 Updates: 5/7000, Avg Grad: 0.289980411529541, Threshold: 0.3080163598060608\n",
            "Epoch: 27, Loss: 0.006249392405152321 Updates: 6/8000, Avg Grad: 0.05039065331220627, Threshold: 0.3593609631061554\n",
            "Epoch: 27, Loss: 0.0020998611580580473 Updates: 6/9000, Avg Grad: 0.16470664739608765, Threshold: 0.3593609631061554\n",
            "Epoch 26 iteration 0 Loss: 0.223 | Acc: 100.000% (1/1)\n",
            "Epoch 26 iteration 100 Loss: 0.674 | Acc: 62.376% (63/101)\n",
            "Epoch 26 iteration 200 Loss: 0.624 | Acc: 64.677% (130/201)\n",
            "Test accuracy: 0.646766185760498\n",
            "Epoch: 27, Loss: 0.011256914585828781 Updates: 6/10000, Avg Grad: 0.30636221170425415, Threshold: 0.3593609631061554\n",
            "Epoch: 27, Loss: 0.008896206505596638 Updates: 7/11000, Avg Grad: 0.07531008869409561, Threshold: 0.4028690755367279\n",
            "Epoch: 27, Loss: 0.0040334961377084255 Updates: 7/12000, Avg Grad: 0.12761904299259186, Threshold: 0.4028690755367279\n",
            "Epoch: 27, Loss: 0.007317773066461086 Updates: 7/13000, Avg Grad: 0.27260974049568176, Threshold: 0.4028690755367279\n",
            "Epoch: 27, Loss: 0.002343089785426855 Updates: 8/14000, Avg Grad: 0.05939117819070816, Threshold: 0.45533332228660583\n",
            "Epoch: 27, Loss: 0.01010467205196619 Updates: 8/15000, Avg Grad: 0.14951401948928833, Threshold: 0.45533332228660583\n",
            "Epoch: 27, Loss: 0.007524409797042608 Updates: 8/16000, Avg Grad: 0.21679149568080902, Threshold: 0.45533332228660583\n",
            "Epoch: 27, Loss: 0.005451772827655077 Updates: 8/17000, Avg Grad: 0.2870374917984009, Threshold: 0.45533332228660583\n",
            "Epoch: 27, Loss: 0.004954592790454626 Updates: 8/18000, Avg Grad: 0.37254124879837036, Threshold: 0.45533332228660583\n",
            "Epoch: 27, Loss: 0.004012092016637325 Updates: 8/19000, Avg Grad: 0.44237327575683594, Threshold: 0.45533332228660583\n",
            "Epoch 26 iteration 0 Loss: 1.191 | Acc: 0.000% (0/1)\n",
            "Epoch 26 iteration 100 Loss: 0.689 | Acc: 62.376% (63/101)\n",
            "Epoch 26 iteration 200 Loss: 0.628 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 27, Loss: 0.005112034734338522 Updates: 9/20000, Avg Grad: 0.057188574224710464, Threshold: 0.4867735803127289\n",
            "Epoch: 27, Loss: 0.0012020434951409698 Updates: 9/21000, Avg Grad: 0.13990448415279388, Threshold: 0.4867735803127289\n",
            "Epoch: 27, Loss: 0.002012294949963689 Updates: 9/22000, Avg Grad: 0.2459261119365692, Threshold: 0.4867735803127289\n",
            "Epoch: 27, Loss: 0.004857218358665705 Updates: 9/23000, Avg Grad: 0.37245339155197144, Threshold: 0.4867735803127289\n",
            "Epoch: 27, Loss: 0.01055461447685957 Updates: 9/24000, Avg Grad: 0.42972704768180847, Threshold: 0.4867735803127289\n",
            "Epoch: 27, Loss: 0.013473971746861935 Updates: 10/25000, Avg Grad: 0.06629922986030579, Threshold: 0.5020272135734558\n",
            "Epoch: 27, Loss: 0.007191232405602932 Updates: 10/26000, Avg Grad: 0.18848057091236115, Threshold: 0.5020272135734558\n",
            "Epoch: 27, Loss: 0.01056727021932602 Updates: 10/27000, Avg Grad: 0.3272237479686737, Threshold: 0.5020272135734558\n",
            "Epoch: 27, Loss: 0.005558651871979237 Updates: 10/28000, Avg Grad: 0.46731650829315186, Threshold: 0.5020272135734558\n",
            "Epoch: 27, Loss: 0.0056320903822779655 Updates: 11/29000, Avg Grad: 0.0844634547829628, Threshold: 0.5079216361045837\n",
            "Epoch 26 iteration 0 Loss: 0.764 | Acc: 0.000% (0/1)\n",
            "Epoch 26 iteration 100 Loss: 0.669 | Acc: 59.406% (60/101)\n",
            "Epoch 26 iteration 200 Loss: 0.648 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 27, Loss: 0.010890591889619827 Updates: 11/30000, Avg Grad: 0.17742913961410522, Threshold: 0.5079216361045837\n",
            "Epoch: 27, Loss: 0.0121738500893116 Updates: 11/31000, Avg Grad: 0.2091960310935974, Threshold: 0.5079216361045837\n",
            "Epoch: 27, Loss: 0.004009888973087072 Updates: 11/32000, Avg Grad: 0.2779500484466553, Threshold: 0.5079216361045837\n",
            "Epoch 27 iteration 0 Loss: 0.330 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.646 | Acc: 66.337% (67/101)\n",
            "Epoch 27 iteration 200 Loss: 0.656 | Acc: 62.189% (125/201)\n",
            "Test accuracy: 0.6218905448913574\n",
            "Epoch: 28, Loss: 0.5615271925926208 Updates: 0/0, Avg Grad: 0.1367744356393814, Threshold: 0.1504518836736679\n",
            "Epoch: 28, Loss: 0.007839605212211609 Updates: 1/1000, Avg Grad: 0.11159839481115341, Threshold: 0.17945143580436707\n",
            "Epoch: 28, Loss: 0.006676454562693834 Updates: 2/2000, Avg Grad: 0.022148411720991135, Threshold: 0.20286519825458527\n",
            "Epoch: 28, Loss: 0.0025866669602692127 Updates: 2/3000, Avg Grad: 0.08738667517900467, Threshold: 0.20286519825458527\n",
            "Epoch: 28, Loss: 0.0018920593429356813 Updates: 3/4000, Avg Grad: 0.01476631686091423, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.0015379437245428562 Updates: 3/5000, Avg Grad: 0.03343614190816879, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.005697808228433132 Updates: 3/6000, Avg Grad: 0.07844847440719604, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.004663967993110418 Updates: 3/7000, Avg Grad: 0.13259023427963257, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.004939399193972349 Updates: 3/8000, Avg Grad: 0.15169258415699005, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.005160288419574499 Updates: 3/9000, Avg Grad: 0.13486002385616302, Threshold: 0.24013115465641022\n",
            "Epoch 27 iteration 0 Loss: 0.412 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.636 | Acc: 60.396% (61/101)\n",
            "Epoch 27 iteration 200 Loss: 0.651 | Acc: 61.194% (123/201)\n",
            "Test accuracy: 0.611940324306488\n",
            "Epoch: 28, Loss: 0.0041534011252224445 Updates: 3/10000, Avg Grad: 0.17090538144111633, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.009249141439795494 Updates: 3/11000, Avg Grad: 0.19799724221229553, Threshold: 0.24013115465641022\n",
            "Epoch: 28, Loss: 0.0009863321902230382 Updates: 4/12000, Avg Grad: 0.24421650171279907, Threshold: 0.26863816380500793\n",
            "Epoch: 28, Loss: 0.0054586040787398815 Updates: 4/13000, Avg Grad: 0.19815559685230255, Threshold: 0.26863816380500793\n",
            "Epoch: 28, Loss: 0.0038093430921435356 Updates: 5/14000, Avg Grad: 0.09727882593870163, Threshold: 0.30550140142440796\n",
            "Epoch: 28, Loss: 0.0038506153505295515 Updates: 6/15000, Avg Grad: 0.3241417407989502, Threshold: 0.34974929690361023\n",
            "Epoch: 28, Loss: 0.0012520856689661741 Updates: 6/16000, Avg Grad: 0.2612057030200958, Threshold: 0.34974929690361023\n",
            "Epoch: 28, Loss: 0.008939503692090511 Updates: 7/17000, Avg Grad: 0.04050266370177269, Threshold: 0.38617023825645447\n",
            "Epoch: 28, Loss: 0.00700406264513731 Updates: 7/18000, Avg Grad: 0.14295361936092377, Threshold: 0.38617023825645447\n",
            "Epoch: 28, Loss: 0.0166102834045887 Updates: 7/19000, Avg Grad: 0.19594615697860718, Threshold: 0.38617023825645447\n",
            "Epoch 27 iteration 0 Loss: 0.342 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.644 | Acc: 65.347% (66/101)\n",
            "Epoch 27 iteration 200 Loss: 0.643 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 28, Loss: 0.013880230486392975 Updates: 7/20000, Avg Grad: 0.2586354911327362, Threshold: 0.38617023825645447\n",
            "Epoch: 28, Loss: 0.0032352758571505547 Updates: 8/21000, Avg Grad: 0.38686251640319824, Threshold: 0.39786049723625183\n",
            "Epoch: 28, Loss: 0.0018135831924155354 Updates: 8/22000, Avg Grad: 0.09193863719701767, Threshold: 0.39786049723625183\n",
            "Epoch: 28, Loss: 0.0034766902681440115 Updates: 8/23000, Avg Grad: 0.24302856624126434, Threshold: 0.39786049723625183\n",
            "Epoch: 28, Loss: 0.011640134267508984 Updates: 8/24000, Avg Grad: 0.34304317831993103, Threshold: 0.39786049723625183\n",
            "Epoch: 28, Loss: 0.01057177409529686 Updates: 9/25000, Avg Grad: 0.14467698335647583, Threshold: 0.41087350249290466\n",
            "Epoch: 28, Loss: 0.0048470948822796345 Updates: 9/26000, Avg Grad: 0.2897137403488159, Threshold: 0.41087350249290466\n",
            "Epoch: 28, Loss: 0.004587586037814617 Updates: 9/27000, Avg Grad: 0.40515831112861633, Threshold: 0.41087350249290466\n",
            "Epoch: 28, Loss: 0.008974172174930573 Updates: 10/28000, Avg Grad: 0.2138139307498932, Threshold: 0.4261581301689148\n",
            "Epoch: 28, Loss: 0.004782772157341242 Updates: 10/29000, Avg Grad: 0.4040735960006714, Threshold: 0.4261581301689148\n",
            "Epoch 27 iteration 0 Loss: 0.428 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.612 | Acc: 62.376% (63/101)\n",
            "Epoch 27 iteration 200 Loss: 0.634 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 28, Loss: 0.0033927701879292727 Updates: 11/30000, Avg Grad: 0.020785776898264885, Threshold: 0.4415290057659149\n",
            "Epoch: 28, Loss: 0.005851175636053085 Updates: 11/31000, Avg Grad: 0.04292793571949005, Threshold: 0.4415290057659149\n",
            "Epoch: 28, Loss: 0.0024255032185465097 Updates: 11/32000, Avg Grad: 0.07143912464380264, Threshold: 0.4415290057659149\n",
            "Epoch 28 iteration 0 Loss: 0.228 | Acc: 100.000% (1/1)\n",
            "Epoch 28 iteration 100 Loss: 0.548 | Acc: 78.218% (79/101)\n",
            "Epoch 28 iteration 200 Loss: 0.584 | Acc: 71.642% (144/201)\n",
            "Test accuracy: 0.7164179086685181\n",
            "Epoch: 29, Loss: 0.23298834264278412 Updates: 0/0, Avg Grad: 0.0792011171579361, Threshold: 0.08712123334407806\n",
            "Epoch: 29, Loss: 0.00905787106603384 Updates: 0/1000, Avg Grad: 0.040743038058280945, Threshold: 0.08712123334407806\n",
            "Epoch: 29, Loss: 0.01811004802584648 Updates: 2/2000, Avg Grad: 0.07320478558540344, Threshold: 0.12595827877521515\n",
            "Epoch: 29, Loss: 0.0148904575034976 Updates: 4/3000, Avg Grad: 0.11484570801258087, Threshold: 0.18229398131370544\n",
            "Epoch: 29, Loss: 0.005023142788559198 Updates: 5/4000, Avg Grad: 0.17029163241386414, Threshold: 0.21072179079055786\n",
            "Epoch: 29, Loss: 0.007367799058556557 Updates: 6/5000, Avg Grad: 0.16715683043003082, Threshold: 0.2353551983833313\n",
            "Epoch: 29, Loss: 0.007876530289649963 Updates: 7/6000, Avg Grad: 0.02659241482615471, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.004251564387232065 Updates: 7/7000, Avg Grad: 0.06975378841161728, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.0019115384202450514 Updates: 7/8000, Avg Grad: 0.06738793104887009, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.01527191512286663 Updates: 7/9000, Avg Grad: 0.1064206212759018, Threshold: 0.2606407105922699\n",
            "Epoch 28 iteration 0 Loss: 1.044 | Acc: 0.000% (0/1)\n",
            "Epoch 28 iteration 100 Loss: 0.643 | Acc: 67.327% (68/101)\n",
            "Epoch 28 iteration 200 Loss: 0.653 | Acc: 63.184% (127/201)\n",
            "Test accuracy: 0.6318408250808716\n",
            "Epoch: 29, Loss: 0.005097818560898304 Updates: 7/10000, Avg Grad: 0.13542738556861877, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.0017836616607382894 Updates: 7/11000, Avg Grad: 0.16689227521419525, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.005945987068116665 Updates: 7/12000, Avg Grad: 0.14482229948043823, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.005207093898206949 Updates: 7/13000, Avg Grad: 0.17659100890159607, Threshold: 0.2606407105922699\n",
            "Epoch: 29, Loss: 0.0047445292584598064 Updates: 8/14000, Avg Grad: 0.018840566277503967, Threshold: 0.30763113498687744\n",
            "Epoch: 29, Loss: 0.004793856758624315 Updates: 8/15000, Avg Grad: 0.07418569922447205, Threshold: 0.30763113498687744\n",
            "Epoch: 29, Loss: 0.003301034914329648 Updates: 8/16000, Avg Grad: 0.1844267100095749, Threshold: 0.30763113498687744\n",
            "Epoch: 29, Loss: 0.0011314155999571085 Updates: 8/17000, Avg Grad: 0.1772046834230423, Threshold: 0.30763113498687744\n",
            "Epoch: 29, Loss: 0.0062505085952579975 Updates: 8/18000, Avg Grad: 0.2315703183412552, Threshold: 0.30763113498687744\n",
            "Epoch: 29, Loss: 0.004784294404089451 Updates: 9/19000, Avg Grad: 0.012650389224290848, Threshold: 0.33007004857063293\n",
            "Epoch 28 iteration 0 Loss: 0.834 | Acc: 0.000% (0/1)\n",
            "Epoch 28 iteration 100 Loss: 0.618 | Acc: 65.347% (66/101)\n",
            "Epoch 28 iteration 200 Loss: 0.602 | Acc: 68.657% (138/201)\n",
            "Test accuracy: 0.6865671873092651\n",
            "Epoch: 29, Loss: 0.007192863151431084 Updates: 9/20000, Avg Grad: 0.06387834250926971, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.005586047191172838 Updates: 9/21000, Avg Grad: 0.06584671139717102, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.0061399070546031 Updates: 9/22000, Avg Grad: 0.09204506129026413, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.004020271822810173 Updates: 9/23000, Avg Grad: 0.1126755028963089, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.0032058111391961575 Updates: 9/24000, Avg Grad: 0.15683339536190033, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.004441669210791588 Updates: 9/25000, Avg Grad: 0.12065199017524719, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.0018398973625153303 Updates: 9/26000, Avg Grad: 0.13791954517364502, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.012558814138174057 Updates: 9/27000, Avg Grad: 0.1520443856716156, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.009493773803114891 Updates: 9/28000, Avg Grad: 0.16436952352523804, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.005471385084092617 Updates: 9/29000, Avg Grad: 0.1310010850429535, Threshold: 0.33007004857063293\n",
            "Epoch 28 iteration 0 Loss: 0.204 | Acc: 100.000% (1/1)\n",
            "Epoch 28 iteration 100 Loss: 0.590 | Acc: 72.277% (73/101)\n",
            "Epoch 28 iteration 200 Loss: 0.610 | Acc: 67.164% (135/201)\n",
            "Test accuracy: 0.6716417670249939\n",
            "Epoch: 29, Loss: 0.008197596296668053 Updates: 9/30000, Avg Grad: 0.16706182062625885, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.003667013719677925 Updates: 9/31000, Avg Grad: 0.18903635442256927, Threshold: 0.33007004857063293\n",
            "Epoch: 29, Loss: 0.01536209974437952 Updates: 9/32000, Avg Grad: 0.15420471131801605, Threshold: 0.33007004857063293\n",
            "Epoch 29 iteration 0 Loss: 1.202 | Acc: 0.000% (0/1)\n",
            "Epoch 29 iteration 100 Loss: 0.573 | Acc: 76.238% (77/101)\n",
            "Epoch 29 iteration 200 Loss: 0.574 | Acc: 73.134% (147/201)\n",
            "Test accuracy: 0.7313432693481445\n",
            "Epoch: 30, Loss: 0.30978456139564514 Updates: 0/0, Avg Grad: 0.09112559258937836, Threshold: 0.10023815184831619\n",
            "Epoch: 30, Loss: 0.01001365389674902 Updates: 0/1000, Avg Grad: 0.09279561787843704, Threshold: 0.10023815184831619\n",
            "Epoch: 30, Loss: 0.0034224085975438356 Updates: 0/2000, Avg Grad: 0.07016368210315704, Threshold: 0.10023815184831619\n",
            "Epoch: 30, Loss: 0.009743476286530495 Updates: 0/3000, Avg Grad: 0.08855830132961273, Threshold: 0.10023815184831619\n",
            "Epoch: 30, Loss: 0.008771037682890892 Updates: 1/4000, Avg Grad: 0.06348662078380585, Threshold: 0.11069341748952866\n",
            "Epoch: 30, Loss: 0.0014921127585694194 Updates: 3/5000, Avg Grad: 0.0409962423145771, Threshold: 0.14138443768024445\n",
            "Epoch: 30, Loss: 0.0050888871774077415 Updates: 4/6000, Avg Grad: 0.06896073371171951, Threshold: 0.15605822205543518\n",
            "Epoch: 30, Loss: 0.0021857484243810177 Updates: 5/7000, Avg Grad: 0.017231464385986328, Threshold: 0.17445476353168488\n",
            "Epoch: 30, Loss: 0.007803397718816996 Updates: 5/8000, Avg Grad: 0.04310661926865578, Threshold: 0.17445476353168488\n",
            "Epoch: 30, Loss: 0.0028408162761479616 Updates: 5/9000, Avg Grad: 0.0642642229795456, Threshold: 0.17445476353168488\n",
            "Epoch 29 iteration 0 Loss: 0.651 | Acc: 100.000% (1/1)\n",
            "Epoch 29 iteration 100 Loss: 0.606 | Acc: 68.317% (69/101)\n",
            "Epoch 29 iteration 200 Loss: 0.616 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 30, Loss: 0.0038503396790474653 Updates: 5/10000, Avg Grad: 0.1049119159579277, Threshold: 0.17445476353168488\n",
            "Epoch: 30, Loss: 0.0032352679409086704 Updates: 5/11000, Avg Grad: 0.12818865478038788, Threshold: 0.17445476353168488\n",
            "Epoch: 30, Loss: 0.004156355746090412 Updates: 5/12000, Avg Grad: 0.1350213587284088, Threshold: 0.17445476353168488\n",
            "Epoch: 30, Loss: 0.005764365196228027 Updates: 5/13000, Avg Grad: 0.15666404366493225, Threshold: 0.17445476353168488\n",
            "Epoch: 30, Loss: 0.013253232464194298 Updates: 6/14000, Avg Grad: 0.021908046677708626, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.004888186231255531 Updates: 6/15000, Avg Grad: 0.03963681682944298, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.009223471395671368 Updates: 6/16000, Avg Grad: 0.06140600144863129, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.006697105243802071 Updates: 6/17000, Avg Grad: 0.07226202636957169, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.008274543099105358 Updates: 6/18000, Avg Grad: 0.0643451139330864, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.008333680219948292 Updates: 6/19000, Avg Grad: 0.08238465338945389, Threshold: 0.19211813807487488\n",
            "Epoch 29 iteration 0 Loss: 0.147 | Acc: 100.000% (1/1)\n",
            "Epoch 29 iteration 100 Loss: 0.628 | Acc: 63.366% (64/101)\n",
            "Epoch 29 iteration 200 Loss: 0.616 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 30, Loss: 0.007586349733173847 Updates: 6/20000, Avg Grad: 0.10415859520435333, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.015352986752986908 Updates: 6/21000, Avg Grad: 0.09011431783437729, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.00402689166367054 Updates: 6/22000, Avg Grad: 0.1382908970117569, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.002977789146825671 Updates: 6/23000, Avg Grad: 0.16753067076206207, Threshold: 0.19211813807487488\n",
            "Epoch: 30, Loss: 0.003669304307550192 Updates: 7/24000, Avg Grad: 0.027618790045380592, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.0028462749905884266 Updates: 7/25000, Avg Grad: 0.10730766505002975, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.005278087221086025 Updates: 7/26000, Avg Grad: 0.08801411837339401, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.004495199304074049 Updates: 7/27000, Avg Grad: 0.1042417362332344, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.003659054869785905 Updates: 7/28000, Avg Grad: 0.11006379127502441, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.006557939108461142 Updates: 7/29000, Avg Grad: 0.1289394348859787, Threshold: 0.20449988543987274\n",
            "Epoch 29 iteration 0 Loss: 0.247 | Acc: 100.000% (1/1)\n",
            "Epoch 29 iteration 100 Loss: 0.638 | Acc: 69.307% (70/101)\n",
            "Epoch 29 iteration 200 Loss: 0.658 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 30, Loss: 0.005649117287248373 Updates: 7/30000, Avg Grad: 0.1502814143896103, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.004899625666439533 Updates: 7/31000, Avg Grad: 0.1837126612663269, Threshold: 0.20449988543987274\n",
            "Epoch: 30, Loss: 0.006935022305697203 Updates: 7/32000, Avg Grad: 0.18149524927139282, Threshold: 0.20449988543987274\n",
            "Epoch 30 iteration 0 Loss: 0.634 | Acc: 100.000% (1/1)\n",
            "Epoch 30 iteration 100 Loss: 0.653 | Acc: 65.347% (66/101)\n",
            "Epoch 30 iteration 200 Loss: 0.643 | Acc: 65.174% (131/201)\n",
            "Test accuracy: 0.6517412662506104\n",
            "Epoch: 31, Loss: 0.5889157056808472 Updates: 0/0, Avg Grad: 0.14100168645381927, Threshold: 0.15510186553001404\n",
            "Epoch: 31, Loss: 0.00518451351672411 Updates: 1/1000, Avg Grad: 0.16259321570396423, Threshold: 0.17885254323482513\n",
            "Epoch: 31, Loss: 0.010598554275929928 Updates: 1/2000, Avg Grad: 0.13653022050857544, Threshold: 0.17885254323482513\n",
            "Epoch: 31, Loss: 0.0050160265527665615 Updates: 2/3000, Avg Grad: 0.18836690485477448, Threshold: 0.1973881721496582\n",
            "Epoch: 31, Loss: 0.0030354473274201155 Updates: 3/4000, Avg Grad: 0.133528932929039, Threshold: 0.2321077585220337\n",
            "Epoch: 31, Loss: 0.00920889526605606 Updates: 4/5000, Avg Grad: 0.10275980085134506, Threshold: 0.265028178691864\n",
            "Epoch: 31, Loss: 0.0034684892743825912 Updates: 4/6000, Avg Grad: 0.19422954320907593, Threshold: 0.265028178691864\n",
            "Epoch: 31, Loss: 0.010856624692678452 Updates: 5/7000, Avg Grad: 0.27196377515792847, Threshold: 0.2991601526737213\n",
            "Epoch: 31, Loss: 0.002915001707151532 Updates: 5/8000, Avg Grad: 0.1751307249069214, Threshold: 0.2991601526737213\n",
            "Epoch: 31, Loss: 0.004327218979597092 Updates: 5/9000, Avg Grad: 0.27190354466438293, Threshold: 0.2991601526737213\n",
            "Epoch 30 iteration 0 Loss: 0.281 | Acc: 100.000% (1/1)\n",
            "Epoch 30 iteration 100 Loss: 0.696 | Acc: 56.436% (57/101)\n",
            "Epoch 30 iteration 200 Loss: 0.680 | Acc: 57.711% (116/201)\n",
            "Test accuracy: 0.5771144032478333\n",
            "Epoch: 31, Loss: 0.0018848949111998081 Updates: 6/10000, Avg Grad: 0.11842665821313858, Threshold: 0.3379466235637665\n",
            "Epoch: 31, Loss: 0.011116856709122658 Updates: 6/11000, Avg Grad: 0.32931891083717346, Threshold: 0.3379466235637665\n",
            "Epoch: 31, Loss: 0.00056069390848279 Updates: 7/12000, Avg Grad: 0.09671986848115921, Threshold: 0.3852173686027527\n",
            "Epoch: 31, Loss: 0.005676606670022011 Updates: 7/13000, Avg Grad: 0.14115387201309204, Threshold: 0.3852173686027527\n",
            "Epoch: 31, Loss: 0.0025362710002809763 Updates: 7/14000, Avg Grad: 0.29521188139915466, Threshold: 0.3852173686027527\n",
            "Epoch: 31, Loss: 0.011063038371503353 Updates: 8/15000, Avg Grad: 0.02309730462729931, Threshold: 0.4205162823200226\n",
            "Epoch: 31, Loss: 0.0028165581170469522 Updates: 8/16000, Avg Grad: 0.08229600638151169, Threshold: 0.4205162823200226\n",
            "Epoch: 31, Loss: 0.009019547142088413 Updates: 8/17000, Avg Grad: 0.14676739275455475, Threshold: 0.4205162823200226\n",
            "Epoch: 31, Loss: 0.0035670935176312923 Updates: 8/18000, Avg Grad: 0.3077009916305542, Threshold: 0.4205162823200226\n",
            "Epoch: 31, Loss: 0.004477693699300289 Updates: 9/19000, Avg Grad: 0.4388973116874695, Threshold: 0.4564392566680908\n",
            "Epoch 30 iteration 0 Loss: 1.452 | Acc: 0.000% (0/1)\n",
            "Epoch 30 iteration 100 Loss: 0.613 | Acc: 67.327% (68/101)\n",
            "Epoch 30 iteration 200 Loss: 0.621 | Acc: 67.662% (136/201)\n",
            "Test accuracy: 0.676616907119751\n",
            "Epoch: 31, Loss: 0.012651619501411915 Updates: 9/20000, Avg Grad: 0.11434770375490189, Threshold: 0.4564392566680908\n",
            "Epoch: 31, Loss: 0.0013962932862341404 Updates: 9/21000, Avg Grad: 0.22092820703983307, Threshold: 0.4564392566680908\n",
            "Epoch: 31, Loss: 0.001805635984055698 Updates: 9/22000, Avg Grad: 0.3387258052825928, Threshold: 0.4564392566680908\n",
            "Epoch: 31, Loss: 0.003374763997271657 Updates: 10/23000, Avg Grad: 0.037407007068395615, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.00396506255492568 Updates: 10/24000, Avg Grad: 0.07672759145498276, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.019525088369846344 Updates: 10/25000, Avg Grad: 0.1375962346792221, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.008924055844545364 Updates: 10/26000, Avg Grad: 0.16567756235599518, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.00597340427339077 Updates: 10/27000, Avg Grad: 0.16355234384536743, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.0011046858271583915 Updates: 10/28000, Avg Grad: 0.21686160564422607, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.0022324901074171066 Updates: 10/29000, Avg Grad: 0.23891344666481018, Threshold: 0.4760597348213196\n",
            "Epoch 30 iteration 0 Loss: 1.545 | Acc: 0.000% (0/1)\n",
            "Epoch 30 iteration 100 Loss: 0.657 | Acc: 60.396% (61/101)\n",
            "Epoch 30 iteration 200 Loss: 0.651 | Acc: 59.204% (119/201)\n",
            "Test accuracy: 0.5920398235321045\n",
            "Epoch: 31, Loss: 0.006594695616513491 Updates: 10/30000, Avg Grad: 0.277858704328537, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.01684212125837803 Updates: 10/31000, Avg Grad: 0.3258245587348938, Threshold: 0.4760597348213196\n",
            "Epoch: 31, Loss: 0.005151236429810524 Updates: 10/32000, Avg Grad: 0.38057371973991394, Threshold: 0.4760597348213196\n",
            "Epoch 31 iteration 0 Loss: 0.251 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.587 | Acc: 66.337% (67/101)\n",
            "Epoch 31 iteration 200 Loss: 0.621 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 32, Loss: 0.7773378491401672 Updates: 0/0, Avg Grad: 0.17133264243602753, Threshold: 0.1884659081697464\n",
            "Epoch: 32, Loss: 0.003633761778473854 Updates: 1/1000, Avg Grad: 0.1241394579410553, Threshold: 0.21162745356559753\n",
            "Epoch: 32, Loss: 0.009213803336024284 Updates: 2/2000, Avg Grad: 0.024995112791657448, Threshold: 0.2330995351076126\n",
            "Epoch: 32, Loss: 0.0061174375005066395 Updates: 2/3000, Avg Grad: 0.08162523061037064, Threshold: 0.2330995351076126\n",
            "Epoch: 32, Loss: 0.004609754309058189 Updates: 2/4000, Avg Grad: 0.18165931105613708, Threshold: 0.2330995351076126\n",
            "Epoch: 32, Loss: 0.010216645896434784 Updates: 3/5000, Avg Grad: 0.08396066725254059, Threshold: 0.2651769816875458\n",
            "Epoch: 32, Loss: 0.011359434574842453 Updates: 3/6000, Avg Grad: 0.25669756531715393, Threshold: 0.2651769816875458\n",
            "Epoch: 32, Loss: 0.00719761848449707 Updates: 4/7000, Avg Grad: 0.1944984495639801, Threshold: 0.30136775970458984\n",
            "Epoch: 32, Loss: 0.004364306107163429 Updates: 5/8000, Avg Grad: 0.022305887192487717, Threshold: 0.3388347327709198\n",
            "Epoch: 32, Loss: 0.0014879952650517225 Updates: 5/9000, Avg Grad: 0.08409282565116882, Threshold: 0.3388347327709198\n",
            "Epoch 31 iteration 0 Loss: 0.515 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.587 | Acc: 69.307% (70/101)\n",
            "Epoch 31 iteration 200 Loss: 0.604 | Acc: 69.154% (139/201)\n",
            "Test accuracy: 0.6915422677993774\n",
            "Epoch: 32, Loss: 0.015333329327404499 Updates: 5/10000, Avg Grad: 0.2168433964252472, Threshold: 0.3388347327709198\n",
            "Epoch: 32, Loss: 0.0022222178522497416 Updates: 5/11000, Avg Grad: 0.3021549582481384, Threshold: 0.3388347327709198\n",
            "Epoch: 32, Loss: 0.007632313761860132 Updates: 6/12000, Avg Grad: 0.08959991484880447, Threshold: 0.37734872102737427\n",
            "Epoch: 32, Loss: 0.015452644787728786 Updates: 6/13000, Avg Grad: 0.15275050699710846, Threshold: 0.37734872102737427\n",
            "Epoch: 32, Loss: 0.0037255508359521627 Updates: 6/14000, Avg Grad: 0.30609413981437683, Threshold: 0.37734872102737427\n",
            "Epoch: 32, Loss: 0.01146996021270752 Updates: 6/15000, Avg Grad: 0.3745611310005188, Threshold: 0.37734872102737427\n",
            "Epoch: 32, Loss: 0.010229173116385937 Updates: 6/16000, Avg Grad: 0.3639373481273651, Threshold: 0.37734872102737427\n",
            "Epoch: 32, Loss: 0.011704975739121437 Updates: 7/17000, Avg Grad: 0.08940961956977844, Threshold: 0.41307544708251953\n",
            "Epoch: 32, Loss: 0.006746345199644566 Updates: 7/18000, Avg Grad: 0.24707747995853424, Threshold: 0.41307544708251953\n",
            "Epoch: 32, Loss: 0.010097827762365341 Updates: 7/19000, Avg Grad: 0.3399719297885895, Threshold: 0.41307544708251953\n",
            "Epoch 31 iteration 0 Loss: 0.651 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.680 | Acc: 57.426% (58/101)\n",
            "Epoch 31 iteration 200 Loss: 0.630 | Acc: 64.677% (130/201)\n",
            "Test accuracy: 0.646766185760498\n",
            "Epoch: 32, Loss: 0.004361302591860294 Updates: 8/20000, Avg Grad: 0.010641975328326225, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.004193244967609644 Updates: 8/21000, Avg Grad: 0.04665575921535492, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.006492012180387974 Updates: 8/22000, Avg Grad: 0.08442854136228561, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.003911739215254784 Updates: 8/23000, Avg Grad: 0.09199180454015732, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.0028239735402166843 Updates: 8/24000, Avg Grad: 0.10112091898918152, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.0037032165564596653 Updates: 8/25000, Avg Grad: 0.11255127191543579, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.008735248818993568 Updates: 8/26000, Avg Grad: 0.09201090037822723, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.005678059998899698 Updates: 8/27000, Avg Grad: 0.10736127942800522, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.007907170802354813 Updates: 8/28000, Avg Grad: 0.09033617377281189, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.006518052890896797 Updates: 8/29000, Avg Grad: 0.1572204828262329, Threshold: 0.4295980930328369\n",
            "Epoch 31 iteration 0 Loss: 0.574 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.603 | Acc: 68.317% (69/101)\n",
            "Epoch 31 iteration 200 Loss: 0.652 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 32, Loss: 0.0007583844708278775 Updates: 8/30000, Avg Grad: 0.1381351798772812, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.0019220727263018489 Updates: 8/31000, Avg Grad: 0.19900183379650116, Threshold: 0.4295980930328369\n",
            "Epoch: 32, Loss: 0.005807752721011639 Updates: 8/32000, Avg Grad: 0.18862396478652954, Threshold: 0.4295980930328369\n",
            "Epoch 32 iteration 0 Loss: 0.994 | Acc: 0.000% (0/1)\n",
            "Epoch 32 iteration 100 Loss: 0.619 | Acc: 67.327% (68/101)\n",
            "Epoch 32 iteration 200 Loss: 0.654 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 33, Loss: 0.2727135121822357 Updates: 0/0, Avg Grad: 0.08325254172086716, Threshold: 0.09157779812812805\n",
            "Epoch: 33, Loss: 0.002467687940225005 Updates: 0/1000, Avg Grad: 0.033960942178964615, Threshold: 0.09157779812812805\n",
            "Epoch: 33, Loss: 0.0011356356553733349 Updates: 0/2000, Avg Grad: 0.0464925654232502, Threshold: 0.09157779812812805\n",
            "Epoch: 33, Loss: 0.001679976237937808 Updates: 1/3000, Avg Grad: 0.09707528352737427, Threshold: 0.10678281635046005\n",
            "Epoch: 33, Loss: 0.004966291133314371 Updates: 1/4000, Avg Grad: 0.10231754928827286, Threshold: 0.10678281635046005\n",
            "Epoch: 33, Loss: 0.0028218934312462807 Updates: 3/5000, Avg Grad: 0.03336905315518379, Threshold: 0.1491764783859253\n",
            "Epoch: 33, Loss: 0.0018976683495566249 Updates: 3/6000, Avg Grad: 0.10140831023454666, Threshold: 0.1491764783859253\n",
            "Epoch: 33, Loss: 0.004177315626293421 Updates: 4/7000, Avg Grad: 0.15917497873306274, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.002885351190343499 Updates: 4/8000, Avg Grad: 0.06559313833713531, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.003965963143855333 Updates: 4/9000, Avg Grad: 0.06404528766870499, Threshold: 0.17509247362613678\n",
            "Epoch 32 iteration 0 Loss: 0.489 | Acc: 100.000% (1/1)\n",
            "Epoch 32 iteration 100 Loss: 0.651 | Acc: 65.347% (66/101)\n",
            "Epoch 32 iteration 200 Loss: 0.646 | Acc: 65.174% (131/201)\n",
            "Test accuracy: 0.6517412662506104\n",
            "Epoch: 33, Loss: 0.0017238175496459007 Updates: 4/10000, Avg Grad: 0.060771599411964417, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.0018522558966651559 Updates: 4/11000, Avg Grad: 0.06985776126384735, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.00227406807243824 Updates: 4/12000, Avg Grad: 0.09644754230976105, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.01678411290049553 Updates: 4/13000, Avg Grad: 0.09171435236930847, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.007866812869906425 Updates: 4/14000, Avg Grad: 0.11875814199447632, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.005768596660345793 Updates: 4/15000, Avg Grad: 0.15032827854156494, Threshold: 0.17509247362613678\n",
            "Epoch: 33, Loss: 0.007649137172847986 Updates: 5/16000, Avg Grad: 0.05895531177520752, Threshold: 0.20109660923480988\n",
            "Epoch: 33, Loss: 0.0029698219150304794 Updates: 5/17000, Avg Grad: 0.18808087706565857, Threshold: 0.20109660923480988\n",
            "Epoch: 33, Loss: 0.006718635093420744 Updates: 7/18000, Avg Grad: 0.014325197786092758, Threshold: 0.2566288411617279\n",
            "Epoch: 33, Loss: 0.0018697847845032811 Updates: 7/19000, Avg Grad: 0.22672952711582184, Threshold: 0.2566288411617279\n",
            "Epoch 32 iteration 0 Loss: 0.272 | Acc: 100.000% (1/1)\n",
            "Epoch 32 iteration 100 Loss: 0.611 | Acc: 69.307% (70/101)\n",
            "Epoch 32 iteration 200 Loss: 0.607 | Acc: 68.159% (137/201)\n",
            "Test accuracy: 0.6815920472145081\n",
            "Epoch: 33, Loss: 0.007456232327967882 Updates: 8/20000, Avg Grad: 0.07264173775911331, Threshold: 0.2745911777019501\n",
            "Epoch: 33, Loss: 0.003435155376791954 Updates: 8/21000, Avg Grad: 0.09196287393569946, Threshold: 0.2745911777019501\n",
            "Epoch: 33, Loss: 0.0013499537017196417 Updates: 8/22000, Avg Grad: 0.08824598044157028, Threshold: 0.2745911777019501\n",
            "Epoch: 33, Loss: 0.005476138088852167 Updates: 8/23000, Avg Grad: 0.0944744199514389, Threshold: 0.2745911777019501\n",
            "Epoch: 33, Loss: 0.006796854082494974 Updates: 8/24000, Avg Grad: 0.1489555984735489, Threshold: 0.2745911777019501\n",
            "Epoch: 33, Loss: 0.011677336879074574 Updates: 8/25000, Avg Grad: 0.16289842128753662, Threshold: 0.2745911777019501\n",
            "Epoch: 33, Loss: 0.0031685661524534225 Updates: 9/26000, Avg Grad: 0.28276336193084717, Threshold: 0.28619328141212463\n",
            "Epoch: 33, Loss: 0.006540853530168533 Updates: 9/27000, Avg Grad: 0.10249514877796173, Threshold: 0.28619328141212463\n",
            "Epoch: 33, Loss: 0.004685739986598492 Updates: 9/28000, Avg Grad: 0.22282253205776215, Threshold: 0.28619328141212463\n",
            "Epoch: 33, Loss: 0.007711742073297501 Updates: 10/29000, Avg Grad: 0.02783563919365406, Threshold: 0.2995304465293884\n",
            "Epoch 32 iteration 0 Loss: 0.735 | Acc: 0.000% (0/1)\n",
            "Epoch 32 iteration 100 Loss: 0.623 | Acc: 66.337% (67/101)\n",
            "Epoch 32 iteration 200 Loss: 0.637 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 33, Loss: 0.0047417315654456615 Updates: 11/30000, Avg Grad: 0.3156420588493347, Threshold: 0.31757909059524536\n",
            "Epoch: 33, Loss: 0.002511771861463785 Updates: 11/31000, Avg Grad: 0.16208893060684204, Threshold: 0.31757909059524536\n",
            "Epoch: 33, Loss: 0.0013106641126796603 Updates: 11/32000, Avg Grad: 0.2799365818500519, Threshold: 0.31757909059524536\n",
            "Epoch 33 iteration 0 Loss: 0.539 | Acc: 100.000% (1/1)\n",
            "Epoch 33 iteration 100 Loss: 0.593 | Acc: 67.327% (68/101)\n",
            "Epoch 33 iteration 200 Loss: 0.605 | Acc: 68.159% (137/201)\n",
            "Test accuracy: 0.6815920472145081\n",
            "Epoch: 34, Loss: 0.421251505613327 Updates: 0/0, Avg Grad: 0.1094081699848175, Threshold: 0.1203489899635315\n",
            "Epoch: 34, Loss: 0.008429164066910744 Updates: 1/1000, Avg Grad: 0.07113540172576904, Threshold: 0.14547456800937653\n",
            "Epoch: 34, Loss: 0.0030742448288947344 Updates: 2/2000, Avg Grad: 0.02253679744899273, Threshold: 0.20315144956111908\n",
            "Epoch: 34, Loss: 0.002617894671857357 Updates: 2/3000, Avg Grad: 0.1267981380224228, Threshold: 0.20315144956111908\n",
            "Epoch: 34, Loss: 0.01959776133298874 Updates: 3/4000, Avg Grad: 0.010691008530557156, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.0037855517584830523 Updates: 3/5000, Avg Grad: 0.054019685834646225, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.011617046780884266 Updates: 3/6000, Avg Grad: 0.05979328975081444, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.0022223887499421835 Updates: 3/7000, Avg Grad: 0.10500792413949966, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.0032487798016518354 Updates: 3/8000, Avg Grad: 0.13345064222812653, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.006282291375100613 Updates: 3/9000, Avg Grad: 0.11039736866950989, Threshold: 0.22739706933498383\n",
            "Epoch 33 iteration 0 Loss: 0.769 | Acc: 0.000% (0/1)\n",
            "Epoch 33 iteration 100 Loss: 0.680 | Acc: 58.416% (59/101)\n",
            "Epoch 33 iteration 200 Loss: 0.670 | Acc: 59.204% (119/201)\n",
            "Test accuracy: 0.5920398235321045\n",
            "Epoch: 34, Loss: 0.00202456908300519 Updates: 3/10000, Avg Grad: 0.17478936910629272, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.021364588290452957 Updates: 3/11000, Avg Grad: 0.17680180072784424, Threshold: 0.22739706933498383\n",
            "Epoch: 34, Loss: 0.014615662395954132 Updates: 4/12000, Avg Grad: 0.0270040612667799, Threshold: 0.2533344030380249\n",
            "Epoch: 34, Loss: 0.00411135284230113 Updates: 4/13000, Avg Grad: 0.06736396998167038, Threshold: 0.2533344030380249\n",
            "Epoch: 34, Loss: 0.0056044007651507854 Updates: 4/14000, Avg Grad: 0.15602535009384155, Threshold: 0.2533344030380249\n",
            "Epoch: 34, Loss: 0.018049780279397964 Updates: 4/15000, Avg Grad: 0.2395690679550171, Threshold: 0.2533344030380249\n",
            "Epoch: 34, Loss: 0.0031215050257742405 Updates: 5/16000, Avg Grad: 0.040321942418813705, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.021631410345435143 Updates: 5/17000, Avg Grad: 0.073928102850914, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.007216943893581629 Updates: 5/18000, Avg Grad: 0.11658547073602676, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.0010396441211923957 Updates: 5/19000, Avg Grad: 0.14338898658752441, Threshold: 0.27839216589927673\n",
            "Epoch 33 iteration 0 Loss: 0.476 | Acc: 100.000% (1/1)\n",
            "Epoch 33 iteration 100 Loss: 0.680 | Acc: 63.366% (64/101)\n",
            "Epoch 33 iteration 200 Loss: 0.673 | Acc: 62.189% (125/201)\n",
            "Test accuracy: 0.6218905448913574\n",
            "Epoch: 34, Loss: 0.004169848747551441 Updates: 5/20000, Avg Grad: 0.1526191085577011, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.0032667776104062796 Updates: 5/21000, Avg Grad: 0.20082469284534454, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.0023902759421616793 Updates: 5/22000, Avg Grad: 0.2062264382839203, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.0049990336410701275 Updates: 5/23000, Avg Grad: 0.2765946090221405, Threshold: 0.27839216589927673\n",
            "Epoch: 34, Loss: 0.006053320597857237 Updates: 6/24000, Avg Grad: 0.03294921666383743, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.009912468492984772 Updates: 6/25000, Avg Grad: 0.06221761554479599, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.0036227679811418056 Updates: 6/26000, Avg Grad: 0.07771111279726028, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.003389681689441204 Updates: 6/27000, Avg Grad: 0.11367979645729065, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.00453744875267148 Updates: 6/28000, Avg Grad: 0.1162908524274826, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.004371989984065294 Updates: 6/29000, Avg Grad: 0.15815439820289612, Threshold: 0.28500545024871826\n",
            "Epoch 33 iteration 0 Loss: 0.381 | Acc: 100.000% (1/1)\n",
            "Epoch 33 iteration 100 Loss: 0.531 | Acc: 75.248% (76/101)\n",
            "Epoch 33 iteration 200 Loss: 0.580 | Acc: 71.144% (143/201)\n",
            "Test accuracy: 0.711442768573761\n",
            "Epoch: 34, Loss: 0.003612068248912692 Updates: 6/30000, Avg Grad: 0.20864658057689667, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.0009808598551899195 Updates: 6/31000, Avg Grad: 0.2656445801258087, Threshold: 0.28500545024871826\n",
            "Epoch: 34, Loss: 0.0010939354542642832 Updates: 7/32000, Avg Grad: 0.039445437490940094, Threshold: 0.29365527629852295\n",
            "Epoch 34 iteration 0 Loss: 0.358 | Acc: 100.000% (1/1)\n",
            "Epoch 34 iteration 100 Loss: 0.700 | Acc: 60.396% (61/101)\n",
            "Epoch 34 iteration 200 Loss: 0.636 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 35, Loss: 0.3177260756492615 Updates: 0/0, Avg Grad: 0.09337661415338516, Threshold: 0.10271427780389786\n",
            "Epoch: 35, Loss: 0.0021369275636970997 Updates: 1/1000, Avg Grad: 0.024759802967309952, Threshold: 0.12137646228075027\n",
            "Epoch: 35, Loss: 0.0022152892779558897 Updates: 3/2000, Avg Grad: 0.007998463697731495, Threshold: 0.15140022337436676\n",
            "Epoch: 35, Loss: 0.007687839213758707 Updates: 4/3000, Avg Grad: 0.05480394512414932, Threshold: 0.16806933283805847\n",
            "Epoch: 35, Loss: 0.005185001064091921 Updates: 4/4000, Avg Grad: 0.1514277160167694, Threshold: 0.16806933283805847\n",
            "Epoch: 35, Loss: 0.012286031618714333 Updates: 5/5000, Avg Grad: 0.09977467358112335, Threshold: 0.1870952546596527\n",
            "Epoch: 35, Loss: 0.0065000588074326515 Updates: 5/6000, Avg Grad: 0.1427481770515442, Threshold: 0.1870952546596527\n",
            "Epoch: 35, Loss: 0.0025549319107085466 Updates: 6/7000, Avg Grad: 0.027503693476319313, Threshold: 0.21216894686222076\n",
            "Epoch: 35, Loss: 0.00851527601480484 Updates: 6/8000, Avg Grad: 0.08191295713186264, Threshold: 0.21216894686222076\n",
            "Epoch: 35, Loss: 0.01177806407213211 Updates: 6/9000, Avg Grad: 0.18081597983837128, Threshold: 0.21216894686222076\n",
            "Epoch 34 iteration 0 Loss: 0.747 | Acc: 0.000% (0/1)\n",
            "Epoch 34 iteration 100 Loss: 0.625 | Acc: 66.337% (67/101)\n",
            "Epoch 34 iteration 200 Loss: 0.630 | Acc: 65.174% (131/201)\n",
            "Test accuracy: 0.6517412662506104\n",
            "Epoch: 35, Loss: 0.01170092262327671 Updates: 7/10000, Avg Grad: 0.06518673896789551, Threshold: 0.2356240302324295\n",
            "Epoch: 35, Loss: 0.001424219342879951 Updates: 7/11000, Avg Grad: 0.11724646389484406, Threshold: 0.2356240302324295\n",
            "Epoch: 35, Loss: 0.0023741049226373434 Updates: 7/12000, Avg Grad: 0.11397501081228256, Threshold: 0.2356240302324295\n",
            "Epoch: 35, Loss: 0.00914864894002676 Updates: 7/13000, Avg Grad: 0.135748952627182, Threshold: 0.2356240302324295\n",
            "Epoch: 35, Loss: 0.016652360558509827 Updates: 7/14000, Avg Grad: 0.18133777379989624, Threshold: 0.2356240302324295\n",
            "Epoch: 35, Loss: 0.0031427773647010326 Updates: 8/15000, Avg Grad: 0.0421551838517189, Threshold: 0.25746071338653564\n",
            "Epoch: 35, Loss: 0.0037284332793205976 Updates: 8/16000, Avg Grad: 0.1570160984992981, Threshold: 0.25746071338653564\n",
            "Epoch: 35, Loss: 0.0016349165234714746 Updates: 8/17000, Avg Grad: 0.2392992228269577, Threshold: 0.25746071338653564\n",
            "Epoch: 35, Loss: 0.002477425616234541 Updates: 9/18000, Avg Grad: 0.17521938681602478, Threshold: 0.29075887799263\n",
            "Epoch: 35, Loss: 0.004950713366270065 Updates: 10/19000, Avg Grad: 0.03040229342877865, Threshold: 0.31143274903297424\n",
            "Epoch 34 iteration 0 Loss: 0.196 | Acc: 100.000% (1/1)\n",
            "Epoch 34 iteration 100 Loss: 0.613 | Acc: 62.376% (63/101)\n",
            "Epoch 34 iteration 200 Loss: 0.619 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 35, Loss: 0.007034996524453163 Updates: 10/20000, Avg Grad: 0.09076385200023651, Threshold: 0.31143274903297424\n",
            "Epoch: 35, Loss: 0.005481405649334192 Updates: 10/21000, Avg Grad: 0.1693320870399475, Threshold: 0.31143274903297424\n",
            "Epoch: 35, Loss: 0.014050153084099293 Updates: 10/22000, Avg Grad: 0.19175869226455688, Threshold: 0.31143274903297424\n",
            "Epoch: 35, Loss: 0.0029691504314541817 Updates: 10/23000, Avg Grad: 0.20198126137256622, Threshold: 0.31143274903297424\n",
            "Epoch: 35, Loss: 0.003787839086726308 Updates: 10/24000, Avg Grad: 0.22836364805698395, Threshold: 0.31143274903297424\n",
            "Epoch: 35, Loss: 0.003213372314348817 Updates: 10/25000, Avg Grad: 0.28193920850753784, Threshold: 0.31143274903297424\n",
            "Epoch: 35, Loss: 0.007682417053729296 Updates: 11/26000, Avg Grad: 0.0841364786028862, Threshold: 0.31818920373916626\n",
            "Epoch: 35, Loss: 0.006723918952047825 Updates: 11/27000, Avg Grad: 0.29924458265304565, Threshold: 0.31818920373916626\n",
            "Epoch: 35, Loss: 0.008562842383980751 Updates: 12/28000, Avg Grad: 0.2489974945783615, Threshold: 0.33601996302604675\n",
            "Epoch: 35, Loss: 0.002381070051342249 Updates: 13/29000, Avg Grad: 0.13020798563957214, Threshold: 0.36395132541656494\n",
            "Epoch 34 iteration 0 Loss: 0.857 | Acc: 0.000% (0/1)\n",
            "Epoch 34 iteration 100 Loss: 0.605 | Acc: 69.307% (70/101)\n",
            "Epoch 34 iteration 200 Loss: 0.631 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 35, Loss: 0.0059530711732804775 Updates: 14/30000, Avg Grad: 0.38827648758888245, Threshold: 0.39065924286842346\n",
            "Epoch: 35, Loss: 0.003079271875321865 Updates: 14/31000, Avg Grad: 0.11082208901643753, Threshold: 0.39065924286842346\n",
            "Epoch: 35, Loss: 0.005543161183595657 Updates: 14/32000, Avg Grad: 0.09598203748464584, Threshold: 0.39065924286842346\n",
            "Epoch 35 iteration 0 Loss: 1.340 | Acc: 0.000% (0/1)\n",
            "Epoch 35 iteration 100 Loss: 0.599 | Acc: 65.347% (66/101)\n",
            "Epoch 35 iteration 200 Loss: 0.599 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 36, Loss: 0.7236541509628296 Updates: 0/0, Avg Grad: 0.1929868459701538, Threshold: 0.21228553354740143\n",
            "Epoch: 36, Loss: 0.003383252304047346 Updates: 1/1000, Avg Grad: 0.039707060903310776, Threshold: 0.23914691805839539\n",
            "Epoch: 36, Loss: 0.006191357504576445 Updates: 1/2000, Avg Grad: 0.09831364452838898, Threshold: 0.23914691805839539\n",
            "Epoch: 36, Loss: 0.0051530892960727215 Updates: 1/3000, Avg Grad: 0.191095769405365, Threshold: 0.23914691805839539\n",
            "Epoch: 36, Loss: 0.0041684359312057495 Updates: 2/4000, Avg Grad: 0.023567328229546547, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.004765675403177738 Updates: 2/5000, Avg Grad: 0.04147583618760109, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.007988794706761837 Updates: 2/6000, Avg Grad: 0.051655203104019165, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.010791460052132607 Updates: 2/7000, Avg Grad: 0.08091077953577042, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.0020851190201938152 Updates: 2/8000, Avg Grad: 0.07618989795446396, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.0020402560476213694 Updates: 2/9000, Avg Grad: 0.09358926117420197, Threshold: 0.272097110748291\n",
            "Epoch 35 iteration 0 Loss: 0.263 | Acc: 100.000% (1/1)\n",
            "Epoch 35 iteration 100 Loss: 0.575 | Acc: 70.297% (71/101)\n",
            "Epoch 35 iteration 200 Loss: 0.615 | Acc: 67.164% (135/201)\n",
            "Test accuracy: 0.6716417670249939\n",
            "Epoch: 36, Loss: 0.007585359737277031 Updates: 2/10000, Avg Grad: 0.07742592692375183, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.0033137043938040733 Updates: 2/11000, Avg Grad: 0.11082292348146439, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.007534238509833813 Updates: 2/12000, Avg Grad: 0.10023297369480133, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.01206225249916315 Updates: 2/13000, Avg Grad: 0.12714318931102753, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.002849136246368289 Updates: 2/14000, Avg Grad: 0.13495996594429016, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.0030473112128674984 Updates: 2/15000, Avg Grad: 0.16655273735523224, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.004243853501975536 Updates: 2/16000, Avg Grad: 0.16446353495121002, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.004918055143207312 Updates: 2/17000, Avg Grad: 0.16795426607131958, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.004682780243456364 Updates: 2/18000, Avg Grad: 0.19581851363182068, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.024464158341288567 Updates: 2/19000, Avg Grad: 0.19120053946971893, Threshold: 0.272097110748291\n",
            "Epoch 35 iteration 0 Loss: 0.334 | Acc: 100.000% (1/1)\n",
            "Epoch 35 iteration 100 Loss: 0.605 | Acc: 71.287% (72/101)\n",
            "Epoch 35 iteration 200 Loss: 0.639 | Acc: 67.662% (136/201)\n",
            "Test accuracy: 0.676616907119751\n",
            "Epoch: 36, Loss: 0.0013247946044430137 Updates: 2/20000, Avg Grad: 0.2255311906337738, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.004412807058542967 Updates: 2/21000, Avg Grad: 0.23293986916542053, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.006243442185223103 Updates: 2/22000, Avg Grad: 0.2287072241306305, Threshold: 0.272097110748291\n",
            "Epoch: 36, Loss: 0.013646670617163181 Updates: 3/23000, Avg Grad: 0.01287134736776352, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.012180035002529621 Updates: 3/24000, Avg Grad: 0.07410842180252075, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.004312379285693169 Updates: 3/25000, Avg Grad: 0.12954308092594147, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.0036211686674505472 Updates: 3/26000, Avg Grad: 0.15236763656139374, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.003754296572878957 Updates: 3/27000, Avg Grad: 0.17975199222564697, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.004854915663599968 Updates: 3/28000, Avg Grad: 0.1940390020608902, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.005197577644139528 Updates: 3/29000, Avg Grad: 0.19116979837417603, Threshold: 0.2785785496234894\n",
            "Epoch 35 iteration 0 Loss: 1.377 | Acc: 0.000% (0/1)\n",
            "Epoch 35 iteration 100 Loss: 0.643 | Acc: 63.366% (64/101)\n",
            "Epoch 35 iteration 200 Loss: 0.620 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 36, Loss: 0.006606339942663908 Updates: 3/30000, Avg Grad: 0.2205502688884735, Threshold: 0.2785785496234894\n",
            "Epoch: 36, Loss: 0.0014297758461907506 Updates: 4/31000, Avg Grad: 0.01851494610309601, Threshold: 0.2917792797088623\n",
            "Epoch: 36, Loss: 0.0018321349052712321 Updates: 4/32000, Avg Grad: 0.09453602135181427, Threshold: 0.2917792797088623\n",
            "Epoch 36 iteration 0 Loss: 1.134 | Acc: 0.000% (0/1)\n",
            "Epoch 36 iteration 100 Loss: 0.583 | Acc: 71.287% (72/101)\n",
            "Epoch 36 iteration 200 Loss: 0.597 | Acc: 70.647% (142/201)\n",
            "Test accuracy: 0.7064676880836487\n",
            "Epoch: 37, Loss: 0.8135477900505066 Updates: 0/0, Avg Grad: 0.20219218730926514, Threshold: 0.2224114090204239\n",
            "Epoch: 37, Loss: 0.0017096103401854634 Updates: 1/1000, Avg Grad: 0.04086557775735855, Threshold: 0.2532918453216553\n",
            "Epoch: 37, Loss: 0.007604493293911219 Updates: 1/2000, Avg Grad: 0.12422392517328262, Threshold: 0.2532918453216553\n",
            "Epoch: 37, Loss: 0.0021494438406080008 Updates: 1/3000, Avg Grad: 0.19953566789627075, Threshold: 0.2532918453216553\n",
            "Epoch: 37, Loss: 0.007735314313322306 Updates: 2/4000, Avg Grad: 0.2561444938182831, Threshold: 0.28175896406173706\n",
            "Epoch: 37, Loss: 0.003577708499506116 Updates: 2/5000, Avg Grad: 0.061867646872997284, Threshold: 0.28175896406173706\n",
            "Epoch: 37, Loss: 0.003709839889779687 Updates: 2/6000, Avg Grad: 0.15327002108097076, Threshold: 0.28175896406173706\n",
            "Epoch: 37, Loss: 0.005706362891942263 Updates: 2/7000, Avg Grad: 0.24023275077342987, Threshold: 0.28175896406173706\n",
            "Epoch: 37, Loss: 0.0012280052760615945 Updates: 3/8000, Avg Grad: 0.024532537907361984, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.003272302681580186 Updates: 3/9000, Avg Grad: 0.05360029637813568, Threshold: 0.3128945827484131\n",
            "Epoch 36 iteration 0 Loss: 0.250 | Acc: 100.000% (1/1)\n",
            "Epoch 36 iteration 100 Loss: 0.597 | Acc: 69.307% (70/101)\n",
            "Epoch 36 iteration 200 Loss: 0.616 | Acc: 67.662% (136/201)\n",
            "Test accuracy: 0.676616907119751\n",
            "Epoch: 37, Loss: 0.004459036979824305 Updates: 3/10000, Avg Grad: 0.06809702515602112, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.004742642864584923 Updates: 3/11000, Avg Grad: 0.10980295389890671, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.0025447397492825985 Updates: 3/12000, Avg Grad: 0.15061424672603607, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.0020227753557264805 Updates: 3/13000, Avg Grad: 0.2064545452594757, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.003129893448203802 Updates: 3/14000, Avg Grad: 0.21275928616523743, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.00344418128952384 Updates: 3/15000, Avg Grad: 0.25512397289276123, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.001630654907785356 Updates: 3/16000, Avg Grad: 0.2955527603626251, Threshold: 0.3128945827484131\n",
            "Epoch: 37, Loss: 0.017228031530976295 Updates: 4/17000, Avg Grad: 0.07246347516775131, Threshold: 0.34204551577568054\n",
            "Epoch: 37, Loss: 0.0042509762570261955 Updates: 4/18000, Avg Grad: 0.12108605355024338, Threshold: 0.34204551577568054\n",
            "Epoch: 37, Loss: 0.004201217088848352 Updates: 4/19000, Avg Grad: 0.132623091340065, Threshold: 0.34204551577568054\n",
            "Epoch 36 iteration 0 Loss: 0.684 | Acc: 100.000% (1/1)\n",
            "Epoch 36 iteration 100 Loss: 0.654 | Acc: 61.386% (62/101)\n",
            "Epoch 36 iteration 200 Loss: 0.658 | Acc: 62.687% (126/201)\n",
            "Test accuracy: 0.6268656849861145\n",
            "Epoch: 37, Loss: 0.001583898556418717 Updates: 4/20000, Avg Grad: 0.24389000236988068, Threshold: 0.34204551577568054\n",
            "Epoch: 37, Loss: 0.004360707476735115 Updates: 4/21000, Avg Grad: 0.2849510908126831, Threshold: 0.34204551577568054\n",
            "Epoch: 37, Loss: 0.008345209062099457 Updates: 4/22000, Avg Grad: 0.30948111414909363, Threshold: 0.34204551577568054\n",
            "Epoch: 37, Loss: 0.0034659227821975946 Updates: 5/23000, Avg Grad: 0.017674395814538002, Threshold: 0.3532412350177765\n",
            "Epoch: 37, Loss: 0.013393356464803219 Updates: 5/24000, Avg Grad: 0.0980079397559166, Threshold: 0.3532412350177765\n",
            "Epoch: 37, Loss: 0.00204063905403018 Updates: 5/25000, Avg Grad: 0.11087805777788162, Threshold: 0.3532412350177765\n",
            "Epoch: 37, Loss: 0.0030150683596730232 Updates: 5/26000, Avg Grad: 0.14501041173934937, Threshold: 0.3532412350177765\n",
            "Epoch: 37, Loss: 0.0019962331280112267 Updates: 5/27000, Avg Grad: 0.2246990203857422, Threshold: 0.3532412350177765\n",
            "Epoch: 37, Loss: 0.0017911021132022142 Updates: 5/28000, Avg Grad: 0.2761164903640747, Threshold: 0.3532412350177765\n",
            "Epoch: 37, Loss: 0.008345343172550201 Updates: 5/29000, Avg Grad: 0.3159278333187103, Threshold: 0.3532412350177765\n",
            "Epoch 36 iteration 0 Loss: 0.636 | Acc: 100.000% (1/1)\n",
            "Epoch 36 iteration 100 Loss: 0.598 | Acc: 66.337% (67/101)\n",
            "Epoch 36 iteration 200 Loss: 0.596 | Acc: 69.154% (139/201)\n",
            "Test accuracy: 0.6915422677993774\n",
            "Epoch: 37, Loss: 0.007234388496726751 Updates: 6/30000, Avg Grad: 0.027436519041657448, Threshold: 0.3564111292362213\n",
            "Epoch: 37, Loss: 0.009520286694169044 Updates: 6/31000, Avg Grad: 0.08108019083738327, Threshold: 0.3564111292362213\n",
            "Epoch: 37, Loss: 0.005241307429969311 Updates: 6/32000, Avg Grad: 0.10964130610227585, Threshold: 0.3564111292362213\n",
            "Epoch 37 iteration 0 Loss: 0.866 | Acc: 0.000% (0/1)\n",
            "Epoch 37 iteration 100 Loss: 0.624 | Acc: 71.287% (72/101)\n",
            "Epoch 37 iteration 200 Loss: 0.608 | Acc: 71.144% (143/201)\n",
            "Test accuracy: 0.711442768573761\n",
            "Epoch: 38, Loss: 0.2390691637992859 Updates: 0/0, Avg Grad: 0.07374536246061325, Threshold: 0.08111990243196487\n",
            "Epoch: 38, Loss: 0.008090240880846977 Updates: 1/1000, Avg Grad: 0.08147155493497849, Threshold: 0.09194125235080719\n",
            "Epoch: 38, Loss: 0.00156546535436064 Updates: 2/2000, Avg Grad: 0.03857464715838432, Threshold: 0.10882674157619476\n",
            "Epoch: 38, Loss: 0.0032698323484510183 Updates: 2/3000, Avg Grad: 0.07959172874689102, Threshold: 0.10882674157619476\n",
            "Epoch: 38, Loss: 0.004507074132561684 Updates: 2/4000, Avg Grad: 0.07224291563034058, Threshold: 0.10882674157619476\n",
            "Epoch: 38, Loss: 0.007010307163000107 Updates: 2/5000, Avg Grad: 0.08906961232423782, Threshold: 0.10882674157619476\n",
            "Epoch: 38, Loss: 0.008513080887496471 Updates: 2/6000, Avg Grad: 0.08391621708869934, Threshold: 0.10882674157619476\n",
            "Epoch: 38, Loss: 0.005341023206710815 Updates: 2/7000, Avg Grad: 0.0907457172870636, Threshold: 0.10882674157619476\n",
            "Epoch: 38, Loss: 0.002919873222708702 Updates: 3/8000, Avg Grad: 0.032114967703819275, Threshold: 0.12260118126869202\n",
            "Epoch: 38, Loss: 0.00199803221039474 Updates: 3/9000, Avg Grad: 0.04976809397339821, Threshold: 0.12260118126869202\n",
            "Epoch 37 iteration 0 Loss: 0.252 | Acc: 100.000% (1/1)\n",
            "Epoch 37 iteration 100 Loss: 0.631 | Acc: 63.366% (64/101)\n",
            "Epoch 37 iteration 200 Loss: 0.590 | Acc: 68.657% (138/201)\n",
            "Test accuracy: 0.6865671873092651\n",
            "Epoch: 38, Loss: 0.0022752475924789906 Updates: 3/10000, Avg Grad: 0.05779103934764862, Threshold: 0.12260118126869202\n",
            "Epoch: 38, Loss: 0.008013328537344933 Updates: 3/11000, Avg Grad: 0.09005918353796005, Threshold: 0.12260118126869202\n",
            "Epoch: 38, Loss: 0.003806921886280179 Updates: 3/12000, Avg Grad: 0.09366634488105774, Threshold: 0.12260118126869202\n",
            "Epoch: 38, Loss: 0.0028590247966349125 Updates: 4/13000, Avg Grad: 0.13125449419021606, Threshold: 0.14437994360923767\n",
            "Epoch: 38, Loss: 0.007416369393467903 Updates: 4/14000, Avg Grad: 0.06290719658136368, Threshold: 0.14437994360923767\n",
            "Epoch: 38, Loss: 0.0048818569630384445 Updates: 4/15000, Avg Grad: 0.057291146367788315, Threshold: 0.14437994360923767\n",
            "Epoch: 38, Loss: 0.0049096448346972466 Updates: 4/16000, Avg Grad: 0.09192197769880295, Threshold: 0.14437994360923767\n",
            "Epoch: 38, Loss: 0.007880293764173985 Updates: 5/17000, Avg Grad: 0.040771231055259705, Threshold: 0.15738415718078613\n",
            "Epoch: 38, Loss: 0.007347599137574434 Updates: 5/18000, Avg Grad: 0.07429435849189758, Threshold: 0.15738415718078613\n",
            "Epoch: 38, Loss: 0.004813605919480324 Updates: 5/19000, Avg Grad: 0.11409186571836472, Threshold: 0.15738415718078613\n",
            "Epoch 37 iteration 0 Loss: 0.258 | Acc: 100.000% (1/1)\n",
            "Epoch 37 iteration 100 Loss: 0.598 | Acc: 67.327% (68/101)\n",
            "Epoch 37 iteration 200 Loss: 0.617 | Acc: 68.159% (137/201)\n",
            "Test accuracy: 0.6815920472145081\n",
            "Epoch: 38, Loss: 0.00518617732450366 Updates: 6/20000, Avg Grad: 0.020835543051362038, Threshold: 0.17351171374320984\n",
            "Epoch: 38, Loss: 0.00597362918779254 Updates: 6/21000, Avg Grad: 0.03921590745449066, Threshold: 0.17351171374320984\n",
            "Epoch: 38, Loss: 0.004897590726613998 Updates: 6/22000, Avg Grad: 0.08664944767951965, Threshold: 0.17351171374320984\n",
            "Epoch: 38, Loss: 0.004908391740173101 Updates: 7/23000, Avg Grad: 0.17858609557151794, Threshold: 0.18219724297523499\n",
            "Epoch: 38, Loss: 0.012081262655556202 Updates: 7/24000, Avg Grad: 0.08725955337285995, Threshold: 0.18219724297523499\n",
            "Epoch: 38, Loss: 0.0029173323418945074 Updates: 8/25000, Avg Grad: 0.029873989522457123, Threshold: 0.1853104531764984\n",
            "Epoch: 38, Loss: 0.009603572078049183 Updates: 8/26000, Avg Grad: 0.03596700355410576, Threshold: 0.1853104531764984\n",
            "Epoch: 38, Loss: 0.004000342916697264 Updates: 8/27000, Avg Grad: 0.09125209599733353, Threshold: 0.1853104531764984\n",
            "Epoch: 38, Loss: 0.014072122983634472 Updates: 8/28000, Avg Grad: 0.1123976930975914, Threshold: 0.1853104531764984\n",
            "Epoch: 38, Loss: 0.005019671283662319 Updates: 8/29000, Avg Grad: 0.11299148201942444, Threshold: 0.1853104531764984\n",
            "Epoch 37 iteration 0 Loss: 0.997 | Acc: 0.000% (0/1)\n",
            "Epoch 37 iteration 100 Loss: 0.600 | Acc: 70.297% (71/101)\n",
            "Epoch 37 iteration 200 Loss: 0.614 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 38, Loss: 0.007741404231637716 Updates: 8/30000, Avg Grad: 0.10572290420532227, Threshold: 0.1853104531764984\n",
            "Epoch: 38, Loss: 0.011382555589079857 Updates: 8/31000, Avg Grad: 0.1738356649875641, Threshold: 0.1853104531764984\n",
            "Epoch: 38, Loss: 0.009872283786535263 Updates: 8/32000, Avg Grad: 0.16134005784988403, Threshold: 0.1853104531764984\n",
            "Epoch 38 iteration 0 Loss: 1.257 | Acc: 0.000% (0/1)\n",
            "Epoch 38 iteration 100 Loss: 0.531 | Acc: 75.248% (76/101)\n",
            "Epoch 38 iteration 200 Loss: 0.565 | Acc: 74.129% (149/201)\n",
            "Test accuracy: 0.7412935495376587\n",
            "Epoch: 39, Loss: 1.449489951133728 Updates: 0/0, Avg Grad: 0.26447340846061707, Threshold: 0.29092076420783997\n",
            "Epoch: 39, Loss: 0.012620082125067711 Updates: 0/1000, Avg Grad: 0.2526584565639496, Threshold: 0.29092076420783997\n",
            "Epoch: 39, Loss: 0.003749096766114235 Updates: 0/2000, Avg Grad: 0.21556760370731354, Threshold: 0.29092076420783997\n",
            "Epoch: 39, Loss: 0.0025822848547250032 Updates: 0/3000, Avg Grad: 0.19384567439556122, Threshold: 0.29092076420783997\n",
            "Epoch: 39, Loss: 0.003004188183695078 Updates: 0/4000, Avg Grad: 0.21928918361663818, Threshold: 0.29092076420783997\n",
            "Epoch: 39, Loss: 0.0008553258376196027 Updates: 0/5000, Avg Grad: 0.2633762061595917, Threshold: 0.29092076420783997\n",
            "Epoch: 39, Loss: 0.004933475982397795 Updates: 1/6000, Avg Grad: 0.048899032175540924, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.00430796155706048 Updates: 1/7000, Avg Grad: 0.07258974760770798, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.002707740990445018 Updates: 1/8000, Avg Grad: 0.10570493340492249, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.012243685312569141 Updates: 1/9000, Avg Grad: 0.1569455862045288, Threshold: 0.32992443442344666\n",
            "Epoch 38 iteration 0 Loss: 0.479 | Acc: 100.000% (1/1)\n",
            "Epoch 38 iteration 100 Loss: 0.587 | Acc: 68.317% (69/101)\n",
            "Epoch 38 iteration 200 Loss: 0.619 | Acc: 65.174% (131/201)\n",
            "Test accuracy: 0.6517412662506104\n",
            "Epoch: 39, Loss: 0.010469372384250164 Updates: 1/10000, Avg Grad: 0.21298298239707947, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.005051088985055685 Updates: 1/11000, Avg Grad: 0.2751554548740387, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.007549768779426813 Updates: 1/12000, Avg Grad: 0.296540230512619, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.009246904402971268 Updates: 1/13000, Avg Grad: 0.2617458403110504, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.008062117733061314 Updates: 1/14000, Avg Grad: 0.23839683830738068, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.006676589138805866 Updates: 1/15000, Avg Grad: 0.22956876456737518, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.0020637616980820894 Updates: 1/16000, Avg Grad: 0.2414395958185196, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.009716047905385494 Updates: 1/17000, Avg Grad: 0.26804524660110474, Threshold: 0.32992443442344666\n",
            "Epoch: 39, Loss: 0.007505136076360941 Updates: 2/18000, Avg Grad: 0.03915988653898239, Threshold: 0.3533167541027069\n",
            "Epoch: 39, Loss: 0.005804745480418205 Updates: 2/19000, Avg Grad: 0.2420484870672226, Threshold: 0.3533167541027069\n",
            "Epoch 38 iteration 0 Loss: 0.518 | Acc: 100.000% (1/1)\n",
            "Epoch 38 iteration 100 Loss: 0.583 | Acc: 67.327% (68/101)\n",
            "Epoch 38 iteration 200 Loss: 0.649 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 39, Loss: 0.007004888728260994 Updates: 3/20000, Avg Grad: 0.06780136376619339, Threshold: 0.3769507110118866\n",
            "Epoch: 39, Loss: 0.012564736418426037 Updates: 3/21000, Avg Grad: 0.19141924381256104, Threshold: 0.3769507110118866\n",
            "Epoch: 39, Loss: 0.002808346413075924 Updates: 4/22000, Avg Grad: 0.013351893052458763, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.002542460337281227 Updates: 4/23000, Avg Grad: 0.0445580780506134, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.007686061784625053 Updates: 4/24000, Avg Grad: 0.07288283109664917, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.008677409030497074 Updates: 4/25000, Avg Grad: 0.07487737387418747, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.004779480863362551 Updates: 4/26000, Avg Grad: 0.08487033098936081, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.005832237657159567 Updates: 4/27000, Avg Grad: 0.08417300879955292, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.015850700438022614 Updates: 4/28000, Avg Grad: 0.10659866034984589, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.004208243917673826 Updates: 4/29000, Avg Grad: 0.11426696181297302, Threshold: 0.41819632053375244\n",
            "Epoch 38 iteration 0 Loss: 0.260 | Acc: 100.000% (1/1)\n",
            "Epoch 38 iteration 100 Loss: 0.621 | Acc: 64.356% (65/101)\n",
            "Epoch 38 iteration 200 Loss: 0.608 | Acc: 67.164% (135/201)\n",
            "Test accuracy: 0.6716417670249939\n",
            "Epoch: 39, Loss: 0.0035623260773718357 Updates: 4/30000, Avg Grad: 0.2123485505580902, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.007263157982379198 Updates: 4/31000, Avg Grad: 0.2868083715438843, Threshold: 0.41819632053375244\n",
            "Epoch: 39, Loss: 0.00668242946267128 Updates: 4/32000, Avg Grad: 0.2885581851005554, Threshold: 0.41819632053375244\n",
            "Epoch 39 iteration 0 Loss: 0.414 | Acc: 100.000% (1/1)\n",
            "Epoch 39 iteration 100 Loss: 0.670 | Acc: 57.426% (58/101)\n",
            "Epoch 39 iteration 200 Loss: 0.692 | Acc: 60.199% (121/201)\n",
            "Test accuracy: 0.6019900441169739\n",
            "Epoch: 40, Loss: 1.4567086696624756 Updates: 0/0, Avg Grad: 0.30904486775398254, Threshold: 0.339949369430542\n",
            "Epoch: 40, Loss: 0.027467530220746994 Updates: 0/1000, Avg Grad: 0.2885783910751343, Threshold: 0.339949369430542\n",
            "Epoch: 40, Loss: 0.00825630035251379 Updates: 0/2000, Avg Grad: 0.31527242064476013, Threshold: 0.339949369430542\n",
            "Epoch: 40, Loss: 0.0004190226609352976 Updates: 1/3000, Avg Grad: 0.31562331318855286, Threshold: 0.3823936879634857\n",
            "Epoch: 40, Loss: 0.008844357915222645 Updates: 2/4000, Avg Grad: 0.2693127393722534, Threshold: 0.4278617203235626\n",
            "Epoch: 40, Loss: 0.02035578340291977 Updates: 3/5000, Avg Grad: 0.0319596491754055, Threshold: 0.4916311204433441\n",
            "Epoch: 40, Loss: 0.0038832342252135277 Updates: 3/6000, Avg Grad: 0.08277126401662827, Threshold: 0.4916311204433441\n",
            "Epoch: 40, Loss: 0.003782902844250202 Updates: 3/7000, Avg Grad: 0.184162899851799, Threshold: 0.4916311204433441\n",
            "Epoch: 40, Loss: 0.005328106228262186 Updates: 3/8000, Avg Grad: 0.2568083703517914, Threshold: 0.4916311204433441\n",
            "Epoch: 40, Loss: 0.0023679486475884914 Updates: 3/9000, Avg Grad: 0.3301135301589966, Threshold: 0.4916311204433441\n",
            "Epoch 39 iteration 0 Loss: 0.856 | Acc: 0.000% (0/1)\n",
            "Epoch 39 iteration 100 Loss: 0.523 | Acc: 77.228% (78/101)\n",
            "Epoch 39 iteration 200 Loss: 0.582 | Acc: 72.139% (145/201)\n",
            "Test accuracy: 0.7213930487632751\n",
            "Epoch: 40, Loss: 0.0064797974191606045 Updates: 3/10000, Avg Grad: 0.45289862155914307, Threshold: 0.4916311204433441\n",
            "Epoch: 40, Loss: 0.008577866479754448 Updates: 4/11000, Avg Grad: 0.22194480895996094, Threshold: 0.5598716139793396\n",
            "Epoch: 40, Loss: 0.005048726685345173 Updates: 4/12000, Avg Grad: 0.49986541271209717, Threshold: 0.5598716139793396\n",
            "Epoch: 40, Loss: 0.0027555639389902353 Updates: 5/13000, Avg Grad: 0.33153846859931946, Threshold: 0.6392621994018555\n",
            "Epoch: 40, Loss: 0.003952665254473686 Updates: 6/14000, Avg Grad: 0.08070531487464905, Threshold: 0.7234212756156921\n",
            "Epoch: 40, Loss: 0.009597046300768852 Updates: 6/15000, Avg Grad: 0.29795393347740173, Threshold: 0.7234212756156921\n",
            "Epoch: 40, Loss: 0.002200068673118949 Updates: 6/16000, Avg Grad: 0.5921677947044373, Threshold: 0.7234212756156921\n",
            "Epoch: 40, Loss: 0.00767539581283927 Updates: 7/17000, Avg Grad: 0.059373639523983, Threshold: 0.7949920296669006\n",
            "Epoch: 40, Loss: 0.011718508787453175 Updates: 7/18000, Avg Grad: 0.18378904461860657, Threshold: 0.7949920296669006\n",
            "Epoch: 40, Loss: 0.004107988905161619 Updates: 7/19000, Avg Grad: 0.31702592968940735, Threshold: 0.7949920296669006\n",
            "Epoch 39 iteration 0 Loss: 0.363 | Acc: 100.000% (1/1)\n",
            "Epoch 39 iteration 100 Loss: 0.619 | Acc: 61.386% (62/101)\n",
            "Epoch 39 iteration 200 Loss: 0.594 | Acc: 68.159% (137/201)\n",
            "Test accuracy: 0.6815920472145081\n",
            "Epoch: 40, Loss: 0.0035132411867380142 Updates: 7/20000, Avg Grad: 0.48165255784988403, Threshold: 0.7949920296669006\n",
            "Epoch: 40, Loss: 0.007273879833519459 Updates: 7/21000, Avg Grad: 0.5565359592437744, Threshold: 0.7949920296669006\n",
            "Epoch: 40, Loss: 0.0033328586723655462 Updates: 7/22000, Avg Grad: 0.7336938977241516, Threshold: 0.7949920296669006\n",
            "Epoch: 40, Loss: 0.003281407291069627 Updates: 8/23000, Avg Grad: 0.12496588379144669, Threshold: 0.8219791650772095\n",
            "Epoch: 40, Loss: 0.003900412702932954 Updates: 8/24000, Avg Grad: 0.31686320900917053, Threshold: 0.8219791650772095\n",
            "Epoch: 40, Loss: 0.007273247465491295 Updates: 8/25000, Avg Grad: 0.5744361877441406, Threshold: 0.8219791650772095\n",
            "Epoch: 40, Loss: 0.005166696384549141 Updates: 9/26000, Avg Grad: 0.8368949294090271, Threshold: 0.8470464944839478\n",
            "Epoch: 40, Loss: 0.0030014372896403074 Updates: 9/27000, Avg Grad: 0.09319251775741577, Threshold: 0.8470464944839478\n",
            "Epoch: 40, Loss: 0.007405176293104887 Updates: 9/28000, Avg Grad: 0.20035289227962494, Threshold: 0.8470464944839478\n",
            "Epoch: 40, Loss: 0.010334176942706108 Updates: 9/29000, Avg Grad: 0.29978927969932556, Threshold: 0.8470464944839478\n",
            "Epoch 39 iteration 0 Loss: 0.875 | Acc: 0.000% (0/1)\n",
            "Epoch 39 iteration 100 Loss: 0.610 | Acc: 65.347% (66/101)\n",
            "Epoch 39 iteration 200 Loss: 0.609 | Acc: 68.657% (138/201)\n",
            "Test accuracy: 0.6865671873092651\n",
            "Epoch: 40, Loss: 0.004181141033768654 Updates: 9/30000, Avg Grad: 0.4697023332118988, Threshold: 0.8470464944839478\n",
            "Epoch: 40, Loss: 0.009738028980791569 Updates: 9/31000, Avg Grad: 0.5198745727539062, Threshold: 0.8470464944839478\n",
            "Epoch: 40, Loss: 0.005778021644800901 Updates: 9/32000, Avg Grad: 0.5765407085418701, Threshold: 0.8470464944839478\n",
            "Epoch 40 iteration 0 Loss: 0.239 | Acc: 100.000% (1/1)\n",
            "Epoch 40 iteration 100 Loss: 0.604 | Acc: 66.337% (67/101)\n",
            "Epoch 40 iteration 200 Loss: 0.609 | Acc: 69.154% (139/201)\n",
            "Test accuracy: 0.6915422677993774\n",
            "Epoch: 41, Loss: 0.3428528606891632 Updates: 0/0, Avg Grad: 0.09893843531608582, Threshold: 0.10883228480815887\n",
            "Epoch: 41, Loss: 0.0018120418535545468 Updates: 2/1000, Avg Grad: 0.03924994170665741, Threshold: 0.1723673790693283\n",
            "Epoch: 41, Loss: 0.007909336127340794 Updates: 3/2000, Avg Grad: 0.13116437196731567, Threshold: 0.20721197128295898\n",
            "Epoch: 41, Loss: 0.007967798970639706 Updates: 4/3000, Avg Grad: 0.13289588689804077, Threshold: 0.23260445892810822\n",
            "Epoch: 41, Loss: 0.002681547775864601 Updates: 5/4000, Avg Grad: 0.08097464591264725, Threshold: 0.2797343134880066\n",
            "Epoch: 41, Loss: 0.0013336734846234322 Updates: 5/5000, Avg Grad: 0.16429667174816132, Threshold: 0.2797343134880066\n",
            "Epoch: 41, Loss: 0.0037624994292855263 Updates: 5/6000, Avg Grad: 0.18075479567050934, Threshold: 0.2797343134880066\n",
            "Epoch: 41, Loss: 0.0033915159292519093 Updates: 5/7000, Avg Grad: 0.27361610531806946, Threshold: 0.2797343134880066\n",
            "Epoch: 41, Loss: 0.0022601226810365915 Updates: 6/8000, Avg Grad: 0.03238970786333084, Threshold: 0.310382604598999\n",
            "Epoch: 41, Loss: 0.00334282242693007 Updates: 6/9000, Avg Grad: 0.26491889357566833, Threshold: 0.310382604598999\n",
            "Epoch 40 iteration 0 Loss: 0.705 | Acc: 0.000% (0/1)\n",
            "Epoch 40 iteration 100 Loss: 0.605 | Acc: 67.327% (68/101)\n",
            "Epoch 40 iteration 200 Loss: 0.619 | Acc: 66.667% (134/201)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 41, Loss: 0.0034507913514971733 Updates: 7/10000, Avg Grad: 0.0770692229270935, Threshold: 0.37956759333610535\n",
            "Epoch: 41, Loss: 0.017478983849287033 Updates: 7/11000, Avg Grad: 0.1881086230278015, Threshold: 0.37956759333610535\n",
            "Epoch: 41, Loss: 0.002809802070260048 Updates: 7/12000, Avg Grad: 0.3137339651584625, Threshold: 0.37956759333610535\n",
            "Epoch: 41, Loss: 0.0014953118516132236 Updates: 8/13000, Avg Grad: 0.03175372630357742, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.00735503388568759 Updates: 8/14000, Avg Grad: 0.1044112965464592, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.009879090823233128 Updates: 8/15000, Avg Grad: 0.1380728781223297, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.01485524419695139 Updates: 8/16000, Avg Grad: 0.21977616846561432, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.016948126256465912 Updates: 8/17000, Avg Grad: 0.2634925842285156, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.009235619567334652 Updates: 8/18000, Avg Grad: 0.27960404753685, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.007018572185188532 Updates: 8/19000, Avg Grad: 0.35526829957962036, Threshold: 0.42873451113700867\n",
            "Epoch 40 iteration 0 Loss: 0.229 | Acc: 100.000% (1/1)\n",
            "Epoch 40 iteration 100 Loss: 0.616 | Acc: 62.376% (63/101)\n",
            "Epoch 40 iteration 200 Loss: 0.626 | Acc: 64.677% (130/201)\n",
            "Test accuracy: 0.646766185760498\n",
            "Epoch: 41, Loss: 0.006135830655694008 Updates: 8/20000, Avg Grad: 0.42632538080215454, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.01485070213675499 Updates: 8/21000, Avg Grad: 0.3857554495334625, Threshold: 0.42873451113700867\n",
            "Epoch: 41, Loss: 0.008534761145710945 Updates: 9/22000, Avg Grad: 0.04145427793264389, Threshold: 0.45557647943496704\n",
            "Epoch: 41, Loss: 0.0006620340282097459 Updates: 9/23000, Avg Grad: 0.17748180031776428, Threshold: 0.45557647943496704\n",
            "Epoch: 41, Loss: 0.004650414921343327 Updates: 9/24000, Avg Grad: 0.31516405940055847, Threshold: 0.45557647943496704\n",
            "Epoch: 41, Loss: 0.009835237637162209 Updates: 10/25000, Avg Grad: 0.10421115905046463, Threshold: 0.46974512934684753\n",
            "Epoch: 41, Loss: 0.00752034317702055 Updates: 10/26000, Avg Grad: 0.2396695464849472, Threshold: 0.46974512934684753\n",
            "Epoch: 41, Loss: 0.004943938925862312 Updates: 11/27000, Avg Grad: 0.47659817337989807, Threshold: 0.48147380352020264\n",
            "Epoch: 41, Loss: 0.00531185045838356 Updates: 11/28000, Avg Grad: 0.22711023688316345, Threshold: 0.48147380352020264\n",
            "Epoch: 41, Loss: 0.004371214658021927 Updates: 11/29000, Avg Grad: 0.35280323028564453, Threshold: 0.48147380352020264\n",
            "Epoch 40 iteration 0 Loss: 0.918 | Acc: 0.000% (0/1)\n",
            "Epoch 40 iteration 100 Loss: 0.568 | Acc: 70.297% (71/101)\n",
            "Epoch 40 iteration 200 Loss: 0.593 | Acc: 69.652% (140/201)\n",
            "Test accuracy: 0.6965174078941345\n",
            "Epoch: 41, Loss: 0.007750325836241245 Updates: 12/30000, Avg Grad: 0.017347827553749084, Threshold: 0.48544207215309143\n",
            "Epoch: 41, Loss: 0.007261037360876799 Updates: 12/31000, Avg Grad: 0.08608498424291611, Threshold: 0.48544207215309143\n",
            "Epoch: 41, Loss: 0.004213346634060144 Updates: 12/32000, Avg Grad: 0.14451806247234344, Threshold: 0.48544207215309143\n",
            "Epoch 41 iteration 0 Loss: 0.333 | Acc: 100.000% (1/1)\n",
            "Epoch 41 iteration 100 Loss: 0.661 | Acc: 63.366% (64/101)\n",
            "Epoch 41 iteration 200 Loss: 0.626 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 42, Loss: 1.8536291122436523 Updates: 0/0, Avg Grad: 0.3205588459968567, Threshold: 0.35261473059654236\n",
            "Epoch: 42, Loss: 0.010702862404286861 Updates: 1/1000, Avg Grad: 0.013896538875997066, Threshold: 0.40369853377342224\n",
            "Epoch: 42, Loss: 0.0025148121640086174 Updates: 1/2000, Avg Grad: 0.06169259548187256, Threshold: 0.40369853377342224\n",
            "Epoch: 42, Loss: 0.0038953572511672974 Updates: 1/3000, Avg Grad: 0.12941665947437286, Threshold: 0.40369853377342224\n",
            "Epoch: 42, Loss: 0.006075967103242874 Updates: 1/4000, Avg Grad: 0.22210201621055603, Threshold: 0.40369853377342224\n",
            "Epoch: 42, Loss: 0.004933774936944246 Updates: 1/5000, Avg Grad: 0.35355448722839355, Threshold: 0.40369853377342224\n",
            "Epoch: 42, Loss: 0.002831418300047517 Updates: 2/6000, Avg Grad: 0.04203977435827255, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.005935268476605415 Updates: 2/7000, Avg Grad: 0.06895340979099274, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.005573310423642397 Updates: 2/8000, Avg Grad: 0.1280205398797989, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.006548873148858547 Updates: 2/9000, Avg Grad: 0.17177444696426392, Threshold: 0.4560779631137848\n",
            "Epoch 41 iteration 0 Loss: 0.694 | Acc: 0.000% (0/1)\n",
            "Epoch 41 iteration 100 Loss: 0.560 | Acc: 71.287% (72/101)\n",
            "Epoch 41 iteration 200 Loss: 0.596 | Acc: 67.662% (136/201)\n",
            "Test accuracy: 0.676616907119751\n",
            "Epoch: 42, Loss: 0.007826125249266624 Updates: 2/10000, Avg Grad: 0.1513376235961914, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.013531884178519249 Updates: 2/11000, Avg Grad: 0.22299262881278992, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.0032437953632324934 Updates: 2/12000, Avg Grad: 0.30535688996315, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.010205867700278759 Updates: 2/13000, Avg Grad: 0.3614513874053955, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.021394772455096245 Updates: 2/14000, Avg Grad: 0.3198448121547699, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.005114347208291292 Updates: 2/15000, Avg Grad: 0.35891714692115784, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.003488576738163829 Updates: 2/16000, Avg Grad: 0.40217894315719604, Threshold: 0.4560779631137848\n",
            "Epoch: 42, Loss: 0.005681372247636318 Updates: 3/17000, Avg Grad: 0.013822388835251331, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.0010341298766434193 Updates: 3/18000, Avg Grad: 0.040986113250255585, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.005771125666797161 Updates: 3/19000, Avg Grad: 0.07690010964870453, Threshold: 0.49025052785873413\n",
            "Epoch 41 iteration 0 Loss: 0.432 | Acc: 100.000% (1/1)\n",
            "Epoch 41 iteration 100 Loss: 0.652 | Acc: 61.386% (62/101)\n",
            "Epoch 41 iteration 200 Loss: 0.629 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 42, Loss: 0.0035938448272645473 Updates: 3/20000, Avg Grad: 0.15152032673358917, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.0030619141180068254 Updates: 3/21000, Avg Grad: 0.19173382222652435, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.0026376356836408377 Updates: 3/22000, Avg Grad: 0.259845495223999, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.002891886280849576 Updates: 3/23000, Avg Grad: 0.23161160945892334, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.013475775718688965 Updates: 3/24000, Avg Grad: 0.17826618254184723, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.003126774914562702 Updates: 3/25000, Avg Grad: 0.17690736055374146, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.006566332653164864 Updates: 3/26000, Avg Grad: 0.2004249393939972, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.00357028073631227 Updates: 3/27000, Avg Grad: 0.2488710731267929, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.007360409013926983 Updates: 3/28000, Avg Grad: 0.2442770004272461, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.009673332795500755 Updates: 3/29000, Avg Grad: 0.2541336119174957, Threshold: 0.49025052785873413\n",
            "Epoch 41 iteration 0 Loss: 0.228 | Acc: 100.000% (1/1)\n",
            "Epoch 41 iteration 100 Loss: 0.614 | Acc: 62.376% (63/101)\n",
            "Epoch 41 iteration 200 Loss: 0.640 | Acc: 61.692% (124/201)\n",
            "Test accuracy: 0.6169154047966003\n",
            "Epoch: 42, Loss: 0.0032657673582434654 Updates: 3/30000, Avg Grad: 0.25371065735816956, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.008217902854084969 Updates: 3/31000, Avg Grad: 0.27530115842819214, Threshold: 0.49025052785873413\n",
            "Epoch: 42, Loss: 0.005545241758227348 Updates: 3/32000, Avg Grad: 0.32184848189353943, Threshold: 0.49025052785873413\n",
            "Epoch 42 iteration 0 Loss: 0.413 | Acc: 100.000% (1/1)\n",
            "Epoch 42 iteration 100 Loss: 0.650 | Acc: 66.337% (67/101)\n",
            "Epoch 42 iteration 200 Loss: 0.592 | Acc: 71.144% (143/201)\n",
            "Test accuracy: 0.711442768573761\n",
            "Epoch: 43, Loss: 0.345798522233963 Updates: 0/0, Avg Grad: 0.10783102363348007, Threshold: 0.11861412972211838\n",
            "Epoch: 43, Loss: 0.0037409511860460043 Updates: 0/1000, Avg Grad: 0.08925420790910721, Threshold: 0.11861412972211838\n",
            "Epoch: 43, Loss: 0.0056424676440656185 Updates: 1/2000, Avg Grad: 0.07322177290916443, Threshold: 0.13820892572402954\n",
            "Epoch: 43, Loss: 0.0022098745685070753 Updates: 2/3000, Avg Grad: 0.027560193091630936, Threshold: 0.1589609533548355\n",
            "Epoch: 43, Loss: 0.004716829862445593 Updates: 3/4000, Avg Grad: 0.026589494198560715, Threshold: 0.18797951936721802\n",
            "Epoch: 43, Loss: 0.0055649918504059315 Updates: 3/5000, Avg Grad: 0.09648866951465607, Threshold: 0.18797951936721802\n",
            "Epoch: 43, Loss: 0.0019330913200974464 Updates: 3/6000, Avg Grad: 0.14915324747562408, Threshold: 0.18797951936721802\n",
            "Epoch: 43, Loss: 0.0023404317907989025 Updates: 4/7000, Avg Grad: 0.2004360407590866, Threshold: 0.22047965228557587\n",
            "Epoch: 43, Loss: 0.0021878464613109827 Updates: 4/8000, Avg Grad: 0.028428861871361732, Threshold: 0.22047965228557587\n",
            "Epoch: 43, Loss: 0.004798687994480133 Updates: 4/9000, Avg Grad: 0.17618411779403687, Threshold: 0.22047965228557587\n",
            "Epoch 42 iteration 0 Loss: 0.171 | Acc: 100.000% (1/1)\n",
            "Epoch 42 iteration 100 Loss: 0.604 | Acc: 66.337% (67/101)\n",
            "Epoch 42 iteration 200 Loss: 0.633 | Acc: 63.682% (128/201)\n",
            "Test accuracy: 0.6368159055709839\n",
            "Epoch: 43, Loss: 0.010860620997846127 Updates: 5/10000, Avg Grad: 0.02639334462583065, Threshold: 0.2532924711704254\n",
            "Epoch: 43, Loss: 0.006537845823913813 Updates: 5/11000, Avg Grad: 0.12308593094348907, Threshold: 0.2532924711704254\n",
            "Epoch: 43, Loss: 0.005737757310271263 Updates: 5/12000, Avg Grad: 0.1958652287721634, Threshold: 0.2532924711704254\n",
            "Epoch: 43, Loss: 0.0024236293975263834 Updates: 6/13000, Avg Grad: 0.027473431080579758, Threshold: 0.30104145407676697\n",
            "Epoch: 43, Loss: 0.007063866127282381 Updates: 6/14000, Avg Grad: 0.055294714868068695, Threshold: 0.30104145407676697\n",
            "Epoch: 43, Loss: 0.004522259347140789 Updates: 6/15000, Avg Grad: 0.08351476490497589, Threshold: 0.30104145407676697\n",
            "Epoch: 43, Loss: 0.008284313604235649 Updates: 6/16000, Avg Grad: 0.19467681646347046, Threshold: 0.30104145407676697\n",
            "Epoch: 43, Loss: 0.004803074989467859 Updates: 6/17000, Avg Grad: 0.24470801651477814, Threshold: 0.30104145407676697\n",
            "Epoch: 43, Loss: 0.002266321098431945 Updates: 6/18000, Avg Grad: 0.28438621759414673, Threshold: 0.30104145407676697\n",
            "Epoch: 43, Loss: 0.008088125847280025 Updates: 7/19000, Avg Grad: 0.12723876535892487, Threshold: 0.33113399147987366\n",
            "Epoch 42 iteration 0 Loss: 0.740 | Acc: 0.000% (0/1)\n",
            "Epoch 42 iteration 100 Loss: 0.633 | Acc: 63.366% (64/101)\n",
            "Epoch 42 iteration 200 Loss: 0.647 | Acc: 62.189% (125/201)\n",
            "Test accuracy: 0.6218905448913574\n",
            "Epoch: 43, Loss: 0.003782544517889619 Updates: 7/20000, Avg Grad: 0.28775689005851746, Threshold: 0.33113399147987366\n",
            "Epoch: 43, Loss: 0.003731687320396304 Updates: 8/21000, Avg Grad: 0.07291460782289505, Threshold: 0.37105467915534973\n",
            "Epoch: 43, Loss: 0.004663886036723852 Updates: 8/22000, Avg Grad: 0.23056800663471222, Threshold: 0.37105467915534973\n",
            "Epoch: 43, Loss: 0.006930384784936905 Updates: 8/23000, Avg Grad: 0.31958431005477905, Threshold: 0.37105467915534973\n",
            "Epoch: 43, Loss: 0.0038491017185151577 Updates: 9/24000, Avg Grad: 0.08738046884536743, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.005448178853839636 Updates: 9/25000, Avg Grad: 0.09049297124147415, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.012477307580411434 Updates: 9/26000, Avg Grad: 0.11785527318716049, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.02032524347305298 Updates: 9/27000, Avg Grad: 0.1735309660434723, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.005018546711653471 Updates: 9/28000, Avg Grad: 0.25148260593414307, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.0036357641220092773 Updates: 9/29000, Avg Grad: 0.30674684047698975, Threshold: 0.38627147674560547\n",
            "Epoch 42 iteration 0 Loss: 0.911 | Acc: 0.000% (0/1)\n",
            "Epoch 42 iteration 100 Loss: 0.632 | Acc: 66.337% (67/101)\n",
            "Epoch 42 iteration 200 Loss: 0.616 | Acc: 68.159% (137/201)\n",
            "Test accuracy: 0.6815920472145081\n",
            "Epoch: 43, Loss: 0.006471702829003334 Updates: 9/30000, Avg Grad: 0.31363219022750854, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.00783848948776722 Updates: 9/31000, Avg Grad: 0.3624638617038727, Threshold: 0.38627147674560547\n",
            "Epoch: 43, Loss: 0.004009856842458248 Updates: 10/32000, Avg Grad: 0.11234492063522339, Threshold: 0.390877902507782\n",
            "Epoch 43 iteration 0 Loss: 0.353 | Acc: 100.000% (1/1)\n",
            "Epoch 43 iteration 100 Loss: 0.603 | Acc: 70.297% (71/101)\n",
            "Epoch 43 iteration 200 Loss: 0.643 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 44, Loss: 0.44333720207214355 Updates: 0/0, Avg Grad: 0.1415356993675232, Threshold: 0.1556892693042755\n",
            "Epoch: 44, Loss: 0.003514963900670409 Updates: 2/1000, Avg Grad: 0.1154419332742691, Threshold: 0.23552007973194122\n",
            "Epoch: 44, Loss: 0.0022082324139773846 Updates: 4/2000, Avg Grad: 0.04663620889186859, Threshold: 0.357785701751709\n",
            "Epoch: 44, Loss: 0.005737170111387968 Updates: 5/3000, Avg Grad: 0.3915129601955414, Threshold: 0.4306642711162567\n",
            "Epoch: 44, Loss: 0.002269730204716325 Updates: 5/4000, Avg Grad: 0.08907663077116013, Threshold: 0.4306642711162567\n",
            "Epoch: 44, Loss: 0.010068628005683422 Updates: 5/5000, Avg Grad: 0.17990505695343018, Threshold: 0.4306642711162567\n",
            "Epoch: 44, Loss: 0.0028351701330393553 Updates: 5/6000, Avg Grad: 0.3377334177494049, Threshold: 0.4306642711162567\n",
            "Epoch: 44, Loss: 0.003605069825425744 Updates: 5/7000, Avg Grad: 0.39586636424064636, Threshold: 0.4306642711162567\n",
            "Epoch: 44, Loss: 0.0046201408840715885 Updates: 6/8000, Avg Grad: 0.19974388182163239, Threshold: 0.48015889525413513\n",
            "Epoch: 44, Loss: 0.005151481833308935 Updates: 6/9000, Avg Grad: 0.41736048460006714, Threshold: 0.48015889525413513\n",
            "Epoch 43 iteration 0 Loss: 0.118 | Acc: 100.000% (1/1)\n",
            "Epoch 43 iteration 100 Loss: 0.660 | Acc: 65.347% (66/101)\n",
            "Epoch 43 iteration 200 Loss: 0.644 | Acc: 64.179% (129/201)\n",
            "Test accuracy: 0.641791045665741\n",
            "Epoch: 44, Loss: 0.008884833194315434 Updates: 7/10000, Avg Grad: 0.29305803775787354, Threshold: 0.5642914772033691\n",
            "Epoch: 44, Loss: 0.002607605652883649 Updates: 8/11000, Avg Grad: 0.12304400652647018, Threshold: 0.6550953984260559\n",
            "Epoch: 44, Loss: 0.015784507617354393 Updates: 8/12000, Avg Grad: 0.5046176910400391, Threshold: 0.6550953984260559\n",
            "Epoch: 44, Loss: 0.0023432718589901924 Updates: 9/13000, Avg Grad: 0.0885254293680191, Threshold: 0.7230890393257141\n",
            "Epoch: 44, Loss: 0.002588824136182666 Updates: 9/14000, Avg Grad: 0.22334012389183044, Threshold: 0.7230890393257141\n",
            "Epoch: 44, Loss: 0.01605900749564171 Updates: 9/15000, Avg Grad: 0.37288016080856323, Threshold: 0.7230890393257141\n",
            "Epoch: 44, Loss: 0.004514225292950869 Updates: 9/16000, Avg Grad: 0.4423733651638031, Threshold: 0.7230890393257141\n",
            "Epoch: 44, Loss: 0.00626293383538723 Updates: 9/17000, Avg Grad: 0.5141844153404236, Threshold: 0.7230890393257141\n",
            "Epoch: 44, Loss: 0.004193146713078022 Updates: 9/18000, Avg Grad: 0.6888595819473267, Threshold: 0.7230890393257141\n",
            "Epoch: 44, Loss: 0.0045329206623137 Updates: 10/19000, Avg Grad: 0.31347694993019104, Threshold: 0.7597624659538269\n",
            "Epoch 43 iteration 0 Loss: 1.819 | Acc: 0.000% (0/1)\n",
            "Epoch 43 iteration 100 Loss: 0.662 | Acc: 61.386% (62/101)\n",
            "Epoch 43 iteration 200 Loss: 0.638 | Acc: 66.169% (133/201)\n",
            "Test accuracy: 0.6616915464401245\n",
            "Epoch: 44, Loss: 0.004070967901498079 Updates: 10/20000, Avg Grad: 0.685161828994751, Threshold: 0.7597624659538269\n",
            "Epoch: 44, Loss: 0.015250448137521744 Updates: 11/21000, Avg Grad: 0.358865886926651, Threshold: 0.7864013314247131\n",
            "Epoch: 44, Loss: 0.006388614885509014 Updates: 12/22000, Avg Grad: 0.08646794408559799, Threshold: 0.814059853553772\n",
            "Epoch: 44, Loss: 0.001569058047607541 Updates: 12/23000, Avg Grad: 0.5226898789405823, Threshold: 0.814059853553772\n",
            "Epoch: 44, Loss: 0.0020403738599270582 Updates: 13/24000, Avg Grad: 0.0766034871339798, Threshold: 0.839670717716217\n",
            "Epoch: 44, Loss: 0.0017633980605751276 Updates: 13/25000, Avg Grad: 0.31989148259162903, Threshold: 0.839670717716217\n",
            "Epoch: 44, Loss: 0.009045450016856194 Updates: 13/26000, Avg Grad: 0.5570221543312073, Threshold: 0.839670717716217\n",
            "Epoch: 44, Loss: 0.009089287370443344 Updates: 13/27000, Avg Grad: 0.8009052872657776, Threshold: 0.839670717716217\n",
            "Epoch: 44, Loss: 0.008658181875944138 Updates: 14/28000, Avg Grad: 0.218041330575943, Threshold: 0.8492023348808289\n",
            "Epoch: 44, Loss: 0.007630066946148872 Updates: 14/29000, Avg Grad: 0.29430121183395386, Threshold: 0.8492023348808289\n",
            "Epoch 43 iteration 0 Loss: 0.230 | Acc: 100.000% (1/1)\n",
            "Epoch 43 iteration 100 Loss: 0.575 | Acc: 72.277% (73/101)\n",
            "Epoch 43 iteration 200 Loss: 0.617 | Acc: 68.657% (138/201)\n",
            "Test accuracy: 0.6865671873092651\n",
            "Epoch: 44, Loss: 0.013649994507431984 Updates: 14/30000, Avg Grad: 0.5738720297813416, Threshold: 0.8492023348808289\n",
            "Epoch: 44, Loss: 0.00959121622145176 Updates: 14/31000, Avg Grad: 0.794312596321106, Threshold: 0.8492023348808289\n",
            "Epoch: 44, Loss: 0.005031385459005833 Updates: 15/32000, Avg Grad: 0.1305883526802063, Threshold: 0.8671838045120239\n",
            "Epoch 44 iteration 0 Loss: 0.969 | Acc: 0.000% (0/1)\n",
            "Epoch 44 iteration 100 Loss: 0.717 | Acc: 52.475% (53/101)\n",
            "Epoch 44 iteration 200 Loss: 0.729 | Acc: 53.731% (108/201)\n",
            "Test accuracy: 0.5373134613037109\n",
            "Epoch: 45, Loss: 0.38839998841285706 Updates: 0/0, Avg Grad: 0.12183649837970734, Threshold: 0.1340201497077942\n",
            "Epoch: 45, Loss: 0.00211016065441072 Updates: 3/1000, Avg Grad: 0.12365102022886276, Threshold: 0.30540430545806885\n",
            "Epoch: 45, Loss: 0.003476576879620552 Updates: 4/2000, Avg Grad: 0.39772671461105347, Threshold: 0.41882437467575073\n",
            "Epoch: 45, Loss: 0.005791012197732925 Updates: 5/3000, Avg Grad: 0.4007568359375, Threshold: 0.518700897693634\n",
            "Epoch: 45, Loss: 0.00434526614844799 Updates: 6/4000, Avg Grad: 0.1893448829650879, Threshold: 0.5720090270042419\n",
            "Epoch: 45, Loss: 0.003014486515894532 Updates: 6/5000, Avg Grad: 0.4819890260696411, Threshold: 0.5720090270042419\n",
            "Epoch: 45, Loss: 0.005866824183613062 Updates: 7/6000, Avg Grad: 0.036784280091524124, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.008881389163434505 Updates: 7/7000, Avg Grad: 0.08549626171588898, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.0075216819532215595 Updates: 7/8000, Avg Grad: 0.10835777223110199, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.004874358884990215 Updates: 7/9000, Avg Grad: 0.13815267384052277, Threshold: 0.6351593732833862\n",
            "Epoch 44 iteration 0 Loss: 0.292 | Acc: 100.000% (1/1)\n",
            "Epoch 44 iteration 100 Loss: 0.626 | Acc: 68.317% (69/101)\n",
            "Epoch 44 iteration 200 Loss: 0.619 | Acc: 68.159% (137/201)\n",
            "Test accuracy: 0.6815920472145081\n",
            "Epoch: 45, Loss: 0.0034532221034169197 Updates: 7/10000, Avg Grad: 0.20874281227588654, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.009298698976635933 Updates: 7/11000, Avg Grad: 0.2744482457637787, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.00359857315197587 Updates: 7/12000, Avg Grad: 0.333940327167511, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.008351410739123821 Updates: 7/13000, Avg Grad: 0.3948894143104553, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.0055929808877408504 Updates: 7/14000, Avg Grad: 0.4653460383415222, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.004480652045458555 Updates: 7/15000, Avg Grad: 0.5125113129615784, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.004024761728942394 Updates: 7/16000, Avg Grad: 0.5639247894287109, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.004826301708817482 Updates: 7/17000, Avg Grad: 0.618022084236145, Threshold: 0.6351593732833862\n",
            "Epoch: 45, Loss: 0.0041438196785748005 Updates: 8/18000, Avg Grad: 0.20135082304477692, Threshold: 0.6742427945137024\n",
            "Epoch: 45, Loss: 0.006600914057344198 Updates: 8/19000, Avg Grad: 0.4865182042121887, Threshold: 0.6742427945137024\n",
            "Epoch 44 iteration 0 Loss: 0.153 | Acc: 100.000% (1/1)\n",
            "Epoch 44 iteration 100 Loss: 0.596 | Acc: 71.287% (72/101)\n",
            "Epoch 44 iteration 200 Loss: 0.633 | Acc: 65.672% (132/201)\n",
            "Test accuracy: 0.6567164063453674\n",
            "Epoch: 45, Loss: 0.004920572973787785 Updates: 9/20000, Avg Grad: 0.08433566987514496, Threshold: 0.6981245279312134\n",
            "Epoch: 45, Loss: 0.004879727493971586 Updates: 9/21000, Avg Grad: 0.4494171142578125, Threshold: 0.6981245279312134\n",
            "Epoch: 45, Loss: 0.0072718169540166855 Updates: 10/22000, Avg Grad: 0.7135358452796936, Threshold: 0.7306435704231262\n",
            "Epoch: 45, Loss: 0.004085481632500887 Updates: 10/23000, Avg Grad: 0.2879619896411896, Threshold: 0.7306435704231262\n",
            "Epoch: 45, Loss: 0.004760747775435448 Updates: 10/24000, Avg Grad: 0.5193710923194885, Threshold: 0.7306435704231262\n",
            "Epoch: 45, Loss: 0.0029120026156306267 Updates: 11/25000, Avg Grad: 0.016078345477581024, Threshold: 0.7479981184005737\n",
            "Epoch: 45, Loss: 0.0051590148359537125 Updates: 11/26000, Avg Grad: 0.10671553760766983, Threshold: 0.7479981184005737\n",
            "Epoch: 45, Loss: 0.006693804170936346 Updates: 11/27000, Avg Grad: 0.17112140357494354, Threshold: 0.7479981184005737\n",
            "Epoch: 45, Loss: 0.006351265124976635 Updates: 11/28000, Avg Grad: 0.23315377533435822, Threshold: 0.7479981184005737\n",
            "Epoch: 45, Loss: 0.004494054242968559 Updates: 11/29000, Avg Grad: 0.27515143156051636, Threshold: 0.7479981184005737\n",
            "Epoch 44 iteration 0 Loss: 0.641 | Acc: 100.000% (1/1)\n",
            "Epoch 44 iteration 100 Loss: 0.596 | Acc: 72.277% (73/101)\n",
            "Epoch 44 iteration 200 Loss: 0.604 | Acc: 70.149% (141/201)\n",
            "Test accuracy: 0.7014925479888916\n",
            "Epoch: 45, Loss: 0.009128659032285213 Updates: 11/30000, Avg Grad: 0.3326167166233063, Threshold: 0.7479981184005737\n",
            "Epoch: 45, Loss: 0.013433609157800674 Updates: 11/31000, Avg Grad: 0.40661635994911194, Threshold: 0.7479981184005737\n",
            "Epoch: 45, Loss: 0.002276040380820632 Updates: 11/32000, Avg Grad: 0.46407702565193176, Threshold: 0.7479981184005737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = {\n",
        "    \"num_epochs\" : 45,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 1,\n",
        "    'learning_rate' : 0.0008,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.1,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "\n",
        "with open('log_baseline_train2.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test2.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "with open('record.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"update\", \"grad\", \"threshold\"])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "net3 = TweetRNN(50, 50, 2)\n",
        "trainloader = TweetBatcher(train, batch_size=1, drop_last=True)\n",
        "validloader = TweetBatcher(valid, batch_size=1, drop_last=False)\n",
        "testloader = TweetBatcher(test, batch_size=1, drop_last=False)\n",
        "c3f1_loss_per_epoch2, c3f1_update_per_epoch2, c3f1_every_loss2, c3f1_thr_per_batch2 = net_trainer3(net3, trainloader, validloader, train_config)\n",
        "\n",
        "# model = TweetRNN(50, 50, 2)\n",
        "\n",
        "# train_rnn_network(model, train_loader, valid_loader, num_epochs=20, learning_rate=2e-4)\n",
        "# get_accuracy(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blHMOXPVHjDy",
        "outputId": "9e11526e-6727-4749-ebb3-d756ed1dbda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 1.611 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 1.179 | Acc: 40.594% (41/101)\n",
            "Epoch 0 iteration 200 Loss: 1.156 | Acc: 40.299% (81/201)\n",
            "Epoch 0 iteration 300 Loss: 1.071 | Acc: 45.847% (138/301)\n",
            "Epoch 0 iteration 400 Loss: 1.085 | Acc: 44.140% (177/401)\n",
            "Epoch 0 iteration 500 Loss: 1.071 | Acc: 45.709% (229/501)\n",
            "Test accuracy: 0.45708581805229187\n",
            "Epoch: 1, Loss: 0.0014463725965470076 Updates: 1/1, Avg Grad: 0.00040840118890628220\n",
            "Epoch: 1, Loss: 0.002563933841884136 Updates: 6/1001, Avg Grad: 0.039178155362606050\n",
            "Epoch: 1, Loss: 0.002838392276316881 Updates: 11/2001, Avg Grad: 0.0136977145448327060\n",
            "Epoch: 1, Loss: 0.0034989137202501297 Updates: 16/3001, Avg Grad: 0.0133654652163386340\n",
            "Epoch: 1, Loss: 0.0029715404380112886 Updates: 21/4001, Avg Grad: 0.0119735272601246830\n",
            "Epoch: 1, Loss: 0.0032116887159645557 Updates: 26/5001, Avg Grad: 0.0128757860511541370\n",
            "Epoch: 1, Loss: 0.002840035827830434 Updates: 31/6001, Avg Grad: 0.015868144109845160\n",
            "Epoch: 1, Loss: 0.004503329284489155 Updates: 36/7001, Avg Grad: 0.0071842945180833340\n",
            "Epoch: 1, Loss: 0.0025291391648352146 Updates: 41/8001, Avg Grad: 0.0115313073620200160\n",
            "Epoch: 1, Loss: 0.0039537446573376656 Updates: 46/9001, Avg Grad: 0.0074987914413213730\n",
            "Epoch 0 iteration 0 Loss: 0.776 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.717 | Acc: 50.495% (51/101)\n",
            "Epoch 0 iteration 200 Loss: 0.702 | Acc: 54.229% (109/201)\n",
            "Epoch 0 iteration 300 Loss: 0.696 | Acc: 55.482% (167/301)\n",
            "Epoch 0 iteration 400 Loss: 0.681 | Acc: 57.357% (230/401)\n",
            "Epoch 0 iteration 500 Loss: 0.665 | Acc: 60.479% (303/501)\n",
            "Test accuracy: 0.6047903895378113\n",
            "Epoch: 1, Loss: 0.0030034715309739113 Updates: 51/10001, Avg Grad: 0.0141082862392067910\n",
            "Epoch: 1, Loss: 0.004617242608219385 Updates: 56/11001, Avg Grad: 0.009732006117701530\n",
            "Epoch: 1, Loss: 0.0037525040097534657 Updates: 61/12001, Avg Grad: 0.0146618308499455450\n",
            "Epoch: 1, Loss: 0.002018410712480545 Updates: 66/13001, Avg Grad: 0.0084677357226610180\n",
            "Epoch: 1, Loss: 0.0032546804286539555 Updates: 71/14001, Avg Grad: 0.018414547666907310\n",
            "Epoch: 1, Loss: 0.002423891332000494 Updates: 76/15001, Avg Grad: 0.01427786797285080\n",
            "Epoch: 1, Loss: 0.0036810431629419327 Updates: 81/16001, Avg Grad: 0.0168061144649982450\n",
            "Epoch: 1, Loss: 0.0011090872576460242 Updates: 86/17001, Avg Grad: 0.0121474182233214380\n",
            "Epoch: 1, Loss: 0.004893190693110228 Updates: 91/18001, Avg Grad: 0.0087141375988721850\n",
            "Epoch: 1, Loss: 0.0023844537790864706 Updates: 96/19001, Avg Grad: 0.017261540517210960\n",
            "Epoch 0 iteration 0 Loss: 1.043 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.632 | Acc: 69.307% (70/101)\n",
            "Epoch 0 iteration 200 Loss: 0.656 | Acc: 64.677% (130/201)\n",
            "Epoch 0 iteration 300 Loss: 0.638 | Acc: 67.110% (202/301)\n",
            "Epoch 0 iteration 400 Loss: 0.627 | Acc: 66.833% (268/401)\n",
            "Epoch 0 iteration 500 Loss: 0.634 | Acc: 65.070% (326/501)\n",
            "Test accuracy: 0.6506986021995544\n",
            "Epoch: 1, Loss: 0.0056877704337239265 Updates: 101/20001, Avg Grad: 0.0366137214004993440\n",
            "Epoch: 1, Loss: 0.0016105228569358587 Updates: 106/21001, Avg Grad: 0.0130259227007627490\n",
            "Epoch: 1, Loss: 0.0025429341476410627 Updates: 111/22001, Avg Grad: 0.0112111149355769160\n",
            "Epoch: 1, Loss: 0.0021340816747397184 Updates: 116/23001, Avg Grad: 0.0436599887907505040\n",
            "Epoch: 1, Loss: 0.0032988383900374174 Updates: 121/24001, Avg Grad: 0.0260727219283580780\n",
            "Epoch: 1, Loss: 0.0031705647706985474 Updates: 126/25001, Avg Grad: 0.0248186308890581130\n",
            "Epoch: 1, Loss: 0.0015666363760828972 Updates: 131/26001, Avg Grad: 0.0145351476967334750\n",
            "Epoch: 1, Loss: 0.005725364666432142 Updates: 136/27001, Avg Grad: 0.0226261634379625320\n",
            "Epoch: 1, Loss: 0.004365005064755678 Updates: 141/28001, Avg Grad: 0.0083499290049076080\n",
            "Epoch: 1, Loss: 0.0022057003807276487 Updates: 146/29001, Avg Grad: 0.0078145042061805730\n",
            "Epoch 0 iteration 0 Loss: 0.763 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 0.652 | Acc: 68.317% (69/101)\n",
            "Epoch 0 iteration 200 Loss: 0.628 | Acc: 68.657% (138/201)\n",
            "Epoch 0 iteration 300 Loss: 0.645 | Acc: 67.774% (204/301)\n",
            "Epoch 0 iteration 400 Loss: 0.634 | Acc: 67.332% (270/401)\n",
            "Epoch 0 iteration 500 Loss: 0.637 | Acc: 66.068% (331/501)\n",
            "Test accuracy: 0.6606786251068115\n",
            "Epoch: 1, Loss: 0.003035439644008875 Updates: 151/30001, Avg Grad: 0.0088550504297018050\n",
            "Epoch: 1, Loss: 0.006075285840779543 Updates: 156/31001, Avg Grad: 0.016609631478786470\n",
            "Epoch: 1, Loss: 0.006405720487236977 Updates: 161/32001, Avg Grad: 0.012765308842062950\n",
            "Epoch 1 iteration 0 Loss: 1.966 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 0.673 | Acc: 65.347% (66/101)\n",
            "Epoch 1 iteration 200 Loss: 0.654 | Acc: 65.174% (131/201)\n",
            "Epoch 1 iteration 300 Loss: 0.669 | Acc: 63.787% (192/301)\n",
            "Epoch 1 iteration 400 Loss: 0.657 | Acc: 64.339% (258/401)\n",
            "Epoch 1 iteration 500 Loss: 0.642 | Acc: 64.671% (324/501)\n",
            "Test accuracy: 0.6467065811157227\n",
            "Epoch: 2, Loss: 0.003459029598161578 Updates: 1/1, Avg Grad: 0.00087193690706044440\n",
            "Epoch: 2, Loss: 0.0005500501138158143 Updates: 6/1001, Avg Grad: 0.0298125650733709340\n",
            "Epoch: 2, Loss: 0.004082116764038801 Updates: 11/2001, Avg Grad: 0.0145226037129759790\n",
            "Epoch: 2, Loss: 0.004694584757089615 Updates: 16/3001, Avg Grad: 0.017492361366748810\n",
            "Epoch: 2, Loss: 0.0012106563663110137 Updates: 21/4001, Avg Grad: 0.0060412120074033740\n",
            "Epoch: 2, Loss: 0.002728796098381281 Updates: 26/5001, Avg Grad: 0.0249390080571174620\n",
            "Epoch: 2, Loss: 0.0008074158686213195 Updates: 31/6001, Avg Grad: 0.0127589693292975430\n",
            "Epoch: 2, Loss: 0.0025752359069883823 Updates: 36/7001, Avg Grad: 0.0106186438351869580\n",
            "Epoch: 2, Loss: 0.002775210654363036 Updates: 41/8001, Avg Grad: 0.017894415184855460\n",
            "Epoch: 2, Loss: 0.002517139073461294 Updates: 46/9001, Avg Grad: 0.0189747754484415050\n",
            "Epoch 1 iteration 0 Loss: 0.216 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.651 | Acc: 62.376% (63/101)\n",
            "Epoch 1 iteration 200 Loss: 0.666 | Acc: 62.687% (126/201)\n",
            "Epoch 1 iteration 300 Loss: 0.650 | Acc: 65.449% (197/301)\n",
            "Epoch 1 iteration 400 Loss: 0.662 | Acc: 63.591% (255/401)\n",
            "Epoch 1 iteration 500 Loss: 0.662 | Acc: 63.473% (318/501)\n",
            "Test accuracy: 0.6347305178642273\n",
            "Epoch: 2, Loss: 0.005121264606714249 Updates: 51/10001, Avg Grad: 0.0245240814983844760\n",
            "Epoch: 2, Loss: 0.0039648208767175674 Updates: 56/11001, Avg Grad: 0.0277285668998956680\n",
            "Epoch: 2, Loss: 0.00494385976344347 Updates: 61/12001, Avg Grad: 0.0144954230636358260\n",
            "Epoch: 2, Loss: 0.001325672841630876 Updates: 66/13001, Avg Grad: 0.0164778567850589750\n",
            "Epoch: 2, Loss: 0.0009571750997565687 Updates: 71/14001, Avg Grad: 0.0291995033621788020\n",
            "Epoch: 2, Loss: 0.002539923181757331 Updates: 76/15001, Avg Grad: 0.0255736019462347030\n",
            "Epoch: 2, Loss: 0.0026355446316301823 Updates: 81/16001, Avg Grad: 0.0140087008476257320\n",
            "Epoch: 2, Loss: 0.004458969458937645 Updates: 86/17001, Avg Grad: 0.011250969953835010\n",
            "Epoch: 2, Loss: 0.005816568620502949 Updates: 91/18001, Avg Grad: 0.034711342304944990\n",
            "Epoch: 2, Loss: 0.002394634298980236 Updates: 96/19001, Avg Grad: 0.0081585524603724480\n",
            "Epoch 1 iteration 0 Loss: 0.455 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.661 | Acc: 61.386% (62/101)\n",
            "Epoch 1 iteration 200 Loss: 0.653 | Acc: 61.194% (123/201)\n",
            "Epoch 1 iteration 300 Loss: 0.663 | Acc: 59.801% (180/301)\n",
            "Epoch 1 iteration 400 Loss: 0.655 | Acc: 61.097% (245/401)\n",
            "Epoch 1 iteration 500 Loss: 0.661 | Acc: 61.078% (306/501)\n",
            "Test accuracy: 0.6107784509658813\n",
            "Epoch: 2, Loss: 0.004581737797707319 Updates: 101/20001, Avg Grad: 0.0077181090600788590\n",
            "Epoch: 2, Loss: 0.005293866619467735 Updates: 106/21001, Avg Grad: 0.007752893492579460\n",
            "Epoch: 2, Loss: 0.004522564820945263 Updates: 111/22001, Avg Grad: 0.0148811219260096550\n",
            "Epoch: 2, Loss: 0.002582369139418006 Updates: 116/23001, Avg Grad: 0.0098903775215148930\n",
            "Epoch: 2, Loss: 0.0018118901643902063 Updates: 121/24001, Avg Grad: 0.0063395230099558830\n",
            "Epoch: 2, Loss: 0.0013035370502620935 Updates: 126/25001, Avg Grad: 0.0108505496755242350\n",
            "Epoch: 2, Loss: 0.005822513252496719 Updates: 131/26001, Avg Grad: 0.01697588898241520\n",
            "Epoch: 2, Loss: 0.0004589290183503181 Updates: 136/27001, Avg Grad: 0.0092500420287251470\n",
            "Epoch: 2, Loss: 0.0006445984472520649 Updates: 141/28001, Avg Grad: 0.0074113188311457630\n",
            "Epoch: 2, Loss: 0.0039742812514305115 Updates: 146/29001, Avg Grad: 0.01806556247174740\n",
            "Epoch 1 iteration 0 Loss: 0.380 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 0.641 | Acc: 69.307% (70/101)\n",
            "Epoch 1 iteration 200 Loss: 0.645 | Acc: 66.169% (133/201)\n",
            "Epoch 1 iteration 300 Loss: 0.632 | Acc: 67.110% (202/301)\n",
            "Epoch 1 iteration 400 Loss: 0.643 | Acc: 64.838% (260/401)\n",
            "Epoch 1 iteration 500 Loss: 0.643 | Acc: 65.669% (329/501)\n",
            "Test accuracy: 0.6566866040229797\n",
            "Epoch: 2, Loss: 0.005998315755277872 Updates: 151/30001, Avg Grad: 0.0122697893530130390\n",
            "Epoch: 2, Loss: 0.005330340936779976 Updates: 156/31001, Avg Grad: 0.030638368800282480\n",
            "Epoch: 2, Loss: 0.0018510554218664765 Updates: 161/32001, Avg Grad: 0.0122261019423604010\n",
            "Epoch 2 iteration 0 Loss: 1.541 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.697 | Acc: 63.366% (64/101)\n",
            "Epoch 2 iteration 200 Loss: 0.770 | Acc: 59.701% (120/201)\n",
            "Epoch 2 iteration 300 Loss: 0.753 | Acc: 60.133% (181/301)\n",
            "Epoch 2 iteration 400 Loss: 0.728 | Acc: 62.095% (249/401)\n",
            "Epoch 2 iteration 500 Loss: 0.729 | Acc: 62.675% (314/501)\n",
            "Test accuracy: 0.6267465353012085\n",
            "Epoch: 3, Loss: 0.003980815876275301 Updates: 1/1, Avg Grad: 0.00090122275287285450\n",
            "Epoch: 3, Loss: 0.000960514007601887 Updates: 6/1001, Avg Grad: 0.033711623400449750\n",
            "Epoch: 3, Loss: 0.0007593237096443772 Updates: 11/2001, Avg Grad: 0.022579882293939590\n",
            "Epoch: 3, Loss: 0.0044099087826907635 Updates: 16/3001, Avg Grad: 0.0080354996025562290\n",
            "Epoch: 3, Loss: 0.002820167224854231 Updates: 21/4001, Avg Grad: 0.0082216253504157070\n",
            "Epoch: 3, Loss: 0.001354569336399436 Updates: 26/5001, Avg Grad: 0.0162658803164958950\n",
            "Epoch: 3, Loss: 0.0019929828122258186 Updates: 31/6001, Avg Grad: 0.0094197131693363190\n",
            "Epoch: 3, Loss: 0.008390096016228199 Updates: 36/7001, Avg Grad: 0.01658146083354950\n",
            "Epoch: 3, Loss: 0.0029942935798317194 Updates: 41/8001, Avg Grad: 0.026877222582697870\n",
            "Epoch: 3, Loss: 0.0058396970853209496 Updates: 46/9001, Avg Grad: 0.0243757870048284530\n",
            "Epoch 2 iteration 0 Loss: 0.866 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.616 | Acc: 63.366% (64/101)\n",
            "Epoch 2 iteration 200 Loss: 0.621 | Acc: 66.169% (133/201)\n",
            "Epoch 2 iteration 300 Loss: 0.630 | Acc: 64.120% (193/301)\n",
            "Epoch 2 iteration 400 Loss: 0.637 | Acc: 62.594% (251/401)\n",
            "Epoch 2 iteration 500 Loss: 0.637 | Acc: 61.677% (309/501)\n",
            "Test accuracy: 0.6167664527893066\n",
            "Epoch: 3, Loss: 0.00449840584769845 Updates: 51/10001, Avg Grad: 0.0119304619729518890\n",
            "Epoch: 3, Loss: 0.009190937504172325 Updates: 56/11001, Avg Grad: 0.0257314704358577730\n",
            "Epoch: 3, Loss: 0.0018699367064982653 Updates: 61/12001, Avg Grad: 0.0247549060732126240\n",
            "Epoch: 3, Loss: 0.006540196947753429 Updates: 66/13001, Avg Grad: 0.0105912741273641590\n",
            "Epoch: 3, Loss: 0.002072274452075362 Updates: 71/14001, Avg Grad: 0.0249753575772047040\n",
            "Epoch: 3, Loss: 0.004223025869578123 Updates: 76/15001, Avg Grad: 0.0152149247005581860\n",
            "Epoch: 3, Loss: 0.002961121266707778 Updates: 81/16001, Avg Grad: 0.0091431066393852230\n",
            "Epoch: 3, Loss: 0.000606496469117701 Updates: 86/17001, Avg Grad: 0.033246848732233050\n",
            "Epoch: 3, Loss: 0.0043900879099965096 Updates: 91/18001, Avg Grad: 0.0124207595363259320\n",
            "Epoch: 3, Loss: 0.0015205203089863062 Updates: 96/19001, Avg Grad: 0.0114561822265386580\n",
            "Epoch 2 iteration 0 Loss: 0.375 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 100 Loss: 0.625 | Acc: 70.297% (71/101)\n",
            "Epoch 2 iteration 200 Loss: 0.644 | Acc: 64.179% (129/201)\n",
            "Epoch 2 iteration 300 Loss: 0.638 | Acc: 63.123% (190/301)\n",
            "Epoch 2 iteration 400 Loss: 0.631 | Acc: 65.586% (263/401)\n",
            "Epoch 2 iteration 500 Loss: 0.612 | Acc: 67.665% (339/501)\n",
            "Test accuracy: 0.6766467094421387\n",
            "Epoch: 3, Loss: 0.005717702675610781 Updates: 101/20001, Avg Grad: 0.0062864352948963640\n",
            "Epoch: 3, Loss: 0.0011637918651103973 Updates: 106/21001, Avg Grad: 0.0147672668099403380\n",
            "Epoch: 3, Loss: 0.004804298281669617 Updates: 111/22001, Avg Grad: 0.0191100277006626130\n",
            "Epoch: 3, Loss: 0.001142114750109613 Updates: 116/23001, Avg Grad: 0.0185611154884099960\n",
            "Epoch: 3, Loss: 0.00484597310423851 Updates: 121/24001, Avg Grad: 0.0060260053724050520\n",
            "Epoch: 3, Loss: 0.006620418280363083 Updates: 126/25001, Avg Grad: 0.024413455277681350\n",
            "Epoch: 3, Loss: 0.0026325529906898737 Updates: 131/26001, Avg Grad: 0.0072424877434968950\n",
            "Epoch: 3, Loss: 0.0020428162533789873 Updates: 136/27001, Avg Grad: 0.0103039694949984550\n",
            "Epoch: 3, Loss: 0.0060210698284208775 Updates: 141/28001, Avg Grad: 0.0130684068426489830\n",
            "Epoch: 3, Loss: 0.003527354681864381 Updates: 146/29001, Avg Grad: 0.0196090061217546460\n",
            "Epoch 2 iteration 0 Loss: 0.838 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 0.645 | Acc: 63.366% (64/101)\n",
            "Epoch 2 iteration 200 Loss: 0.643 | Acc: 65.174% (131/201)\n",
            "Epoch 2 iteration 300 Loss: 0.649 | Acc: 64.452% (194/301)\n",
            "Epoch 2 iteration 400 Loss: 0.633 | Acc: 66.085% (265/401)\n",
            "Epoch 2 iteration 500 Loss: 0.620 | Acc: 67.665% (339/501)\n",
            "Test accuracy: 0.6766467094421387\n",
            "Epoch: 3, Loss: 0.004420232493430376 Updates: 151/30001, Avg Grad: 0.0110472850501537320\n",
            "Epoch: 3, Loss: 0.0022097951732575893 Updates: 156/31001, Avg Grad: 0.0074247168377041820\n",
            "Epoch: 3, Loss: 0.007736950647085905 Updates: 161/32001, Avg Grad: 0.008194199763238430\n",
            "Epoch 3 iteration 0 Loss: 0.639 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 100 Loss: 0.726 | Acc: 60.396% (61/101)\n",
            "Epoch 3 iteration 200 Loss: 0.673 | Acc: 64.179% (129/201)\n",
            "Epoch 3 iteration 300 Loss: 0.680 | Acc: 64.784% (195/301)\n",
            "Epoch 3 iteration 400 Loss: 0.678 | Acc: 64.589% (259/401)\n",
            "Epoch 3 iteration 500 Loss: 0.685 | Acc: 64.271% (322/501)\n",
            "Test accuracy: 0.6427145600318909\n",
            "Epoch: 4, Loss: 0.0008771908469498158 Updates: 1/1, Avg Grad: 0.0003386702446732670\n",
            "Epoch: 4, Loss: 0.007701222784817219 Updates: 6/1001, Avg Grad: 0.016699584200978280\n",
            "Epoch: 4, Loss: 0.0009795485530048609 Updates: 11/2001, Avg Grad: 0.0145129188895225520\n",
            "Epoch: 4, Loss: 0.0008843485848046839 Updates: 16/3001, Avg Grad: 0.0077394633553922180\n",
            "Epoch: 4, Loss: 0.0070530627854168415 Updates: 21/4001, Avg Grad: 0.0267222933471202850\n",
            "Epoch: 4, Loss: 0.0016290328931063414 Updates: 26/5001, Avg Grad: 0.0261554885655641560\n",
            "Epoch: 4, Loss: 0.0016478607431054115 Updates: 31/6001, Avg Grad: 0.021132381632924080\n",
            "Epoch: 4, Loss: 0.0026417223270982504 Updates: 36/7001, Avg Grad: 0.025336250662803650\n",
            "Epoch: 4, Loss: 0.002046160865575075 Updates: 41/8001, Avg Grad: 0.0145269576460123060\n",
            "Epoch: 4, Loss: 0.0007241565035656095 Updates: 46/9001, Avg Grad: 0.0066988803446292880\n",
            "Epoch 3 iteration 0 Loss: 0.274 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 100 Loss: 0.655 | Acc: 65.347% (66/101)\n",
            "Epoch 3 iteration 200 Loss: 0.673 | Acc: 64.179% (129/201)\n",
            "Epoch 3 iteration 300 Loss: 0.652 | Acc: 65.116% (196/301)\n",
            "Epoch 3 iteration 400 Loss: 0.645 | Acc: 65.835% (264/401)\n",
            "Epoch 3 iteration 500 Loss: 0.631 | Acc: 67.066% (336/501)\n",
            "Test accuracy: 0.6706587076187134\n",
            "Epoch: 4, Loss: 0.0018634290900081396 Updates: 51/10001, Avg Grad: 0.0084395455196499820\n",
            "Epoch: 4, Loss: 0.003664561314508319 Updates: 56/11001, Avg Grad: 0.0110957119613885880\n",
            "Epoch: 4, Loss: 0.0030936473049223423 Updates: 61/12001, Avg Grad: 0.01963685452938080\n",
            "Epoch: 4, Loss: 0.010046265088021755 Updates: 66/13001, Avg Grad: 0.0153364352881908420\n",
            "Epoch: 4, Loss: 0.0027302680537104607 Updates: 71/14001, Avg Grad: 0.021723311394453050\n",
            "Epoch: 4, Loss: 0.007774182595312595 Updates: 76/15001, Avg Grad: 0.0102279512211680410\n",
            "Epoch: 4, Loss: 0.0070882230065763 Updates: 81/16001, Avg Grad: 0.0132906315848231320\n",
            "Epoch: 4, Loss: 0.002340934472158551 Updates: 86/17001, Avg Grad: 0.020422341302037240\n",
            "Epoch: 4, Loss: 0.0006894689286127687 Updates: 91/18001, Avg Grad: 0.018713254481554030\n",
            "Epoch: 4, Loss: 0.008984386920928955 Updates: 96/19001, Avg Grad: 0.017574053257703780\n",
            "Epoch 3 iteration 0 Loss: 1.489 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.694 | Acc: 63.366% (64/101)\n",
            "Epoch 3 iteration 200 Loss: 0.632 | Acc: 68.657% (138/201)\n",
            "Epoch 3 iteration 300 Loss: 0.606 | Acc: 69.435% (209/301)\n",
            "Epoch 3 iteration 400 Loss: 0.596 | Acc: 70.324% (282/401)\n",
            "Epoch 3 iteration 500 Loss: 0.606 | Acc: 69.461% (348/501)\n",
            "Test accuracy: 0.6946107745170593\n",
            "Epoch: 4, Loss: 0.001144970883615315 Updates: 101/20001, Avg Grad: 0.0134843774139881130\n",
            "Epoch: 4, Loss: 0.0016757261473685503 Updates: 106/21001, Avg Grad: 0.0123182544484734540\n",
            "Epoch: 4, Loss: 0.0050855837762355804 Updates: 111/22001, Avg Grad: 0.0182020068168640140\n",
            "Epoch: 4, Loss: 0.0013396331341937184 Updates: 116/23001, Avg Grad: 0.0078823380172252660\n",
            "Epoch: 4, Loss: 0.0016654718201607466 Updates: 121/24001, Avg Grad: 0.0108303399756550790\n",
            "Epoch: 4, Loss: 0.004868121352046728 Updates: 126/25001, Avg Grad: 0.0106131648644804950\n",
            "Epoch: 4, Loss: 0.004136768169701099 Updates: 131/26001, Avg Grad: 0.0107257394120097160\n",
            "Epoch: 4, Loss: 0.004223353695124388 Updates: 136/27001, Avg Grad: 0.0142695521935820580\n",
            "Epoch: 4, Loss: 0.0038670729845762253 Updates: 141/28001, Avg Grad: 0.0160180851817131040\n",
            "Epoch: 4, Loss: 0.0032169255428016186 Updates: 146/29001, Avg Grad: 0.0123491231352090840\n",
            "Epoch 3 iteration 0 Loss: 0.846 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 0.604 | Acc: 68.317% (69/101)\n",
            "Epoch 3 iteration 200 Loss: 0.652 | Acc: 66.169% (133/201)\n",
            "Epoch 3 iteration 300 Loss: 0.662 | Acc: 65.449% (197/301)\n",
            "Epoch 3 iteration 400 Loss: 0.641 | Acc: 68.080% (273/401)\n",
            "Epoch 3 iteration 500 Loss: 0.630 | Acc: 68.064% (341/501)\n",
            "Test accuracy: 0.6806387305259705\n",
            "Epoch: 4, Loss: 0.0053596957586705685 Updates: 151/30001, Avg Grad: 0.0120828207582235340\n",
            "Epoch: 4, Loss: 0.0016128792194649577 Updates: 156/31001, Avg Grad: 0.0170613210648298260\n",
            "Epoch: 4, Loss: 0.002903390908613801 Updates: 161/32001, Avg Grad: 0.0142715778201818470\n",
            "Epoch 4 iteration 0 Loss: 2.095 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.758 | Acc: 57.426% (58/101)\n",
            "Epoch 4 iteration 200 Loss: 0.680 | Acc: 60.199% (121/201)\n",
            "Epoch 4 iteration 300 Loss: 0.692 | Acc: 59.468% (179/301)\n",
            "Epoch 4 iteration 400 Loss: 0.731 | Acc: 57.606% (231/401)\n",
            "Epoch 4 iteration 500 Loss: 0.711 | Acc: 58.483% (293/501)\n",
            "Test accuracy: 0.5848303437232971\n",
            "Epoch: 5, Loss: 0.006657688412815332 Updates: 1/1, Avg Grad: 0.0012694196775555610\n",
            "Epoch: 5, Loss: 0.0021134684793651104 Updates: 6/1001, Avg Grad: 0.0120813976973295210\n",
            "Epoch: 5, Loss: 0.0023607828188687563 Updates: 11/2001, Avg Grad: 0.019116330891847610\n",
            "Epoch: 5, Loss: 0.005812118761241436 Updates: 16/3001, Avg Grad: 0.0167042631655931470\n",
            "Epoch: 5, Loss: 0.0035069608129560947 Updates: 21/4001, Avg Grad: 0.0290977284312248230\n",
            "Epoch: 5, Loss: 0.008469541557133198 Updates: 26/5001, Avg Grad: 0.0087380819022655490\n",
            "Epoch: 5, Loss: 0.0004410117107909173 Updates: 31/6001, Avg Grad: 0.0079987924546003340\n",
            "Epoch: 5, Loss: 0.003843932645395398 Updates: 36/7001, Avg Grad: 0.0072302450425922870\n",
            "Epoch: 5, Loss: 0.0033232602290809155 Updates: 41/8001, Avg Grad: 0.0109386434778571130\n",
            "Epoch: 5, Loss: 0.0022224499844014645 Updates: 46/9001, Avg Grad: 0.017931427806615830\n",
            "Epoch 4 iteration 0 Loss: 1.320 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.696 | Acc: 60.396% (61/101)\n",
            "Epoch 4 iteration 200 Loss: 0.713 | Acc: 55.721% (112/201)\n",
            "Epoch 4 iteration 300 Loss: 0.666 | Acc: 60.797% (183/301)\n",
            "Epoch 4 iteration 400 Loss: 0.652 | Acc: 62.344% (250/401)\n",
            "Epoch 4 iteration 500 Loss: 0.637 | Acc: 63.273% (317/501)\n",
            "Test accuracy: 0.6327345371246338\n",
            "Epoch: 5, Loss: 0.00037092698039487004 Updates: 51/10001, Avg Grad: 0.0090896822512149810\n",
            "Epoch: 5, Loss: 0.0014291219413280487 Updates: 56/11001, Avg Grad: 0.0161361861974000930\n",
            "Epoch: 5, Loss: 0.0020234629046171904 Updates: 61/12001, Avg Grad: 0.0101583525538444520\n",
            "Epoch: 5, Loss: 0.005180856212973595 Updates: 66/13001, Avg Grad: 0.0269865412265062330\n",
            "Epoch: 5, Loss: 0.0021823919378221035 Updates: 71/14001, Avg Grad: 0.0184795446693897250\n",
            "Epoch: 5, Loss: 0.0012240582145750523 Updates: 76/15001, Avg Grad: 0.019115556031465530\n",
            "Epoch: 5, Loss: 0.0004698925476986915 Updates: 81/16001, Avg Grad: 0.0125276995822787280\n",
            "Epoch: 5, Loss: 0.0036252387799322605 Updates: 86/17001, Avg Grad: 0.0137148639187216760\n",
            "Epoch: 5, Loss: 0.0011197230778634548 Updates: 91/18001, Avg Grad: 0.014306182041764260\n",
            "Epoch: 5, Loss: 0.0037523284554481506 Updates: 96/19001, Avg Grad: 0.0155464624986052510\n",
            "Epoch 4 iteration 0 Loss: 0.775 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.646 | Acc: 62.376% (63/101)\n",
            "Epoch 4 iteration 200 Loss: 0.618 | Acc: 68.657% (138/201)\n",
            "Epoch 4 iteration 300 Loss: 0.610 | Acc: 70.100% (211/301)\n",
            "Epoch 4 iteration 400 Loss: 0.601 | Acc: 70.574% (283/401)\n",
            "Epoch 4 iteration 500 Loss: 0.603 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 5, Loss: 0.001136268605478108 Updates: 101/20001, Avg Grad: 0.0152175836265087130\n",
            "Epoch: 5, Loss: 0.0019724282901734114 Updates: 106/21001, Avg Grad: 0.0128631750121712680\n",
            "Epoch: 5, Loss: 0.0025305352173745632 Updates: 111/22001, Avg Grad: 0.0233107917010784150\n",
            "Epoch: 5, Loss: 0.0014655705308541656 Updates: 116/23001, Avg Grad: 0.0138533739373087880\n",
            "Epoch: 5, Loss: 0.0038010512944310904 Updates: 121/24001, Avg Grad: 0.021370898932218550\n",
            "Epoch: 5, Loss: 0.0007407946395687759 Updates: 126/25001, Avg Grad: 0.0219999868422746660\n",
            "Epoch: 5, Loss: 0.0017792676808312535 Updates: 131/26001, Avg Grad: 0.021271890029311180\n",
            "Epoch: 5, Loss: 0.0072524272836744785 Updates: 136/27001, Avg Grad: 0.0111616281792521480\n",
            "Epoch: 5, Loss: 0.0016724992310628295 Updates: 141/28001, Avg Grad: 0.014894842170178890\n",
            "Epoch: 5, Loss: 0.002976421732455492 Updates: 146/29001, Avg Grad: 0.0189917739480733870\n",
            "Epoch 4 iteration 0 Loss: 1.404 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 0.593 | Acc: 73.267% (74/101)\n",
            "Epoch 4 iteration 200 Loss: 0.637 | Acc: 69.652% (140/201)\n",
            "Epoch 4 iteration 300 Loss: 0.654 | Acc: 66.777% (201/301)\n",
            "Epoch 4 iteration 400 Loss: 0.632 | Acc: 68.080% (273/401)\n",
            "Epoch 4 iteration 500 Loss: 0.632 | Acc: 67.864% (340/501)\n",
            "Test accuracy: 0.6786426901817322\n",
            "Epoch: 5, Loss: 0.0018711293814703822 Updates: 151/30001, Avg Grad: 0.0138792917132377620\n",
            "Epoch: 5, Loss: 0.0032222114969044924 Updates: 156/31001, Avg Grad: 0.0173701830208301540\n",
            "Epoch: 5, Loss: 0.0026992587372660637 Updates: 161/32001, Avg Grad: 0.0095265842974185940\n",
            "Epoch 5 iteration 0 Loss: 0.564 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.653 | Acc: 64.356% (65/101)\n",
            "Epoch 5 iteration 200 Loss: 0.586 | Acc: 71.642% (144/201)\n",
            "Epoch 5 iteration 300 Loss: 0.569 | Acc: 72.757% (219/301)\n",
            "Epoch 5 iteration 400 Loss: 0.599 | Acc: 70.324% (282/401)\n",
            "Epoch 5 iteration 500 Loss: 0.600 | Acc: 70.060% (351/501)\n",
            "Test accuracy: 0.7005987763404846\n",
            "Epoch: 6, Loss: 0.005685725715011358 Updates: 1/1, Avg Grad: 0.00140214560087770220\n",
            "Epoch: 6, Loss: 0.002787249628454447 Updates: 6/1001, Avg Grad: 0.021128892898559570\n",
            "Epoch: 6, Loss: 0.003149741794914007 Updates: 11/2001, Avg Grad: 0.008832683786749840\n",
            "Epoch: 6, Loss: 0.0012638280168175697 Updates: 16/3001, Avg Grad: 0.0112336631864309310\n",
            "Epoch: 6, Loss: 0.0014224316691979766 Updates: 21/4001, Avg Grad: 0.0143756652250885960\n",
            "Epoch: 6, Loss: 0.004949275404214859 Updates: 26/5001, Avg Grad: 0.0427371896803379060\n",
            "Epoch: 6, Loss: 0.0010928866686299443 Updates: 31/6001, Avg Grad: 0.0064869089983403680\n",
            "Epoch: 6, Loss: 0.0011807993287220597 Updates: 36/7001, Avg Grad: 0.0087546054273843770\n",
            "Epoch: 6, Loss: 0.0065899984911084175 Updates: 41/8001, Avg Grad: 0.0135460915043950080\n",
            "Epoch: 6, Loss: 0.0019745398312807083 Updates: 46/9001, Avg Grad: 0.0164565965533256530\n",
            "Epoch 5 iteration 0 Loss: 0.693 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.595 | Acc: 67.327% (68/101)\n",
            "Epoch 5 iteration 200 Loss: 0.602 | Acc: 66.667% (134/201)\n",
            "Epoch 5 iteration 300 Loss: 0.605 | Acc: 66.445% (200/301)\n",
            "Epoch 5 iteration 400 Loss: 0.615 | Acc: 65.586% (263/401)\n",
            "Epoch 5 iteration 500 Loss: 0.611 | Acc: 66.667% (334/501)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 6, Loss: 0.0046955617144703865 Updates: 51/10001, Avg Grad: 0.023197019472718240\n",
            "Epoch: 6, Loss: 0.001064467360265553 Updates: 56/11001, Avg Grad: 0.0239234194159507750\n",
            "Epoch: 6, Loss: 0.0010069512063637376 Updates: 61/12001, Avg Grad: 0.029426207765936850\n",
            "Epoch: 6, Loss: 0.001599566312506795 Updates: 66/13001, Avg Grad: 0.0085989860817790030\n",
            "Epoch: 6, Loss: 0.003994460683315992 Updates: 71/14001, Avg Grad: 0.0119092455133795740\n",
            "Epoch: 6, Loss: 0.0019713109359145164 Updates: 76/15001, Avg Grad: 0.0375826023519039150\n",
            "Epoch: 6, Loss: 0.0020483890548348427 Updates: 81/16001, Avg Grad: 0.02714647538959980\n",
            "Epoch: 6, Loss: 0.0006501952302642167 Updates: 86/17001, Avg Grad: 0.0112645160406827930\n",
            "Epoch: 6, Loss: 0.005432952661067247 Updates: 91/18001, Avg Grad: 0.0083325281739234920\n",
            "Epoch: 6, Loss: 0.0034475158900022507 Updates: 96/19001, Avg Grad: 0.0133831156417727470\n",
            "Epoch 5 iteration 0 Loss: 0.560 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.557 | Acc: 72.277% (73/101)\n",
            "Epoch 5 iteration 200 Loss: 0.578 | Acc: 70.149% (141/201)\n",
            "Epoch 5 iteration 300 Loss: 0.604 | Acc: 67.774% (204/301)\n",
            "Epoch 5 iteration 400 Loss: 0.612 | Acc: 67.332% (270/401)\n",
            "Epoch 5 iteration 500 Loss: 0.606 | Acc: 68.064% (341/501)\n",
            "Test accuracy: 0.6806387305259705\n",
            "Epoch: 6, Loss: 0.005308225750923157 Updates: 101/20001, Avg Grad: 0.0083334213122725490\n",
            "Epoch: 6, Loss: 0.00871669314801693 Updates: 106/21001, Avg Grad: 0.0110153099521994590\n",
            "Epoch: 6, Loss: 0.0012202473590150476 Updates: 111/22001, Avg Grad: 0.0299613885581493380\n",
            "Epoch: 6, Loss: 0.0019037468591704965 Updates: 116/23001, Avg Grad: 0.0147173879668116570\n",
            "Epoch: 6, Loss: 0.006540773436427116 Updates: 121/24001, Avg Grad: 0.0172765143215656280\n",
            "Epoch: 6, Loss: 0.0005451588076539338 Updates: 126/25001, Avg Grad: 0.0097798136994242670\n",
            "Epoch: 6, Loss: 0.0012160545447841287 Updates: 131/26001, Avg Grad: 0.0211262460798025130\n",
            "Epoch: 6, Loss: 0.0018495874246582389 Updates: 136/27001, Avg Grad: 0.019627381116151810\n",
            "Epoch: 6, Loss: 0.0005198596627451479 Updates: 141/28001, Avg Grad: 0.0182624980807304380\n",
            "Epoch: 6, Loss: 0.0035639642737805843 Updates: 146/29001, Avg Grad: 0.0165850259363651280\n",
            "Epoch 5 iteration 0 Loss: 0.168 | Acc: 100.000% (1/1)\n",
            "Epoch 5 iteration 100 Loss: 0.615 | Acc: 70.297% (71/101)\n",
            "Epoch 5 iteration 200 Loss: 0.593 | Acc: 71.144% (143/201)\n",
            "Epoch 5 iteration 300 Loss: 0.593 | Acc: 71.429% (215/301)\n",
            "Epoch 5 iteration 400 Loss: 0.609 | Acc: 68.579% (275/401)\n",
            "Epoch 5 iteration 500 Loss: 0.594 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 6, Loss: 0.0036650679539889097 Updates: 151/30001, Avg Grad: 0.0084580434486269950\n",
            "Epoch: 6, Loss: 0.0027164206840097904 Updates: 156/31001, Avg Grad: 0.0175757855176925660\n",
            "Epoch: 6, Loss: 0.005553603172302246 Updates: 161/32001, Avg Grad: 0.00617614854127168660\n",
            "Epoch 6 iteration 0 Loss: 0.862 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.678 | Acc: 61.386% (62/101)\n",
            "Epoch 6 iteration 200 Loss: 0.638 | Acc: 66.169% (133/201)\n",
            "Epoch 6 iteration 300 Loss: 0.631 | Acc: 66.113% (199/301)\n",
            "Epoch 6 iteration 400 Loss: 0.633 | Acc: 66.085% (265/401)\n",
            "Epoch 6 iteration 500 Loss: 0.638 | Acc: 66.068% (331/501)\n",
            "Test accuracy: 0.6606786251068115\n",
            "Epoch: 7, Loss: 0.0008714384748600423 Updates: 1/1, Avg Grad: 0.00035721671883948150\n",
            "Epoch: 7, Loss: 0.002603469882160425 Updates: 6/1001, Avg Grad: 0.017752198502421380\n",
            "Epoch: 7, Loss: 0.0014232249232009053 Updates: 11/2001, Avg Grad: 0.0150075200945138930\n",
            "Epoch: 7, Loss: 0.001049591926857829 Updates: 16/3001, Avg Grad: 0.0078403102234005930\n",
            "Epoch: 7, Loss: 0.0006872552912682295 Updates: 21/4001, Avg Grad: 0.0110148377716541290\n",
            "Epoch: 7, Loss: 0.0012981654144823551 Updates: 26/5001, Avg Grad: 0.0105152055621147160\n",
            "Epoch: 7, Loss: 0.002804124727845192 Updates: 31/6001, Avg Grad: 0.020763650536537170\n",
            "Epoch: 7, Loss: 0.005630037747323513 Updates: 36/7001, Avg Grad: 0.0102762924507260320\n",
            "Epoch: 7, Loss: 0.003738577011972666 Updates: 41/8001, Avg Grad: 0.0090767024084925650\n",
            "Epoch: 7, Loss: 0.0039931610226631165 Updates: 46/9001, Avg Grad: 0.0163831636309623720\n",
            "Epoch 6 iteration 0 Loss: 1.010 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.675 | Acc: 61.386% (62/101)\n",
            "Epoch 6 iteration 200 Loss: 0.648 | Acc: 62.687% (126/201)\n",
            "Epoch 6 iteration 300 Loss: 0.644 | Acc: 62.791% (189/301)\n",
            "Epoch 6 iteration 400 Loss: 0.653 | Acc: 62.843% (252/401)\n",
            "Epoch 6 iteration 500 Loss: 0.657 | Acc: 62.675% (314/501)\n",
            "Test accuracy: 0.6267465353012085\n",
            "Epoch: 7, Loss: 0.0024140505120158195 Updates: 51/10001, Avg Grad: 0.0077455695718526840\n",
            "Epoch: 7, Loss: 0.001794168259948492 Updates: 56/11001, Avg Grad: 0.0096739465370774270\n",
            "Epoch: 7, Loss: 0.0031025714706629515 Updates: 61/12001, Avg Grad: 0.0141946682706475260\n",
            "Epoch: 7, Loss: 0.0016334950923919678 Updates: 66/13001, Avg Grad: 0.0155816962942481040\n",
            "Epoch: 7, Loss: 0.003180702915415168 Updates: 71/14001, Avg Grad: 0.0085893748328089710\n",
            "Epoch: 7, Loss: 0.0015196187887340784 Updates: 76/15001, Avg Grad: 0.0141051756218075750\n",
            "Epoch: 7, Loss: 0.007324857171624899 Updates: 81/16001, Avg Grad: 0.0146927163004875180\n",
            "Epoch: 7, Loss: 0.00215613073669374 Updates: 86/17001, Avg Grad: 0.010061765089631080\n",
            "Epoch: 7, Loss: 0.00262663746252656 Updates: 91/18001, Avg Grad: 0.0130851389840245250\n",
            "Epoch: 7, Loss: 0.006104240193963051 Updates: 96/19001, Avg Grad: 0.0085195507854223250\n",
            "Epoch 6 iteration 0 Loss: 0.701 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.639 | Acc: 61.386% (62/101)\n",
            "Epoch 6 iteration 200 Loss: 0.696 | Acc: 56.716% (114/201)\n",
            "Epoch 6 iteration 300 Loss: 0.651 | Acc: 61.794% (186/301)\n",
            "Epoch 6 iteration 400 Loss: 0.633 | Acc: 65.087% (261/401)\n",
            "Epoch 6 iteration 500 Loss: 0.638 | Acc: 65.269% (327/501)\n",
            "Test accuracy: 0.652694582939148\n",
            "Epoch: 7, Loss: 0.00819401815533638 Updates: 101/20001, Avg Grad: 0.0272004716098308560\n",
            "Epoch: 7, Loss: 0.0010740939760580659 Updates: 106/21001, Avg Grad: 0.0150345731526613240\n",
            "Epoch: 7, Loss: 0.0012799478136003017 Updates: 111/22001, Avg Grad: 0.0103354267776012420\n",
            "Epoch: 7, Loss: 0.003041271585971117 Updates: 116/23001, Avg Grad: 0.01143464446067810\n",
            "Epoch: 7, Loss: 0.003221900202333927 Updates: 121/24001, Avg Grad: 0.0302630662918090820\n",
            "Epoch: 7, Loss: 0.004067271947860718 Updates: 126/25001, Avg Grad: 0.0136573361232876780\n",
            "Epoch: 7, Loss: 0.0004946035332977772 Updates: 131/26001, Avg Grad: 0.018499534577131270\n",
            "Epoch: 7, Loss: 0.0031101717613637447 Updates: 136/27001, Avg Grad: 0.0132112680003046990\n",
            "Epoch: 7, Loss: 0.0013972623273730278 Updates: 141/28001, Avg Grad: 0.0101868715137243270\n",
            "Epoch: 7, Loss: 0.0051439618691802025 Updates: 146/29001, Avg Grad: 0.0133589114993810650\n",
            "Epoch 6 iteration 0 Loss: 1.013 | Acc: 0.000% (0/1)\n",
            "Epoch 6 iteration 100 Loss: 0.618 | Acc: 68.317% (69/101)\n",
            "Epoch 6 iteration 200 Loss: 0.641 | Acc: 65.174% (131/201)\n",
            "Epoch 6 iteration 300 Loss: 0.650 | Acc: 64.120% (193/301)\n",
            "Epoch 6 iteration 400 Loss: 0.657 | Acc: 63.840% (256/401)\n",
            "Epoch 6 iteration 500 Loss: 0.642 | Acc: 64.671% (324/501)\n",
            "Test accuracy: 0.6467065811157227\n",
            "Epoch: 7, Loss: 0.006513414438813925 Updates: 151/30001, Avg Grad: 0.0149323344230651860\n",
            "Epoch: 7, Loss: 0.001391156343743205 Updates: 156/31001, Avg Grad: 0.02076261118054390\n",
            "Epoch: 7, Loss: 0.009057210758328438 Updates: 161/32001, Avg Grad: 0.0127188395708799360\n",
            "Epoch 7 iteration 0 Loss: 0.349 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.676 | Acc: 62.376% (63/101)\n",
            "Epoch 7 iteration 200 Loss: 0.639 | Acc: 67.164% (135/201)\n",
            "Epoch 7 iteration 300 Loss: 0.640 | Acc: 65.781% (198/301)\n",
            "Epoch 7 iteration 400 Loss: 0.651 | Acc: 65.337% (262/401)\n",
            "Epoch 7 iteration 500 Loss: 0.648 | Acc: 65.269% (327/501)\n",
            "Test accuracy: 0.652694582939148\n",
            "Epoch: 8, Loss: 0.005887787323445082 Updates: 1/1, Avg Grad: 0.00146332103759050370\n",
            "Epoch: 8, Loss: 0.005982739385217428 Updates: 6/1001, Avg Grad: 0.0096317837014794350\n",
            "Epoch: 8, Loss: 0.0013683652505278587 Updates: 11/2001, Avg Grad: 0.016060626134276390\n",
            "Epoch: 8, Loss: 0.0019456829177215695 Updates: 16/3001, Avg Grad: 0.0296560432761907580\n",
            "Epoch: 8, Loss: 0.004277379717677832 Updates: 21/4001, Avg Grad: 0.0155717814341187480\n",
            "Epoch: 8, Loss: 0.002016812562942505 Updates: 26/5001, Avg Grad: 0.0187508184462785720\n",
            "Epoch: 8, Loss: 0.0019289972260594368 Updates: 31/6001, Avg Grad: 0.0176472421735525130\n",
            "Epoch: 8, Loss: 0.005225458648055792 Updates: 36/7001, Avg Grad: 0.0163326747715473180\n",
            "Epoch: 8, Loss: 0.0009733869228512049 Updates: 41/8001, Avg Grad: 0.0130274090915918350\n",
            "Epoch: 8, Loss: 0.0017401250079274178 Updates: 46/9001, Avg Grad: 0.016694547608494760\n",
            "Epoch 7 iteration 0 Loss: 0.301 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.590 | Acc: 69.307% (70/101)\n",
            "Epoch 7 iteration 200 Loss: 0.599 | Acc: 66.667% (134/201)\n",
            "Epoch 7 iteration 300 Loss: 0.619 | Acc: 64.120% (193/301)\n",
            "Epoch 7 iteration 400 Loss: 0.609 | Acc: 66.085% (265/401)\n",
            "Epoch 7 iteration 500 Loss: 0.606 | Acc: 66.866% (335/501)\n",
            "Test accuracy: 0.6686626672744751\n",
            "Epoch: 8, Loss: 0.0004306042683310807 Updates: 51/10001, Avg Grad: 0.015259755775332450\n",
            "Epoch: 8, Loss: 0.000668083259370178 Updates: 56/11001, Avg Grad: 0.0122362086549401280\n",
            "Epoch: 8, Loss: 0.0017029541777446866 Updates: 61/12001, Avg Grad: 0.0059073036536574360\n",
            "Epoch: 8, Loss: 0.0014978803228586912 Updates: 66/13001, Avg Grad: 0.0233057308942079540\n",
            "Epoch: 8, Loss: 0.005450908560305834 Updates: 71/14001, Avg Grad: 0.0213347505778074260\n",
            "Epoch: 8, Loss: 0.004803745076060295 Updates: 76/15001, Avg Grad: 0.0228322669863700870\n",
            "Epoch: 8, Loss: 0.0006579277687706053 Updates: 81/16001, Avg Grad: 0.011336931027472020\n",
            "Epoch: 8, Loss: 0.0031063389033079147 Updates: 86/17001, Avg Grad: 0.011185045354068280\n",
            "Epoch: 8, Loss: 0.006792367901653051 Updates: 91/18001, Avg Grad: 0.010251700878143310\n",
            "Epoch: 8, Loss: 0.002086281543597579 Updates: 96/19001, Avg Grad: 0.0080737695097923280\n",
            "Epoch 7 iteration 0 Loss: 0.087 | Acc: 100.000% (1/1)\n",
            "Epoch 7 iteration 100 Loss: 0.623 | Acc: 62.376% (63/101)\n",
            "Epoch 7 iteration 200 Loss: 0.633 | Acc: 64.677% (130/201)\n",
            "Epoch 7 iteration 300 Loss: 0.632 | Acc: 63.787% (192/301)\n",
            "Epoch 7 iteration 400 Loss: 0.650 | Acc: 62.843% (252/401)\n",
            "Epoch 7 iteration 500 Loss: 0.639 | Acc: 63.673% (319/501)\n",
            "Test accuracy: 0.6367265582084656\n",
            "Epoch: 8, Loss: 0.0013670142507180572 Updates: 101/20001, Avg Grad: 0.0054600764997303490\n",
            "Epoch: 8, Loss: 0.005764718167483807 Updates: 106/21001, Avg Grad: 0.0126635525375604630\n",
            "Epoch: 8, Loss: 0.0007900111377239227 Updates: 111/22001, Avg Grad: 0.0114188566803932190\n",
            "Epoch: 8, Loss: 0.0008822556701488793 Updates: 116/23001, Avg Grad: 0.012422375380992890\n",
            "Epoch: 8, Loss: 0.002323197666555643 Updates: 121/24001, Avg Grad: 0.0286089330911636350\n",
            "Epoch: 8, Loss: 0.001852388959378004 Updates: 126/25001, Avg Grad: 0.0116579262539744380\n",
            "Epoch: 8, Loss: 0.00870224554091692 Updates: 131/26001, Avg Grad: 0.010431490838527680\n",
            "Epoch: 8, Loss: 0.0026495070196688175 Updates: 136/27001, Avg Grad: 0.0183309279382228850\n",
            "Epoch: 8, Loss: 0.0011994425440207124 Updates: 141/28001, Avg Grad: 0.017945088446140290\n",
            "Epoch: 8, Loss: 0.009358616545796394 Updates: 146/29001, Avg Grad: 0.015083254314959050\n",
            "Epoch 7 iteration 0 Loss: 0.886 | Acc: 0.000% (0/1)\n",
            "Epoch 7 iteration 100 Loss: 0.646 | Acc: 67.327% (68/101)\n",
            "Epoch 7 iteration 200 Loss: 0.629 | Acc: 68.159% (137/201)\n",
            "Epoch 7 iteration 300 Loss: 0.634 | Acc: 66.777% (201/301)\n",
            "Epoch 7 iteration 400 Loss: 0.648 | Acc: 64.589% (259/401)\n",
            "Epoch 7 iteration 500 Loss: 0.644 | Acc: 65.269% (327/501)\n",
            "Test accuracy: 0.652694582939148\n",
            "Epoch: 8, Loss: 0.0033004835713654757 Updates: 151/30001, Avg Grad: 0.0076656057499349120\n",
            "Epoch: 8, Loss: 0.002443406032398343 Updates: 156/31001, Avg Grad: 0.025137448683381080\n",
            "Epoch: 8, Loss: 0.002312628086656332 Updates: 161/32001, Avg Grad: 0.0097468448802828790\n",
            "Epoch 8 iteration 0 Loss: 0.285 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.651 | Acc: 65.347% (66/101)\n",
            "Epoch 8 iteration 200 Loss: 0.657 | Acc: 63.682% (128/201)\n",
            "Epoch 8 iteration 300 Loss: 0.631 | Acc: 64.784% (195/301)\n",
            "Epoch 8 iteration 400 Loss: 0.619 | Acc: 66.584% (267/401)\n",
            "Epoch 8 iteration 500 Loss: 0.619 | Acc: 67.066% (336/501)\n",
            "Test accuracy: 0.6706587076187134\n",
            "Epoch: 9, Loss: 0.007088329643011093 Updates: 1/1, Avg Grad: 0.00135862233582884070\n",
            "Epoch: 9, Loss: 0.0014304174110293388 Updates: 6/1001, Avg Grad: 0.0237234551459550860\n",
            "Epoch: 9, Loss: 0.0011246894719079137 Updates: 11/2001, Avg Grad: 0.018021091818809510\n",
            "Epoch: 9, Loss: 0.006405127700418234 Updates: 16/3001, Avg Grad: 0.025337655097246170\n",
            "Epoch: 9, Loss: 0.002783603733405471 Updates: 21/4001, Avg Grad: 0.0180069636553525920\n",
            "Epoch: 9, Loss: 0.0008561257855035365 Updates: 26/5001, Avg Grad: 0.0141566367819905280\n",
            "Epoch: 9, Loss: 0.0007165800780057907 Updates: 31/6001, Avg Grad: 0.0183536689728498460\n",
            "Epoch: 9, Loss: 0.0037638749927282333 Updates: 36/7001, Avg Grad: 0.01857455074787140\n",
            "Epoch: 9, Loss: 0.0024787879083305597 Updates: 41/8001, Avg Grad: 0.0423381105065345760\n",
            "Epoch: 9, Loss: 0.00250835414044559 Updates: 46/9001, Avg Grad: 0.0182439945638179780\n",
            "Epoch 8 iteration 0 Loss: 0.171 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.612 | Acc: 67.327% (68/101)\n",
            "Epoch 8 iteration 200 Loss: 0.601 | Acc: 68.159% (137/201)\n",
            "Epoch 8 iteration 300 Loss: 0.607 | Acc: 67.774% (204/301)\n",
            "Epoch 8 iteration 400 Loss: 0.609 | Acc: 67.581% (271/401)\n",
            "Epoch 8 iteration 500 Loss: 0.607 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 9, Loss: 0.0011811329750344157 Updates: 51/10001, Avg Grad: 0.0133437700569629670\n",
            "Epoch: 9, Loss: 0.00361602078191936 Updates: 56/11001, Avg Grad: 0.0200071856379508970\n",
            "Epoch: 9, Loss: 0.003521572332829237 Updates: 61/12001, Avg Grad: 0.0120213674381375310\n",
            "Epoch: 9, Loss: 0.006300911772996187 Updates: 66/13001, Avg Grad: 0.0078941164538264270\n",
            "Epoch: 9, Loss: 0.0016323259333148599 Updates: 71/14001, Avg Grad: 0.030372332781553270\n",
            "Epoch: 9, Loss: 0.006758393719792366 Updates: 76/15001, Avg Grad: 0.0144781991839408870\n",
            "Epoch: 9, Loss: 0.0014254802372306585 Updates: 81/16001, Avg Grad: 0.0134364785626530650\n",
            "Epoch: 9, Loss: 0.0026261890307068825 Updates: 86/17001, Avg Grad: 0.0199479274451732640\n",
            "Epoch: 9, Loss: 0.004905564710497856 Updates: 91/18001, Avg Grad: 0.0240107793360948560\n",
            "Epoch: 9, Loss: 0.003184984903782606 Updates: 96/19001, Avg Grad: 0.0214961115270853040\n",
            "Epoch 8 iteration 0 Loss: 0.520 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.586 | Acc: 75.248% (76/101)\n",
            "Epoch 8 iteration 200 Loss: 0.582 | Acc: 76.119% (153/201)\n",
            "Epoch 8 iteration 300 Loss: 0.601 | Acc: 71.761% (216/301)\n",
            "Epoch 8 iteration 400 Loss: 0.594 | Acc: 71.820% (288/401)\n",
            "Epoch 8 iteration 500 Loss: 0.595 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 9, Loss: 0.0014822316588833928 Updates: 101/20001, Avg Grad: 0.0120372101664543150\n",
            "Epoch: 9, Loss: 0.0028364467434585094 Updates: 106/21001, Avg Grad: 0.0167043916881084440\n",
            "Epoch: 9, Loss: 0.002461760537698865 Updates: 111/22001, Avg Grad: 0.0157320648431777950\n",
            "Epoch: 9, Loss: 0.002321977401152253 Updates: 116/23001, Avg Grad: 0.038623608648777010\n",
            "Epoch: 9, Loss: 0.0014394484460353851 Updates: 121/24001, Avg Grad: 0.016189478337764740\n",
            "Epoch: 9, Loss: 0.0014246489154174924 Updates: 126/25001, Avg Grad: 0.0175990145653486250\n",
            "Epoch: 9, Loss: 0.00466395728290081 Updates: 131/26001, Avg Grad: 0.0089097600430250170\n",
            "Epoch: 9, Loss: 0.0013116125483065844 Updates: 136/27001, Avg Grad: 0.0242259502410888670\n",
            "Epoch: 9, Loss: 0.001658382243476808 Updates: 141/28001, Avg Grad: 0.025232465937733650\n",
            "Epoch: 9, Loss: 0.0009843326406553388 Updates: 146/29001, Avg Grad: 0.0122506441548466680\n",
            "Epoch 8 iteration 0 Loss: 0.326 | Acc: 100.000% (1/1)\n",
            "Epoch 8 iteration 100 Loss: 0.594 | Acc: 69.307% (70/101)\n",
            "Epoch 8 iteration 200 Loss: 0.633 | Acc: 67.662% (136/201)\n",
            "Epoch 8 iteration 300 Loss: 0.607 | Acc: 71.429% (215/301)\n",
            "Epoch 8 iteration 400 Loss: 0.593 | Acc: 70.823% (284/401)\n",
            "Epoch 8 iteration 500 Loss: 0.601 | Acc: 69.661% (349/501)\n",
            "Test accuracy: 0.6966068148612976\n",
            "Epoch: 9, Loss: 0.002828688593581319 Updates: 151/30001, Avg Grad: 0.015715317800641060\n",
            "Epoch: 9, Loss: 0.001475286902859807 Updates: 156/31001, Avg Grad: 0.019373549148440360\n",
            "Epoch: 9, Loss: 0.002290363423526287 Updates: 161/32001, Avg Grad: 0.0157595109194517140\n",
            "Epoch 9 iteration 0 Loss: 0.136 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 100 Loss: 0.636 | Acc: 66.337% (67/101)\n",
            "Epoch 9 iteration 200 Loss: 0.618 | Acc: 68.159% (137/201)\n",
            "Epoch 9 iteration 300 Loss: 0.642 | Acc: 66.113% (199/301)\n",
            "Epoch 9 iteration 400 Loss: 0.626 | Acc: 67.581% (271/401)\n",
            "Epoch 9 iteration 500 Loss: 0.632 | Acc: 66.866% (335/501)\n",
            "Test accuracy: 0.6686626672744751\n",
            "Epoch: 10, Loss: 0.0015134802088141441 Updates: 1/1, Avg Grad: 0.00065945618553087120\n",
            "Epoch: 10, Loss: 0.000876508594956249 Updates: 6/1001, Avg Grad: 0.02316126786172390\n",
            "Epoch: 10, Loss: 0.006598941516131163 Updates: 11/2001, Avg Grad: 0.013974278233945370\n",
            "Epoch: 10, Loss: 0.0035357156302779913 Updates: 16/3001, Avg Grad: 0.023808723315596580\n",
            "Epoch: 10, Loss: 0.0010214231442660093 Updates: 21/4001, Avg Grad: 0.016920445486903190\n",
            "Epoch: 10, Loss: 0.0036327517591416836 Updates: 26/5001, Avg Grad: 0.0124896727502346040\n",
            "Epoch: 10, Loss: 0.006797062698751688 Updates: 31/6001, Avg Grad: 0.018760031089186670\n",
            "Epoch: 10, Loss: 0.003115689381957054 Updates: 36/7001, Avg Grad: 0.0211574584245681760\n",
            "Epoch: 10, Loss: 0.0028738845139741898 Updates: 41/8001, Avg Grad: 0.0135286310687661170\n",
            "Epoch: 10, Loss: 0.0015146092046052217 Updates: 46/9001, Avg Grad: 0.0125718414783477780\n",
            "Epoch 9 iteration 0 Loss: 1.087 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 100 Loss: 0.695 | Acc: 58.416% (59/101)\n",
            "Epoch 9 iteration 200 Loss: 0.635 | Acc: 64.179% (129/201)\n",
            "Epoch 9 iteration 300 Loss: 0.618 | Acc: 65.781% (198/301)\n",
            "Epoch 9 iteration 400 Loss: 0.624 | Acc: 66.584% (267/401)\n",
            "Epoch 9 iteration 500 Loss: 0.620 | Acc: 67.066% (336/501)\n",
            "Test accuracy: 0.6706587076187134\n",
            "Epoch: 10, Loss: 0.0015056864358484745 Updates: 51/10001, Avg Grad: 0.0137465391308069230\n",
            "Epoch: 10, Loss: 0.0022947152610868216 Updates: 56/11001, Avg Grad: 0.019968450069427490\n",
            "Epoch: 10, Loss: 0.003280394012108445 Updates: 61/12001, Avg Grad: 0.033143453299999240\n",
            "Epoch: 10, Loss: 0.0044556353241205215 Updates: 66/13001, Avg Grad: 0.018008979037404060\n",
            "Epoch: 10, Loss: 0.0022680291440337896 Updates: 71/14001, Avg Grad: 0.0110458685085177420\n",
            "Epoch: 10, Loss: 0.0010540960356593132 Updates: 76/15001, Avg Grad: 0.022039746865630150\n",
            "Epoch: 10, Loss: 0.0045766280964016914 Updates: 81/16001, Avg Grad: 0.011940073221921920\n",
            "Epoch: 10, Loss: 0.0014645516639575362 Updates: 86/17001, Avg Grad: 0.0068189431913197040\n",
            "Epoch: 10, Loss: 0.0019500560592859983 Updates: 91/18001, Avg Grad: 0.0149282626807689670\n",
            "Epoch: 10, Loss: 0.0010800089221447706 Updates: 96/19001, Avg Grad: 0.0148116759955883030\n",
            "Epoch 9 iteration 0 Loss: 1.312 | Acc: 0.000% (0/1)\n",
            "Epoch 9 iteration 100 Loss: 0.578 | Acc: 69.307% (70/101)\n",
            "Epoch 9 iteration 200 Loss: 0.594 | Acc: 68.159% (137/201)\n",
            "Epoch 9 iteration 300 Loss: 0.598 | Acc: 68.439% (206/301)\n",
            "Epoch 9 iteration 400 Loss: 0.602 | Acc: 67.830% (272/401)\n",
            "Epoch 9 iteration 500 Loss: 0.603 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 10, Loss: 0.0007034814334474504 Updates: 101/20001, Avg Grad: 0.029151603579521180\n",
            "Epoch: 10, Loss: 0.0016132944729179144 Updates: 106/21001, Avg Grad: 0.016922703012824060\n",
            "Epoch: 10, Loss: 0.0063225035555660725 Updates: 111/22001, Avg Grad: 0.0234285313636064530\n",
            "Epoch: 10, Loss: 0.005420255474746227 Updates: 116/23001, Avg Grad: 0.0296599715948104860\n",
            "Epoch: 10, Loss: 0.0033671848941594362 Updates: 121/24001, Avg Grad: 0.0133152473717927930\n",
            "Epoch: 10, Loss: 0.0029315713327378035 Updates: 126/25001, Avg Grad: 0.0194339230656623840\n",
            "Epoch: 10, Loss: 0.005834062583744526 Updates: 131/26001, Avg Grad: 0.022067820653319360\n",
            "Epoch: 10, Loss: 0.0010892103891819715 Updates: 136/27001, Avg Grad: 0.021369645372033120\n",
            "Epoch: 10, Loss: 0.007073144428431988 Updates: 141/28001, Avg Grad: 0.008032190613448620\n",
            "Epoch: 10, Loss: 0.004562308080494404 Updates: 146/29001, Avg Grad: 0.0267213806509971620\n",
            "Epoch 9 iteration 0 Loss: 0.378 | Acc: 100.000% (1/1)\n",
            "Epoch 9 iteration 100 Loss: 0.666 | Acc: 52.475% (53/101)\n",
            "Epoch 9 iteration 200 Loss: 0.689 | Acc: 55.224% (111/201)\n",
            "Epoch 9 iteration 300 Loss: 0.652 | Acc: 61.130% (184/301)\n",
            "Epoch 9 iteration 400 Loss: 0.644 | Acc: 61.097% (245/401)\n",
            "Epoch 9 iteration 500 Loss: 0.625 | Acc: 64.072% (321/501)\n",
            "Test accuracy: 0.6407185792922974\n",
            "Epoch: 10, Loss: 0.005383695941418409 Updates: 151/30001, Avg Grad: 0.0190558098256587980\n",
            "Epoch: 10, Loss: 0.0011587062617763877 Updates: 156/31001, Avg Grad: 0.0249979496002197270\n",
            "Epoch: 10, Loss: 0.005004588980227709 Updates: 161/32001, Avg Grad: 0.0109235998243093490\n",
            "Epoch 10 iteration 0 Loss: 0.276 | Acc: 100.000% (1/1)\n",
            "Epoch 10 iteration 100 Loss: 0.602 | Acc: 68.317% (69/101)\n",
            "Epoch 10 iteration 200 Loss: 0.641 | Acc: 65.174% (131/201)\n",
            "Epoch 10 iteration 300 Loss: 0.621 | Acc: 68.106% (205/301)\n",
            "Epoch 10 iteration 400 Loss: 0.609 | Acc: 69.077% (277/401)\n",
            "Epoch 10 iteration 500 Loss: 0.611 | Acc: 69.461% (348/501)\n",
            "Test accuracy: 0.6946107745170593\n",
            "Epoch: 11, Loss: 0.006989786401391029 Updates: 1/1, Avg Grad: 0.00177524238824844360\n",
            "Epoch: 11, Loss: 0.0032612306531518698 Updates: 6/1001, Avg Grad: 0.0205324608832597730\n",
            "Epoch: 11, Loss: 0.0027276906184852123 Updates: 11/2001, Avg Grad: 0.0354429595172405240\n",
            "Epoch: 11, Loss: 0.004787130281329155 Updates: 16/3001, Avg Grad: 0.0129068978130817410\n",
            "Epoch: 11, Loss: 0.0011793517041951418 Updates: 21/4001, Avg Grad: 0.017202053219079970\n",
            "Epoch: 11, Loss: 0.006278593558818102 Updates: 26/5001, Avg Grad: 0.0127917937934398650\n",
            "Epoch: 11, Loss: 0.0009385786834172904 Updates: 31/6001, Avg Grad: 0.0229257121682167050\n",
            "Epoch: 11, Loss: 0.004581349901854992 Updates: 36/7001, Avg Grad: 0.0087768519297242160\n",
            "Epoch: 11, Loss: 0.0010675563244149089 Updates: 41/8001, Avg Grad: 0.029535252600908280\n",
            "Epoch: 11, Loss: 0.00048560055438429117 Updates: 46/9001, Avg Grad: 0.0327650681138038640\n",
            "Epoch 10 iteration 0 Loss: 1.161 | Acc: 0.000% (0/1)\n",
            "Epoch 10 iteration 100 Loss: 0.651 | Acc: 62.376% (63/101)\n",
            "Epoch 10 iteration 200 Loss: 0.629 | Acc: 66.667% (134/201)\n",
            "Epoch 10 iteration 300 Loss: 0.634 | Acc: 66.113% (199/301)\n",
            "Epoch 10 iteration 400 Loss: 0.630 | Acc: 66.584% (267/401)\n",
            "Epoch 10 iteration 500 Loss: 0.617 | Acc: 67.465% (338/501)\n",
            "Test accuracy: 0.6746506690979004\n",
            "Epoch: 11, Loss: 0.002260996960103512 Updates: 51/10001, Avg Grad: 0.017967369407415390\n",
            "Epoch: 11, Loss: 0.0051809572614729404 Updates: 56/11001, Avg Grad: 0.0225811377167701720\n",
            "Epoch: 11, Loss: 0.002178207505494356 Updates: 61/12001, Avg Grad: 0.048215880990028380\n",
            "Epoch: 11, Loss: 0.0010041643399745226 Updates: 66/13001, Avg Grad: 0.0319836959242820740\n",
            "Epoch: 11, Loss: 0.0024043924640864134 Updates: 71/14001, Avg Grad: 0.034548532217741010\n",
            "Epoch: 11, Loss: 0.006854035891592503 Updates: 76/15001, Avg Grad: 0.0210983585566282270\n",
            "Epoch: 11, Loss: 0.005580844357609749 Updates: 81/16001, Avg Grad: 0.014898595400154590\n",
            "Epoch: 11, Loss: 0.006778435315936804 Updates: 86/17001, Avg Grad: 0.033667787909507750\n",
            "Epoch: 11, Loss: 0.0029604530427604914 Updates: 91/18001, Avg Grad: 0.0175307150930166240\n",
            "Epoch: 11, Loss: 0.006765950471162796 Updates: 96/19001, Avg Grad: 0.024405488744378090\n",
            "Epoch 10 iteration 0 Loss: 1.441 | Acc: 0.000% (0/1)\n",
            "Epoch 10 iteration 100 Loss: 0.626 | Acc: 65.347% (66/101)\n",
            "Epoch 10 iteration 200 Loss: 0.628 | Acc: 67.164% (135/201)\n",
            "Epoch 10 iteration 300 Loss: 0.632 | Acc: 66.113% (199/301)\n",
            "Epoch 10 iteration 400 Loss: 0.624 | Acc: 67.581% (271/401)\n",
            "Epoch 10 iteration 500 Loss: 0.621 | Acc: 67.665% (339/501)\n",
            "Test accuracy: 0.6766467094421387\n",
            "Epoch: 11, Loss: 0.0025656751822680235 Updates: 101/20001, Avg Grad: 0.0133373988792300220\n",
            "Epoch: 11, Loss: 0.00139078707434237 Updates: 106/21001, Avg Grad: 0.0216399524360895160\n",
            "Epoch: 11, Loss: 0.0006343310233205557 Updates: 111/22001, Avg Grad: 0.0073794899508357050\n",
            "Epoch: 11, Loss: 0.0015728280413895845 Updates: 116/23001, Avg Grad: 0.0154825458303093910\n",
            "Epoch: 11, Loss: 0.0037041339091956615 Updates: 121/24001, Avg Grad: 0.0248854514211416240\n",
            "Epoch: 11, Loss: 0.002116634277626872 Updates: 126/25001, Avg Grad: 0.0097520891577005390\n",
            "Epoch: 11, Loss: 0.006092439871281385 Updates: 131/26001, Avg Grad: 0.040738865733146670\n",
            "Epoch: 11, Loss: 0.0016335140680894256 Updates: 136/27001, Avg Grad: 0.0444619208574295040\n",
            "Epoch: 11, Loss: 0.003207049099728465 Updates: 141/28001, Avg Grad: 0.024333892390131950\n",
            "Epoch: 11, Loss: 0.0023780004121363163 Updates: 146/29001, Avg Grad: 0.012030742131173610\n",
            "Epoch 10 iteration 0 Loss: 0.171 | Acc: 100.000% (1/1)\n",
            "Epoch 10 iteration 100 Loss: 0.577 | Acc: 71.287% (72/101)\n",
            "Epoch 10 iteration 200 Loss: 0.584 | Acc: 70.149% (141/201)\n",
            "Epoch 10 iteration 300 Loss: 0.586 | Acc: 69.103% (208/301)\n",
            "Epoch 10 iteration 400 Loss: 0.619 | Acc: 65.835% (264/401)\n",
            "Epoch 10 iteration 500 Loss: 0.613 | Acc: 66.068% (331/501)\n",
            "Test accuracy: 0.6606786251068115\n",
            "Epoch: 11, Loss: 0.0015441294526681304 Updates: 151/30001, Avg Grad: 0.008397322148084640\n",
            "Epoch: 11, Loss: 0.00040365345194004476 Updates: 156/31001, Avg Grad: 0.011360190808773040\n",
            "Epoch: 11, Loss: 0.0017223061295226216 Updates: 161/32001, Avg Grad: 0.0128176668658852580\n",
            "Epoch 11 iteration 0 Loss: 0.278 | Acc: 100.000% (1/1)\n",
            "Epoch 11 iteration 100 Loss: 0.596 | Acc: 71.287% (72/101)\n",
            "Epoch 11 iteration 200 Loss: 0.600 | Acc: 71.642% (144/201)\n",
            "Epoch 11 iteration 300 Loss: 0.575 | Acc: 73.422% (221/301)\n",
            "Epoch 11 iteration 400 Loss: 0.585 | Acc: 71.322% (286/401)\n",
            "Epoch 11 iteration 500 Loss: 0.597 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 12, Loss: 0.001091408310458064 Updates: 1/1, Avg Grad: 0.000473684602184221150\n",
            "Epoch: 12, Loss: 0.0008670141687616706 Updates: 6/1001, Avg Grad: 0.0239507704973220830\n",
            "Epoch: 12, Loss: 0.0008966451860032976 Updates: 11/2001, Avg Grad: 0.0144824907183647160\n",
            "Epoch: 12, Loss: 0.0022293278016149998 Updates: 16/3001, Avg Grad: 0.0073668565601110460\n",
            "Epoch: 12, Loss: 0.0020243399776518345 Updates: 21/4001, Avg Grad: 0.037404693663120270\n",
            "Epoch: 12, Loss: 0.00661012576892972 Updates: 26/5001, Avg Grad: 0.0158345699310302730\n",
            "Epoch: 12, Loss: 0.0009434632956981659 Updates: 31/6001, Avg Grad: 0.0236364845186471940\n",
            "Epoch: 12, Loss: 0.005223215091973543 Updates: 36/7001, Avg Grad: 0.0500670783221721650\n",
            "Epoch: 12, Loss: 0.0007575521012768149 Updates: 41/8001, Avg Grad: 0.0393950305879116060\n",
            "Epoch: 12, Loss: 0.004295787308365107 Updates: 46/9001, Avg Grad: 0.020391995087265970\n",
            "Epoch 11 iteration 0 Loss: 0.692 | Acc: 100.000% (1/1)\n",
            "Epoch 11 iteration 100 Loss: 0.749 | Acc: 57.426% (58/101)\n",
            "Epoch 11 iteration 200 Loss: 0.689 | Acc: 60.697% (122/201)\n",
            "Epoch 11 iteration 300 Loss: 0.681 | Acc: 60.133% (181/301)\n",
            "Epoch 11 iteration 400 Loss: 0.663 | Acc: 62.095% (249/401)\n",
            "Epoch 11 iteration 500 Loss: 0.646 | Acc: 63.074% (316/501)\n",
            "Test accuracy: 0.6307384967803955\n",
            "Epoch: 12, Loss: 0.001253516529686749 Updates: 51/10001, Avg Grad: 0.0148895410820841790\n",
            "Epoch: 12, Loss: 0.002504199743270874 Updates: 56/11001, Avg Grad: 0.0198041815310716630\n",
            "Epoch: 12, Loss: 0.0007666255696676672 Updates: 61/12001, Avg Grad: 0.0079600801691412930\n",
            "Epoch: 12, Loss: 0.0019026075024157763 Updates: 66/13001, Avg Grad: 0.0145235573872923850\n",
            "Epoch: 12, Loss: 0.0032997203525155783 Updates: 71/14001, Avg Grad: 0.0093538649380207060\n",
            "Epoch: 12, Loss: 0.0007556272321380675 Updates: 76/15001, Avg Grad: 0.0287245623767375950\n",
            "Epoch: 12, Loss: 0.0012348664458841085 Updates: 81/16001, Avg Grad: 0.018141098320484160\n",
            "Epoch: 12, Loss: 0.0018727375427260995 Updates: 86/17001, Avg Grad: 0.0107764005661010740\n",
            "Epoch: 12, Loss: 0.0006589539116248488 Updates: 91/18001, Avg Grad: 0.015602610073983670\n",
            "Epoch: 12, Loss: 0.0021377033554017544 Updates: 96/19001, Avg Grad: 0.0319659188389778140\n",
            "Epoch 11 iteration 0 Loss: 0.328 | Acc: 100.000% (1/1)\n",
            "Epoch 11 iteration 100 Loss: 0.644 | Acc: 59.406% (60/101)\n",
            "Epoch 11 iteration 200 Loss: 0.651 | Acc: 59.701% (120/201)\n",
            "Epoch 11 iteration 300 Loss: 0.633 | Acc: 61.794% (186/301)\n",
            "Epoch 11 iteration 400 Loss: 0.625 | Acc: 64.090% (257/401)\n",
            "Epoch 11 iteration 500 Loss: 0.631 | Acc: 63.673% (319/501)\n",
            "Test accuracy: 0.6367265582084656\n",
            "Epoch: 12, Loss: 0.0007418522145599127 Updates: 101/20001, Avg Grad: 0.0147354677319526670\n",
            "Epoch: 12, Loss: 0.0013895193114876747 Updates: 106/21001, Avg Grad: 0.0289053600281476970\n",
            "Epoch: 12, Loss: 0.001410602475516498 Updates: 111/22001, Avg Grad: 0.0308768972754478450\n",
            "Epoch: 12, Loss: 0.0037787174805998802 Updates: 116/23001, Avg Grad: 0.0157241653650999070\n",
            "Epoch: 12, Loss: 0.0016633481718599796 Updates: 121/24001, Avg Grad: 0.0213378854095935820\n",
            "Epoch: 12, Loss: 0.0009891282534226775 Updates: 126/25001, Avg Grad: 0.051063932478427890\n",
            "Epoch: 12, Loss: 0.0016016950830817223 Updates: 131/26001, Avg Grad: 0.0133742149919271470\n",
            "Epoch: 12, Loss: 0.005761634558439255 Updates: 136/27001, Avg Grad: 0.031101191416382790\n",
            "Epoch: 12, Loss: 0.0006072619580663741 Updates: 141/28001, Avg Grad: 0.027028229087591170\n",
            "Epoch: 12, Loss: 0.005392030347138643 Updates: 146/29001, Avg Grad: 0.01855625770986080\n",
            "Epoch 11 iteration 0 Loss: 1.761 | Acc: 0.000% (0/1)\n",
            "Epoch 11 iteration 100 Loss: 0.684 | Acc: 63.366% (64/101)\n",
            "Epoch 11 iteration 200 Loss: 0.652 | Acc: 64.179% (129/201)\n",
            "Epoch 11 iteration 300 Loss: 0.629 | Acc: 65.116% (196/301)\n",
            "Epoch 11 iteration 400 Loss: 0.610 | Acc: 66.334% (266/401)\n",
            "Epoch 11 iteration 500 Loss: 0.608 | Acc: 67.465% (338/501)\n",
            "Test accuracy: 0.6746506690979004\n",
            "Epoch: 12, Loss: 0.0005858768126927316 Updates: 151/30001, Avg Grad: 0.0288278814405202870\n",
            "Epoch: 12, Loss: 0.0008620969019830227 Updates: 156/31001, Avg Grad: 0.0171187296509742740\n",
            "Epoch: 12, Loss: 0.002026308560743928 Updates: 161/32001, Avg Grad: 0.0141421211883425710\n",
            "Epoch 12 iteration 0 Loss: 0.169 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.540 | Acc: 78.218% (79/101)\n",
            "Epoch 12 iteration 200 Loss: 0.552 | Acc: 74.627% (150/201)\n",
            "Epoch 12 iteration 300 Loss: 0.570 | Acc: 72.757% (219/301)\n",
            "Epoch 12 iteration 400 Loss: 0.587 | Acc: 71.571% (287/401)\n",
            "Epoch 12 iteration 500 Loss: 0.585 | Acc: 71.058% (356/501)\n",
            "Test accuracy: 0.7105788588523865\n",
            "Epoch: 13, Loss: 0.00846280436962843 Updates: 1/1, Avg Grad: 0.00216087652370333670\n",
            "Epoch: 13, Loss: 0.0008314240840263665 Updates: 6/1001, Avg Grad: 0.0116399647668004040\n",
            "Epoch: 13, Loss: 0.0011450580786913633 Updates: 11/2001, Avg Grad: 0.019432144239544870\n",
            "Epoch: 13, Loss: 0.0018541199387982488 Updates: 16/3001, Avg Grad: 0.0109920613467693330\n",
            "Epoch: 13, Loss: 0.0006801991257816553 Updates: 21/4001, Avg Grad: 0.038532394915819170\n",
            "Epoch: 13, Loss: 0.006134130526334047 Updates: 26/5001, Avg Grad: 0.016870819032192230\n",
            "Epoch: 13, Loss: 0.0028624378610402346 Updates: 31/6001, Avg Grad: 0.0097667183727025990\n",
            "Epoch: 13, Loss: 0.011088878847658634 Updates: 36/7001, Avg Grad: 0.039496932178735730\n",
            "Epoch: 13, Loss: 0.003870240179821849 Updates: 41/8001, Avg Grad: 0.0083148386329412460\n",
            "Epoch: 13, Loss: 0.0031822926830500364 Updates: 46/9001, Avg Grad: 0.008320400491356850\n",
            "Epoch 12 iteration 0 Loss: 1.636 | Acc: 0.000% (0/1)\n",
            "Epoch 12 iteration 100 Loss: 0.617 | Acc: 64.356% (65/101)\n",
            "Epoch 12 iteration 200 Loss: 0.590 | Acc: 70.149% (141/201)\n",
            "Epoch 12 iteration 300 Loss: 0.604 | Acc: 69.767% (210/301)\n",
            "Epoch 12 iteration 400 Loss: 0.607 | Acc: 69.327% (278/401)\n",
            "Epoch 12 iteration 500 Loss: 0.596 | Acc: 69.661% (349/501)\n",
            "Test accuracy: 0.6966068148612976\n",
            "Epoch: 13, Loss: 0.0023994443472474813 Updates: 51/10001, Avg Grad: 0.0425972491502761840\n",
            "Epoch: 13, Loss: 0.001967614982277155 Updates: 56/11001, Avg Grad: 0.030740184709429740\n",
            "Epoch: 13, Loss: 0.0021714612375944853 Updates: 61/12001, Avg Grad: 0.026213945820927620\n",
            "Epoch: 13, Loss: 0.0032507837750017643 Updates: 66/13001, Avg Grad: 0.0213704593479633330\n",
            "Epoch: 13, Loss: 0.004684933926910162 Updates: 71/14001, Avg Grad: 0.0167028401046991350\n",
            "Epoch: 13, Loss: 0.008029351010918617 Updates: 76/15001, Avg Grad: 0.024343941360712050\n",
            "Epoch: 13, Loss: 0.007096835412085056 Updates: 81/16001, Avg Grad: 0.0273841340094804760\n",
            "Epoch: 13, Loss: 0.003389088436961174 Updates: 86/17001, Avg Grad: 0.014634072780609130\n",
            "Epoch: 13, Loss: 0.0027855474036186934 Updates: 91/18001, Avg Grad: 0.0097371246665716170\n",
            "Epoch: 13, Loss: 0.0014701420441269875 Updates: 96/19001, Avg Grad: 0.021903963759541510\n",
            "Epoch 12 iteration 0 Loss: 0.485 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.546 | Acc: 76.238% (77/101)\n",
            "Epoch 12 iteration 200 Loss: 0.565 | Acc: 73.134% (147/201)\n",
            "Epoch 12 iteration 300 Loss: 0.568 | Acc: 72.093% (217/301)\n",
            "Epoch 12 iteration 400 Loss: 0.581 | Acc: 71.322% (286/401)\n",
            "Epoch 12 iteration 500 Loss: 0.569 | Acc: 71.657% (359/501)\n",
            "Test accuracy: 0.7165668606758118\n",
            "Epoch: 13, Loss: 0.0002544164308346808 Updates: 101/20001, Avg Grad: 0.0445685349404811860\n",
            "Epoch: 13, Loss: 0.0019936198368668556 Updates: 106/21001, Avg Grad: 0.0183927379548549650\n",
            "Epoch: 13, Loss: 0.0036253652069717646 Updates: 111/22001, Avg Grad: 0.008318183012306690\n",
            "Epoch: 13, Loss: 0.007843036204576492 Updates: 116/23001, Avg Grad: 0.0456561744213104250\n",
            "Epoch: 13, Loss: 0.0015695183537900448 Updates: 121/24001, Avg Grad: 0.0282883737236261370\n",
            "Epoch: 13, Loss: 0.0011066194856539369 Updates: 126/25001, Avg Grad: 0.0097350990399718280\n",
            "Epoch: 13, Loss: 0.0017679109005257487 Updates: 131/26001, Avg Grad: 0.0427158549427986150\n",
            "Epoch: 13, Loss: 0.0011621273588389158 Updates: 136/27001, Avg Grad: 0.011354100890457630\n",
            "Epoch: 13, Loss: 0.00220710807479918 Updates: 141/28001, Avg Grad: 0.0116197606548666950\n",
            "Epoch: 13, Loss: 0.012623710557818413 Updates: 146/29001, Avg Grad: 0.0261508207768201830\n",
            "Epoch 12 iteration 0 Loss: 0.564 | Acc: 100.000% (1/1)\n",
            "Epoch 12 iteration 100 Loss: 0.710 | Acc: 53.465% (54/101)\n",
            "Epoch 12 iteration 200 Loss: 0.649 | Acc: 63.184% (127/201)\n",
            "Epoch 12 iteration 300 Loss: 0.638 | Acc: 65.781% (198/301)\n",
            "Epoch 12 iteration 400 Loss: 0.630 | Acc: 66.833% (268/401)\n",
            "Epoch 12 iteration 500 Loss: 0.636 | Acc: 66.467% (333/501)\n",
            "Test accuracy: 0.6646706461906433\n",
            "Epoch: 13, Loss: 0.004349000286310911 Updates: 151/30001, Avg Grad: 0.0166342798620462420\n",
            "Epoch: 13, Loss: 0.0070116352289915085 Updates: 156/31001, Avg Grad: 0.020737398415803910\n",
            "Epoch: 13, Loss: 0.0007600625394843519 Updates: 161/32001, Avg Grad: 0.0096076764166355130\n",
            "Epoch 13 iteration 0 Loss: 0.312 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.650 | Acc: 65.347% (66/101)\n",
            "Epoch 13 iteration 200 Loss: 0.600 | Acc: 70.647% (142/201)\n",
            "Epoch 13 iteration 300 Loss: 0.607 | Acc: 68.771% (207/301)\n",
            "Epoch 13 iteration 400 Loss: 0.601 | Acc: 68.828% (276/401)\n",
            "Epoch 13 iteration 500 Loss: 0.591 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 14, Loss: 0.009382165968418121 Updates: 1/1, Avg Grad: 0.00226711714640259740\n",
            "Epoch: 14, Loss: 0.00612728763371706 Updates: 6/1001, Avg Grad: 0.0347110442817211150\n",
            "Epoch: 14, Loss: 0.0003617510083131492 Updates: 11/2001, Avg Grad: 0.051390711218118670\n",
            "Epoch: 14, Loss: 0.002538179513067007 Updates: 16/3001, Avg Grad: 0.026172017678618430\n",
            "Epoch: 14, Loss: 0.002062695100903511 Updates: 21/4001, Avg Grad: 0.0086408779025077820\n",
            "Epoch: 14, Loss: 0.004633048083633184 Updates: 26/5001, Avg Grad: 0.024263195693492890\n",
            "Epoch: 14, Loss: 0.006179511081427336 Updates: 31/6001, Avg Grad: 0.0250222776085138320\n",
            "Epoch: 14, Loss: 0.0016226477455347776 Updates: 36/7001, Avg Grad: 0.0106791686266660690\n",
            "Epoch: 14, Loss: 0.0010870363330468535 Updates: 41/8001, Avg Grad: 0.0252190884202718730\n",
            "Epoch: 14, Loss: 0.0032930849120020866 Updates: 46/9001, Avg Grad: 0.008766464889049530\n",
            "Epoch 13 iteration 0 Loss: 0.728 | Acc: 0.000% (0/1)\n",
            "Epoch 13 iteration 100 Loss: 0.563 | Acc: 75.248% (76/101)\n",
            "Epoch 13 iteration 200 Loss: 0.584 | Acc: 70.647% (142/201)\n",
            "Epoch 13 iteration 300 Loss: 0.581 | Acc: 71.761% (216/301)\n",
            "Epoch 13 iteration 400 Loss: 0.570 | Acc: 73.067% (293/401)\n",
            "Epoch 13 iteration 500 Loss: 0.586 | Acc: 71.856% (360/501)\n",
            "Test accuracy: 0.71856290102005\n",
            "Epoch: 14, Loss: 0.005464684218168259 Updates: 51/10001, Avg Grad: 0.0137311294674873350\n",
            "Epoch: 14, Loss: 0.0007691083592362702 Updates: 56/11001, Avg Grad: 0.03070601075887680\n",
            "Epoch: 14, Loss: 0.0019130912842229009 Updates: 61/12001, Avg Grad: 0.027855353429913520\n",
            "Epoch: 14, Loss: 0.0037679984234273434 Updates: 66/13001, Avg Grad: 0.0216866955161094670\n",
            "Epoch: 14, Loss: 0.0034569769632071257 Updates: 71/14001, Avg Grad: 0.0231249742209911350\n",
            "Epoch: 14, Loss: 0.006079678423702717 Updates: 76/15001, Avg Grad: 0.0220225434750318530\n",
            "Epoch: 14, Loss: 0.0024338208604604006 Updates: 81/16001, Avg Grad: 0.021584304049611090\n",
            "Epoch: 14, Loss: 0.0014280021423473954 Updates: 86/17001, Avg Grad: 0.018883263692259790\n",
            "Epoch: 14, Loss: 0.0008154545212164521 Updates: 91/18001, Avg Grad: 0.0350764356553554530\n",
            "Epoch: 14, Loss: 0.009740625508129597 Updates: 96/19001, Avg Grad: 0.0272864010185003280\n",
            "Epoch 13 iteration 0 Loss: 0.251 | Acc: 100.000% (1/1)\n",
            "Epoch 13 iteration 100 Loss: 0.587 | Acc: 69.307% (70/101)\n",
            "Epoch 13 iteration 200 Loss: 0.565 | Acc: 70.149% (141/201)\n",
            "Epoch 13 iteration 300 Loss: 0.577 | Acc: 70.764% (213/301)\n",
            "Epoch 13 iteration 400 Loss: 0.601 | Acc: 70.075% (281/401)\n",
            "Epoch 13 iteration 500 Loss: 0.598 | Acc: 70.060% (351/501)\n",
            "Test accuracy: 0.7005987763404846\n",
            "Epoch: 14, Loss: 0.003984618932008743 Updates: 101/20001, Avg Grad: 0.0323187336325645450\n",
            "Epoch: 14, Loss: 0.002405528211966157 Updates: 106/21001, Avg Grad: 0.020984783768653870\n",
            "Epoch: 14, Loss: 0.0009964583441615105 Updates: 111/22001, Avg Grad: 0.0100418534129858020\n",
            "Epoch: 14, Loss: 0.0022353550884872675 Updates: 116/23001, Avg Grad: 0.0231405273079872130\n",
            "Epoch: 14, Loss: 0.002711221342906356 Updates: 121/24001, Avg Grad: 0.0275197774171829220\n",
            "Epoch: 14, Loss: 0.00462875422090292 Updates: 126/25001, Avg Grad: 0.018064951524138450\n",
            "Epoch: 14, Loss: 0.0023499985691159964 Updates: 131/26001, Avg Grad: 0.01764109358191490\n",
            "Epoch: 14, Loss: 0.0025332358200103045 Updates: 136/27001, Avg Grad: 0.0148156620562076570\n",
            "Epoch: 14, Loss: 0.0009468075004406273 Updates: 141/28001, Avg Grad: 0.0173219069838523860\n",
            "Epoch: 14, Loss: 0.006782000884413719 Updates: 146/29001, Avg Grad: 0.0192600954324007030\n",
            "Epoch 13 iteration 0 Loss: 0.931 | Acc: 0.000% (0/1)\n",
            "Epoch 13 iteration 100 Loss: 0.565 | Acc: 74.257% (75/101)\n",
            "Epoch 13 iteration 200 Loss: 0.596 | Acc: 70.647% (142/201)\n",
            "Epoch 13 iteration 300 Loss: 0.588 | Acc: 72.093% (217/301)\n",
            "Epoch 13 iteration 400 Loss: 0.590 | Acc: 70.823% (284/401)\n",
            "Epoch 13 iteration 500 Loss: 0.588 | Acc: 69.661% (349/501)\n",
            "Test accuracy: 0.6966068148612976\n",
            "Epoch: 14, Loss: 0.003669500583782792 Updates: 151/30001, Avg Grad: 0.017383052036166190\n",
            "Epoch: 14, Loss: 0.003108760342001915 Updates: 156/31001, Avg Grad: 0.0247983615845441820\n",
            "Epoch: 14, Loss: 0.0068708620965480804 Updates: 161/32001, Avg Grad: 0.0183999594300985340\n",
            "Epoch 14 iteration 0 Loss: 0.080 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.667 | Acc: 68.317% (69/101)\n",
            "Epoch 14 iteration 200 Loss: 0.700 | Acc: 64.179% (129/201)\n",
            "Epoch 14 iteration 300 Loss: 0.651 | Acc: 65.116% (196/301)\n",
            "Epoch 14 iteration 400 Loss: 0.637 | Acc: 66.584% (267/401)\n",
            "Epoch 14 iteration 500 Loss: 0.642 | Acc: 66.667% (334/501)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 15, Loss: 0.007449299097061157 Updates: 1/1, Avg Grad: 0.0021204159129410980\n",
            "Epoch: 15, Loss: 0.0012230008142068982 Updates: 6/1001, Avg Grad: 0.0224296282976865770\n",
            "Epoch: 15, Loss: 0.0024512666277587414 Updates: 11/2001, Avg Grad: 0.0186886880546808240\n",
            "Epoch: 15, Loss: 0.0019031676929444075 Updates: 16/3001, Avg Grad: 0.0252273734658956530\n",
            "Epoch: 15, Loss: 0.0022303375881165266 Updates: 21/4001, Avg Grad: 0.0138855576515197750\n",
            "Epoch: 15, Loss: 0.0003171622520312667 Updates: 26/5001, Avg Grad: 0.0229987427592277530\n",
            "Epoch: 15, Loss: 0.0006389396730810404 Updates: 31/6001, Avg Grad: 0.0182328186929225920\n",
            "Epoch: 15, Loss: 0.0031145133543759584 Updates: 36/7001, Avg Grad: 0.0074605289846658710\n",
            "Epoch: 15, Loss: 0.00347335753031075 Updates: 41/8001, Avg Grad: 0.0138418106362223630\n",
            "Epoch: 15, Loss: 0.002735608257353306 Updates: 46/9001, Avg Grad: 0.046379551291465760\n",
            "Epoch 14 iteration 0 Loss: 0.515 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.520 | Acc: 79.208% (80/101)\n",
            "Epoch 14 iteration 200 Loss: 0.539 | Acc: 75.622% (152/201)\n",
            "Epoch 14 iteration 300 Loss: 0.554 | Acc: 74.086% (223/301)\n",
            "Epoch 14 iteration 400 Loss: 0.568 | Acc: 72.569% (291/401)\n",
            "Epoch 14 iteration 500 Loss: 0.571 | Acc: 71.856% (360/501)\n",
            "Test accuracy: 0.71856290102005\n",
            "Epoch: 15, Loss: 0.0027537343557924032 Updates: 51/10001, Avg Grad: 0.023795552551746370\n",
            "Epoch: 15, Loss: 0.0014993228251114488 Updates: 56/11001, Avg Grad: 0.013701399788260460\n",
            "Epoch: 15, Loss: 0.0007063964731059968 Updates: 61/12001, Avg Grad: 0.024212572723627090\n",
            "Epoch: 15, Loss: 0.003092127852141857 Updates: 66/13001, Avg Grad: 0.037109754979610440\n",
            "Epoch: 15, Loss: 0.00022492952120956033 Updates: 71/14001, Avg Grad: 0.02471439167857170\n",
            "Epoch: 15, Loss: 0.0027373614721000195 Updates: 76/15001, Avg Grad: 0.0084350723773241040\n",
            "Epoch: 15, Loss: 0.0005784110981039703 Updates: 81/16001, Avg Grad: 0.039460584521293640\n",
            "Epoch: 15, Loss: 0.00484719593077898 Updates: 86/17001, Avg Grad: 0.0176790412515401840\n",
            "Epoch: 15, Loss: 0.0016883881762623787 Updates: 91/18001, Avg Grad: 0.0163010284304618840\n",
            "Epoch: 15, Loss: 0.0008138121920637786 Updates: 96/19001, Avg Grad: 0.0155726363882422450\n",
            "Epoch 14 iteration 0 Loss: 0.465 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.548 | Acc: 72.277% (73/101)\n",
            "Epoch 14 iteration 200 Loss: 0.540 | Acc: 75.622% (152/201)\n",
            "Epoch 14 iteration 300 Loss: 0.546 | Acc: 76.080% (229/301)\n",
            "Epoch 14 iteration 400 Loss: 0.566 | Acc: 72.569% (291/401)\n",
            "Epoch 14 iteration 500 Loss: 0.573 | Acc: 70.858% (355/501)\n",
            "Test accuracy: 0.7085828185081482\n",
            "Epoch: 15, Loss: 0.0030846488662064075 Updates: 101/20001, Avg Grad: 0.0102443424984812740\n",
            "Epoch: 15, Loss: 0.00044285482726991177 Updates: 106/21001, Avg Grad: 0.0135310767218470570\n",
            "Epoch: 15, Loss: 0.0028600499499589205 Updates: 111/22001, Avg Grad: 0.0253341514617204670\n",
            "Epoch: 15, Loss: 0.0019878121092915535 Updates: 116/23001, Avg Grad: 0.031549319624900820\n",
            "Epoch: 15, Loss: 0.004017304629087448 Updates: 121/24001, Avg Grad: 0.0072445380501449110\n",
            "Epoch: 15, Loss: 0.002635039621964097 Updates: 126/25001, Avg Grad: 0.014214809052646160\n",
            "Epoch: 15, Loss: 0.0030847426969558 Updates: 131/26001, Avg Grad: 0.025280375033617020\n",
            "Epoch: 15, Loss: 0.0009584359358996153 Updates: 136/27001, Avg Grad: 0.0175268650054931640\n",
            "Epoch: 15, Loss: 0.002961309626698494 Updates: 141/28001, Avg Grad: 0.0168298054486513140\n",
            "Epoch: 15, Loss: 0.0035347638186067343 Updates: 146/29001, Avg Grad: 0.0338629186153411870\n",
            "Epoch 14 iteration 0 Loss: 0.385 | Acc: 100.000% (1/1)\n",
            "Epoch 14 iteration 100 Loss: 0.633 | Acc: 64.356% (65/101)\n",
            "Epoch 14 iteration 200 Loss: 0.584 | Acc: 69.154% (139/201)\n",
            "Epoch 14 iteration 300 Loss: 0.611 | Acc: 67.110% (202/301)\n",
            "Epoch 14 iteration 400 Loss: 0.625 | Acc: 66.085% (265/401)\n",
            "Epoch 14 iteration 500 Loss: 0.620 | Acc: 66.667% (334/501)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 15, Loss: 0.002095332369208336 Updates: 151/30001, Avg Grad: 0.031767133623361590\n",
            "Epoch: 15, Loss: 0.001377990934997797 Updates: 156/31001, Avg Grad: 0.0073024672456085680\n",
            "Epoch: 15, Loss: 0.001378247863613069 Updates: 161/32001, Avg Grad: 0.01231378410011530\n",
            "Epoch 15 iteration 0 Loss: 0.307 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.642 | Acc: 62.376% (63/101)\n",
            "Epoch 15 iteration 200 Loss: 0.614 | Acc: 66.667% (134/201)\n",
            "Epoch 15 iteration 300 Loss: 0.607 | Acc: 67.774% (204/301)\n",
            "Epoch 15 iteration 400 Loss: 0.627 | Acc: 65.586% (263/401)\n",
            "Epoch 15 iteration 500 Loss: 0.627 | Acc: 65.868% (330/501)\n",
            "Test accuracy: 0.658682644367218\n",
            "Epoch: 16, Loss: 0.005450259894132614 Updates: 1/1, Avg Grad: 0.00150895910337567330\n",
            "Epoch: 16, Loss: 0.0034040433820337057 Updates: 6/1001, Avg Grad: 0.019063919782638550\n",
            "Epoch: 16, Loss: 0.0007480640197172761 Updates: 11/2001, Avg Grad: 0.0307681709527969360\n",
            "Epoch: 16, Loss: 0.0054778908379375935 Updates: 16/3001, Avg Grad: 0.0248805545270442960\n",
            "Epoch: 16, Loss: 0.0017716133734211326 Updates: 21/4001, Avg Grad: 0.026617750525474550\n",
            "Epoch: 16, Loss: 0.0012870533391833305 Updates: 26/5001, Avg Grad: 0.039384093135595320\n",
            "Epoch: 16, Loss: 0.0010529329301789403 Updates: 31/6001, Avg Grad: 0.0296076871454715730\n",
            "Epoch: 16, Loss: 0.0005492026684805751 Updates: 36/7001, Avg Grad: 0.044817637652158740\n",
            "Epoch: 16, Loss: 0.002905488247051835 Updates: 41/8001, Avg Grad: 0.0410268977284431460\n",
            "Epoch: 16, Loss: 0.001366339740343392 Updates: 46/9001, Avg Grad: 0.027696611359715460\n",
            "Epoch 15 iteration 0 Loss: 0.211 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.574 | Acc: 70.297% (71/101)\n",
            "Epoch 15 iteration 200 Loss: 0.599 | Acc: 70.149% (141/201)\n",
            "Epoch 15 iteration 300 Loss: 0.597 | Acc: 68.439% (206/301)\n",
            "Epoch 15 iteration 400 Loss: 0.610 | Acc: 67.581% (271/401)\n",
            "Epoch 15 iteration 500 Loss: 0.604 | Acc: 67.465% (338/501)\n",
            "Test accuracy: 0.6746506690979004\n",
            "Epoch: 16, Loss: 0.003336722496896982 Updates: 51/10001, Avg Grad: 0.0309958644211292270\n",
            "Epoch: 16, Loss: 0.0015648840926587582 Updates: 56/11001, Avg Grad: 0.020178388804197310\n",
            "Epoch: 16, Loss: 0.0016189583111554384 Updates: 61/12001, Avg Grad: 0.024891735985875130\n",
            "Epoch: 16, Loss: 0.0015863359440118074 Updates: 66/13001, Avg Grad: 0.0276278425008058550\n",
            "Epoch: 16, Loss: 0.0019359299913048744 Updates: 71/14001, Avg Grad: 0.050347171723842620\n",
            "Epoch: 16, Loss: 0.004031877964735031 Updates: 76/15001, Avg Grad: 0.0155994566157460210\n",
            "Epoch: 16, Loss: 0.005530208349227905 Updates: 81/16001, Avg Grad: 0.0154681839048862460\n",
            "Epoch: 16, Loss: 0.0032020611688494682 Updates: 86/17001, Avg Grad: 0.0343852117657661440\n",
            "Epoch: 16, Loss: 0.004812582861632109 Updates: 91/18001, Avg Grad: 0.0117926411330699920\n",
            "Epoch: 16, Loss: 0.0064026955515146255 Updates: 96/19001, Avg Grad: 0.0381993241608142850\n",
            "Epoch 15 iteration 0 Loss: 0.205 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.589 | Acc: 68.317% (69/101)\n",
            "Epoch 15 iteration 200 Loss: 0.603 | Acc: 68.159% (137/201)\n",
            "Epoch 15 iteration 300 Loss: 0.604 | Acc: 67.774% (204/301)\n",
            "Epoch 15 iteration 400 Loss: 0.599 | Acc: 68.080% (273/401)\n",
            "Epoch 15 iteration 500 Loss: 0.597 | Acc: 68.064% (341/501)\n",
            "Test accuracy: 0.6806387305259705\n",
            "Epoch: 16, Loss: 0.00583622558042407 Updates: 101/20001, Avg Grad: 0.022672183811664580\n",
            "Epoch: 16, Loss: 0.0006790081388317049 Updates: 106/21001, Avg Grad: 0.0173308271914720540\n",
            "Epoch: 16, Loss: 0.004423917271196842 Updates: 111/22001, Avg Grad: 0.0141369523480534550\n",
            "Epoch: 16, Loss: 0.0010715055977925658 Updates: 116/23001, Avg Grad: 0.023977175354957580\n",
            "Epoch: 16, Loss: 0.0009212050354108214 Updates: 121/24001, Avg Grad: 0.027957445010542870\n",
            "Epoch: 16, Loss: 0.0005789398564957082 Updates: 126/25001, Avg Grad: 0.031384333968162540\n",
            "Epoch: 16, Loss: 0.0012680661166086793 Updates: 131/26001, Avg Grad: 0.0092737087979912760\n",
            "Epoch: 16, Loss: 0.004008681979030371 Updates: 136/27001, Avg Grad: 0.02491724304854870\n",
            "Epoch: 16, Loss: 0.0031870368402451277 Updates: 141/28001, Avg Grad: 0.0205623432993888850\n",
            "Epoch: 16, Loss: 0.0009452615049667656 Updates: 146/29001, Avg Grad: 0.031862989068031310\n",
            "Epoch 15 iteration 0 Loss: 0.305 | Acc: 100.000% (1/1)\n",
            "Epoch 15 iteration 100 Loss: 0.651 | Acc: 67.327% (68/101)\n",
            "Epoch 15 iteration 200 Loss: 0.643 | Acc: 66.169% (133/201)\n",
            "Epoch 15 iteration 300 Loss: 0.625 | Acc: 67.774% (204/301)\n",
            "Epoch 15 iteration 400 Loss: 0.627 | Acc: 67.581% (271/401)\n",
            "Epoch 15 iteration 500 Loss: 0.635 | Acc: 65.669% (329/501)\n",
            "Test accuracy: 0.6566866040229797\n",
            "Epoch: 16, Loss: 0.0011250741081312299 Updates: 151/30001, Avg Grad: 0.0131393345072865490\n",
            "Epoch: 16, Loss: 0.0025304073933511972 Updates: 156/31001, Avg Grad: 0.045486591756343840\n",
            "Epoch: 16, Loss: 0.001993213314563036 Updates: 161/32001, Avg Grad: 0.0145009625703096390\n",
            "Epoch 16 iteration 0 Loss: 1.542 | Acc: 0.000% (0/1)\n",
            "Epoch 16 iteration 100 Loss: 0.698 | Acc: 65.347% (66/101)\n",
            "Epoch 16 iteration 200 Loss: 0.681 | Acc: 66.667% (134/201)\n",
            "Epoch 16 iteration 300 Loss: 0.674 | Acc: 65.449% (197/301)\n",
            "Epoch 16 iteration 400 Loss: 0.666 | Acc: 65.337% (262/401)\n",
            "Epoch 16 iteration 500 Loss: 0.663 | Acc: 64.870% (325/501)\n",
            "Test accuracy: 0.6487026214599609\n",
            "Epoch: 17, Loss: 0.003914660774171352 Updates: 1/1, Avg Grad: 0.00144794886000454430\n",
            "Epoch: 17, Loss: 0.005844261031597853 Updates: 6/1001, Avg Grad: 0.050196886062622070\n",
            "Epoch: 17, Loss: 0.0032872131559997797 Updates: 11/2001, Avg Grad: 0.03248693048954010\n",
            "Epoch: 17, Loss: 0.0013060549972578883 Updates: 16/3001, Avg Grad: 0.0090912729501724240\n",
            "Epoch: 17, Loss: 0.0026837766636162996 Updates: 21/4001, Avg Grad: 0.032190423458814620\n",
            "Epoch: 17, Loss: 0.002229280536994338 Updates: 26/5001, Avg Grad: 0.0349441133439540860\n",
            "Epoch: 17, Loss: 0.0009329158929176629 Updates: 31/6001, Avg Grad: 0.0305040795356035230\n",
            "Epoch: 17, Loss: 0.00040323176654055715 Updates: 36/7001, Avg Grad: 0.0211773440241813660\n",
            "Epoch: 17, Loss: 0.008079931139945984 Updates: 41/8001, Avg Grad: 0.0120221767574548720\n",
            "Epoch: 17, Loss: 0.0021101622842252254 Updates: 46/9001, Avg Grad: 0.051445055752992630\n",
            "Epoch 16 iteration 0 Loss: 0.539 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.602 | Acc: 70.297% (71/101)\n",
            "Epoch 16 iteration 200 Loss: 0.600 | Acc: 68.159% (137/201)\n",
            "Epoch 16 iteration 300 Loss: 0.609 | Acc: 65.781% (198/301)\n",
            "Epoch 16 iteration 400 Loss: 0.618 | Acc: 65.586% (263/401)\n",
            "Epoch 16 iteration 500 Loss: 0.614 | Acc: 66.667% (334/501)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 17, Loss: 0.0017013774486258626 Updates: 51/10001, Avg Grad: 0.020946390926837920\n",
            "Epoch: 17, Loss: 0.003519602818414569 Updates: 56/11001, Avg Grad: 0.037123251706361770\n",
            "Epoch: 17, Loss: 0.001590147614479065 Updates: 61/12001, Avg Grad: 0.014954422600567340\n",
            "Epoch: 17, Loss: 0.001781263155862689 Updates: 66/13001, Avg Grad: 0.009682583622634410\n",
            "Epoch: 17, Loss: 0.0005803905660286546 Updates: 71/14001, Avg Grad: 0.0452687963843345640\n",
            "Epoch: 17, Loss: 0.0014844724209979177 Updates: 76/15001, Avg Grad: 0.040989294648170470\n",
            "Epoch: 17, Loss: 0.0006991936243139207 Updates: 81/16001, Avg Grad: 0.033429916948080060\n",
            "Epoch: 17, Loss: 0.0020182374864816666 Updates: 86/17001, Avg Grad: 0.0243640635162591930\n",
            "Epoch: 17, Loss: 0.0028454260900616646 Updates: 91/18001, Avg Grad: 0.04377552121877670\n",
            "Epoch: 17, Loss: 0.002775017637759447 Updates: 96/19001, Avg Grad: 0.0304260402917861940\n",
            "Epoch 16 iteration 0 Loss: 0.368 | Acc: 100.000% (1/1)\n",
            "Epoch 16 iteration 100 Loss: 0.577 | Acc: 71.287% (72/101)\n",
            "Epoch 16 iteration 200 Loss: 0.560 | Acc: 71.642% (144/201)\n",
            "Epoch 16 iteration 300 Loss: 0.576 | Acc: 70.432% (212/301)\n",
            "Epoch 16 iteration 400 Loss: 0.569 | Acc: 72.319% (290/401)\n",
            "Epoch 16 iteration 500 Loss: 0.576 | Acc: 71.457% (358/501)\n",
            "Test accuracy: 0.7145708799362183\n",
            "Epoch: 17, Loss: 0.002754610264673829 Updates: 101/20001, Avg Grad: 0.0089479396119713780\n",
            "Epoch: 17, Loss: 0.003076992928981781 Updates: 106/21001, Avg Grad: 0.0163318496197462080\n",
            "Epoch: 17, Loss: 0.0011603643652051687 Updates: 111/22001, Avg Grad: 0.031678073108196260\n",
            "Epoch: 17, Loss: 0.0025109266862273216 Updates: 116/23001, Avg Grad: 0.0171645544469356540\n",
            "Epoch: 17, Loss: 0.0029684666078537703 Updates: 121/24001, Avg Grad: 0.017536247149109840\n",
            "Epoch: 17, Loss: 0.0017685352358967066 Updates: 126/25001, Avg Grad: 0.0151573801413178440\n",
            "Epoch: 17, Loss: 0.004304785747081041 Updates: 131/26001, Avg Grad: 0.0150354830548167230\n",
            "Epoch: 17, Loss: 0.0007116696797311306 Updates: 136/27001, Avg Grad: 0.034905500710010530\n",
            "Epoch: 17, Loss: 0.002339140046387911 Updates: 141/28001, Avg Grad: 0.0094024157151579860\n",
            "Epoch: 17, Loss: 0.0008592557860538363 Updates: 146/29001, Avg Grad: 0.0101211545988917350\n",
            "Epoch 16 iteration 0 Loss: 1.339 | Acc: 0.000% (0/1)\n",
            "Epoch 16 iteration 100 Loss: 0.793 | Acc: 54.455% (55/101)\n",
            "Epoch 16 iteration 200 Loss: 0.757 | Acc: 58.209% (117/201)\n",
            "Epoch 16 iteration 300 Loss: 0.715 | Acc: 61.794% (186/301)\n",
            "Epoch 16 iteration 400 Loss: 0.656 | Acc: 66.833% (268/401)\n",
            "Epoch 16 iteration 500 Loss: 0.636 | Acc: 67.265% (337/501)\n",
            "Test accuracy: 0.6726546883583069\n",
            "Epoch: 17, Loss: 0.0018101510358974338 Updates: 151/30001, Avg Grad: 0.0272011514753103260\n",
            "Epoch: 17, Loss: 0.001483622007071972 Updates: 156/31001, Avg Grad: 0.0118960728868842120\n",
            "Epoch: 17, Loss: 0.0013832810800522566 Updates: 161/32001, Avg Grad: 0.0260697789490222930\n",
            "Epoch 17 iteration 0 Loss: 0.206 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.593 | Acc: 72.277% (73/101)\n",
            "Epoch 17 iteration 200 Loss: 0.616 | Acc: 69.154% (139/201)\n",
            "Epoch 17 iteration 300 Loss: 0.637 | Acc: 65.449% (197/301)\n",
            "Epoch 17 iteration 400 Loss: 0.614 | Acc: 68.080% (273/401)\n",
            "Epoch 17 iteration 500 Loss: 0.595 | Acc: 69.860% (350/501)\n",
            "Test accuracy: 0.6986027956008911\n",
            "Epoch: 18, Loss: 0.007712981663644314 Updates: 1/1, Avg Grad: 0.00180101231671869750\n",
            "Epoch: 18, Loss: 0.00025516300229355693 Updates: 6/1001, Avg Grad: 0.024551581591367720\n",
            "Epoch: 18, Loss: 0.0022440135944634676 Updates: 11/2001, Avg Grad: 0.041954774409532550\n",
            "Epoch: 18, Loss: 0.0024633510038256645 Updates: 16/3001, Avg Grad: 0.0311991255730390550\n",
            "Epoch: 18, Loss: 0.0015055604744702578 Updates: 21/4001, Avg Grad: 0.0089179538190364840\n",
            "Epoch: 18, Loss: 0.003693617647513747 Updates: 26/5001, Avg Grad: 0.0113140465691685680\n",
            "Epoch: 18, Loss: 0.0035421508364379406 Updates: 31/6001, Avg Grad: 0.017666988074779510\n",
            "Epoch: 18, Loss: 0.005182653199881315 Updates: 36/7001, Avg Grad: 0.0106333950534462930\n",
            "Epoch: 18, Loss: 0.002456885762512684 Updates: 41/8001, Avg Grad: 0.0154792126268148420\n",
            "Epoch: 18, Loss: 0.005355333909392357 Updates: 46/9001, Avg Grad: 0.034534703940153120\n",
            "Epoch 17 iteration 0 Loss: 1.577 | Acc: 0.000% (0/1)\n",
            "Epoch 17 iteration 100 Loss: 0.712 | Acc: 58.416% (59/101)\n",
            "Epoch 17 iteration 200 Loss: 0.658 | Acc: 64.677% (130/201)\n",
            "Epoch 17 iteration 300 Loss: 0.643 | Acc: 65.781% (198/301)\n",
            "Epoch 17 iteration 400 Loss: 0.630 | Acc: 68.080% (273/401)\n",
            "Epoch 17 iteration 500 Loss: 0.636 | Acc: 66.667% (334/501)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 18, Loss: 0.006031198892742395 Updates: 51/10001, Avg Grad: 0.026862902566790580\n",
            "Epoch: 18, Loss: 0.0017678375588729978 Updates: 56/11001, Avg Grad: 0.041460867971181870\n",
            "Epoch: 18, Loss: 0.0006371737108565867 Updates: 61/12001, Avg Grad: 0.0236686412245035170\n",
            "Epoch: 18, Loss: 0.0004986096755601466 Updates: 66/13001, Avg Grad: 0.0248824693262577060\n",
            "Epoch: 18, Loss: 0.009120053611695766 Updates: 71/14001, Avg Grad: 0.074288591742515560\n",
            "Epoch: 18, Loss: 0.0009682997479103506 Updates: 76/15001, Avg Grad: 0.0086325127631425860\n",
            "Epoch: 18, Loss: 0.00228647468611598 Updates: 81/16001, Avg Grad: 0.0105159059166908260\n",
            "Epoch: 18, Loss: 0.0118012186139822 Updates: 86/17001, Avg Grad: 0.0288237184286117550\n",
            "Epoch: 18, Loss: 0.002203945303335786 Updates: 91/18001, Avg Grad: 0.037948533892631530\n",
            "Epoch: 18, Loss: 0.006166745442897081 Updates: 96/19001, Avg Grad: 0.0423207506537437440\n",
            "Epoch 17 iteration 0 Loss: 1.591 | Acc: 0.000% (0/1)\n",
            "Epoch 17 iteration 100 Loss: 0.527 | Acc: 75.248% (76/101)\n",
            "Epoch 17 iteration 200 Loss: 0.576 | Acc: 73.134% (147/201)\n",
            "Epoch 17 iteration 300 Loss: 0.624 | Acc: 68.771% (207/301)\n",
            "Epoch 17 iteration 400 Loss: 0.629 | Acc: 65.835% (264/401)\n",
            "Epoch 17 iteration 500 Loss: 0.644 | Acc: 65.070% (326/501)\n",
            "Test accuracy: 0.6506986021995544\n",
            "Epoch: 18, Loss: 0.004525023512542248 Updates: 101/20001, Avg Grad: 0.0082766124978661540\n",
            "Epoch: 18, Loss: 0.005858609452843666 Updates: 106/21001, Avg Grad: 0.0082437628880143170\n",
            "Epoch: 18, Loss: 0.0033459654077887535 Updates: 111/22001, Avg Grad: 0.0111960126087069510\n",
            "Epoch: 18, Loss: 0.0034844514448195696 Updates: 116/23001, Avg Grad: 0.042467478662729260\n",
            "Epoch: 18, Loss: 0.004207308869808912 Updates: 121/24001, Avg Grad: 0.014766232110559940\n",
            "Epoch: 18, Loss: 0.005380505695939064 Updates: 126/25001, Avg Grad: 0.021522182971239090\n",
            "Epoch: 18, Loss: 0.0038872205186635256 Updates: 131/26001, Avg Grad: 0.035871282219886780\n",
            "Epoch: 18, Loss: 0.0017800717614591122 Updates: 136/27001, Avg Grad: 0.0215807650238275530\n",
            "Epoch: 18, Loss: 0.00032562625710852444 Updates: 141/28001, Avg Grad: 0.0230133198201656340\n",
            "Epoch: 18, Loss: 0.0030026219319552183 Updates: 146/29001, Avg Grad: 0.0214489214122295380\n",
            "Epoch 17 iteration 0 Loss: 0.483 | Acc: 100.000% (1/1)\n",
            "Epoch 17 iteration 100 Loss: 0.595 | Acc: 69.307% (70/101)\n",
            "Epoch 17 iteration 200 Loss: 0.601 | Acc: 71.144% (143/201)\n",
            "Epoch 17 iteration 300 Loss: 0.610 | Acc: 68.771% (207/301)\n",
            "Epoch 17 iteration 400 Loss: 0.600 | Acc: 69.576% (279/401)\n",
            "Epoch 17 iteration 500 Loss: 0.604 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 18, Loss: 0.0010614324128255248 Updates: 151/30001, Avg Grad: 0.0089356414973735810\n",
            "Epoch: 18, Loss: 0.000283837434835732 Updates: 156/31001, Avg Grad: 0.023029679432511330\n",
            "Epoch: 18, Loss: 0.0013763807946816087 Updates: 161/32001, Avg Grad: 0.017845831811428070\n",
            "Epoch 18 iteration 0 Loss: 0.243 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.612 | Acc: 68.317% (69/101)\n",
            "Epoch 18 iteration 200 Loss: 0.649 | Acc: 63.184% (127/201)\n",
            "Epoch 18 iteration 300 Loss: 0.662 | Acc: 64.452% (194/301)\n",
            "Epoch 18 iteration 400 Loss: 0.646 | Acc: 64.838% (260/401)\n",
            "Epoch 18 iteration 500 Loss: 0.639 | Acc: 65.669% (329/501)\n",
            "Test accuracy: 0.6566866040229797\n",
            "Epoch: 19, Loss: 0.005419747903943062 Updates: 1/1, Avg Grad: 0.0019125288818031550\n",
            "Epoch: 19, Loss: 0.00857594981789589 Updates: 6/1001, Avg Grad: 0.016994317993521690\n",
            "Epoch: 19, Loss: 0.0013992893509566784 Updates: 11/2001, Avg Grad: 0.0063739055767655370\n",
            "Epoch: 19, Loss: 0.006923648063093424 Updates: 16/3001, Avg Grad: 0.0327188298106193540\n",
            "Epoch: 19, Loss: 0.0020868785213679075 Updates: 21/4001, Avg Grad: 0.0208021812140941620\n",
            "Epoch: 19, Loss: 0.007923084311187267 Updates: 26/5001, Avg Grad: 0.0139034362509846690\n",
            "Epoch: 19, Loss: 0.0034895665012300014 Updates: 31/6001, Avg Grad: 0.0249876119196414950\n",
            "Epoch: 19, Loss: 0.0004918357590213418 Updates: 36/7001, Avg Grad: 0.0344551838934421540\n",
            "Epoch: 19, Loss: 0.004339191596955061 Updates: 41/8001, Avg Grad: 0.0200215857475996020\n",
            "Epoch: 19, Loss: 0.0009709907462820411 Updates: 46/9001, Avg Grad: 0.021913118660449980\n",
            "Epoch 18 iteration 0 Loss: 1.052 | Acc: 0.000% (0/1)\n",
            "Epoch 18 iteration 100 Loss: 0.625 | Acc: 63.366% (64/101)\n",
            "Epoch 18 iteration 200 Loss: 0.610 | Acc: 63.184% (127/201)\n",
            "Epoch 18 iteration 300 Loss: 0.575 | Acc: 67.110% (202/301)\n",
            "Epoch 18 iteration 400 Loss: 0.567 | Acc: 69.077% (277/401)\n",
            "Epoch 18 iteration 500 Loss: 0.564 | Acc: 69.860% (350/501)\n",
            "Test accuracy: 0.6986027956008911\n",
            "Epoch: 19, Loss: 0.0017101832199841738 Updates: 51/10001, Avg Grad: 0.0307369735091924670\n",
            "Epoch: 19, Loss: 0.004230371210724115 Updates: 56/11001, Avg Grad: 0.0179593674838542940\n",
            "Epoch: 19, Loss: 0.0026534113567322493 Updates: 61/12001, Avg Grad: 0.040520895272493360\n",
            "Epoch: 19, Loss: 0.0031816568225622177 Updates: 66/13001, Avg Grad: 0.023932857438921930\n",
            "Epoch: 19, Loss: 0.0043563516810536385 Updates: 71/14001, Avg Grad: 0.044215146452188490\n",
            "Epoch: 19, Loss: 0.0008809719001874328 Updates: 76/15001, Avg Grad: 0.00754904141649603840\n",
            "Epoch: 19, Loss: 0.0021908546332269907 Updates: 81/16001, Avg Grad: 0.019387330859899520\n",
            "Epoch: 19, Loss: 0.007135372143238783 Updates: 86/17001, Avg Grad: 0.031355135142803190\n",
            "Epoch: 19, Loss: 0.002164226258173585 Updates: 91/18001, Avg Grad: 0.02871946617960930\n",
            "Epoch: 19, Loss: 0.003350457176566124 Updates: 96/19001, Avg Grad: 0.04830302670598030\n",
            "Epoch 18 iteration 0 Loss: 0.091 | Acc: 100.000% (1/1)\n",
            "Epoch 18 iteration 100 Loss: 0.707 | Acc: 62.376% (63/101)\n",
            "Epoch 18 iteration 200 Loss: 0.635 | Acc: 66.169% (133/201)\n",
            "Epoch 18 iteration 300 Loss: 0.640 | Acc: 67.442% (203/301)\n",
            "Epoch 18 iteration 400 Loss: 0.626 | Acc: 67.332% (270/401)\n",
            "Epoch 18 iteration 500 Loss: 0.628 | Acc: 66.467% (333/501)\n",
            "Test accuracy: 0.6646706461906433\n",
            "Epoch: 19, Loss: 0.0020889979787170887 Updates: 101/20001, Avg Grad: 0.018881808966398240\n",
            "Epoch: 19, Loss: 0.002765767276287079 Updates: 106/21001, Avg Grad: 0.0321891792118549350\n",
            "Epoch: 19, Loss: 0.0008453726768493652 Updates: 111/22001, Avg Grad: 0.0294662900269031520\n",
            "Epoch: 19, Loss: 0.006051764357835054 Updates: 116/23001, Avg Grad: 0.036087349057197570\n",
            "Epoch: 19, Loss: 0.006695198826491833 Updates: 121/24001, Avg Grad: 0.0190064813941717150\n",
            "Epoch: 19, Loss: 0.0019171461462974548 Updates: 126/25001, Avg Grad: 0.0524670891463756560\n",
            "Epoch: 19, Loss: 0.0007294812239706516 Updates: 131/26001, Avg Grad: 0.037261761724948880\n",
            "Epoch: 19, Loss: 0.0006441849982365966 Updates: 136/27001, Avg Grad: 0.035054653882980350\n",
            "Epoch: 19, Loss: 0.0012560300529003143 Updates: 141/28001, Avg Grad: 0.0117959631606936450\n",
            "Epoch: 19, Loss: 0.0006852215738035738 Updates: 146/29001, Avg Grad: 0.0106756091117858890\n",
            "Epoch 18 iteration 0 Loss: 0.731 | Acc: 0.000% (0/1)\n",
            "Epoch 18 iteration 100 Loss: 0.497 | Acc: 75.248% (76/101)\n",
            "Epoch 18 iteration 200 Loss: 0.519 | Acc: 74.627% (150/201)\n",
            "Epoch 18 iteration 300 Loss: 0.522 | Acc: 74.086% (223/301)\n",
            "Epoch 18 iteration 400 Loss: 0.538 | Acc: 74.065% (297/401)\n",
            "Epoch 18 iteration 500 Loss: 0.539 | Acc: 73.453% (368/501)\n",
            "Test accuracy: 0.7345309257507324\n",
            "Epoch: 19, Loss: 0.004005535040050745 Updates: 151/30001, Avg Grad: 0.0091135380789637570\n",
            "Epoch: 19, Loss: 0.0024981058668345213 Updates: 156/31001, Avg Grad: 0.0191163178533315660\n",
            "Epoch: 19, Loss: 0.0006741295219399035 Updates: 161/32001, Avg Grad: 0.0310906935483217240\n",
            "Epoch 19 iteration 0 Loss: 0.987 | Acc: 0.000% (0/1)\n",
            "Epoch 19 iteration 100 Loss: 0.529 | Acc: 74.257% (75/101)\n",
            "Epoch 19 iteration 200 Loss: 0.576 | Acc: 73.632% (148/201)\n",
            "Epoch 19 iteration 300 Loss: 0.569 | Acc: 73.754% (222/301)\n",
            "Epoch 19 iteration 400 Loss: 0.573 | Acc: 73.815% (296/401)\n",
            "Epoch 19 iteration 500 Loss: 0.555 | Acc: 74.850% (375/501)\n",
            "Test accuracy: 0.7485029697418213\n",
            "Epoch: 20, Loss: 0.012689182534813881 Updates: 1/1, Avg Grad: 0.00275182398036122320\n",
            "Epoch: 20, Loss: 0.0013114092871546745 Updates: 6/1001, Avg Grad: 0.0111281322315335270\n",
            "Epoch: 20, Loss: 0.0032154435757547617 Updates: 11/2001, Avg Grad: 0.0571060031652450560\n",
            "Epoch: 20, Loss: 0.0010963259264826775 Updates: 16/3001, Avg Grad: 0.0167670026421546940\n",
            "Epoch: 20, Loss: 0.0036298949271440506 Updates: 21/4001, Avg Grad: 0.0222949869930744170\n",
            "Epoch: 20, Loss: 0.005551159381866455 Updates: 26/5001, Avg Grad: 0.0175759978592395780\n",
            "Epoch: 20, Loss: 0.000817422813270241 Updates: 31/6001, Avg Grad: 0.041049361228942870\n",
            "Epoch: 20, Loss: 0.002040834166109562 Updates: 36/7001, Avg Grad: 0.0246688183397054670\n",
            "Epoch: 20, Loss: 0.0020863618701696396 Updates: 41/8001, Avg Grad: 0.0200995653867721560\n",
            "Epoch: 20, Loss: 0.0016768323257565498 Updates: 46/9001, Avg Grad: 0.0168924387544393540\n",
            "Epoch 19 iteration 0 Loss: 1.012 | Acc: 0.000% (0/1)\n",
            "Epoch 19 iteration 100 Loss: 0.557 | Acc: 68.317% (69/101)\n",
            "Epoch 19 iteration 200 Loss: 0.539 | Acc: 72.139% (145/201)\n",
            "Epoch 19 iteration 300 Loss: 0.579 | Acc: 70.100% (211/301)\n",
            "Epoch 19 iteration 400 Loss: 0.550 | Acc: 72.070% (289/401)\n",
            "Epoch 19 iteration 500 Loss: 0.550 | Acc: 71.856% (360/501)\n",
            "Test accuracy: 0.71856290102005\n",
            "Epoch: 20, Loss: 0.0006563101778738201 Updates: 51/10001, Avg Grad: 0.0132675096392631530\n",
            "Epoch: 20, Loss: 0.009040102362632751 Updates: 56/11001, Avg Grad: 0.041026845574378970\n",
            "Epoch: 20, Loss: 0.0026925369165837765 Updates: 61/12001, Avg Grad: 0.0102172279730439190\n",
            "Epoch: 20, Loss: 0.0022779100108891726 Updates: 66/13001, Avg Grad: 0.0283636376261711120\n",
            "Epoch: 20, Loss: 0.004778469912707806 Updates: 71/14001, Avg Grad: 0.0169100500643253330\n",
            "Epoch: 20, Loss: 0.004731112625449896 Updates: 76/15001, Avg Grad: 0.0312182046473026280\n",
            "Epoch: 20, Loss: 0.0039414591155946255 Updates: 81/16001, Avg Grad: 0.0091853505000472070\n",
            "Epoch: 20, Loss: 0.004058892838656902 Updates: 86/17001, Avg Grad: 0.0152365472167730330\n",
            "Epoch: 20, Loss: 0.0009342145058326423 Updates: 91/18001, Avg Grad: 0.0313463620841503140\n",
            "Epoch: 20, Loss: 0.004973048344254494 Updates: 96/19001, Avg Grad: 0.0261750314384698870\n",
            "Epoch 19 iteration 0 Loss: 0.153 | Acc: 100.000% (1/1)\n",
            "Epoch 19 iteration 100 Loss: 0.613 | Acc: 68.317% (69/101)\n",
            "Epoch 19 iteration 200 Loss: 0.617 | Acc: 67.662% (136/201)\n",
            "Epoch 19 iteration 300 Loss: 0.613 | Acc: 67.110% (202/301)\n",
            "Epoch 19 iteration 400 Loss: 0.626 | Acc: 65.586% (263/401)\n",
            "Epoch 19 iteration 500 Loss: 0.641 | Acc: 65.070% (326/501)\n",
            "Test accuracy: 0.6506986021995544\n",
            "Epoch: 20, Loss: 0.0016851612599566579 Updates: 101/20001, Avg Grad: 0.034379851073026660\n",
            "Epoch: 20, Loss: 0.0032925810664892197 Updates: 106/21001, Avg Grad: 0.0248423889279365540\n",
            "Epoch: 20, Loss: 0.0009963099146261811 Updates: 111/22001, Avg Grad: 0.0183011107146739960\n",
            "Epoch: 20, Loss: 0.003274321323260665 Updates: 116/23001, Avg Grad: 0.0160291008651256560\n",
            "Epoch: 20, Loss: 0.0010596889769658446 Updates: 121/24001, Avg Grad: 0.031986437737941740\n",
            "Epoch: 20, Loss: 0.0069093527272343636 Updates: 126/25001, Avg Grad: 0.0301196817308664320\n",
            "Epoch: 20, Loss: 0.005390016362071037 Updates: 131/26001, Avg Grad: 0.02304391935467720\n",
            "Epoch: 20, Loss: 0.0017435195622965693 Updates: 136/27001, Avg Grad: 0.0136241642758250240\n",
            "Epoch: 20, Loss: 0.004031652584671974 Updates: 141/28001, Avg Grad: 0.017166983336210250\n",
            "Epoch: 20, Loss: 0.0005473807104863226 Updates: 146/29001, Avg Grad: 0.0096534527838230130\n",
            "Epoch 19 iteration 0 Loss: 0.347 | Acc: 100.000% (1/1)\n",
            "Epoch 19 iteration 100 Loss: 0.664 | Acc: 58.416% (59/101)\n",
            "Epoch 19 iteration 200 Loss: 0.591 | Acc: 65.672% (132/201)\n",
            "Epoch 19 iteration 300 Loss: 0.587 | Acc: 68.106% (205/301)\n",
            "Epoch 19 iteration 400 Loss: 0.572 | Acc: 69.327% (278/401)\n",
            "Epoch 19 iteration 500 Loss: 0.557 | Acc: 70.459% (353/501)\n",
            "Test accuracy: 0.7045907974243164\n",
            "Epoch: 20, Loss: 0.0013020524056628346 Updates: 151/30001, Avg Grad: 0.0177502222359180450\n",
            "Epoch: 20, Loss: 0.0006790097104385495 Updates: 156/31001, Avg Grad: 0.032519076019525530\n",
            "Epoch: 20, Loss: 0.009088915772736073 Updates: 161/32001, Avg Grad: 0.039088588207960130\n",
            "Epoch 20 iteration 0 Loss: 0.390 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.630 | Acc: 62.376% (63/101)\n",
            "Epoch 20 iteration 200 Loss: 0.612 | Acc: 67.662% (136/201)\n",
            "Epoch 20 iteration 300 Loss: 0.652 | Acc: 63.787% (192/301)\n",
            "Epoch 20 iteration 400 Loss: 0.658 | Acc: 63.840% (256/401)\n",
            "Epoch 20 iteration 500 Loss: 0.641 | Acc: 65.070% (326/501)\n",
            "Test accuracy: 0.6506986021995544\n",
            "Epoch: 21, Loss: 0.0036365571431815624 Updates: 1/1, Avg Grad: 0.00146719010081142190\n",
            "Epoch: 21, Loss: 0.004500268492847681 Updates: 6/1001, Avg Grad: 0.06131635233759880\n",
            "Epoch: 21, Loss: 0.006342914886772633 Updates: 11/2001, Avg Grad: 0.018294999375939370\n",
            "Epoch: 21, Loss: 0.0044640121050179005 Updates: 16/3001, Avg Grad: 0.0266312006860971450\n",
            "Epoch: 21, Loss: 0.0023078948725014925 Updates: 21/4001, Avg Grad: 0.0243064910173416140\n",
            "Epoch: 21, Loss: 0.0008419256773777306 Updates: 26/5001, Avg Grad: 0.0283374935388565060\n",
            "Epoch: 21, Loss: 0.002699450356885791 Updates: 31/6001, Avg Grad: 0.0122266523540019990\n",
            "Epoch: 21, Loss: 0.0008005273411981761 Updates: 36/7001, Avg Grad: 0.027792839333415030\n",
            "Epoch: 21, Loss: 0.0015462595038115978 Updates: 41/8001, Avg Grad: 0.0097690485417842860\n",
            "Epoch: 21, Loss: 0.007586725987493992 Updates: 46/9001, Avg Grad: 0.0236496068537235260\n",
            "Epoch 20 iteration 0 Loss: 0.120 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.501 | Acc: 77.228% (78/101)\n",
            "Epoch 20 iteration 200 Loss: 0.555 | Acc: 71.642% (144/201)\n",
            "Epoch 20 iteration 300 Loss: 0.575 | Acc: 69.435% (209/301)\n",
            "Epoch 20 iteration 400 Loss: 0.606 | Acc: 68.828% (276/401)\n",
            "Epoch 20 iteration 500 Loss: 0.613 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 21, Loss: 0.008734804578125477 Updates: 51/10001, Avg Grad: 0.009871006011962890\n",
            "Epoch: 21, Loss: 0.00497778644785285 Updates: 56/11001, Avg Grad: 0.035964339971542360\n",
            "Epoch: 21, Loss: 0.001322825439274311 Updates: 61/12001, Avg Grad: 0.0139426253736019130\n",
            "Epoch: 21, Loss: 0.0041171410121023655 Updates: 66/13001, Avg Grad: 0.034146580845117570\n",
            "Epoch: 21, Loss: 0.008882214315235615 Updates: 71/14001, Avg Grad: 0.0245094038546085360\n",
            "Epoch: 21, Loss: 0.003113951999694109 Updates: 76/15001, Avg Grad: 0.0171552710235118870\n",
            "Epoch: 21, Loss: 0.0010000517359003425 Updates: 81/16001, Avg Grad: 0.0142377149313688280\n",
            "Epoch: 21, Loss: 0.0031271670013666153 Updates: 86/17001, Avg Grad: 0.03063722327351570\n",
            "Epoch: 21, Loss: 0.0052985441870987415 Updates: 91/18001, Avg Grad: 0.0247040912508964540\n",
            "Epoch: 21, Loss: 0.008466733619570732 Updates: 96/19001, Avg Grad: 0.048468045890331270\n",
            "Epoch 20 iteration 0 Loss: 0.374 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.572 | Acc: 70.297% (71/101)\n",
            "Epoch 20 iteration 200 Loss: 0.589 | Acc: 70.647% (142/201)\n",
            "Epoch 20 iteration 300 Loss: 0.601 | Acc: 70.432% (212/301)\n",
            "Epoch 20 iteration 400 Loss: 0.588 | Acc: 71.322% (286/401)\n",
            "Epoch 20 iteration 500 Loss: 0.590 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 21, Loss: 0.0024734875187277794 Updates: 101/20001, Avg Grad: 0.0095403976738452910\n",
            "Epoch: 21, Loss: 0.0017228289507329464 Updates: 106/21001, Avg Grad: 0.0217128992080688480\n",
            "Epoch: 21, Loss: 0.0025007957592606544 Updates: 111/22001, Avg Grad: 0.0207284279167652130\n",
            "Epoch: 21, Loss: 0.0006115538999438286 Updates: 116/23001, Avg Grad: 0.0268009677529335020\n",
            "Epoch: 21, Loss: 0.0013664205325767398 Updates: 121/24001, Avg Grad: 0.0165056586265563960\n",
            "Epoch: 21, Loss: 0.003239545039832592 Updates: 126/25001, Avg Grad: 0.0291459634900093080\n",
            "Epoch: 21, Loss: 0.0008016882347874343 Updates: 131/26001, Avg Grad: 0.0439232140779495240\n",
            "Epoch: 21, Loss: 0.004307325929403305 Updates: 136/27001, Avg Grad: 0.0180685073137283330\n",
            "Epoch: 21, Loss: 0.00046621845103800297 Updates: 141/28001, Avg Grad: 0.0189457535743713380\n",
            "Epoch: 21, Loss: 0.00602988013997674 Updates: 146/29001, Avg Grad: 0.032492667436599730\n",
            "Epoch 20 iteration 0 Loss: 0.439 | Acc: 100.000% (1/1)\n",
            "Epoch 20 iteration 100 Loss: 0.622 | Acc: 71.287% (72/101)\n",
            "Epoch 20 iteration 200 Loss: 0.610 | Acc: 69.154% (139/201)\n",
            "Epoch 20 iteration 300 Loss: 0.614 | Acc: 67.442% (203/301)\n",
            "Epoch 20 iteration 400 Loss: 0.615 | Acc: 67.332% (270/401)\n",
            "Epoch 20 iteration 500 Loss: 0.603 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 21, Loss: 0.0017491995822638273 Updates: 151/30001, Avg Grad: 0.0237679053097963330\n",
            "Epoch: 21, Loss: 0.0018198627512902021 Updates: 156/31001, Avg Grad: 0.0173463523387908940\n",
            "Epoch: 21, Loss: 0.005444572772830725 Updates: 161/32001, Avg Grad: 0.028504462912678720\n",
            "Epoch 21 iteration 0 Loss: 1.032 | Acc: 0.000% (0/1)\n",
            "Epoch 21 iteration 100 Loss: 0.547 | Acc: 75.248% (76/101)\n",
            "Epoch 21 iteration 200 Loss: 0.531 | Acc: 76.119% (153/201)\n",
            "Epoch 21 iteration 300 Loss: 0.538 | Acc: 74.751% (225/301)\n",
            "Epoch 21 iteration 400 Loss: 0.543 | Acc: 73.815% (296/401)\n",
            "Epoch 21 iteration 500 Loss: 0.549 | Acc: 72.655% (364/501)\n",
            "Test accuracy: 0.7265468835830688\n",
            "Epoch: 22, Loss: 0.001906871679238975 Updates: 1/1, Avg Grad: 0.0008447822183370590\n",
            "Epoch: 22, Loss: 0.0013216233346611261 Updates: 6/1001, Avg Grad: 0.04104445129632950\n",
            "Epoch: 22, Loss: 0.006967803463339806 Updates: 11/2001, Avg Grad: 0.03855290636420250\n",
            "Epoch: 22, Loss: 0.00963837094604969 Updates: 16/3001, Avg Grad: 0.068643920123577120\n",
            "Epoch: 22, Loss: 0.0028624963015317917 Updates: 21/4001, Avg Grad: 0.063200473785400390\n",
            "Epoch: 22, Loss: 0.0011956217931583524 Updates: 26/5001, Avg Grad: 0.009161702357232570\n",
            "Epoch: 22, Loss: 0.004200647585093975 Updates: 31/6001, Avg Grad: 0.0082504255697131160\n",
            "Epoch: 22, Loss: 0.006080812308937311 Updates: 36/7001, Avg Grad: 0.052289538085460660\n",
            "Epoch: 22, Loss: 0.004935178905725479 Updates: 41/8001, Avg Grad: 0.043959535658359530\n",
            "Epoch: 22, Loss: 0.0006460332660935819 Updates: 46/9001, Avg Grad: 0.0192490033805370330\n",
            "Epoch 21 iteration 0 Loss: 0.753 | Acc: 0.000% (0/1)\n",
            "Epoch 21 iteration 100 Loss: 0.558 | Acc: 71.287% (72/101)\n",
            "Epoch 21 iteration 200 Loss: 0.542 | Acc: 70.149% (141/201)\n",
            "Epoch 21 iteration 300 Loss: 0.573 | Acc: 68.106% (205/301)\n",
            "Epoch 21 iteration 400 Loss: 0.564 | Acc: 69.077% (277/401)\n",
            "Epoch 21 iteration 500 Loss: 0.572 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 22, Loss: 0.002951425965875387 Updates: 51/10001, Avg Grad: 0.0198144745081663130\n",
            "Epoch: 22, Loss: 0.0011569552589207888 Updates: 56/11001, Avg Grad: 0.0285636130720376970\n",
            "Epoch: 22, Loss: 0.0034492556005716324 Updates: 61/12001, Avg Grad: 0.0317356288433074950\n",
            "Epoch: 22, Loss: 0.0019923902582377195 Updates: 66/13001, Avg Grad: 0.0177664756774902340\n",
            "Epoch: 22, Loss: 0.0009002364240586758 Updates: 71/14001, Avg Grad: 0.0314912945032119750\n",
            "Epoch: 22, Loss: 0.004826094023883343 Updates: 76/15001, Avg Grad: 0.0240290425717830660\n",
            "Epoch: 22, Loss: 0.003102110233157873 Updates: 81/16001, Avg Grad: 0.025580607354640960\n",
            "Epoch: 22, Loss: 0.00041208989568986 Updates: 86/17001, Avg Grad: 0.0258804373443126680\n",
            "Epoch: 22, Loss: 0.006828967947512865 Updates: 91/18001, Avg Grad: 0.0391798578202724460\n",
            "Epoch: 22, Loss: 0.006322028581053019 Updates: 96/19001, Avg Grad: 0.020882358774542810\n",
            "Epoch 21 iteration 0 Loss: 0.465 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.646 | Acc: 65.347% (66/101)\n",
            "Epoch 21 iteration 200 Loss: 0.608 | Acc: 67.662% (136/201)\n",
            "Epoch 21 iteration 300 Loss: 0.604 | Acc: 67.110% (202/301)\n",
            "Epoch 21 iteration 400 Loss: 0.605 | Acc: 67.082% (269/401)\n",
            "Epoch 21 iteration 500 Loss: 0.594 | Acc: 67.066% (336/501)\n",
            "Test accuracy: 0.6706587076187134\n",
            "Epoch: 22, Loss: 0.001326136407442391 Updates: 101/20001, Avg Grad: 0.0371549949049949650\n",
            "Epoch: 22, Loss: 0.0025828396901488304 Updates: 106/21001, Avg Grad: 0.0278219897300004960\n",
            "Epoch: 22, Loss: 0.0034448066726326942 Updates: 111/22001, Avg Grad: 0.036177571862936020\n",
            "Epoch: 22, Loss: 0.004607468377798796 Updates: 116/23001, Avg Grad: 0.012200152501463890\n",
            "Epoch: 22, Loss: 0.0015529260272160172 Updates: 121/24001, Avg Grad: 0.04352654516696930\n",
            "Epoch: 22, Loss: 0.0014843781245872378 Updates: 126/25001, Avg Grad: 0.0381852015852928160\n",
            "Epoch: 22, Loss: 0.0011906224535778165 Updates: 131/26001, Avg Grad: 0.0133884986862540250\n",
            "Epoch: 22, Loss: 0.0042629968374967575 Updates: 136/27001, Avg Grad: 0.0344663746654987340\n",
            "Epoch: 22, Loss: 0.000722475117072463 Updates: 141/28001, Avg Grad: 0.024154953658580780\n",
            "Epoch: 22, Loss: 0.0014131289208307862 Updates: 146/29001, Avg Grad: 0.0221450868993997570\n",
            "Epoch 21 iteration 0 Loss: 0.247 | Acc: 100.000% (1/1)\n",
            "Epoch 21 iteration 100 Loss: 0.626 | Acc: 67.327% (68/101)\n",
            "Epoch 21 iteration 200 Loss: 0.642 | Acc: 65.174% (131/201)\n",
            "Epoch 21 iteration 300 Loss: 0.627 | Acc: 67.442% (203/301)\n",
            "Epoch 21 iteration 400 Loss: 0.607 | Acc: 69.825% (280/401)\n",
            "Epoch 21 iteration 500 Loss: 0.599 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 22, Loss: 0.0002122399164363742 Updates: 151/30001, Avg Grad: 0.0195942632853984830\n",
            "Epoch: 22, Loss: 0.0009793422650545835 Updates: 156/31001, Avg Grad: 0.0291055720299482350\n",
            "Epoch: 22, Loss: 0.0015612452989444137 Updates: 161/32001, Avg Grad: 0.031948834657669070\n",
            "Epoch 22 iteration 0 Loss: 1.821 | Acc: 0.000% (0/1)\n",
            "Epoch 22 iteration 100 Loss: 0.629 | Acc: 69.307% (70/101)\n",
            "Epoch 22 iteration 200 Loss: 0.656 | Acc: 68.159% (137/201)\n",
            "Epoch 22 iteration 300 Loss: 0.678 | Acc: 64.784% (195/301)\n",
            "Epoch 22 iteration 400 Loss: 0.698 | Acc: 64.090% (257/401)\n",
            "Epoch 22 iteration 500 Loss: 0.695 | Acc: 64.072% (321/501)\n",
            "Test accuracy: 0.6407185792922974\n",
            "Epoch: 23, Loss: 0.006706431973725557 Updates: 1/1, Avg Grad: 0.00185207475442439320\n",
            "Epoch: 23, Loss: 0.003733188845217228 Updates: 6/1001, Avg Grad: 0.0302830152213573460\n",
            "Epoch: 23, Loss: 0.011933059431612492 Updates: 11/2001, Avg Grad: 0.0258292462676763530\n",
            "Epoch: 23, Loss: 0.001287464750930667 Updates: 16/3001, Avg Grad: 0.0190646387636661530\n",
            "Epoch: 23, Loss: 0.005862932652235031 Updates: 21/4001, Avg Grad: 0.0344575792551040650\n",
            "Epoch: 23, Loss: 0.0020301654003560543 Updates: 26/5001, Avg Grad: 0.038078065961599350\n",
            "Epoch: 23, Loss: 0.0002750936255324632 Updates: 31/6001, Avg Grad: 0.060510519891977310\n",
            "Epoch: 23, Loss: 0.00513486610725522 Updates: 36/7001, Avg Grad: 0.0155938779935240750\n",
            "Epoch: 23, Loss: 0.000862276996485889 Updates: 41/8001, Avg Grad: 0.0208291057497262950\n",
            "Epoch: 23, Loss: 0.0006516557768918574 Updates: 46/9001, Avg Grad: 0.048761561512947080\n",
            "Epoch 22 iteration 0 Loss: 1.076 | Acc: 0.000% (0/1)\n",
            "Epoch 22 iteration 100 Loss: 0.561 | Acc: 66.337% (67/101)\n",
            "Epoch 22 iteration 200 Loss: 0.621 | Acc: 66.169% (133/201)\n",
            "Epoch 22 iteration 300 Loss: 0.636 | Acc: 66.445% (200/301)\n",
            "Epoch 22 iteration 400 Loss: 0.612 | Acc: 67.830% (272/401)\n",
            "Epoch 22 iteration 500 Loss: 0.607 | Acc: 68.463% (343/501)\n",
            "Test accuracy: 0.6846307516098022\n",
            "Epoch: 23, Loss: 0.0007095026085153222 Updates: 51/10001, Avg Grad: 0.0298849828541278840\n",
            "Epoch: 23, Loss: 0.0006178579060360789 Updates: 56/11001, Avg Grad: 0.0168295148760080340\n",
            "Epoch: 23, Loss: 0.0008070445619523525 Updates: 61/12001, Avg Grad: 0.0115252872928977010\n",
            "Epoch: 23, Loss: 0.0016751919174566865 Updates: 66/13001, Avg Grad: 0.0288573447614908220\n",
            "Epoch: 23, Loss: 0.007642021868377924 Updates: 71/14001, Avg Grad: 0.014112731441855430\n",
            "Epoch: 23, Loss: 0.001208376488648355 Updates: 76/15001, Avg Grad: 0.046639218926429750\n",
            "Epoch: 23, Loss: 0.0014064015122130513 Updates: 81/16001, Avg Grad: 0.0180966407060623170\n",
            "Epoch: 23, Loss: 0.0019781985320150852 Updates: 86/17001, Avg Grad: 0.0213704816997051240\n",
            "Epoch: 23, Loss: 0.001325487159192562 Updates: 91/18001, Avg Grad: 0.03952922299504280\n",
            "Epoch: 23, Loss: 0.0008675128337927163 Updates: 96/19001, Avg Grad: 0.0349389091134071350\n",
            "Epoch 22 iteration 0 Loss: 0.477 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.618 | Acc: 68.317% (69/101)\n",
            "Epoch 22 iteration 200 Loss: 0.604 | Acc: 67.164% (135/201)\n",
            "Epoch 22 iteration 300 Loss: 0.594 | Acc: 68.771% (207/301)\n",
            "Epoch 22 iteration 400 Loss: 0.591 | Acc: 68.579% (275/401)\n",
            "Epoch 22 iteration 500 Loss: 0.583 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 23, Loss: 0.001371036167256534 Updates: 101/20001, Avg Grad: 0.03599838167428970\n",
            "Epoch: 23, Loss: 0.004538452718406916 Updates: 106/21001, Avg Grad: 0.018113462254405020\n",
            "Epoch: 23, Loss: 0.003327376442030072 Updates: 111/22001, Avg Grad: 0.038731426000595090\n",
            "Epoch: 23, Loss: 0.0008361365762539208 Updates: 116/23001, Avg Grad: 0.0133216548711061480\n",
            "Epoch: 23, Loss: 0.0007221424020826817 Updates: 121/24001, Avg Grad: 0.0305881220847368240\n",
            "Epoch: 23, Loss: 0.0005745034432038665 Updates: 126/25001, Avg Grad: 0.043230518698692320\n",
            "Epoch: 23, Loss: 0.002005126792937517 Updates: 131/26001, Avg Grad: 0.028559951111674310\n",
            "Epoch: 23, Loss: 0.000322751555358991 Updates: 136/27001, Avg Grad: 0.0208757352083921430\n",
            "Epoch: 23, Loss: 0.0030340508092194796 Updates: 141/28001, Avg Grad: 0.0147294886410236360\n",
            "Epoch: 23, Loss: 0.0010747200576588511 Updates: 146/29001, Avg Grad: 0.0343545228242874150\n",
            "Epoch 22 iteration 0 Loss: 0.220 | Acc: 100.000% (1/1)\n",
            "Epoch 22 iteration 100 Loss: 0.588 | Acc: 70.297% (71/101)\n",
            "Epoch 22 iteration 200 Loss: 0.604 | Acc: 68.657% (138/201)\n",
            "Epoch 22 iteration 300 Loss: 0.596 | Acc: 70.764% (213/301)\n",
            "Epoch 22 iteration 400 Loss: 0.594 | Acc: 70.324% (282/401)\n",
            "Epoch 22 iteration 500 Loss: 0.601 | Acc: 68.862% (345/501)\n",
            "Test accuracy: 0.688622772693634\n",
            "Epoch: 23, Loss: 0.0007017438765615225 Updates: 151/30001, Avg Grad: 0.0384889990091323850\n",
            "Epoch: 23, Loss: 0.0020257257856428623 Updates: 156/31001, Avg Grad: 0.022277781739830970\n",
            "Epoch: 23, Loss: 0.005171825177967548 Updates: 161/32001, Avg Grad: 0.017199994996190070\n",
            "Epoch 23 iteration 0 Loss: 2.370 | Acc: 0.000% (0/1)\n",
            "Epoch 23 iteration 100 Loss: 0.550 | Acc: 74.257% (75/101)\n",
            "Epoch 23 iteration 200 Loss: 0.542 | Acc: 75.622% (152/201)\n",
            "Epoch 23 iteration 300 Loss: 0.564 | Acc: 72.757% (219/301)\n",
            "Epoch 23 iteration 400 Loss: 0.577 | Acc: 72.319% (290/401)\n",
            "Epoch 23 iteration 500 Loss: 0.581 | Acc: 71.457% (358/501)\n",
            "Test accuracy: 0.7145708799362183\n",
            "Epoch: 24, Loss: 0.008048078045248985 Updates: 1/1, Avg Grad: 0.00209863134659826760\n",
            "Epoch: 24, Loss: 0.006003645248711109 Updates: 6/1001, Avg Grad: 0.0242061112076044080\n",
            "Epoch: 24, Loss: 0.003099640365689993 Updates: 11/2001, Avg Grad: 0.054716348648071290\n",
            "Epoch: 24, Loss: 0.0019726050086319447 Updates: 16/3001, Avg Grad: 0.023040575906634330\n",
            "Epoch: 24, Loss: 0.0019099533092230558 Updates: 21/4001, Avg Grad: 0.0268914252519607540\n",
            "Epoch: 24, Loss: 0.0076002138666808605 Updates: 26/5001, Avg Grad: 0.0157048162072896960\n",
            "Epoch: 24, Loss: 0.0041965944692492485 Updates: 31/6001, Avg Grad: 0.019058544188737870\n",
            "Epoch: 24, Loss: 0.00037696401705034077 Updates: 36/7001, Avg Grad: 0.056408893316984180\n",
            "Epoch: 24, Loss: 0.0018849964253604412 Updates: 41/8001, Avg Grad: 0.035849727690219880\n",
            "Epoch: 24, Loss: 0.0016042017377912998 Updates: 46/9001, Avg Grad: 0.043805398046970370\n",
            "Epoch 23 iteration 0 Loss: 1.692 | Acc: 0.000% (0/1)\n",
            "Epoch 23 iteration 100 Loss: 0.633 | Acc: 65.347% (66/101)\n",
            "Epoch 23 iteration 200 Loss: 0.630 | Acc: 65.174% (131/201)\n",
            "Epoch 23 iteration 300 Loss: 0.601 | Acc: 66.445% (200/301)\n",
            "Epoch 23 iteration 400 Loss: 0.598 | Acc: 66.334% (266/401)\n",
            "Epoch 23 iteration 500 Loss: 0.604 | Acc: 65.269% (327/501)\n",
            "Test accuracy: 0.652694582939148\n",
            "Epoch: 24, Loss: 0.009705866686999798 Updates: 51/10001, Avg Grad: 0.0215832367539405820\n",
            "Epoch: 24, Loss: 0.0038081579841673374 Updates: 56/11001, Avg Grad: 0.021480793133378030\n",
            "Epoch: 24, Loss: 0.001497665187343955 Updates: 61/12001, Avg Grad: 0.038746479898691180\n",
            "Epoch: 24, Loss: 0.004142908379435539 Updates: 66/13001, Avg Grad: 0.0290905833244323730\n",
            "Epoch: 24, Loss: 0.010085845366120338 Updates: 71/14001, Avg Grad: 0.044416584074497220\n",
            "Epoch: 24, Loss: 0.0024513753596693277 Updates: 76/15001, Avg Grad: 0.0238217208534479140\n",
            "Epoch: 24, Loss: 0.0025669578462839127 Updates: 81/16001, Avg Grad: 0.0167249552905559540\n",
            "Epoch: 24, Loss: 0.0034712033811956644 Updates: 86/17001, Avg Grad: 0.025046091526746750\n",
            "Epoch: 24, Loss: 0.0003253525937907398 Updates: 91/18001, Avg Grad: 0.017364166676998140\n",
            "Epoch: 24, Loss: 0.0029907941352576017 Updates: 96/19001, Avg Grad: 0.0243038441985845570\n",
            "Epoch 23 iteration 0 Loss: 0.548 | Acc: 100.000% (1/1)\n",
            "Epoch 23 iteration 100 Loss: 0.627 | Acc: 67.327% (68/101)\n",
            "Epoch 23 iteration 200 Loss: 0.622 | Acc: 66.667% (134/201)\n",
            "Epoch 23 iteration 300 Loss: 0.595 | Acc: 69.103% (208/301)\n",
            "Epoch 23 iteration 400 Loss: 0.578 | Acc: 71.072% (285/401)\n",
            "Epoch 23 iteration 500 Loss: 0.581 | Acc: 71.257% (357/501)\n",
            "Test accuracy: 0.71257483959198\n",
            "Epoch: 24, Loss: 0.0007419231696985662 Updates: 101/20001, Avg Grad: 0.0161180403083562850\n",
            "Epoch: 24, Loss: 0.008435219526290894 Updates: 106/21001, Avg Grad: 0.0231757704168558120\n",
            "Epoch: 24, Loss: 0.00564012723043561 Updates: 111/22001, Avg Grad: 0.0220237188041210170\n",
            "Epoch: 24, Loss: 0.001526329666376114 Updates: 116/23001, Avg Grad: 0.034365724772214890\n",
            "Epoch: 24, Loss: 0.007864157669246197 Updates: 121/24001, Avg Grad: 0.0095399785786867140\n",
            "Epoch: 24, Loss: 0.003485630266368389 Updates: 126/25001, Avg Grad: 0.0186856891959905620\n",
            "Epoch: 24, Loss: 0.0013467710232362151 Updates: 131/26001, Avg Grad: 0.031782481819391250\n",
            "Epoch: 24, Loss: 0.003069296246394515 Updates: 136/27001, Avg Grad: 0.0290936119854450230\n",
            "Epoch: 24, Loss: 0.0014695428544655442 Updates: 141/28001, Avg Grad: 0.0185313560068607330\n",
            "Epoch: 24, Loss: 0.009031352587044239 Updates: 146/29001, Avg Grad: 0.0117575656622648240\n",
            "Epoch 23 iteration 0 Loss: 0.898 | Acc: 0.000% (0/1)\n",
            "Epoch 23 iteration 100 Loss: 0.606 | Acc: 73.267% (74/101)\n",
            "Epoch 23 iteration 200 Loss: 0.568 | Acc: 74.129% (149/201)\n",
            "Epoch 23 iteration 300 Loss: 0.561 | Acc: 74.419% (224/301)\n",
            "Epoch 23 iteration 400 Loss: 0.568 | Acc: 73.317% (294/401)\n",
            "Epoch 23 iteration 500 Loss: 0.599 | Acc: 70.858% (355/501)\n",
            "Test accuracy: 0.7085828185081482\n",
            "Epoch: 24, Loss: 0.0005310839624144137 Updates: 151/30001, Avg Grad: 0.066270649433135990\n",
            "Epoch: 24, Loss: 0.010116100311279297 Updates: 156/31001, Avg Grad: 0.011962614022195340\n",
            "Epoch: 24, Loss: 0.0017885841662064195 Updates: 161/32001, Avg Grad: 0.012361846864223480\n",
            "Epoch 24 iteration 0 Loss: 0.149 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.513 | Acc: 76.238% (77/101)\n",
            "Epoch 24 iteration 200 Loss: 0.542 | Acc: 73.134% (147/201)\n",
            "Epoch 24 iteration 300 Loss: 0.546 | Acc: 72.757% (219/301)\n",
            "Epoch 24 iteration 400 Loss: 0.557 | Acc: 72.070% (289/401)\n",
            "Epoch 24 iteration 500 Loss: 0.552 | Acc: 72.655% (364/501)\n",
            "Test accuracy: 0.7265468835830688\n",
            "Epoch: 25, Loss: 0.0036483686417341232 Updates: 1/1, Avg Grad: 0.00153070141095668080\n",
            "Epoch: 25, Loss: 0.0012117917649447918 Updates: 6/1001, Avg Grad: 0.0375835001468658450\n",
            "Epoch: 25, Loss: 0.001474804594181478 Updates: 11/2001, Avg Grad: 0.0110153779387474060\n",
            "Epoch: 25, Loss: 0.00505633931607008 Updates: 16/3001, Avg Grad: 0.033632412552833560\n",
            "Epoch: 25, Loss: 0.013071970082819462 Updates: 21/4001, Avg Grad: 0.0191034879535436630\n",
            "Epoch: 25, Loss: 0.0014977764803916216 Updates: 26/5001, Avg Grad: 0.0204955749213695530\n",
            "Epoch: 25, Loss: 0.0028101103380322456 Updates: 31/6001, Avg Grad: 0.0265764854848384860\n",
            "Epoch: 25, Loss: 0.0007810764946043491 Updates: 36/7001, Avg Grad: 0.0284457374364137650\n",
            "Epoch: 25, Loss: 0.0011143996380269527 Updates: 41/8001, Avg Grad: 0.0235141701996326450\n",
            "Epoch: 25, Loss: 0.0003116540319751948 Updates: 46/9001, Avg Grad: 0.0273153539746999740\n",
            "Epoch 24 iteration 0 Loss: 0.407 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.585 | Acc: 71.287% (72/101)\n",
            "Epoch 24 iteration 200 Loss: 0.575 | Acc: 73.134% (147/201)\n",
            "Epoch 24 iteration 300 Loss: 0.576 | Acc: 72.093% (217/301)\n",
            "Epoch 24 iteration 400 Loss: 0.597 | Acc: 68.828% (276/401)\n",
            "Epoch 24 iteration 500 Loss: 0.595 | Acc: 69.261% (347/501)\n",
            "Test accuracy: 0.6926147937774658\n",
            "Epoch: 25, Loss: 0.0018724256660789251 Updates: 51/10001, Avg Grad: 0.0208216290920972820\n",
            "Epoch: 25, Loss: 0.005286309868097305 Updates: 56/11001, Avg Grad: 0.014357759617269040\n",
            "Epoch: 25, Loss: 0.0028606278356164694 Updates: 61/12001, Avg Grad: 0.0135992523282766340\n",
            "Epoch: 25, Loss: 0.00046146620297804475 Updates: 66/13001, Avg Grad: 0.0256200581789016720\n",
            "Epoch: 25, Loss: 0.0006977516459301114 Updates: 71/14001, Avg Grad: 0.082194045186042790\n",
            "Epoch: 25, Loss: 0.004383165389299393 Updates: 76/15001, Avg Grad: 0.02412402816116810\n",
            "Epoch: 25, Loss: 0.006099992897361517 Updates: 81/16001, Avg Grad: 0.024978347122669220\n",
            "Epoch: 25, Loss: 0.0010093809105455875 Updates: 86/17001, Avg Grad: 0.0548230223357677460\n",
            "Epoch: 25, Loss: 0.00043530893162824214 Updates: 91/18001, Avg Grad: 0.0235001761466264720\n",
            "Epoch: 25, Loss: 0.007201183587312698 Updates: 96/19001, Avg Grad: 0.009660108946263790\n",
            "Epoch 24 iteration 0 Loss: 0.232 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.595 | Acc: 72.277% (73/101)\n",
            "Epoch 24 iteration 200 Loss: 0.646 | Acc: 65.672% (132/201)\n",
            "Epoch 24 iteration 300 Loss: 0.617 | Acc: 66.777% (201/301)\n",
            "Epoch 24 iteration 400 Loss: 0.623 | Acc: 67.830% (272/401)\n",
            "Epoch 24 iteration 500 Loss: 0.621 | Acc: 67.864% (340/501)\n",
            "Test accuracy: 0.6786426901817322\n",
            "Epoch: 25, Loss: 0.0032745280768722296 Updates: 101/20001, Avg Grad: 0.032403945922851560\n",
            "Epoch: 25, Loss: 0.0012262961827218533 Updates: 106/21001, Avg Grad: 0.028091475367546080\n",
            "Epoch: 25, Loss: 0.0042253159917891026 Updates: 111/22001, Avg Grad: 0.034568406641483310\n",
            "Epoch: 25, Loss: 0.0010575094493106008 Updates: 116/23001, Avg Grad: 0.054795417934656140\n",
            "Epoch: 25, Loss: 0.00125548813957721 Updates: 121/24001, Avg Grad: 0.0410637892782688140\n",
            "Epoch: 25, Loss: 0.0020643535535782576 Updates: 126/25001, Avg Grad: 0.032113362103700640\n",
            "Epoch: 25, Loss: 0.0013214540667831898 Updates: 131/26001, Avg Grad: 0.04683189466595650\n",
            "Epoch: 25, Loss: 0.0018544193590059876 Updates: 136/27001, Avg Grad: 0.0401763282716274260\n",
            "Epoch: 25, Loss: 0.00043321377597749233 Updates: 141/28001, Avg Grad: 0.0151946134865283970\n",
            "Epoch: 25, Loss: 0.002411993220448494 Updates: 146/29001, Avg Grad: 0.056634396314620970\n",
            "Epoch 24 iteration 0 Loss: 0.256 | Acc: 100.000% (1/1)\n",
            "Epoch 24 iteration 100 Loss: 0.567 | Acc: 72.277% (73/101)\n",
            "Epoch 24 iteration 200 Loss: 0.551 | Acc: 73.134% (147/201)\n",
            "Epoch 24 iteration 300 Loss: 0.543 | Acc: 73.754% (222/301)\n",
            "Epoch 24 iteration 400 Loss: 0.560 | Acc: 71.322% (286/401)\n",
            "Epoch 24 iteration 500 Loss: 0.556 | Acc: 71.657% (359/501)\n",
            "Test accuracy: 0.7165668606758118\n",
            "Epoch: 25, Loss: 0.0021136938594281673 Updates: 151/30001, Avg Grad: 0.02763281576335430\n",
            "Epoch: 25, Loss: 0.0009214553865604103 Updates: 156/31001, Avg Grad: 0.0228861197829246520\n",
            "Epoch: 25, Loss: 0.0017278410959988832 Updates: 161/32001, Avg Grad: 0.0218572169542312620\n",
            "Epoch 25 iteration 0 Loss: 0.797 | Acc: 0.000% (0/1)\n",
            "Epoch 25 iteration 100 Loss: 0.620 | Acc: 68.317% (69/101)\n",
            "Epoch 25 iteration 200 Loss: 0.687 | Acc: 63.682% (128/201)\n",
            "Epoch 25 iteration 300 Loss: 0.688 | Acc: 63.455% (191/301)\n",
            "Epoch 25 iteration 400 Loss: 0.684 | Acc: 63.840% (256/401)\n",
            "Epoch 25 iteration 500 Loss: 0.689 | Acc: 63.273% (317/501)\n",
            "Test accuracy: 0.6327345371246338\n",
            "Epoch: 26, Loss: 0.001039811410009861 Updates: 1/1, Avg Grad: 0.00054916500812396410\n",
            "Epoch: 26, Loss: 0.010976722463965416 Updates: 6/1001, Avg Grad: 0.0204965397715568540\n",
            "Epoch: 26, Loss: 0.0056518507190048695 Updates: 11/2001, Avg Grad: 0.041606020182371140\n",
            "Epoch: 26, Loss: 0.0007174579659476876 Updates: 16/3001, Avg Grad: 0.064501196146011350\n",
            "Epoch: 26, Loss: 0.0006261544185690582 Updates: 21/4001, Avg Grad: 0.0099853435531258580\n",
            "Epoch: 26, Loss: 0.0015499511500820518 Updates: 26/5001, Avg Grad: 0.0374231636524200440\n",
            "Epoch: 26, Loss: 0.00721370056271553 Updates: 31/6001, Avg Grad: 0.022298347204923630\n",
            "Epoch: 26, Loss: 9.654526365920901e-05 Updates: 36/7001, Avg Grad: 0.016003686934709550\n",
            "Epoch: 26, Loss: 0.0004899727646261454 Updates: 41/8001, Avg Grad: 0.040896855294704440\n",
            "Epoch: 26, Loss: 0.002951239701360464 Updates: 46/9001, Avg Grad: 0.0621545836329460140\n",
            "Epoch 25 iteration 0 Loss: 0.255 | Acc: 100.000% (1/1)\n",
            "Epoch 25 iteration 100 Loss: 0.579 | Acc: 71.287% (72/101)\n",
            "Epoch 25 iteration 200 Loss: 0.568 | Acc: 72.637% (146/201)\n",
            "Epoch 25 iteration 300 Loss: 0.609 | Acc: 69.103% (208/301)\n",
            "Epoch 25 iteration 400 Loss: 0.612 | Acc: 68.828% (276/401)\n",
            "Epoch 25 iteration 500 Loss: 0.610 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 26, Loss: 0.0014014708576723933 Updates: 51/10001, Avg Grad: 0.02329072728753090\n",
            "Epoch: 26, Loss: 0.0020943828858435154 Updates: 56/11001, Avg Grad: 0.0237075276672840120\n",
            "Epoch: 26, Loss: 0.004073953256011009 Updates: 61/12001, Avg Grad: 0.070636712014675140\n",
            "Epoch: 26, Loss: 0.006291577592492104 Updates: 66/13001, Avg Grad: 0.0160114783793687820\n",
            "Epoch: 26, Loss: 0.0015718505019322038 Updates: 71/14001, Avg Grad: 0.044445514678955080\n",
            "Epoch: 26, Loss: 0.001848316635005176 Updates: 76/15001, Avg Grad: 0.024440074339509010\n",
            "Epoch: 26, Loss: 0.002980196150019765 Updates: 81/16001, Avg Grad: 0.056193660944700240\n",
            "Epoch: 26, Loss: 0.002529746387153864 Updates: 86/17001, Avg Grad: 0.0168114192783832550\n",
            "Epoch: 26, Loss: 0.0011801073560491204 Updates: 91/18001, Avg Grad: 0.033649619668722150\n",
            "Epoch: 26, Loss: 0.0023239220026880503 Updates: 96/19001, Avg Grad: 0.0147600593045353890\n",
            "Epoch 25 iteration 0 Loss: 1.084 | Acc: 0.000% (0/1)\n",
            "Epoch 25 iteration 100 Loss: 0.507 | Acc: 74.257% (75/101)\n",
            "Epoch 25 iteration 200 Loss: 0.537 | Acc: 73.632% (148/201)\n",
            "Epoch 25 iteration 300 Loss: 0.555 | Acc: 73.090% (220/301)\n",
            "Epoch 25 iteration 400 Loss: 0.586 | Acc: 71.571% (287/401)\n",
            "Epoch 25 iteration 500 Loss: 0.601 | Acc: 69.860% (350/501)\n",
            "Test accuracy: 0.6986027956008911\n",
            "Epoch: 26, Loss: 0.0007310443324968219 Updates: 101/20001, Avg Grad: 0.0134303690865635870\n",
            "Epoch: 26, Loss: 0.0018294363981112838 Updates: 106/21001, Avg Grad: 0.0370232611894607540\n",
            "Epoch: 26, Loss: 0.006677593570202589 Updates: 111/22001, Avg Grad: 0.028590220957994460\n",
            "Epoch: 26, Loss: 0.004039323423057795 Updates: 116/23001, Avg Grad: 0.0227131266146898270\n",
            "Epoch: 26, Loss: 0.0026768276002258062 Updates: 121/24001, Avg Grad: 0.015995301306247710\n",
            "Epoch: 26, Loss: 0.00039626946090720594 Updates: 126/25001, Avg Grad: 0.0144923552870750430\n",
            "Epoch: 26, Loss: 0.0014277902664616704 Updates: 131/26001, Avg Grad: 0.0339418798685073850\n",
            "Epoch: 26, Loss: 0.0003415077517274767 Updates: 136/27001, Avg Grad: 0.04114399105310440\n",
            "Epoch: 26, Loss: 0.0013452754355967045 Updates: 141/28001, Avg Grad: 0.0235782042145729060\n",
            "Epoch: 26, Loss: 0.001229685265570879 Updates: 146/29001, Avg Grad: 0.0204795766621828080\n",
            "Epoch 25 iteration 0 Loss: 0.551 | Acc: 100.000% (1/1)\n",
            "Epoch 25 iteration 100 Loss: 0.622 | Acc: 68.317% (69/101)\n",
            "Epoch 25 iteration 200 Loss: 0.573 | Acc: 69.652% (140/201)\n",
            "Epoch 25 iteration 300 Loss: 0.552 | Acc: 71.429% (215/301)\n",
            "Epoch 25 iteration 400 Loss: 0.552 | Acc: 72.569% (291/401)\n",
            "Epoch 25 iteration 500 Loss: 0.549 | Acc: 72.854% (365/501)\n",
            "Test accuracy: 0.7285429239273071\n",
            "Epoch: 26, Loss: 0.0021736558992415667 Updates: 151/30001, Avg Grad: 0.0184612888842821120\n",
            "Epoch: 26, Loss: 0.0006328602321445942 Updates: 156/31001, Avg Grad: 0.0272622928023338320\n",
            "Epoch: 26, Loss: 0.0021614478901028633 Updates: 161/32001, Avg Grad: 0.0357847772538661960\n",
            "Epoch 26 iteration 0 Loss: 0.286 | Acc: 100.000% (1/1)\n",
            "Epoch 26 iteration 100 Loss: 0.599 | Acc: 65.347% (66/101)\n",
            "Epoch 26 iteration 200 Loss: 0.646 | Acc: 65.174% (131/201)\n",
            "Epoch 26 iteration 300 Loss: 0.651 | Acc: 66.445% (200/301)\n",
            "Epoch 26 iteration 400 Loss: 0.639 | Acc: 66.833% (268/401)\n",
            "Epoch 26 iteration 500 Loss: 0.638 | Acc: 66.866% (335/501)\n",
            "Test accuracy: 0.6686626672744751\n",
            "Epoch: 27, Loss: 0.004259122535586357 Updates: 1/1, Avg Grad: 0.00159979821182787420\n",
            "Epoch: 27, Loss: 0.000364594510756433 Updates: 6/1001, Avg Grad: 0.0431923903524875640\n",
            "Epoch: 27, Loss: 0.0015545074129477143 Updates: 11/2001, Avg Grad: 0.0082926228642463680\n",
            "Epoch: 27, Loss: 0.0006896121776662767 Updates: 16/3001, Avg Grad: 0.024858940392732620\n",
            "Epoch: 27, Loss: 0.005176228936761618 Updates: 21/4001, Avg Grad: 0.0280540212988853450\n",
            "Epoch: 27, Loss: 0.004808025434613228 Updates: 26/5001, Avg Grad: 0.0074716522358357910\n",
            "Epoch: 27, Loss: 0.006987714674323797 Updates: 31/6001, Avg Grad: 0.0138935912400484090\n",
            "Epoch: 27, Loss: 0.0048927487805485725 Updates: 36/7001, Avg Grad: 0.0461166277527809140\n",
            "Epoch: 27, Loss: 0.0021861016284674406 Updates: 41/8001, Avg Grad: 0.018351841717958450\n",
            "Epoch: 27, Loss: 0.001250938163138926 Updates: 46/9001, Avg Grad: 0.0202973578125238420\n",
            "Epoch 26 iteration 0 Loss: 1.212 | Acc: 0.000% (0/1)\n",
            "Epoch 26 iteration 100 Loss: 0.598 | Acc: 68.317% (69/101)\n",
            "Epoch 26 iteration 200 Loss: 0.615 | Acc: 68.159% (137/201)\n",
            "Epoch 26 iteration 300 Loss: 0.615 | Acc: 69.103% (208/301)\n",
            "Epoch 26 iteration 400 Loss: 0.584 | Acc: 71.322% (286/401)\n",
            "Epoch 26 iteration 500 Loss: 0.592 | Acc: 70.459% (353/501)\n",
            "Test accuracy: 0.7045907974243164\n",
            "Epoch: 27, Loss: 0.004850356839597225 Updates: 51/10001, Avg Grad: 0.0272907726466655730\n",
            "Epoch: 27, Loss: 0.0018526650965213776 Updates: 56/11001, Avg Grad: 0.0210274457931518550\n",
            "Epoch: 27, Loss: 0.0008394953329116106 Updates: 61/12001, Avg Grad: 0.043577626347541810\n",
            "Epoch: 27, Loss: 0.0016534387832507491 Updates: 66/13001, Avg Grad: 0.038457214832305910\n",
            "Epoch: 27, Loss: 0.00043616059701889753 Updates: 71/14001, Avg Grad: 0.036640852689743040\n",
            "Epoch: 27, Loss: 0.002878783270716667 Updates: 76/15001, Avg Grad: 0.025711204856634140\n",
            "Epoch: 27, Loss: 0.0030552225653082132 Updates: 81/16001, Avg Grad: 0.020854935050010680\n",
            "Epoch: 27, Loss: 0.004402420949190855 Updates: 86/17001, Avg Grad: 0.045317366719245910\n",
            "Epoch: 27, Loss: 0.003036946291103959 Updates: 91/18001, Avg Grad: 0.0269357655197381970\n",
            "Epoch: 27, Loss: 0.000683064223267138 Updates: 96/19001, Avg Grad: 0.0189140252768993380\n",
            "Epoch 26 iteration 0 Loss: 0.417 | Acc: 100.000% (1/1)\n",
            "Epoch 26 iteration 100 Loss: 0.602 | Acc: 62.376% (63/101)\n",
            "Epoch 26 iteration 200 Loss: 0.629 | Acc: 60.697% (122/201)\n",
            "Epoch 26 iteration 300 Loss: 0.598 | Acc: 65.449% (197/301)\n",
            "Epoch 26 iteration 400 Loss: 0.592 | Acc: 66.334% (266/401)\n",
            "Epoch 26 iteration 500 Loss: 0.591 | Acc: 66.667% (334/501)\n",
            "Test accuracy: 0.6666666865348816\n",
            "Epoch: 27, Loss: 0.0006119220051914454 Updates: 101/20001, Avg Grad: 0.026298355311155320\n",
            "Epoch: 27, Loss: 0.002889051800593734 Updates: 106/21001, Avg Grad: 0.0202103443443775180\n",
            "Epoch: 27, Loss: 0.002725236816331744 Updates: 111/22001, Avg Grad: 0.041422601789236070\n",
            "Epoch: 27, Loss: 0.000501219998113811 Updates: 116/23001, Avg Grad: 0.0380445346236228940\n",
            "Epoch: 27, Loss: 0.01188027486205101 Updates: 121/24001, Avg Grad: 0.0223340895026922230\n",
            "Epoch: 27, Loss: 0.005028196610510349 Updates: 126/25001, Avg Grad: 0.034986563026905060\n",
            "Epoch: 27, Loss: 0.0012432157527655363 Updates: 131/26001, Avg Grad: 0.0236896872520446780\n",
            "Epoch: 27, Loss: 0.00867967214435339 Updates: 136/27001, Avg Grad: 0.054636728018522260\n",
            "Epoch: 27, Loss: 0.0008759528282098472 Updates: 141/28001, Avg Grad: 0.0121923228725790980\n",
            "Epoch: 27, Loss: 0.0007093184976838529 Updates: 146/29001, Avg Grad: 0.009445807896554470\n",
            "Epoch 26 iteration 0 Loss: 0.198 | Acc: 100.000% (1/1)\n",
            "Epoch 26 iteration 100 Loss: 0.614 | Acc: 68.317% (69/101)\n",
            "Epoch 26 iteration 200 Loss: 0.615 | Acc: 65.672% (132/201)\n",
            "Epoch 26 iteration 300 Loss: 0.589 | Acc: 68.106% (205/301)\n",
            "Epoch 26 iteration 400 Loss: 0.589 | Acc: 68.828% (276/401)\n",
            "Epoch 26 iteration 500 Loss: 0.579 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 27, Loss: 0.009589015506207943 Updates: 151/30001, Avg Grad: 0.0305182170122861860\n",
            "Epoch: 27, Loss: 0.0011946704471483827 Updates: 156/31001, Avg Grad: 0.0205360967665910720\n",
            "Epoch: 27, Loss: 0.002958822064101696 Updates: 161/32001, Avg Grad: 0.020160881802439690\n",
            "Epoch 27 iteration 0 Loss: 0.077 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.578 | Acc: 65.347% (66/101)\n",
            "Epoch 27 iteration 200 Loss: 0.580 | Acc: 67.164% (135/201)\n",
            "Epoch 27 iteration 300 Loss: 0.609 | Acc: 68.106% (205/301)\n",
            "Epoch 27 iteration 400 Loss: 0.596 | Acc: 70.075% (281/401)\n",
            "Epoch 27 iteration 500 Loss: 0.588 | Acc: 70.459% (353/501)\n",
            "Test accuracy: 0.7045907974243164\n",
            "Epoch: 28, Loss: 0.001197921228595078 Updates: 1/1, Avg Grad: 0.00056700146524235610\n",
            "Epoch: 28, Loss: 0.01111224852502346 Updates: 6/1001, Avg Grad: 0.064276158809661870\n",
            "Epoch: 28, Loss: 0.0016183792613446712 Updates: 11/2001, Avg Grad: 0.058372236788272860\n",
            "Epoch: 28, Loss: 0.0055573140271008015 Updates: 16/3001, Avg Grad: 0.0137234292924404140\n",
            "Epoch: 28, Loss: 0.0010582831455394626 Updates: 21/4001, Avg Grad: 0.0302035752683877940\n",
            "Epoch: 28, Loss: 0.0013869025278836489 Updates: 26/5001, Avg Grad: 0.025528483092784880\n",
            "Epoch: 28, Loss: 0.0007087428239174187 Updates: 31/6001, Avg Grad: 0.0233011953532695770\n",
            "Epoch: 28, Loss: 0.0009171322453767061 Updates: 36/7001, Avg Grad: 0.0248894244432449340\n",
            "Epoch: 28, Loss: 0.00029124587308615446 Updates: 41/8001, Avg Grad: 0.0362865030765533450\n",
            "Epoch: 28, Loss: 0.0038286258932203054 Updates: 46/9001, Avg Grad: 0.0236868597567081450\n",
            "Epoch 27 iteration 0 Loss: 0.306 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.607 | Acc: 71.287% (72/101)\n",
            "Epoch 27 iteration 200 Loss: 0.575 | Acc: 71.642% (144/201)\n",
            "Epoch 27 iteration 300 Loss: 0.591 | Acc: 70.432% (212/301)\n",
            "Epoch 27 iteration 400 Loss: 0.582 | Acc: 71.322% (286/401)\n",
            "Epoch 27 iteration 500 Loss: 0.582 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 28, Loss: 0.0003721720422618091 Updates: 51/10001, Avg Grad: 0.031225027516484260\n",
            "Epoch: 28, Loss: 0.0010861880145967007 Updates: 56/11001, Avg Grad: 0.0397841110825538640\n",
            "Epoch: 28, Loss: 0.000916534336283803 Updates: 61/12001, Avg Grad: 0.018454704433679580\n",
            "Epoch: 28, Loss: 0.006229040678590536 Updates: 66/13001, Avg Grad: 0.05236355960369110\n",
            "Epoch: 28, Loss: 0.007514151278883219 Updates: 71/14001, Avg Grad: 0.024575227871537210\n",
            "Epoch: 28, Loss: 0.00734666595235467 Updates: 76/15001, Avg Grad: 0.035098005086183550\n",
            "Epoch: 28, Loss: 0.004475932568311691 Updates: 81/16001, Avg Grad: 0.037787076085805890\n",
            "Epoch: 28, Loss: 0.001494844094850123 Updates: 86/17001, Avg Grad: 0.041924562305212020\n",
            "Epoch: 28, Loss: 0.0018951575038954616 Updates: 91/18001, Avg Grad: 0.018901471048593520\n",
            "Epoch: 28, Loss: 0.0014974633231759071 Updates: 96/19001, Avg Grad: 0.02979884110391140\n",
            "Epoch 27 iteration 0 Loss: 1.008 | Acc: 0.000% (0/1)\n",
            "Epoch 27 iteration 100 Loss: 0.749 | Acc: 59.406% (60/101)\n",
            "Epoch 27 iteration 200 Loss: 0.689 | Acc: 62.189% (125/201)\n",
            "Epoch 27 iteration 300 Loss: 0.676 | Acc: 61.462% (185/301)\n",
            "Epoch 27 iteration 400 Loss: 0.648 | Acc: 63.840% (256/401)\n",
            "Epoch 27 iteration 500 Loss: 0.646 | Acc: 63.273% (317/501)\n",
            "Test accuracy: 0.6327345371246338\n",
            "Epoch: 28, Loss: 0.001894803368486464 Updates: 101/20001, Avg Grad: 0.020534165203571320\n",
            "Epoch: 28, Loss: 0.0031380720902234316 Updates: 106/21001, Avg Grad: 0.0132645061239600180\n",
            "Epoch: 28, Loss: 0.008766818791627884 Updates: 111/22001, Avg Grad: 0.064090952277183530\n",
            "Epoch: 28, Loss: 0.0015519617591053247 Updates: 116/23001, Avg Grad: 0.018095564097166060\n",
            "Epoch: 28, Loss: 0.0012965629575774074 Updates: 121/24001, Avg Grad: 0.0342474654316902160\n",
            "Epoch: 28, Loss: 0.0031480491161346436 Updates: 126/25001, Avg Grad: 0.0252679325640201570\n",
            "Epoch: 28, Loss: 0.0018461027648299932 Updates: 131/26001, Avg Grad: 0.0227033440023660660\n",
            "Epoch: 28, Loss: 0.0031354110687971115 Updates: 136/27001, Avg Grad: 0.0240755788981914520\n",
            "Epoch: 28, Loss: 0.002739339368417859 Updates: 141/28001, Avg Grad: 0.0287311021238565440\n",
            "Epoch: 28, Loss: 0.002811940386891365 Updates: 146/29001, Avg Grad: 0.012503342702984810\n",
            "Epoch 27 iteration 0 Loss: 0.170 | Acc: 100.000% (1/1)\n",
            "Epoch 27 iteration 100 Loss: 0.601 | Acc: 64.356% (65/101)\n",
            "Epoch 27 iteration 200 Loss: 0.600 | Acc: 69.652% (140/201)\n",
            "Epoch 27 iteration 300 Loss: 0.599 | Acc: 68.439% (206/301)\n",
            "Epoch 27 iteration 400 Loss: 0.585 | Acc: 70.574% (283/401)\n",
            "Epoch 27 iteration 500 Loss: 0.572 | Acc: 71.058% (356/501)\n",
            "Test accuracy: 0.7105788588523865\n",
            "Epoch: 28, Loss: 0.005533031187951565 Updates: 151/30001, Avg Grad: 0.0366813391447067260\n",
            "Epoch: 28, Loss: 0.0023924594279378653 Updates: 156/31001, Avg Grad: 0.0264486558735370640\n",
            "Epoch: 28, Loss: 0.003927168436348438 Updates: 161/32001, Avg Grad: 0.02065168321132660\n",
            "Epoch 28 iteration 0 Loss: 0.875 | Acc: 0.000% (0/1)\n",
            "Epoch 28 iteration 100 Loss: 0.605 | Acc: 67.327% (68/101)\n",
            "Epoch 28 iteration 200 Loss: 0.608 | Acc: 66.667% (134/201)\n",
            "Epoch 28 iteration 300 Loss: 0.615 | Acc: 67.110% (202/301)\n",
            "Epoch 28 iteration 400 Loss: 0.600 | Acc: 67.830% (272/401)\n",
            "Epoch 28 iteration 500 Loss: 0.599 | Acc: 67.864% (340/501)\n",
            "Test accuracy: 0.6786426901817322\n",
            "Epoch: 29, Loss: 0.0033839482348412275 Updates: 1/1, Avg Grad: 0.00140911166090518240\n",
            "Epoch: 29, Loss: 0.002413445618003607 Updates: 6/1001, Avg Grad: 0.0309257581830024720\n",
            "Epoch: 29, Loss: 0.0007559843943454325 Updates: 11/2001, Avg Grad: 0.0364382192492485050\n",
            "Epoch: 29, Loss: 0.0036368847358971834 Updates: 16/3001, Avg Grad: 0.049882944673299790\n",
            "Epoch: 29, Loss: 0.0016267896862700582 Updates: 21/4001, Avg Grad: 0.0137657653540372850\n",
            "Epoch: 29, Loss: 0.0007486582617275417 Updates: 26/5001, Avg Grad: 0.0170410014688968660\n",
            "Epoch: 29, Loss: 0.003931907005608082 Updates: 31/6001, Avg Grad: 0.03349425643682480\n",
            "Epoch: 29, Loss: 0.0040010008960962296 Updates: 36/7001, Avg Grad: 0.020730350166559220\n",
            "Epoch: 29, Loss: 0.004618864506483078 Updates: 41/8001, Avg Grad: 0.0228577777743339540\n",
            "Epoch: 29, Loss: 0.006425837986171246 Updates: 46/9001, Avg Grad: 0.04829430952668190\n",
            "Epoch 28 iteration 0 Loss: 0.364 | Acc: 100.000% (1/1)\n",
            "Epoch 28 iteration 100 Loss: 0.596 | Acc: 66.337% (67/101)\n",
            "Epoch 28 iteration 200 Loss: 0.603 | Acc: 64.677% (130/201)\n",
            "Epoch 28 iteration 300 Loss: 0.597 | Acc: 66.445% (200/301)\n",
            "Epoch 28 iteration 400 Loss: 0.580 | Acc: 67.082% (269/401)\n",
            "Epoch 28 iteration 500 Loss: 0.575 | Acc: 67.066% (336/501)\n",
            "Test accuracy: 0.6706587076187134\n",
            "Epoch: 29, Loss: 0.002106160158291459 Updates: 51/10001, Avg Grad: 0.0445017106831073760\n",
            "Epoch: 29, Loss: 0.0005706343217752874 Updates: 56/11001, Avg Grad: 0.0260670445859432220\n",
            "Epoch: 29, Loss: 0.0015042528975754976 Updates: 61/12001, Avg Grad: 0.0246169473975896840\n",
            "Epoch: 29, Loss: 0.0055238534696400166 Updates: 66/13001, Avg Grad: 0.0280038211494684220\n",
            "Epoch: 29, Loss: 0.0010585363488644361 Updates: 71/14001, Avg Grad: 0.0253063254058361050\n",
            "Epoch: 29, Loss: 0.0020929155871272087 Updates: 76/15001, Avg Grad: 0.050081811845302580\n",
            "Epoch: 29, Loss: 0.003033980028703809 Updates: 81/16001, Avg Grad: 0.017171008512377740\n",
            "Epoch: 29, Loss: 0.0014507989399135113 Updates: 86/17001, Avg Grad: 0.040430415421724320\n",
            "Epoch: 29, Loss: 0.0017121279379352927 Updates: 91/18001, Avg Grad: 0.014946566894650460\n",
            "Epoch: 29, Loss: 0.0005370737053453922 Updates: 96/19001, Avg Grad: 0.0175754185765981670\n",
            "Epoch 28 iteration 0 Loss: 0.175 | Acc: 100.000% (1/1)\n",
            "Epoch 28 iteration 100 Loss: 0.594 | Acc: 67.327% (68/101)\n",
            "Epoch 28 iteration 200 Loss: 0.581 | Acc: 70.647% (142/201)\n",
            "Epoch 28 iteration 300 Loss: 0.572 | Acc: 73.090% (220/301)\n",
            "Epoch 28 iteration 400 Loss: 0.570 | Acc: 71.820% (288/401)\n",
            "Epoch 28 iteration 500 Loss: 0.574 | Acc: 71.058% (356/501)\n",
            "Test accuracy: 0.7105788588523865\n",
            "Epoch: 29, Loss: 0.0041361115872859955 Updates: 101/20001, Avg Grad: 0.0209646243602037430\n",
            "Epoch: 29, Loss: 0.0018309931037947536 Updates: 106/21001, Avg Grad: 0.055505704134702680\n",
            "Epoch: 29, Loss: 0.0018623642390593886 Updates: 111/22001, Avg Grad: 0.017576722428202630\n",
            "Epoch: 29, Loss: 0.012096438556909561 Updates: 116/23001, Avg Grad: 0.0241455510258674620\n",
            "Epoch: 29, Loss: 0.001408934942446649 Updates: 121/24001, Avg Grad: 0.038437344133853910\n",
            "Epoch: 29, Loss: 0.0017847255803644657 Updates: 126/25001, Avg Grad: 0.039945263415575030\n",
            "Epoch: 29, Loss: 0.002790295984596014 Updates: 131/26001, Avg Grad: 0.0454509072005748750\n",
            "Epoch: 29, Loss: 0.006408380344510078 Updates: 136/27001, Avg Grad: 0.022355556488037110\n",
            "Epoch: 29, Loss: 0.004647932481020689 Updates: 141/28001, Avg Grad: 0.017783347517251970\n",
            "Epoch: 29, Loss: 0.0011112941429018974 Updates: 146/29001, Avg Grad: 0.0155797488987445830\n",
            "Epoch 28 iteration 0 Loss: 1.493 | Acc: 0.000% (0/1)\n",
            "Epoch 28 iteration 100 Loss: 0.555 | Acc: 70.297% (71/101)\n",
            "Epoch 28 iteration 200 Loss: 0.585 | Acc: 68.159% (137/201)\n",
            "Epoch 28 iteration 300 Loss: 0.610 | Acc: 66.113% (199/301)\n",
            "Epoch 28 iteration 400 Loss: 0.597 | Acc: 67.830% (272/401)\n",
            "Epoch 28 iteration 500 Loss: 0.591 | Acc: 68.064% (341/501)\n",
            "Test accuracy: 0.6806387305259705\n",
            "Epoch: 29, Loss: 0.0004334498953539878 Updates: 151/30001, Avg Grad: 0.0117111178115010260\n",
            "Epoch: 29, Loss: 0.001612054300494492 Updates: 156/31001, Avg Grad: 0.0456656180322170260\n",
            "Epoch: 29, Loss: 0.0018257409101352096 Updates: 161/32001, Avg Grad: 0.0127220544964075090\n",
            "Epoch 29 iteration 0 Loss: 0.587 | Acc: 100.000% (1/1)\n",
            "Epoch 29 iteration 100 Loss: 0.711 | Acc: 65.347% (66/101)\n",
            "Epoch 29 iteration 200 Loss: 0.804 | Acc: 59.204% (119/201)\n",
            "Epoch 29 iteration 300 Loss: 0.795 | Acc: 59.136% (178/301)\n",
            "Epoch 29 iteration 400 Loss: 0.781 | Acc: 60.349% (242/401)\n",
            "Epoch 29 iteration 500 Loss: 0.776 | Acc: 60.878% (305/501)\n",
            "Test accuracy: 0.6087824106216431\n",
            "Epoch: 30, Loss: 0.002301948145031929 Updates: 1/1, Avg Grad: 0.00100907962769269940\n",
            "Epoch: 30, Loss: 0.003985053393989801 Updates: 6/1001, Avg Grad: 0.0208658948540687560\n",
            "Epoch: 30, Loss: 0.0011097996030002832 Updates: 11/2001, Avg Grad: 0.0294167306274175640\n",
            "Epoch: 30, Loss: 0.004612727556377649 Updates: 16/3001, Avg Grad: 0.026900300756096840\n",
            "Epoch: 30, Loss: 0.0022309739142656326 Updates: 21/4001, Avg Grad: 0.0153415892273187640\n",
            "Epoch: 30, Loss: 0.0033270579297095537 Updates: 26/5001, Avg Grad: 0.017062108963727950\n",
            "Epoch: 30, Loss: 0.00026072809123434126 Updates: 31/6001, Avg Grad: 0.021237039938569070\n",
            "Epoch: 30, Loss: 0.0022016342263668776 Updates: 36/7001, Avg Grad: 0.0133109558373689650\n",
            "Epoch: 30, Loss: 8.804428216535598e-05 Updates: 41/8001, Avg Grad: 0.0285291057080030440\n",
            "Epoch: 30, Loss: 0.007746509276330471 Updates: 46/9001, Avg Grad: 0.0409594476222991940\n",
            "Epoch 29 iteration 0 Loss: 0.788 | Acc: 0.000% (0/1)\n",
            "Epoch 29 iteration 100 Loss: 0.562 | Acc: 76.238% (77/101)\n",
            "Epoch 29 iteration 200 Loss: 0.631 | Acc: 69.154% (139/201)\n",
            "Epoch 29 iteration 300 Loss: 0.632 | Acc: 66.777% (201/301)\n",
            "Epoch 29 iteration 400 Loss: 0.618 | Acc: 67.830% (272/401)\n",
            "Epoch 29 iteration 500 Loss: 0.618 | Acc: 68.064% (341/501)\n",
            "Test accuracy: 0.6806387305259705\n",
            "Epoch: 30, Loss: 0.0005376502522267401 Updates: 51/10001, Avg Grad: 0.034786380827426910\n",
            "Epoch: 30, Loss: 0.0037262639962136745 Updates: 56/11001, Avg Grad: 0.021429343149065970\n",
            "Epoch: 30, Loss: 0.0028430738020688295 Updates: 61/12001, Avg Grad: 0.028981355950236320\n",
            "Epoch: 30, Loss: 0.010798032395541668 Updates: 66/13001, Avg Grad: 0.0110916830599308010\n",
            "Epoch: 30, Loss: 0.0003928394871763885 Updates: 71/14001, Avg Grad: 0.03073325939476490\n",
            "Epoch: 30, Loss: 0.0008475354989059269 Updates: 76/15001, Avg Grad: 0.035275854170322420\n",
            "Epoch: 30, Loss: 0.0038201597053557634 Updates: 81/16001, Avg Grad: 0.0188125465065240860\n",
            "Epoch: 30, Loss: 0.0029351720586419106 Updates: 86/17001, Avg Grad: 0.036474633961915970\n",
            "Epoch: 30, Loss: 0.005171122495085001 Updates: 91/18001, Avg Grad: 0.0186305865645408630\n",
            "Epoch: 30, Loss: 0.00941848661750555 Updates: 96/19001, Avg Grad: 0.066954337060451510\n",
            "Epoch 29 iteration 0 Loss: 0.435 | Acc: 100.000% (1/1)\n",
            "Epoch 29 iteration 100 Loss: 0.668 | Acc: 67.327% (68/101)\n",
            "Epoch 29 iteration 200 Loss: 0.627 | Acc: 66.667% (134/201)\n",
            "Epoch 29 iteration 300 Loss: 0.622 | Acc: 67.442% (203/301)\n",
            "Epoch 29 iteration 400 Loss: 0.619 | Acc: 67.082% (269/401)\n",
            "Epoch 29 iteration 500 Loss: 0.606 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 30, Loss: 0.000932869384996593 Updates: 101/20001, Avg Grad: 0.024613417685031890\n",
            "Epoch: 30, Loss: 0.0014896825887262821 Updates: 106/21001, Avg Grad: 0.0278631933033466340\n",
            "Epoch: 30, Loss: 0.0027704969979822636 Updates: 111/22001, Avg Grad: 0.0402809903025627140\n",
            "Epoch: 30, Loss: 0.003871460212394595 Updates: 116/23001, Avg Grad: 0.0262050945311784740\n",
            "Epoch: 30, Loss: 0.0005068255704827607 Updates: 121/24001, Avg Grad: 0.0247678812593221660\n",
            "Epoch: 30, Loss: 0.001219137222506106 Updates: 126/25001, Avg Grad: 0.083319947123527530\n",
            "Epoch: 30, Loss: 0.0008697472512722015 Updates: 131/26001, Avg Grad: 0.025606717914342880\n",
            "Epoch: 30, Loss: 0.0016779879806563258 Updates: 136/27001, Avg Grad: 0.025063889101147650\n",
            "Epoch: 30, Loss: 0.0031424295157194138 Updates: 141/28001, Avg Grad: 0.017846435308456420\n",
            "Epoch: 30, Loss: 0.003618645714595914 Updates: 146/29001, Avg Grad: 0.026262795552611350\n",
            "Epoch 29 iteration 0 Loss: 0.213 | Acc: 100.000% (1/1)\n",
            "Epoch 29 iteration 100 Loss: 0.832 | Acc: 53.465% (54/101)\n",
            "Epoch 29 iteration 200 Loss: 0.759 | Acc: 58.209% (117/201)\n",
            "Epoch 29 iteration 300 Loss: 0.732 | Acc: 60.465% (182/301)\n",
            "Epoch 29 iteration 400 Loss: 0.741 | Acc: 59.601% (239/401)\n",
            "Epoch 29 iteration 500 Loss: 0.721 | Acc: 61.078% (306/501)\n",
            "Test accuracy: 0.6107784509658813\n",
            "Epoch: 30, Loss: 0.0010489709675312042 Updates: 151/30001, Avg Grad: 0.0154608860611915590\n",
            "Epoch: 30, Loss: 0.001687808195129037 Updates: 156/31001, Avg Grad: 0.0192032475024461750\n",
            "Epoch: 30, Loss: 0.0014599135611206293 Updates: 161/32001, Avg Grad: 0.049686178565025330\n",
            "Epoch 30 iteration 0 Loss: 0.895 | Acc: 0.000% (0/1)\n",
            "Epoch 30 iteration 100 Loss: 0.604 | Acc: 72.277% (73/101)\n",
            "Epoch 30 iteration 200 Loss: 0.601 | Acc: 72.139% (145/201)\n",
            "Epoch 30 iteration 300 Loss: 0.602 | Acc: 70.100% (211/301)\n",
            "Epoch 30 iteration 400 Loss: 0.597 | Acc: 69.077% (277/401)\n",
            "Epoch 30 iteration 500 Loss: 0.612 | Acc: 67.665% (339/501)\n",
            "Test accuracy: 0.6766467094421387\n",
            "Epoch: 31, Loss: 0.0005439294036477804 Updates: 1/1, Avg Grad: 0.00027860829140990970\n",
            "Epoch: 31, Loss: 0.0015331503236666322 Updates: 6/1001, Avg Grad: 0.0232972092926502230\n",
            "Epoch: 31, Loss: 0.0023757743183523417 Updates: 11/2001, Avg Grad: 0.035605505108833310\n",
            "Epoch: 31, Loss: 0.0034192854072898626 Updates: 16/3001, Avg Grad: 0.032634157687425610\n",
            "Epoch: 31, Loss: 0.0011877829674631357 Updates: 21/4001, Avg Grad: 0.040686834603548050\n",
            "Epoch: 31, Loss: 0.0007129687000997365 Updates: 26/5001, Avg Grad: 0.079579941928386690\n",
            "Epoch: 31, Loss: 0.0068664876744151115 Updates: 31/6001, Avg Grad: 0.0148669090121984480\n",
            "Epoch: 31, Loss: 0.007115492131561041 Updates: 36/7001, Avg Grad: 0.0239828880876302720\n",
            "Epoch: 31, Loss: 0.002910689450800419 Updates: 41/8001, Avg Grad: 0.0284518115222454070\n",
            "Epoch: 31, Loss: 0.0031625586561858654 Updates: 46/9001, Avg Grad: 0.011573502793908120\n",
            "Epoch 30 iteration 0 Loss: 0.581 | Acc: 100.000% (1/1)\n",
            "Epoch 30 iteration 100 Loss: 0.655 | Acc: 65.347% (66/101)\n",
            "Epoch 30 iteration 200 Loss: 0.657 | Acc: 65.174% (131/201)\n",
            "Epoch 30 iteration 300 Loss: 0.648 | Acc: 64.452% (194/301)\n",
            "Epoch 30 iteration 400 Loss: 0.623 | Acc: 66.085% (265/401)\n",
            "Epoch 30 iteration 500 Loss: 0.615 | Acc: 66.467% (333/501)\n",
            "Test accuracy: 0.6646706461906433\n",
            "Epoch: 31, Loss: 0.001105115981772542 Updates: 51/10001, Avg Grad: 0.036904696375131610\n",
            "Epoch: 31, Loss: 0.001291195279918611 Updates: 56/11001, Avg Grad: 0.0276185590773820880\n",
            "Epoch: 31, Loss: 0.0014560814015567303 Updates: 61/12001, Avg Grad: 0.0191041398793458940\n",
            "Epoch: 31, Loss: 0.0016114383470267057 Updates: 66/13001, Avg Grad: 0.037431050091981890\n",
            "Epoch: 31, Loss: 0.001913165207952261 Updates: 71/14001, Avg Grad: 0.037709403783082960\n",
            "Epoch: 31, Loss: 0.00405548233538866 Updates: 76/15001, Avg Grad: 0.0095554273575544360\n",
            "Epoch: 31, Loss: 0.0043308064341545105 Updates: 81/16001, Avg Grad: 0.038216996937990190\n",
            "Epoch: 31, Loss: 0.0022751251235604286 Updates: 86/17001, Avg Grad: 0.0293078832328319550\n",
            "Epoch: 31, Loss: 0.0009642813238315284 Updates: 91/18001, Avg Grad: 0.031498476862907410\n",
            "Epoch: 31, Loss: 0.0013929104898124933 Updates: 96/19001, Avg Grad: 0.04246229678392410\n",
            "Epoch 30 iteration 0 Loss: 0.207 | Acc: 100.000% (1/1)\n",
            "Epoch 30 iteration 100 Loss: 0.556 | Acc: 73.267% (74/101)\n",
            "Epoch 30 iteration 200 Loss: 0.599 | Acc: 70.647% (142/201)\n",
            "Epoch 30 iteration 300 Loss: 0.571 | Acc: 71.096% (214/301)\n",
            "Epoch 30 iteration 400 Loss: 0.562 | Acc: 71.072% (285/401)\n",
            "Epoch 30 iteration 500 Loss: 0.545 | Acc: 72.455% (363/501)\n",
            "Test accuracy: 0.7245509028434753\n",
            "Epoch: 31, Loss: 0.004099822137504816 Updates: 101/20001, Avg Grad: 0.0122515344992280\n",
            "Epoch: 31, Loss: 0.0010641999542713165 Updates: 106/21001, Avg Grad: 0.035615339875221250\n",
            "Epoch: 31, Loss: 0.001836554380133748 Updates: 111/22001, Avg Grad: 0.0145823098719120030\n",
            "Epoch: 31, Loss: 0.004244440235197544 Updates: 116/23001, Avg Grad: 0.02345082536339760\n",
            "Epoch: 31, Loss: 0.006412904243916273 Updates: 121/24001, Avg Grad: 0.0155742987990379330\n",
            "Epoch: 31, Loss: 0.0017487147124484181 Updates: 126/25001, Avg Grad: 0.0150527413934469220\n",
            "Epoch: 31, Loss: 0.0037332975771278143 Updates: 131/26001, Avg Grad: 0.0236556120216846470\n",
            "Epoch: 31, Loss: 0.0025664290878921747 Updates: 136/27001, Avg Grad: 0.0282301455736160280\n",
            "Epoch: 31, Loss: 0.004087891895323992 Updates: 141/28001, Avg Grad: 0.032312858849763870\n",
            "Epoch: 31, Loss: 0.0064174942672252655 Updates: 146/29001, Avg Grad: 0.0083346990868449210\n",
            "Epoch 30 iteration 0 Loss: 0.356 | Acc: 100.000% (1/1)\n",
            "Epoch 30 iteration 100 Loss: 0.576 | Acc: 70.297% (71/101)\n",
            "Epoch 30 iteration 200 Loss: 0.589 | Acc: 70.149% (141/201)\n",
            "Epoch 30 iteration 300 Loss: 0.603 | Acc: 67.774% (204/301)\n",
            "Epoch 30 iteration 400 Loss: 0.600 | Acc: 67.332% (270/401)\n",
            "Epoch 30 iteration 500 Loss: 0.579 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 31, Loss: 0.0008175296243280172 Updates: 151/30001, Avg Grad: 0.015207732096314430\n",
            "Epoch: 31, Loss: 0.005600686185061932 Updates: 156/31001, Avg Grad: 0.0259005445986986160\n",
            "Epoch: 31, Loss: 0.00037036355934105814 Updates: 161/32001, Avg Grad: 0.0150780901312828060\n",
            "Epoch 31 iteration 0 Loss: 0.806 | Acc: 0.000% (0/1)\n",
            "Epoch 31 iteration 100 Loss: 0.597 | Acc: 69.307% (70/101)\n",
            "Epoch 31 iteration 200 Loss: 0.581 | Acc: 69.154% (139/201)\n",
            "Epoch 31 iteration 300 Loss: 0.575 | Acc: 70.432% (212/301)\n",
            "Epoch 31 iteration 400 Loss: 0.579 | Acc: 71.322% (286/401)\n",
            "Epoch 31 iteration 500 Loss: 0.577 | Acc: 71.058% (356/501)\n",
            "Test accuracy: 0.7105788588523865\n",
            "Epoch: 32, Loss: 0.0021225472446531057 Updates: 1/1, Avg Grad: 0.00092241168022155760\n",
            "Epoch: 32, Loss: 0.0003941491013392806 Updates: 6/1001, Avg Grad: 0.0187532361596822740\n",
            "Epoch: 32, Loss: 0.0002198201254941523 Updates: 11/2001, Avg Grad: 0.0111762927845120430\n",
            "Epoch: 32, Loss: 0.0011951210908591747 Updates: 16/3001, Avg Grad: 0.018816651776432990\n",
            "Epoch: 32, Loss: 0.0007082819356583059 Updates: 21/4001, Avg Grad: 0.0568442828953266140\n",
            "Epoch: 32, Loss: 0.004551785998046398 Updates: 26/5001, Avg Grad: 0.036980401724576950\n",
            "Epoch: 32, Loss: 0.0022348789498209953 Updates: 31/6001, Avg Grad: 0.020215401425957680\n",
            "Epoch: 32, Loss: 0.00042372546158730984 Updates: 36/7001, Avg Grad: 0.0274723283946514130\n",
            "Epoch: 32, Loss: 0.0007177053485065699 Updates: 41/8001, Avg Grad: 0.0121757257729768750\n",
            "Epoch: 32, Loss: 0.003649770515039563 Updates: 46/9001, Avg Grad: 0.0313405506312847140\n",
            "Epoch 31 iteration 0 Loss: 0.234 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.619 | Acc: 64.356% (65/101)\n",
            "Epoch 31 iteration 200 Loss: 0.577 | Acc: 68.159% (137/201)\n",
            "Epoch 31 iteration 300 Loss: 0.600 | Acc: 66.777% (201/301)\n",
            "Epoch 31 iteration 400 Loss: 0.606 | Acc: 65.835% (264/401)\n",
            "Epoch 31 iteration 500 Loss: 0.587 | Acc: 67.864% (340/501)\n",
            "Test accuracy: 0.6786426901817322\n",
            "Epoch: 32, Loss: 0.001720026833936572 Updates: 51/10001, Avg Grad: 0.0208369307219982150\n",
            "Epoch: 32, Loss: 0.006967267487198114 Updates: 56/11001, Avg Grad: 0.0101278433576226230\n",
            "Epoch: 32, Loss: 0.004424785729497671 Updates: 61/12001, Avg Grad: 0.0135597856715321540\n",
            "Epoch: 32, Loss: 0.0003970353282056749 Updates: 66/13001, Avg Grad: 0.0124637316912412640\n",
            "Epoch: 32, Loss: 0.000519155350048095 Updates: 71/14001, Avg Grad: 0.0584399402141571040\n",
            "Epoch: 32, Loss: 0.0011142970761284232 Updates: 76/15001, Avg Grad: 0.008696212433278560\n",
            "Epoch: 32, Loss: 0.0010158164659515023 Updates: 81/16001, Avg Grad: 0.029498824849724770\n",
            "Epoch: 32, Loss: 0.0029721304308623075 Updates: 86/17001, Avg Grad: 0.0122616030275821690\n",
            "Epoch: 32, Loss: 0.002210833365097642 Updates: 91/18001, Avg Grad: 0.037050135433673860\n",
            "Epoch: 32, Loss: 0.0017610746435821056 Updates: 96/19001, Avg Grad: 0.0261037852615118030\n",
            "Epoch 31 iteration 0 Loss: 0.163 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.680 | Acc: 62.376% (63/101)\n",
            "Epoch 31 iteration 200 Loss: 0.656 | Acc: 65.672% (132/201)\n",
            "Epoch 31 iteration 300 Loss: 0.626 | Acc: 68.439% (206/301)\n",
            "Epoch 31 iteration 400 Loss: 0.645 | Acc: 67.332% (270/401)\n",
            "Epoch 31 iteration 500 Loss: 0.633 | Acc: 68.263% (342/501)\n",
            "Test accuracy: 0.682634711265564\n",
            "Epoch: 32, Loss: 0.005791546311229467 Updates: 101/20001, Avg Grad: 0.0400008298456668850\n",
            "Epoch: 32, Loss: 0.006568741984665394 Updates: 106/21001, Avg Grad: 0.0364322625100612640\n",
            "Epoch: 32, Loss: 0.00036196611472405493 Updates: 111/22001, Avg Grad: 0.043741259723901750\n",
            "Epoch: 32, Loss: 0.0019492005230858922 Updates: 116/23001, Avg Grad: 0.018526323139667510\n",
            "Epoch: 32, Loss: 0.0027123335748910904 Updates: 121/24001, Avg Grad: 0.0141697358340024950\n",
            "Epoch: 32, Loss: 0.0017572806682437658 Updates: 126/25001, Avg Grad: 0.042872127145528790\n",
            "Epoch: 32, Loss: 0.0026677430141717196 Updates: 131/26001, Avg Grad: 0.0320776663720607760\n",
            "Epoch: 32, Loss: 0.0009659604984335601 Updates: 136/27001, Avg Grad: 0.0158985387533903120\n",
            "Epoch: 32, Loss: 0.0018200756749138236 Updates: 141/28001, Avg Grad: 0.028410824015736580\n",
            "Epoch: 32, Loss: 0.0004218917456455529 Updates: 146/29001, Avg Grad: 0.047409728169441220\n",
            "Epoch 31 iteration 0 Loss: 0.179 | Acc: 100.000% (1/1)\n",
            "Epoch 31 iteration 100 Loss: 0.555 | Acc: 70.297% (71/101)\n",
            "Epoch 31 iteration 200 Loss: 0.597 | Acc: 67.662% (136/201)\n",
            "Epoch 31 iteration 300 Loss: 0.611 | Acc: 66.777% (201/301)\n",
            "Epoch 31 iteration 400 Loss: 0.578 | Acc: 69.077% (277/401)\n",
            "Epoch 31 iteration 500 Loss: 0.597 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 32, Loss: 0.008268770761787891 Updates: 151/30001, Avg Grad: 0.02633376419544220\n",
            "Epoch: 32, Loss: 0.0026394459418952465 Updates: 156/31001, Avg Grad: 0.0384915918111801150\n",
            "Epoch: 32, Loss: 0.004496193025261164 Updates: 161/32001, Avg Grad: 0.043354634195566180\n",
            "Epoch 32 iteration 0 Loss: 1.745 | Acc: 0.000% (0/1)\n",
            "Epoch 32 iteration 100 Loss: 1.009 | Acc: 51.485% (52/101)\n",
            "Epoch 32 iteration 200 Loss: 0.969 | Acc: 52.239% (105/201)\n",
            "Epoch 32 iteration 300 Loss: 0.996 | Acc: 50.831% (153/301)\n",
            "Epoch 32 iteration 400 Loss: 1.002 | Acc: 51.621% (207/401)\n",
            "Epoch 32 iteration 500 Loss: 0.920 | Acc: 55.888% (280/501)\n",
            "Test accuracy: 0.5588822364807129\n",
            "Epoch: 33, Loss: 0.00132868648506701 Updates: 1/1, Avg Grad: 0.00066568457987159490\n",
            "Epoch: 33, Loss: 0.0014824536629021168 Updates: 6/1001, Avg Grad: 0.0139609882608056070\n",
            "Epoch: 33, Loss: 0.004408024251461029 Updates: 11/2001, Avg Grad: 0.0141354426741600040\n",
            "Epoch: 33, Loss: 0.0008589370409026742 Updates: 16/3001, Avg Grad: 0.050890330225229260\n",
            "Epoch: 33, Loss: 0.00314866891130805 Updates: 21/4001, Avg Grad: 0.022173855453729630\n",
            "Epoch: 33, Loss: 0.00033819154486991465 Updates: 26/5001, Avg Grad: 0.0113364113494753840\n",
            "Epoch: 33, Loss: 0.0048222774639725685 Updates: 31/6001, Avg Grad: 0.0280508063733577730\n",
            "Epoch: 33, Loss: 0.0003763774875551462 Updates: 36/7001, Avg Grad: 0.02188825048506260\n",
            "Epoch: 33, Loss: 0.0007568441214971244 Updates: 41/8001, Avg Grad: 0.0169183518737554550\n",
            "Epoch: 33, Loss: 0.0001371109246974811 Updates: 46/9001, Avg Grad: 0.0136292455717921260\n",
            "Epoch 32 iteration 0 Loss: 1.277 | Acc: 0.000% (0/1)\n",
            "Epoch 32 iteration 100 Loss: 0.794 | Acc: 55.446% (56/101)\n",
            "Epoch 32 iteration 200 Loss: 0.700 | Acc: 60.199% (121/201)\n",
            "Epoch 32 iteration 300 Loss: 0.676 | Acc: 62.458% (188/301)\n",
            "Epoch 32 iteration 400 Loss: 0.633 | Acc: 65.337% (262/401)\n",
            "Epoch 32 iteration 500 Loss: 0.623 | Acc: 66.068% (331/501)\n",
            "Test accuracy: 0.6606786251068115\n",
            "Epoch: 33, Loss: 0.004735369235277176 Updates: 51/10001, Avg Grad: 0.0277792476117610930\n",
            "Epoch: 33, Loss: 0.005657011177390814 Updates: 56/11001, Avg Grad: 0.032705500721931460\n",
            "Epoch: 33, Loss: 0.0009560734033584595 Updates: 61/12001, Avg Grad: 0.0131911151111125950\n",
            "Epoch: 33, Loss: 0.0021791416220366955 Updates: 66/13001, Avg Grad: 0.0325853489339351650\n",
            "Epoch: 33, Loss: 0.010024738498032093 Updates: 71/14001, Avg Grad: 0.030549814924597740\n",
            "Epoch: 33, Loss: 0.001568229985423386 Updates: 76/15001, Avg Grad: 0.0224811546504497530\n",
            "Epoch: 33, Loss: 0.012794689275324345 Updates: 81/16001, Avg Grad: 0.031101046130061150\n",
            "Epoch: 33, Loss: 0.001563683501444757 Updates: 86/17001, Avg Grad: 0.009274752810597420\n",
            "Epoch: 33, Loss: 0.0011005514534190297 Updates: 91/18001, Avg Grad: 0.010312217287719250\n",
            "Epoch: 33, Loss: 0.0017799038905650377 Updates: 96/19001, Avg Grad: 0.011267053894698620\n",
            "Epoch 32 iteration 0 Loss: 0.996 | Acc: 0.000% (0/1)\n",
            "Epoch 32 iteration 100 Loss: 0.515 | Acc: 75.248% (76/101)\n",
            "Epoch 32 iteration 200 Loss: 0.586 | Acc: 70.149% (141/201)\n",
            "Epoch 32 iteration 300 Loss: 0.595 | Acc: 70.100% (211/301)\n",
            "Epoch 32 iteration 400 Loss: 0.574 | Acc: 71.072% (285/401)\n",
            "Epoch 32 iteration 500 Loss: 0.582 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 33, Loss: 0.0012989225797355175 Updates: 101/20001, Avg Grad: 0.0204952545464038850\n",
            "Epoch: 33, Loss: 0.0015925122424960136 Updates: 106/21001, Avg Grad: 0.034357082098722460\n",
            "Epoch: 33, Loss: 0.00670660100877285 Updates: 111/22001, Avg Grad: 0.0291030034422874450\n",
            "Epoch: 33, Loss: 0.0032675524707883596 Updates: 116/23001, Avg Grad: 0.0203946251422166820\n",
            "Epoch: 33, Loss: 0.0023668790236115456 Updates: 121/24001, Avg Grad: 0.035531233996152880\n",
            "Epoch: 33, Loss: 0.0032073059119284153 Updates: 126/25001, Avg Grad: 0.0089499056339263920\n",
            "Epoch: 33, Loss: 0.0014541958225890994 Updates: 131/26001, Avg Grad: 0.0229672975838184360\n",
            "Epoch: 33, Loss: 0.0031720460392534733 Updates: 136/27001, Avg Grad: 0.0218385346233844760\n",
            "Epoch: 33, Loss: 0.0017075254581868649 Updates: 141/28001, Avg Grad: 0.0130954375490546230\n",
            "Epoch: 33, Loss: 0.00126123265363276 Updates: 146/29001, Avg Grad: 0.03539482504129410\n",
            "Epoch 32 iteration 0 Loss: 0.907 | Acc: 0.000% (0/1)\n",
            "Epoch 32 iteration 100 Loss: 0.649 | Acc: 65.347% (66/101)\n",
            "Epoch 32 iteration 200 Loss: 0.645 | Acc: 64.677% (130/201)\n",
            "Epoch 32 iteration 300 Loss: 0.624 | Acc: 65.116% (196/301)\n",
            "Epoch 32 iteration 400 Loss: 0.638 | Acc: 64.589% (259/401)\n",
            "Epoch 32 iteration 500 Loss: 0.642 | Acc: 63.872% (320/501)\n",
            "Test accuracy: 0.6387225389480591\n",
            "Epoch: 33, Loss: 0.0014139062259346247 Updates: 151/30001, Avg Grad: 0.027434621006250380\n",
            "Epoch: 33, Loss: 0.005154266953468323 Updates: 156/31001, Avg Grad: 0.0104056736454367640\n",
            "Epoch: 33, Loss: 0.0010863605421036482 Updates: 161/32001, Avg Grad: 0.0137054370716214180\n",
            "Epoch 33 iteration 0 Loss: 1.305 | Acc: 0.000% (0/1)\n",
            "Epoch 33 iteration 100 Loss: 0.635 | Acc: 70.297% (71/101)\n",
            "Epoch 33 iteration 200 Loss: 0.611 | Acc: 73.134% (147/201)\n",
            "Epoch 33 iteration 300 Loss: 0.585 | Acc: 73.090% (220/301)\n",
            "Epoch 33 iteration 400 Loss: 0.591 | Acc: 73.067% (293/401)\n",
            "Epoch 33 iteration 500 Loss: 0.582 | Acc: 74.052% (371/501)\n",
            "Test accuracy: 0.7405189871788025\n",
            "Epoch: 34, Loss: 0.0021379864774644375 Updates: 1/1, Avg Grad: 0.00085303367814049120\n",
            "Epoch: 34, Loss: 0.00026343733770772815 Updates: 6/1001, Avg Grad: 0.0196391157805919650\n",
            "Epoch: 34, Loss: 0.005303589161485434 Updates: 11/2001, Avg Grad: 0.0515176355838775630\n",
            "Epoch: 34, Loss: 0.0018144710920751095 Updates: 16/3001, Avg Grad: 0.0214560981839895250\n",
            "Epoch: 34, Loss: 0.0013941668439656496 Updates: 21/4001, Avg Grad: 0.0204902403056621550\n",
            "Epoch: 34, Loss: 0.003797082230448723 Updates: 26/5001, Avg Grad: 0.0355252586305141450\n",
            "Epoch: 34, Loss: 0.0028406830970197916 Updates: 31/6001, Avg Grad: 0.0153497420251369480\n",
            "Epoch: 34, Loss: 0.001068800687789917 Updates: 36/7001, Avg Grad: 0.05287726968526840\n",
            "Epoch: 34, Loss: 0.0016525882529094815 Updates: 41/8001, Avg Grad: 0.04003809764981270\n",
            "Epoch: 34, Loss: 0.0031334480736404657 Updates: 46/9001, Avg Grad: 0.0283706728368997570\n",
            "Epoch 33 iteration 0 Loss: 0.758 | Acc: 0.000% (0/1)\n",
            "Epoch 33 iteration 100 Loss: 0.630 | Acc: 69.307% (70/101)\n",
            "Epoch 33 iteration 200 Loss: 0.649 | Acc: 65.174% (131/201)\n",
            "Epoch 33 iteration 300 Loss: 0.633 | Acc: 67.442% (203/301)\n",
            "Epoch 33 iteration 400 Loss: 0.608 | Acc: 68.828% (276/401)\n",
            "Epoch 33 iteration 500 Loss: 0.590 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 34, Loss: 0.006198695860803127 Updates: 51/10001, Avg Grad: 0.047425605356693270\n",
            "Epoch: 34, Loss: 0.002371106995269656 Updates: 56/11001, Avg Grad: 0.0310678463429212570\n",
            "Epoch: 34, Loss: 0.004169634077697992 Updates: 61/12001, Avg Grad: 0.032081991434097290\n",
            "Epoch: 34, Loss: 0.002682444406673312 Updates: 66/13001, Avg Grad: 0.0293667558580636980\n",
            "Epoch: 34, Loss: 0.005855188239365816 Updates: 71/14001, Avg Grad: 0.0284368749707937240\n",
            "Epoch: 34, Loss: 0.001584712415933609 Updates: 76/15001, Avg Grad: 0.038409978151321410\n",
            "Epoch: 34, Loss: 0.006067873444408178 Updates: 81/16001, Avg Grad: 0.0327167920768260960\n",
            "Epoch: 34, Loss: 0.00103170913644135 Updates: 86/17001, Avg Grad: 0.0310826245695352550\n",
            "Epoch: 34, Loss: 0.0019962287042289972 Updates: 91/18001, Avg Grad: 0.032415188848972320\n",
            "Epoch: 34, Loss: 0.0021390162874013186 Updates: 96/19001, Avg Grad: 0.0084368214011192320\n",
            "Epoch 33 iteration 0 Loss: 0.202 | Acc: 100.000% (1/1)\n",
            "Epoch 33 iteration 100 Loss: 0.616 | Acc: 66.337% (67/101)\n",
            "Epoch 33 iteration 200 Loss: 0.574 | Acc: 70.647% (142/201)\n",
            "Epoch 33 iteration 300 Loss: 0.557 | Acc: 72.757% (219/301)\n",
            "Epoch 33 iteration 400 Loss: 0.568 | Acc: 72.319% (290/401)\n",
            "Epoch 33 iteration 500 Loss: 0.573 | Acc: 71.856% (360/501)\n",
            "Test accuracy: 0.71856290102005\n",
            "Epoch: 34, Loss: 0.0027707091066986322 Updates: 101/20001, Avg Grad: 0.016026351600885390\n",
            "Epoch: 34, Loss: 0.003941895440220833 Updates: 106/21001, Avg Grad: 0.0113060129806399350\n",
            "Epoch: 34, Loss: 0.0028867595829069614 Updates: 111/22001, Avg Grad: 0.025574240833520890\n",
            "Epoch: 34, Loss: 0.0028330134227871895 Updates: 116/23001, Avg Grad: 0.024197898805141450\n",
            "Epoch: 34, Loss: 0.001974766608327627 Updates: 121/24001, Avg Grad: 0.0100933918729424480\n",
            "Epoch: 34, Loss: 0.002773526357486844 Updates: 126/25001, Avg Grad: 0.0369241274893283840\n",
            "Epoch: 34, Loss: 0.003286228282377124 Updates: 131/26001, Avg Grad: 0.0255532748997211460\n",
            "Epoch: 34, Loss: 0.0022893331479281187 Updates: 136/27001, Avg Grad: 0.0211225524544715880\n",
            "Epoch: 34, Loss: 0.004122386686503887 Updates: 141/28001, Avg Grad: 0.0154896471649408340\n",
            "Epoch: 34, Loss: 0.00040187340346165 Updates: 146/29001, Avg Grad: 0.0162560734897851940\n",
            "Epoch 33 iteration 0 Loss: 0.237 | Acc: 100.000% (1/1)\n",
            "Epoch 33 iteration 100 Loss: 0.456 | Acc: 82.178% (83/101)\n",
            "Epoch 33 iteration 200 Loss: 0.504 | Acc: 77.114% (155/201)\n",
            "Epoch 33 iteration 300 Loss: 0.551 | Acc: 73.422% (221/301)\n",
            "Epoch 33 iteration 400 Loss: 0.565 | Acc: 71.072% (285/401)\n",
            "Epoch 33 iteration 500 Loss: 0.570 | Acc: 71.657% (359/501)\n",
            "Test accuracy: 0.7165668606758118\n",
            "Epoch: 34, Loss: 0.0010829688981175423 Updates: 151/30001, Avg Grad: 0.0145328398793935780\n",
            "Epoch: 34, Loss: 0.0067943427711725235 Updates: 156/31001, Avg Grad: 0.0098331924527883530\n",
            "Epoch: 34, Loss: 0.00014685920905321836 Updates: 161/32001, Avg Grad: 0.0231283828616142270\n",
            "Epoch 34 iteration 0 Loss: 1.935 | Acc: 0.000% (0/1)\n",
            "Epoch 34 iteration 100 Loss: 0.611 | Acc: 67.327% (68/101)\n",
            "Epoch 34 iteration 200 Loss: 0.641 | Acc: 66.169% (133/201)\n",
            "Epoch 34 iteration 300 Loss: 0.625 | Acc: 66.777% (201/301)\n",
            "Epoch 34 iteration 400 Loss: 0.656 | Acc: 64.838% (260/401)\n",
            "Epoch 34 iteration 500 Loss: 0.660 | Acc: 65.269% (327/501)\n",
            "Test accuracy: 0.652694582939148\n",
            "Epoch: 35, Loss: 0.00311621418222785 Updates: 1/1, Avg Grad: 0.00127689121291041370\n",
            "Epoch: 35, Loss: 0.002392175607383251 Updates: 6/1001, Avg Grad: 0.0210852771997451780\n",
            "Epoch: 35, Loss: 0.0035701412707567215 Updates: 11/2001, Avg Grad: 0.050784669816493990\n",
            "Epoch: 35, Loss: 0.0034670245368033648 Updates: 16/3001, Avg Grad: 0.02970268763601780\n",
            "Epoch: 35, Loss: 0.0020423748064786196 Updates: 21/4001, Avg Grad: 0.0344232134521007540\n",
            "Epoch: 35, Loss: 0.0026370862033218145 Updates: 26/5001, Avg Grad: 0.0374035462737083440\n",
            "Epoch: 35, Loss: 0.00040275606443174183 Updates: 31/6001, Avg Grad: 0.025224065408110620\n",
            "Epoch: 35, Loss: 0.004984111059457064 Updates: 36/7001, Avg Grad: 0.0239605810493230820\n",
            "Epoch: 35, Loss: 0.0012392273638397455 Updates: 41/8001, Avg Grad: 0.051789663732051850\n",
            "Epoch: 35, Loss: 0.0006366395391523838 Updates: 46/9001, Avg Grad: 0.028250608593225480\n",
            "Epoch 34 iteration 0 Loss: 0.229 | Acc: 100.000% (1/1)\n",
            "Epoch 34 iteration 100 Loss: 0.587 | Acc: 72.277% (73/101)\n",
            "Epoch 34 iteration 200 Loss: 0.591 | Acc: 70.149% (141/201)\n",
            "Epoch 34 iteration 300 Loss: 0.590 | Acc: 69.435% (209/301)\n",
            "Epoch 34 iteration 400 Loss: 0.598 | Acc: 68.579% (275/401)\n",
            "Epoch 34 iteration 500 Loss: 0.596 | Acc: 67.864% (340/501)\n",
            "Test accuracy: 0.6786426901817322\n",
            "Epoch: 35, Loss: 0.003736149752512574 Updates: 51/10001, Avg Grad: 0.0419777184724807740\n",
            "Epoch: 35, Loss: 0.0014363385271281004 Updates: 56/11001, Avg Grad: 0.048709567636251450\n",
            "Epoch: 35, Loss: 0.002466273959726095 Updates: 61/12001, Avg Grad: 0.067780636250972750\n",
            "Epoch: 35, Loss: 0.0010813255794346333 Updates: 66/13001, Avg Grad: 0.0309478230774402620\n",
            "Epoch: 35, Loss: 0.0035347514785826206 Updates: 71/14001, Avg Grad: 0.011543142609298230\n",
            "Epoch: 35, Loss: 0.003924516960978508 Updates: 76/15001, Avg Grad: 0.0242189075797796250\n",
            "Epoch: 35, Loss: 0.012176441960036755 Updates: 81/16001, Avg Grad: 0.0129553088918328290\n",
            "Epoch: 35, Loss: 0.0005599712603725493 Updates: 86/17001, Avg Grad: 0.0227812845259904860\n",
            "Epoch: 35, Loss: 0.0011466614669188857 Updates: 91/18001, Avg Grad: 0.0193853098899126050\n",
            "Epoch: 35, Loss: 0.0024980269372463226 Updates: 96/19001, Avg Grad: 0.0391849428415298460\n",
            "Epoch 34 iteration 0 Loss: 0.359 | Acc: 100.000% (1/1)\n",
            "Epoch 34 iteration 100 Loss: 0.541 | Acc: 71.287% (72/101)\n",
            "Epoch 34 iteration 200 Loss: 0.575 | Acc: 68.657% (138/201)\n",
            "Epoch 34 iteration 300 Loss: 0.586 | Acc: 67.110% (202/301)\n",
            "Epoch 34 iteration 400 Loss: 0.597 | Acc: 67.082% (269/401)\n",
            "Epoch 34 iteration 500 Loss: 0.606 | Acc: 67.265% (337/501)\n",
            "Test accuracy: 0.6726546883583069\n",
            "Epoch: 35, Loss: 0.001667560194619 Updates: 101/20001, Avg Grad: 0.04216246306896210\n",
            "Epoch: 35, Loss: 0.0016974004684016109 Updates: 106/21001, Avg Grad: 0.0470603220164775850\n",
            "Epoch: 35, Loss: 0.00940759014338255 Updates: 111/22001, Avg Grad: 0.0360234193503856660\n",
            "Epoch: 35, Loss: 0.00038067332934588194 Updates: 116/23001, Avg Grad: 0.0218730457127094270\n",
            "Epoch: 35, Loss: 0.004535006359219551 Updates: 121/24001, Avg Grad: 0.022715831175446510\n",
            "Epoch: 35, Loss: 0.0013848672388121486 Updates: 126/25001, Avg Grad: 0.0515602603554725650\n",
            "Epoch: 35, Loss: 0.0019387811189517379 Updates: 131/26001, Avg Grad: 0.024505371227860450\n",
            "Epoch: 35, Loss: 0.0025454110000282526 Updates: 136/27001, Avg Grad: 0.0191302895545959470\n",
            "Epoch: 35, Loss: 0.006081982050091028 Updates: 141/28001, Avg Grad: 0.012033821083605290\n",
            "Epoch: 35, Loss: 0.008485707454383373 Updates: 146/29001, Avg Grad: 0.0128954015672206880\n",
            "Epoch 34 iteration 0 Loss: 0.505 | Acc: 100.000% (1/1)\n",
            "Epoch 34 iteration 100 Loss: 0.567 | Acc: 67.327% (68/101)\n",
            "Epoch 34 iteration 200 Loss: 0.572 | Acc: 68.657% (138/201)\n",
            "Epoch 34 iteration 300 Loss: 0.588 | Acc: 68.106% (205/301)\n",
            "Epoch 34 iteration 400 Loss: 0.598 | Acc: 67.082% (269/401)\n",
            "Epoch 34 iteration 500 Loss: 0.603 | Acc: 67.465% (338/501)\n",
            "Test accuracy: 0.6746506690979004\n",
            "Epoch: 35, Loss: 0.006174723617732525 Updates: 151/30001, Avg Grad: 0.008881196379661560\n",
            "Epoch: 35, Loss: 0.0010030949488282204 Updates: 156/31001, Avg Grad: 0.0080487122759222980\n",
            "Epoch: 35, Loss: 0.002966278465464711 Updates: 161/32001, Avg Grad: 0.0120922699570655820\n",
            "Epoch 35 iteration 0 Loss: 0.650 | Acc: 100.000% (1/1)\n",
            "Epoch 35 iteration 100 Loss: 0.647 | Acc: 70.297% (71/101)\n",
            "Epoch 35 iteration 200 Loss: 0.592 | Acc: 73.134% (147/201)\n",
            "Epoch 35 iteration 300 Loss: 0.573 | Acc: 73.422% (221/301)\n",
            "Epoch 35 iteration 400 Loss: 0.585 | Acc: 73.317% (294/401)\n",
            "Epoch 35 iteration 500 Loss: 0.612 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 36, Loss: 0.0032591880299150944 Updates: 1/1, Avg Grad: 0.0013042780337855220\n",
            "Epoch: 36, Loss: 0.003206350142136216 Updates: 6/1001, Avg Grad: 0.0306269936263561250\n",
            "Epoch: 36, Loss: 0.00778418080881238 Updates: 11/2001, Avg Grad: 0.0299203135073184970\n",
            "Epoch: 36, Loss: 0.0009486320777796209 Updates: 16/3001, Avg Grad: 0.0297907330095767970\n",
            "Epoch: 36, Loss: 0.00047117614303715527 Updates: 21/4001, Avg Grad: 0.0312161091715097430\n",
            "Epoch: 36, Loss: 0.0017147334292531013 Updates: 26/5001, Avg Grad: 0.022349067032337190\n",
            "Epoch: 36, Loss: 0.0025528017431497574 Updates: 31/6001, Avg Grad: 0.026705469936132430\n",
            "Epoch: 36, Loss: 0.0012421944411471486 Updates: 36/7001, Avg Grad: 0.0239806324243545530\n",
            "Epoch: 36, Loss: 0.006947895511984825 Updates: 41/8001, Avg Grad: 0.0206217095255851750\n",
            "Epoch: 36, Loss: 0.0011312994174659252 Updates: 46/9001, Avg Grad: 0.040264844894409180\n",
            "Epoch 35 iteration 0 Loss: 0.299 | Acc: 100.000% (1/1)\n",
            "Epoch 35 iteration 100 Loss: 0.644 | Acc: 69.307% (70/101)\n",
            "Epoch 35 iteration 200 Loss: 0.610 | Acc: 71.642% (144/201)\n",
            "Epoch 35 iteration 300 Loss: 0.602 | Acc: 71.429% (215/301)\n",
            "Epoch 35 iteration 400 Loss: 0.592 | Acc: 72.319% (290/401)\n",
            "Epoch 35 iteration 500 Loss: 0.593 | Acc: 71.657% (359/501)\n",
            "Test accuracy: 0.7165668606758118\n",
            "Epoch: 36, Loss: 0.0014169493224471807 Updates: 51/10001, Avg Grad: 0.033352389931678770\n",
            "Epoch: 36, Loss: 0.0011966972379013896 Updates: 56/11001, Avg Grad: 0.037091106176376340\n",
            "Epoch: 36, Loss: 0.0008690444519743323 Updates: 61/12001, Avg Grad: 0.044052004814147950\n",
            "Epoch: 36, Loss: 0.0003370767517480999 Updates: 66/13001, Avg Grad: 0.0189266931265592580\n",
            "Epoch: 36, Loss: 0.0009194716112688184 Updates: 71/14001, Avg Grad: 0.0098267402499914170\n",
            "Epoch: 36, Loss: 0.007980379275977612 Updates: 76/15001, Avg Grad: 0.021011888980865480\n",
            "Epoch: 36, Loss: 0.003519507357850671 Updates: 81/16001, Avg Grad: 0.064258418977260590\n",
            "Epoch: 36, Loss: 0.0010385946370661259 Updates: 86/17001, Avg Grad: 0.051854096353054050\n",
            "Epoch: 36, Loss: 0.0012251505395397544 Updates: 91/18001, Avg Grad: 0.031972855329513550\n",
            "Epoch: 36, Loss: 0.003800341859459877 Updates: 96/19001, Avg Grad: 0.0281434394419193270\n",
            "Epoch 35 iteration 0 Loss: 0.785 | Acc: 0.000% (0/1)\n",
            "Epoch 35 iteration 100 Loss: 0.602 | Acc: 70.297% (71/101)\n",
            "Epoch 35 iteration 200 Loss: 0.619 | Acc: 70.149% (141/201)\n",
            "Epoch 35 iteration 300 Loss: 0.596 | Acc: 72.757% (219/301)\n",
            "Epoch 35 iteration 400 Loss: 0.605 | Acc: 72.070% (289/401)\n",
            "Epoch 35 iteration 500 Loss: 0.601 | Acc: 71.257% (357/501)\n",
            "Test accuracy: 0.71257483959198\n",
            "Epoch: 36, Loss: 0.0011865588603541255 Updates: 101/20001, Avg Grad: 0.029876561835408210\n",
            "Epoch: 36, Loss: 0.002654107753187418 Updates: 106/21001, Avg Grad: 0.0139453504234552380\n",
            "Epoch: 36, Loss: 0.0016462926287204027 Updates: 111/22001, Avg Grad: 0.0118044624105095860\n",
            "Epoch: 36, Loss: 0.00411073537543416 Updates: 116/23001, Avg Grad: 0.029317338019609450\n",
            "Epoch: 36, Loss: 0.007800789549946785 Updates: 121/24001, Avg Grad: 0.052231956273317340\n",
            "Epoch: 36, Loss: 0.0017610506620258093 Updates: 126/25001, Avg Grad: 0.0531526692211627960\n",
            "Epoch: 36, Loss: 0.009294543415307999 Updates: 131/26001, Avg Grad: 0.0194823089987039570\n",
            "Epoch: 36, Loss: 0.0010840659961104393 Updates: 136/27001, Avg Grad: 0.037812028080224990\n",
            "Epoch: 36, Loss: 0.008514726534485817 Updates: 141/28001, Avg Grad: 0.0203945916146039960\n",
            "Epoch: 36, Loss: 0.0003151619457639754 Updates: 146/29001, Avg Grad: 0.0443453490734100340\n",
            "Epoch 35 iteration 0 Loss: 0.308 | Acc: 100.000% (1/1)\n",
            "Epoch 35 iteration 100 Loss: 0.620 | Acc: 64.356% (65/101)\n",
            "Epoch 35 iteration 200 Loss: 0.591 | Acc: 68.159% (137/201)\n",
            "Epoch 35 iteration 300 Loss: 0.551 | Acc: 70.432% (212/301)\n",
            "Epoch 35 iteration 400 Loss: 0.566 | Acc: 70.075% (281/401)\n",
            "Epoch 35 iteration 500 Loss: 0.588 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 36, Loss: 0.006150482222437859 Updates: 151/30001, Avg Grad: 0.0319677777588367460\n",
            "Epoch: 36, Loss: 0.001966373063623905 Updates: 156/31001, Avg Grad: 0.026042953133583070\n",
            "Epoch: 36, Loss: 0.0006836719694547355 Updates: 161/32001, Avg Grad: 0.0178146660327911380\n",
            "Epoch 36 iteration 0 Loss: 0.754 | Acc: 0.000% (0/1)\n",
            "Epoch 36 iteration 100 Loss: 0.765 | Acc: 65.347% (66/101)\n",
            "Epoch 36 iteration 200 Loss: 0.756 | Acc: 64.179% (129/201)\n",
            "Epoch 36 iteration 300 Loss: 0.748 | Acc: 64.452% (194/301)\n",
            "Epoch 36 iteration 400 Loss: 0.728 | Acc: 65.337% (262/401)\n",
            "Epoch 36 iteration 500 Loss: 0.720 | Acc: 65.669% (329/501)\n",
            "Test accuracy: 0.6566866040229797\n",
            "Epoch: 37, Loss: 0.0015501085435971618 Updates: 1/1, Avg Grad: 0.00068208441371098160\n",
            "Epoch: 37, Loss: 0.001081670867279172 Updates: 6/1001, Avg Grad: 0.0347174853086471560\n",
            "Epoch: 37, Loss: 0.00291439495049417 Updates: 11/2001, Avg Grad: 0.032243061810731890\n",
            "Epoch: 37, Loss: 0.0008739453624002635 Updates: 16/3001, Avg Grad: 0.0365694165229797360\n",
            "Epoch: 37, Loss: 0.009868982248008251 Updates: 21/4001, Avg Grad: 0.0351366885006427760\n",
            "Epoch: 37, Loss: 0.00043651944724842906 Updates: 26/5001, Avg Grad: 0.0433123260736465450\n",
            "Epoch: 37, Loss: 0.00274260644800961 Updates: 31/6001, Avg Grad: 0.030234415084123610\n",
            "Epoch: 37, Loss: 0.0026110801845788956 Updates: 36/7001, Avg Grad: 0.030710102990269660\n",
            "Epoch: 37, Loss: 0.005382361356168985 Updates: 41/8001, Avg Grad: 0.023299781605601310\n",
            "Epoch: 37, Loss: 0.0007488599512726068 Updates: 46/9001, Avg Grad: 0.041008047759532930\n",
            "Epoch 36 iteration 0 Loss: 0.410 | Acc: 100.000% (1/1)\n",
            "Epoch 36 iteration 100 Loss: 0.546 | Acc: 70.297% (71/101)\n",
            "Epoch 36 iteration 200 Loss: 0.524 | Acc: 72.637% (146/201)\n",
            "Epoch 36 iteration 300 Loss: 0.540 | Acc: 70.764% (213/301)\n",
            "Epoch 36 iteration 400 Loss: 0.556 | Acc: 68.579% (275/401)\n",
            "Epoch 36 iteration 500 Loss: 0.568 | Acc: 68.463% (343/501)\n",
            "Test accuracy: 0.6846307516098022\n",
            "Epoch: 37, Loss: 0.00295922439545393 Updates: 51/10001, Avg Grad: 0.0205209478735923770\n",
            "Epoch: 37, Loss: 0.0020399601198732853 Updates: 56/11001, Avg Grad: 0.0115327835083007810\n",
            "Epoch: 37, Loss: 0.005819634068757296 Updates: 61/12001, Avg Grad: 0.0162264741957187650\n",
            "Epoch: 37, Loss: 0.0016932920552790165 Updates: 66/13001, Avg Grad: 0.0199247747659683230\n",
            "Epoch: 37, Loss: 0.002968434477224946 Updates: 71/14001, Avg Grad: 0.0153070380911231040\n",
            "Epoch: 37, Loss: 0.005947814788669348 Updates: 76/15001, Avg Grad: 0.0344555899500846860\n",
            "Epoch: 37, Loss: 0.0015980472089722753 Updates: 81/16001, Avg Grad: 0.052154488861560820\n",
            "Epoch: 37, Loss: 0.00023355212761089206 Updates: 86/17001, Avg Grad: 0.021701557561755180\n",
            "Epoch: 37, Loss: 0.003066327655687928 Updates: 91/18001, Avg Grad: 0.041422214359045030\n",
            "Epoch: 37, Loss: 0.005907108075916767 Updates: 96/19001, Avg Grad: 0.0263473391532897950\n",
            "Epoch 36 iteration 0 Loss: 0.556 | Acc: 100.000% (1/1)\n",
            "Epoch 36 iteration 100 Loss: 0.538 | Acc: 76.238% (77/101)\n",
            "Epoch 36 iteration 200 Loss: 0.537 | Acc: 75.622% (152/201)\n",
            "Epoch 36 iteration 300 Loss: 0.566 | Acc: 72.757% (219/301)\n",
            "Epoch 36 iteration 400 Loss: 0.573 | Acc: 72.569% (291/401)\n",
            "Epoch 36 iteration 500 Loss: 0.573 | Acc: 72.854% (365/501)\n",
            "Test accuracy: 0.7285429239273071\n",
            "Epoch: 37, Loss: 0.004473069217056036 Updates: 101/20001, Avg Grad: 0.064866431057453160\n",
            "Epoch: 37, Loss: 0.00029417750192806125 Updates: 106/21001, Avg Grad: 0.0097242007032036780\n",
            "Epoch: 37, Loss: 0.0011689889943227172 Updates: 111/22001, Avg Grad: 0.031505938619375230\n",
            "Epoch: 37, Loss: 0.0006396073731593788 Updates: 116/23001, Avg Grad: 0.0134842572733759880\n",
            "Epoch: 37, Loss: 0.0007163993432186544 Updates: 121/24001, Avg Grad: 0.0133798299357295040\n",
            "Epoch: 37, Loss: 0.0005388271529227495 Updates: 126/25001, Avg Grad: 0.0492036230862140660\n",
            "Epoch: 37, Loss: 0.0009684269316494465 Updates: 131/26001, Avg Grad: 0.0158383995294570920\n",
            "Epoch: 37, Loss: 0.002594505436718464 Updates: 136/27001, Avg Grad: 0.0437919124960899350\n",
            "Epoch: 37, Loss: 0.0043545677326619625 Updates: 141/28001, Avg Grad: 0.0273788031190633770\n",
            "Epoch: 37, Loss: 0.0015106702921912074 Updates: 146/29001, Avg Grad: 0.0108028054237365720\n",
            "Epoch 36 iteration 0 Loss: 0.213 | Acc: 100.000% (1/1)\n",
            "Epoch 36 iteration 100 Loss: 0.652 | Acc: 71.287% (72/101)\n",
            "Epoch 36 iteration 200 Loss: 0.593 | Acc: 72.637% (146/201)\n",
            "Epoch 36 iteration 300 Loss: 0.603 | Acc: 71.096% (214/301)\n",
            "Epoch 36 iteration 400 Loss: 0.615 | Acc: 70.324% (282/401)\n",
            "Epoch 36 iteration 500 Loss: 0.612 | Acc: 70.459% (353/501)\n",
            "Test accuracy: 0.7045907974243164\n",
            "Epoch: 37, Loss: 0.007778653874993324 Updates: 151/30001, Avg Grad: 0.0307221766561269760\n",
            "Epoch: 37, Loss: 0.0002912233758252114 Updates: 156/31001, Avg Grad: 0.043286692351102830\n",
            "Epoch: 37, Loss: 0.0007350055384449661 Updates: 161/32001, Avg Grad: 0.0183723140507936480\n",
            "Epoch 37 iteration 0 Loss: 0.382 | Acc: 100.000% (1/1)\n",
            "Epoch 37 iteration 100 Loss: 0.621 | Acc: 68.317% (69/101)\n",
            "Epoch 37 iteration 200 Loss: 0.685 | Acc: 62.189% (125/201)\n",
            "Epoch 37 iteration 300 Loss: 0.689 | Acc: 62.458% (188/301)\n",
            "Epoch 37 iteration 400 Loss: 0.666 | Acc: 63.591% (255/401)\n",
            "Epoch 37 iteration 500 Loss: 0.652 | Acc: 64.271% (322/501)\n",
            "Test accuracy: 0.6427145600318909\n",
            "Epoch: 38, Loss: 0.0025593172758817673 Updates: 1/1, Avg Grad: 0.00120569556020200250\n",
            "Epoch: 38, Loss: 0.00168634916190058 Updates: 6/1001, Avg Grad: 0.0135549427941441540\n",
            "Epoch: 38, Loss: 0.0008744993829168379 Updates: 11/2001, Avg Grad: 0.011313046328723430\n",
            "Epoch: 38, Loss: 0.002966645872220397 Updates: 16/3001, Avg Grad: 0.013591077178716660\n",
            "Epoch: 38, Loss: 0.0007879480253905058 Updates: 21/4001, Avg Grad: 0.0234522577375173570\n",
            "Epoch: 38, Loss: 0.001468396047130227 Updates: 26/5001, Avg Grad: 0.0233585070818662640\n",
            "Epoch: 38, Loss: 0.00030722166411578655 Updates: 31/6001, Avg Grad: 0.041034959256649020\n",
            "Epoch: 38, Loss: 0.0013302215375006199 Updates: 36/7001, Avg Grad: 0.0343972742557525630\n",
            "Epoch: 38, Loss: 0.00033894687658175826 Updates: 41/8001, Avg Grad: 0.0189964361488819120\n",
            "Epoch: 38, Loss: 0.0025433721020817757 Updates: 46/9001, Avg Grad: 0.0275114588439464570\n",
            "Epoch 37 iteration 0 Loss: 0.394 | Acc: 100.000% (1/1)\n",
            "Epoch 37 iteration 100 Loss: 0.546 | Acc: 72.277% (73/101)\n",
            "Epoch 37 iteration 200 Loss: 0.530 | Acc: 73.632% (148/201)\n",
            "Epoch 37 iteration 300 Loss: 0.552 | Acc: 70.764% (213/301)\n",
            "Epoch 37 iteration 400 Loss: 0.556 | Acc: 72.070% (289/401)\n",
            "Epoch 37 iteration 500 Loss: 0.558 | Acc: 72.455% (363/501)\n",
            "Test accuracy: 0.7245509028434753\n",
            "Epoch: 38, Loss: 0.0005322522483766079 Updates: 51/10001, Avg Grad: 0.037721596658229830\n",
            "Epoch: 38, Loss: 0.00268258573487401 Updates: 56/11001, Avg Grad: 0.0272284690290689470\n",
            "Epoch: 38, Loss: 0.008124928921461105 Updates: 61/12001, Avg Grad: 0.039301533252000810\n",
            "Epoch: 38, Loss: 0.0017365901730954647 Updates: 66/13001, Avg Grad: 0.008810376748442650\n",
            "Epoch: 38, Loss: 0.0002863625413738191 Updates: 71/14001, Avg Grad: 0.03346493840217590\n",
            "Epoch: 38, Loss: 0.005940109957009554 Updates: 76/15001, Avg Grad: 0.049262218177318570\n",
            "Epoch: 38, Loss: 0.004613425582647324 Updates: 81/16001, Avg Grad: 0.0182970874011516570\n",
            "Epoch: 38, Loss: 0.002658738987520337 Updates: 86/17001, Avg Grad: 0.0465092621743679050\n",
            "Epoch: 38, Loss: 0.003534309333190322 Updates: 91/18001, Avg Grad: 0.0244549028575420380\n",
            "Epoch: 38, Loss: 0.0033646393567323685 Updates: 96/19001, Avg Grad: 0.055242456495761870\n",
            "Epoch 37 iteration 0 Loss: 0.940 | Acc: 0.000% (0/1)\n",
            "Epoch 37 iteration 100 Loss: 0.610 | Acc: 68.317% (69/101)\n",
            "Epoch 37 iteration 200 Loss: 0.621 | Acc: 67.662% (136/201)\n",
            "Epoch 37 iteration 300 Loss: 0.605 | Acc: 69.435% (209/301)\n",
            "Epoch 37 iteration 400 Loss: 0.592 | Acc: 70.324% (282/401)\n",
            "Epoch 37 iteration 500 Loss: 0.579 | Acc: 72.056% (361/501)\n",
            "Test accuracy: 0.7205588817596436\n",
            "Epoch: 38, Loss: 0.0021044700406491756 Updates: 101/20001, Avg Grad: 0.0143335452303290370\n",
            "Epoch: 38, Loss: 0.00160382897593081 Updates: 106/21001, Avg Grad: 0.0203424207866191860\n",
            "Epoch: 38, Loss: 0.0029019592329859734 Updates: 111/22001, Avg Grad: 0.0109855700284242630\n",
            "Epoch: 38, Loss: 0.0074782585725188255 Updates: 116/23001, Avg Grad: 0.038354564458131790\n",
            "Epoch: 38, Loss: 0.0017379873897880316 Updates: 121/24001, Avg Grad: 0.0249050036072731020\n",
            "Epoch: 38, Loss: 0.003935821820050478 Updates: 126/25001, Avg Grad: 0.031408593058586120\n",
            "Epoch: 38, Loss: 0.0002356467448407784 Updates: 131/26001, Avg Grad: 0.033660840243101120\n",
            "Epoch: 38, Loss: 0.003773944452404976 Updates: 136/27001, Avg Grad: 0.033603213727474210\n",
            "Epoch: 38, Loss: 0.0004923504893667996 Updates: 141/28001, Avg Grad: 0.0351146049797534940\n",
            "Epoch: 38, Loss: 0.0005592688685283065 Updates: 146/29001, Avg Grad: 0.0376400277018547060\n",
            "Epoch 37 iteration 0 Loss: 0.578 | Acc: 100.000% (1/1)\n",
            "Epoch 37 iteration 100 Loss: 0.673 | Acc: 66.337% (67/101)\n",
            "Epoch 37 iteration 200 Loss: 0.599 | Acc: 68.159% (137/201)\n",
            "Epoch 37 iteration 300 Loss: 0.573 | Acc: 70.432% (212/301)\n",
            "Epoch 37 iteration 400 Loss: 0.574 | Acc: 69.825% (280/401)\n",
            "Epoch 37 iteration 500 Loss: 0.577 | Acc: 69.661% (349/501)\n",
            "Test accuracy: 0.6966068148612976\n",
            "Epoch: 38, Loss: 0.0024318224750459194 Updates: 151/30001, Avg Grad: 0.046704143285751340\n",
            "Epoch: 38, Loss: 0.004351775161921978 Updates: 156/31001, Avg Grad: 0.0207517687231302260\n",
            "Epoch: 38, Loss: 0.0012629415141418576 Updates: 161/32001, Avg Grad: 0.0281367842108011250\n",
            "Epoch 38 iteration 0 Loss: 0.240 | Acc: 100.000% (1/1)\n",
            "Epoch 38 iteration 100 Loss: 0.613 | Acc: 73.267% (74/101)\n",
            "Epoch 38 iteration 200 Loss: 0.625 | Acc: 69.154% (139/201)\n",
            "Epoch 38 iteration 300 Loss: 0.586 | Acc: 70.100% (211/301)\n",
            "Epoch 38 iteration 400 Loss: 0.609 | Acc: 68.080% (273/401)\n",
            "Epoch 38 iteration 500 Loss: 0.601 | Acc: 69.461% (348/501)\n",
            "Test accuracy: 0.6946107745170593\n",
            "Epoch: 39, Loss: 0.005283462814986706 Updates: 1/1, Avg Grad: 0.0019543748348951340\n",
            "Epoch: 39, Loss: 0.0006143089267425239 Updates: 6/1001, Avg Grad: 0.0516458302736282350\n",
            "Epoch: 39, Loss: 0.0017173450905829668 Updates: 11/2001, Avg Grad: 0.0531994923949241640\n",
            "Epoch: 39, Loss: 0.0031782761216163635 Updates: 16/3001, Avg Grad: 0.017030233517289160\n",
            "Epoch: 39, Loss: 0.0002830264566000551 Updates: 21/4001, Avg Grad: 0.05643233284354210\n",
            "Epoch: 39, Loss: 0.0010097792837768793 Updates: 26/5001, Avg Grad: 0.0225166790187358860\n",
            "Epoch: 39, Loss: 0.0010092586744576693 Updates: 31/6001, Avg Grad: 0.0219631809741258620\n",
            "Epoch: 39, Loss: 0.001991590717807412 Updates: 36/7001, Avg Grad: 0.0202395655214786530\n",
            "Epoch: 39, Loss: 0.0015793874626979232 Updates: 41/8001, Avg Grad: 0.0425629988312721250\n",
            "Epoch: 39, Loss: 0.0008875688654370606 Updates: 46/9001, Avg Grad: 0.020438974723219870\n",
            "Epoch 38 iteration 0 Loss: 0.422 | Acc: 100.000% (1/1)\n",
            "Epoch 38 iteration 100 Loss: 0.588 | Acc: 70.297% (71/101)\n",
            "Epoch 38 iteration 200 Loss: 0.597 | Acc: 68.159% (137/201)\n",
            "Epoch 38 iteration 300 Loss: 0.602 | Acc: 67.110% (202/301)\n",
            "Epoch 38 iteration 400 Loss: 0.582 | Acc: 68.828% (276/401)\n",
            "Epoch 38 iteration 500 Loss: 0.589 | Acc: 68.263% (342/501)\n",
            "Test accuracy: 0.682634711265564\n",
            "Epoch: 39, Loss: 0.004755094647407532 Updates: 51/10001, Avg Grad: 0.025433836504817010\n",
            "Epoch: 39, Loss: 0.0019629301968961954 Updates: 56/11001, Avg Grad: 0.043014839291572570\n",
            "Epoch: 39, Loss: 0.0011960737174376845 Updates: 61/12001, Avg Grad: 0.077003285288810730\n",
            "Epoch: 39, Loss: 0.006868891883641481 Updates: 66/13001, Avg Grad: 0.018291270360350610\n",
            "Epoch: 39, Loss: 0.0004928144626319408 Updates: 71/14001, Avg Grad: 0.036139424890279770\n",
            "Epoch: 39, Loss: 0.000735458277631551 Updates: 76/15001, Avg Grad: 0.040308907628059390\n",
            "Epoch: 39, Loss: 0.002316921716555953 Updates: 81/16001, Avg Grad: 0.0223479866981506350\n",
            "Epoch: 39, Loss: 0.006471404805779457 Updates: 86/17001, Avg Grad: 0.036106217652559280\n",
            "Epoch: 39, Loss: 0.0011223837500438094 Updates: 91/18001, Avg Grad: 0.012198507785797120\n",
            "Epoch: 39, Loss: 0.0024988611694425344 Updates: 96/19001, Avg Grad: 0.04189627245068550\n",
            "Epoch 38 iteration 0 Loss: 1.019 | Acc: 0.000% (0/1)\n",
            "Epoch 38 iteration 100 Loss: 0.657 | Acc: 63.366% (64/101)\n",
            "Epoch 38 iteration 200 Loss: 0.637 | Acc: 65.174% (131/201)\n",
            "Epoch 38 iteration 300 Loss: 0.606 | Acc: 68.771% (207/301)\n",
            "Epoch 38 iteration 400 Loss: 0.594 | Acc: 69.825% (280/401)\n",
            "Epoch 38 iteration 500 Loss: 0.576 | Acc: 71.856% (360/501)\n",
            "Test accuracy: 0.71856290102005\n",
            "Epoch: 39, Loss: 0.0021565884817391634 Updates: 101/20001, Avg Grad: 0.024915643036365510\n",
            "Epoch: 39, Loss: 0.0029418435879051685 Updates: 106/21001, Avg Grad: 0.0220628138631582260\n",
            "Epoch: 39, Loss: 0.001015921588987112 Updates: 111/22001, Avg Grad: 0.0345077142119407650\n",
            "Epoch: 39, Loss: 0.003950722515583038 Updates: 116/23001, Avg Grad: 0.0187558084726333620\n",
            "Epoch: 39, Loss: 0.01007971540093422 Updates: 121/24001, Avg Grad: 0.0380511656403541560\n",
            "Epoch: 39, Loss: 0.0013988554710522294 Updates: 126/25001, Avg Grad: 0.041670419275760650\n",
            "Epoch: 39, Loss: 0.004166601691395044 Updates: 131/26001, Avg Grad: 0.0357025265693664550\n",
            "Epoch: 39, Loss: 0.0005683796480298042 Updates: 136/27001, Avg Grad: 0.0243437904864549640\n",
            "Epoch: 39, Loss: 0.002550846664234996 Updates: 141/28001, Avg Grad: 0.008869038894772530\n",
            "Epoch: 39, Loss: 0.0049163042567670345 Updates: 146/29001, Avg Grad: 0.0302036665380001070\n",
            "Epoch 38 iteration 0 Loss: 0.886 | Acc: 0.000% (0/1)\n",
            "Epoch 38 iteration 100 Loss: 0.587 | Acc: 72.277% (73/101)\n",
            "Epoch 38 iteration 200 Loss: 0.569 | Acc: 72.139% (145/201)\n",
            "Epoch 38 iteration 300 Loss: 0.571 | Acc: 72.757% (219/301)\n",
            "Epoch 38 iteration 400 Loss: 0.565 | Acc: 72.070% (289/401)\n",
            "Epoch 38 iteration 500 Loss: 0.573 | Acc: 71.457% (358/501)\n",
            "Test accuracy: 0.7145708799362183\n",
            "Epoch: 39, Loss: 0.0014083200367167592 Updates: 151/30001, Avg Grad: 0.0226928573101758960\n",
            "Epoch: 39, Loss: 0.0008040323737077415 Updates: 156/31001, Avg Grad: 0.034778628498315810\n",
            "Epoch: 39, Loss: 0.004946708679199219 Updates: 161/32001, Avg Grad: 0.046574305742979050\n",
            "Epoch 39 iteration 0 Loss: 0.058 | Acc: 100.000% (1/1)\n",
            "Epoch 39 iteration 100 Loss: 0.729 | Acc: 63.366% (64/101)\n",
            "Epoch 39 iteration 200 Loss: 0.699 | Acc: 64.179% (129/201)\n",
            "Epoch 39 iteration 300 Loss: 0.709 | Acc: 65.781% (198/301)\n",
            "Epoch 39 iteration 400 Loss: 0.657 | Acc: 68.080% (273/401)\n",
            "Epoch 39 iteration 500 Loss: 0.661 | Acc: 67.066% (336/501)\n",
            "Test accuracy: 0.6706587076187134\n",
            "Epoch: 40, Loss: 0.0009268588619306684 Updates: 1/1, Avg Grad: 0.00050908932462334630\n",
            "Epoch: 40, Loss: 0.0013156054774299264 Updates: 6/1001, Avg Grad: 0.039951171725988390\n",
            "Epoch: 40, Loss: 0.00020321019110269845 Updates: 11/2001, Avg Grad: 0.041299406439065930\n",
            "Epoch: 40, Loss: 0.0003954230633098632 Updates: 16/3001, Avg Grad: 0.0425715744495391850\n",
            "Epoch: 40, Loss: 0.001010886742733419 Updates: 21/4001, Avg Grad: 0.016097757965326310\n",
            "Epoch: 40, Loss: 0.0013112809974700212 Updates: 26/5001, Avg Grad: 0.0114782545715570450\n",
            "Epoch: 40, Loss: 0.006859802175313234 Updates: 31/6001, Avg Grad: 0.0218367222696542740\n",
            "Epoch: 40, Loss: 0.0024886580649763346 Updates: 36/7001, Avg Grad: 0.0345656238496303560\n",
            "Epoch: 40, Loss: 0.00039044092409312725 Updates: 41/8001, Avg Grad: 0.0309368744492530820\n",
            "Epoch: 40, Loss: 0.0018398393876850605 Updates: 46/9001, Avg Grad: 0.020578270778059960\n",
            "Epoch 39 iteration 0 Loss: 1.316 | Acc: 0.000% (0/1)\n",
            "Epoch 39 iteration 100 Loss: 0.539 | Acc: 71.287% (72/101)\n",
            "Epoch 39 iteration 200 Loss: 0.527 | Acc: 71.144% (143/201)\n",
            "Epoch 39 iteration 300 Loss: 0.544 | Acc: 71.096% (214/301)\n",
            "Epoch 39 iteration 400 Loss: 0.538 | Acc: 71.571% (287/401)\n",
            "Epoch 39 iteration 500 Loss: 0.540 | Acc: 71.856% (360/501)\n",
            "Test accuracy: 0.71856290102005\n",
            "Epoch: 40, Loss: 0.000954928167629987 Updates: 51/10001, Avg Grad: 0.0141527904197573660\n",
            "Epoch: 40, Loss: 0.002622621599584818 Updates: 56/11001, Avg Grad: 0.0118830678984522820\n",
            "Epoch: 40, Loss: 0.0007770006195642054 Updates: 61/12001, Avg Grad: 0.043921124190092090\n",
            "Epoch: 40, Loss: 0.0005906722508370876 Updates: 66/13001, Avg Grad: 0.079001203179359440\n",
            "Epoch: 40, Loss: 0.0031470933463424444 Updates: 71/14001, Avg Grad: 0.033704530447721480\n",
            "Epoch: 40, Loss: 0.0001566272258060053 Updates: 76/15001, Avg Grad: 0.011703079566359520\n",
            "Epoch: 40, Loss: 0.0011945456499233842 Updates: 81/16001, Avg Grad: 0.029397513717412950\n",
            "Epoch: 40, Loss: 0.002011156640946865 Updates: 86/17001, Avg Grad: 0.0252310223877429960\n",
            "Epoch: 40, Loss: 0.006754433736205101 Updates: 91/18001, Avg Grad: 0.0162769313901662830\n",
            "Epoch: 40, Loss: 0.0030737805645912886 Updates: 96/19001, Avg Grad: 0.055931497365236280\n",
            "Epoch 39 iteration 0 Loss: 0.855 | Acc: 0.000% (0/1)\n",
            "Epoch 39 iteration 100 Loss: 0.610 | Acc: 70.297% (71/101)\n",
            "Epoch 39 iteration 200 Loss: 0.597 | Acc: 69.652% (140/201)\n",
            "Epoch 39 iteration 300 Loss: 0.603 | Acc: 70.100% (211/301)\n",
            "Epoch 39 iteration 400 Loss: 0.609 | Acc: 67.830% (272/401)\n",
            "Epoch 39 iteration 500 Loss: 0.599 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 40, Loss: 0.0011604499304667115 Updates: 101/20001, Avg Grad: 0.0131883379071950910\n",
            "Epoch: 40, Loss: 0.0009458268177695572 Updates: 106/21001, Avg Grad: 0.0411428436636924740\n",
            "Epoch: 40, Loss: 0.0006458887364715338 Updates: 111/22001, Avg Grad: 0.038658086210489270\n",
            "Epoch: 40, Loss: 0.0024538577999919653 Updates: 116/23001, Avg Grad: 0.030394854024052620\n",
            "Epoch: 40, Loss: 0.0021037994883954525 Updates: 121/24001, Avg Grad: 0.012798019684851170\n",
            "Epoch: 40, Loss: 0.007724372670054436 Updates: 126/25001, Avg Grad: 0.051590133458375930\n",
            "Epoch: 40, Loss: 0.0012425542809069157 Updates: 131/26001, Avg Grad: 0.0139807201921939850\n",
            "Epoch: 40, Loss: 0.0008061000844463706 Updates: 136/27001, Avg Grad: 0.0277416799217462540\n",
            "Epoch: 40, Loss: 0.008282582275569439 Updates: 141/28001, Avg Grad: 0.0114666102454066280\n",
            "Epoch: 40, Loss: 0.004132392350584269 Updates: 146/29001, Avg Grad: 0.0209906604140996930\n",
            "Epoch 39 iteration 0 Loss: 0.103 | Acc: 100.000% (1/1)\n",
            "Epoch 39 iteration 100 Loss: 0.703 | Acc: 64.356% (65/101)\n",
            "Epoch 39 iteration 200 Loss: 0.620 | Acc: 70.149% (141/201)\n",
            "Epoch 39 iteration 300 Loss: 0.591 | Acc: 72.425% (218/301)\n",
            "Epoch 39 iteration 400 Loss: 0.586 | Acc: 72.569% (291/401)\n",
            "Epoch 39 iteration 500 Loss: 0.589 | Acc: 71.257% (357/501)\n",
            "Test accuracy: 0.71257483959198\n",
            "Epoch: 40, Loss: 0.004899525083601475 Updates: 151/30001, Avg Grad: 0.036119107156991960\n",
            "Epoch: 40, Loss: 0.0033565685153007507 Updates: 156/31001, Avg Grad: 0.0139156617224216460\n",
            "Epoch: 40, Loss: 0.0009653379675000906 Updates: 161/32001, Avg Grad: 0.020521979779005050\n",
            "Epoch 40 iteration 0 Loss: 0.083 | Acc: 100.000% (1/1)\n",
            "Epoch 40 iteration 100 Loss: 0.832 | Acc: 64.356% (65/101)\n",
            "Epoch 40 iteration 200 Loss: 0.832 | Acc: 63.682% (128/201)\n",
            "Epoch 40 iteration 300 Loss: 0.887 | Acc: 61.130% (184/301)\n",
            "Epoch 40 iteration 400 Loss: 0.878 | Acc: 61.347% (246/401)\n",
            "Epoch 40 iteration 500 Loss: 0.851 | Acc: 61.078% (306/501)\n",
            "Test accuracy: 0.6107784509658813\n",
            "Epoch: 41, Loss: 0.001522097853012383 Updates: 1/1, Avg Grad: 0.00073250551940873270\n",
            "Epoch: 41, Loss: 0.0007135780178941786 Updates: 6/1001, Avg Grad: 0.0210201330482959750\n",
            "Epoch: 41, Loss: 0.0004893652512691915 Updates: 11/2001, Avg Grad: 0.0369790568947792050\n",
            "Epoch: 41, Loss: 0.000816567859146744 Updates: 16/3001, Avg Grad: 0.0600580982863903050\n",
            "Epoch: 41, Loss: 0.002216269727796316 Updates: 21/4001, Avg Grad: 0.0133588481694459920\n",
            "Epoch: 41, Loss: 0.001828750129789114 Updates: 26/5001, Avg Grad: 0.0145689807832241060\n",
            "Epoch: 41, Loss: 0.0009852494113147259 Updates: 31/6001, Avg Grad: 0.0252615362405776980\n",
            "Epoch: 41, Loss: 0.002948590787127614 Updates: 36/7001, Avg Grad: 0.0282835550606250760\n",
            "Epoch: 41, Loss: 0.0008414254989475012 Updates: 41/8001, Avg Grad: 0.0339842401444911960\n",
            "Epoch: 41, Loss: 0.0065999929793179035 Updates: 46/9001, Avg Grad: 0.0257152672857046130\n",
            "Epoch 40 iteration 0 Loss: 0.151 | Acc: 100.000% (1/1)\n",
            "Epoch 40 iteration 100 Loss: 0.540 | Acc: 75.248% (76/101)\n",
            "Epoch 40 iteration 200 Loss: 0.572 | Acc: 73.134% (147/201)\n",
            "Epoch 40 iteration 300 Loss: 0.591 | Acc: 71.096% (214/301)\n",
            "Epoch 40 iteration 400 Loss: 0.599 | Acc: 71.322% (286/401)\n",
            "Epoch 40 iteration 500 Loss: 0.598 | Acc: 70.858% (355/501)\n",
            "Test accuracy: 0.7085828185081482\n",
            "Epoch: 41, Loss: 0.0018357501830905676 Updates: 51/10001, Avg Grad: 0.018127733841538430\n",
            "Epoch: 41, Loss: 0.0006577412132173777 Updates: 56/11001, Avg Grad: 0.054013233631849290\n",
            "Epoch: 41, Loss: 0.0017776885069906712 Updates: 61/12001, Avg Grad: 0.0151026332750916480\n",
            "Epoch: 41, Loss: 0.008263451047241688 Updates: 66/13001, Avg Grad: 0.0104884691536426540\n",
            "Epoch: 41, Loss: 0.0007005410152487457 Updates: 71/14001, Avg Grad: 0.0482556931674480440\n",
            "Epoch: 41, Loss: 0.005751432850956917 Updates: 76/15001, Avg Grad: 0.034377101808786390\n",
            "Epoch: 41, Loss: 0.0006134032155387104 Updates: 81/16001, Avg Grad: 0.0121630253270268440\n",
            "Epoch: 41, Loss: 0.0014893561601638794 Updates: 86/17001, Avg Grad: 0.0324009880423545840\n",
            "Epoch: 41, Loss: 0.001197082456201315 Updates: 91/18001, Avg Grad: 0.041160564869642260\n",
            "Epoch: 41, Loss: 0.0021409790497273207 Updates: 96/19001, Avg Grad: 0.0425935164093971250\n",
            "Epoch 40 iteration 0 Loss: 0.070 | Acc: 100.000% (1/1)\n",
            "Epoch 40 iteration 100 Loss: 0.670 | Acc: 68.317% (69/101)\n",
            "Epoch 40 iteration 200 Loss: 0.641 | Acc: 69.154% (139/201)\n",
            "Epoch 40 iteration 300 Loss: 0.656 | Acc: 68.106% (205/301)\n",
            "Epoch 40 iteration 400 Loss: 0.644 | Acc: 67.581% (271/401)\n",
            "Epoch 40 iteration 500 Loss: 0.645 | Acc: 66.267% (332/501)\n",
            "Test accuracy: 0.6626746654510498\n",
            "Epoch: 41, Loss: 0.002983080456033349 Updates: 101/20001, Avg Grad: 0.0130009083077311520\n",
            "Epoch: 41, Loss: 0.0034389537759125233 Updates: 106/21001, Avg Grad: 0.0220867712050676350\n",
            "Epoch: 41, Loss: 0.005434858612716198 Updates: 111/22001, Avg Grad: 0.0364861749112606050\n",
            "Epoch: 41, Loss: 0.0006587737007066607 Updates: 116/23001, Avg Grad: 0.0166820902377367020\n",
            "Epoch: 41, Loss: 0.00045025537838228047 Updates: 121/24001, Avg Grad: 0.0219290424138307570\n",
            "Epoch: 41, Loss: 0.0007814690470695496 Updates: 126/25001, Avg Grad: 0.0225225687026977540\n",
            "Epoch: 41, Loss: 0.0009004275780171156 Updates: 131/26001, Avg Grad: 0.010088747367262840\n",
            "Epoch: 41, Loss: 0.0011639982694759965 Updates: 136/27001, Avg Grad: 0.0258269123733043670\n",
            "Epoch: 41, Loss: 0.0015535883139818907 Updates: 141/28001, Avg Grad: 0.048272319138050080\n",
            "Epoch: 41, Loss: 0.0020892133470624685 Updates: 146/29001, Avg Grad: 0.031428597867488860\n",
            "Epoch 40 iteration 0 Loss: 0.107 | Acc: 100.000% (1/1)\n",
            "Epoch 40 iteration 100 Loss: 0.602 | Acc: 72.277% (73/101)\n",
            "Epoch 40 iteration 200 Loss: 0.657 | Acc: 69.154% (139/201)\n",
            "Epoch 40 iteration 300 Loss: 0.659 | Acc: 67.774% (204/301)\n",
            "Epoch 40 iteration 400 Loss: 0.666 | Acc: 66.334% (266/401)\n",
            "Epoch 40 iteration 500 Loss: 0.652 | Acc: 66.267% (332/501)\n",
            "Test accuracy: 0.6626746654510498\n",
            "Epoch: 41, Loss: 0.001590412575751543 Updates: 151/30001, Avg Grad: 0.0315246582031250\n",
            "Epoch: 41, Loss: 0.0006172473658807576 Updates: 156/31001, Avg Grad: 0.047697447240352630\n",
            "Epoch: 41, Loss: 0.001103179412893951 Updates: 161/32001, Avg Grad: 0.0166980884969234470\n",
            "Epoch 41 iteration 0 Loss: 0.332 | Acc: 100.000% (1/1)\n",
            "Epoch 41 iteration 100 Loss: 0.702 | Acc: 60.396% (61/101)\n",
            "Epoch 41 iteration 200 Loss: 0.603 | Acc: 67.164% (135/201)\n",
            "Epoch 41 iteration 300 Loss: 0.579 | Acc: 70.764% (213/301)\n",
            "Epoch 41 iteration 400 Loss: 0.578 | Acc: 70.324% (282/401)\n",
            "Epoch 41 iteration 500 Loss: 0.572 | Acc: 70.858% (355/501)\n",
            "Test accuracy: 0.7085828185081482\n",
            "Epoch: 42, Loss: 0.0014126033056527376 Updates: 1/1, Avg Grad: 0.00079394073691219090\n",
            "Epoch: 42, Loss: 0.00183088646735996 Updates: 6/1001, Avg Grad: 0.017401460558176040\n",
            "Epoch: 42, Loss: 0.0034853722900152206 Updates: 11/2001, Avg Grad: 0.0120299514383077620\n",
            "Epoch: 42, Loss: 0.004694273695349693 Updates: 16/3001, Avg Grad: 0.038059715181589130\n",
            "Epoch: 42, Loss: 0.0005154742975719273 Updates: 21/4001, Avg Grad: 0.034670412540435790\n",
            "Epoch: 42, Loss: 0.004991901107132435 Updates: 26/5001, Avg Grad: 0.0119704985991120340\n",
            "Epoch: 42, Loss: 0.0008997514960356057 Updates: 31/6001, Avg Grad: 0.0491214469075202940\n",
            "Epoch: 42, Loss: 0.006061675492674112 Updates: 36/7001, Avg Grad: 0.0268643517047166820\n",
            "Epoch: 42, Loss: 0.002797828521579504 Updates: 41/8001, Avg Grad: 0.049680728465318680\n",
            "Epoch: 42, Loss: 0.0036249083932489157 Updates: 46/9001, Avg Grad: 0.0462634749710559840\n",
            "Epoch 41 iteration 0 Loss: 0.218 | Acc: 100.000% (1/1)\n",
            "Epoch 41 iteration 100 Loss: 0.603 | Acc: 70.297% (71/101)\n",
            "Epoch 41 iteration 200 Loss: 0.577 | Acc: 73.134% (147/201)\n",
            "Epoch 41 iteration 300 Loss: 0.601 | Acc: 71.761% (216/301)\n",
            "Epoch 41 iteration 400 Loss: 0.608 | Acc: 70.075% (281/401)\n",
            "Epoch 41 iteration 500 Loss: 0.620 | Acc: 69.261% (347/501)\n",
            "Test accuracy: 0.6926147937774658\n",
            "Epoch: 42, Loss: 0.001707899384200573 Updates: 51/10001, Avg Grad: 0.035052008926868440\n",
            "Epoch: 42, Loss: 0.0006918673170730472 Updates: 56/11001, Avg Grad: 0.015127210877835750\n",
            "Epoch: 42, Loss: 0.004120803438127041 Updates: 61/12001, Avg Grad: 0.038637991994619370\n",
            "Epoch: 42, Loss: 0.0008340884232893586 Updates: 66/13001, Avg Grad: 0.0142533257603645320\n",
            "Epoch: 42, Loss: 0.000600947707425803 Updates: 71/14001, Avg Grad: 0.02110859565436840\n",
            "Epoch: 42, Loss: 0.004221292212605476 Updates: 76/15001, Avg Grad: 0.0189487952739000320\n",
            "Epoch: 42, Loss: 0.0025344546884298325 Updates: 81/16001, Avg Grad: 0.0486375726759433750\n",
            "Epoch: 42, Loss: 0.001659153145737946 Updates: 86/17001, Avg Grad: 0.049370918422937390\n",
            "Epoch: 42, Loss: 0.0011572971707209945 Updates: 91/18001, Avg Grad: 0.068938136100769040\n",
            "Epoch: 42, Loss: 0.003094926243647933 Updates: 96/19001, Avg Grad: 0.025334944948554040\n",
            "Epoch 41 iteration 0 Loss: 1.730 | Acc: 0.000% (0/1)\n",
            "Epoch 41 iteration 100 Loss: 0.582 | Acc: 65.347% (66/101)\n",
            "Epoch 41 iteration 200 Loss: 0.591 | Acc: 64.677% (130/201)\n",
            "Epoch 41 iteration 300 Loss: 0.576 | Acc: 67.110% (202/301)\n",
            "Epoch 41 iteration 400 Loss: 0.585 | Acc: 66.334% (266/401)\n",
            "Epoch 41 iteration 500 Loss: 0.590 | Acc: 66.068% (331/501)\n",
            "Test accuracy: 0.6606786251068115\n",
            "Epoch: 42, Loss: 0.0014934029895812273 Updates: 101/20001, Avg Grad: 0.0101424464955925940\n",
            "Epoch: 42, Loss: 0.0010427492670714855 Updates: 106/21001, Avg Grad: 0.045215748250484470\n",
            "Epoch: 42, Loss: 0.0011682136682793498 Updates: 111/22001, Avg Grad: 0.0097039751708507540\n",
            "Epoch: 42, Loss: 0.001673588645644486 Updates: 116/23001, Avg Grad: 0.0210682135075330730\n",
            "Epoch: 42, Loss: 0.001382378744892776 Updates: 121/24001, Avg Grad: 0.0174652989953756330\n",
            "Epoch: 42, Loss: 0.0014133815420791507 Updates: 126/25001, Avg Grad: 0.016563877463340760\n",
            "Epoch: 42, Loss: 0.0029341010376811028 Updates: 131/26001, Avg Grad: 0.02626061439514160\n",
            "Epoch: 42, Loss: 0.0015194842126220465 Updates: 136/27001, Avg Grad: 0.0138648329302668570\n",
            "Epoch: 42, Loss: 0.0030111304949969053 Updates: 141/28001, Avg Grad: 0.043709792196750640\n",
            "Epoch: 42, Loss: 0.001365517033264041 Updates: 146/29001, Avg Grad: 0.01744334213435650\n",
            "Epoch 41 iteration 0 Loss: 0.293 | Acc: 100.000% (1/1)\n",
            "Epoch 41 iteration 100 Loss: 0.603 | Acc: 65.347% (66/101)\n",
            "Epoch 41 iteration 200 Loss: 0.616 | Acc: 65.174% (131/201)\n",
            "Epoch 41 iteration 300 Loss: 0.588 | Acc: 67.442% (203/301)\n",
            "Epoch 41 iteration 400 Loss: 0.591 | Acc: 68.080% (273/401)\n",
            "Epoch 41 iteration 500 Loss: 0.599 | Acc: 67.864% (340/501)\n",
            "Test accuracy: 0.6786426901817322\n",
            "Epoch: 42, Loss: 0.005134989507496357 Updates: 151/30001, Avg Grad: 0.03275602310895920\n",
            "Epoch: 42, Loss: 0.0006173885776661336 Updates: 156/31001, Avg Grad: 0.039347831159830090\n",
            "Epoch: 42, Loss: 0.0006927153444848955 Updates: 161/32001, Avg Grad: 0.046651877462863920\n",
            "Epoch 42 iteration 0 Loss: 0.756 | Acc: 0.000% (0/1)\n",
            "Epoch 42 iteration 100 Loss: 0.543 | Acc: 71.287% (72/101)\n",
            "Epoch 42 iteration 200 Loss: 0.606 | Acc: 67.164% (135/201)\n",
            "Epoch 42 iteration 300 Loss: 0.600 | Acc: 68.439% (206/301)\n",
            "Epoch 42 iteration 400 Loss: 0.608 | Acc: 68.080% (273/401)\n",
            "Epoch 42 iteration 500 Loss: 0.587 | Acc: 70.259% (352/501)\n",
            "Test accuracy: 0.7025948166847229\n",
            "Epoch: 43, Loss: 0.007563693914562464 Updates: 1/1, Avg Grad: 0.00224983948282897470\n",
            "Epoch: 43, Loss: 0.001085950993001461 Updates: 6/1001, Avg Grad: 0.0376131460070610050\n",
            "Epoch: 43, Loss: 0.010859671980142593 Updates: 11/2001, Avg Grad: 0.0622214078903198240\n",
            "Epoch: 43, Loss: 0.00034866895293816924 Updates: 16/3001, Avg Grad: 0.064075842499732970\n",
            "Epoch: 43, Loss: 0.0022722845897078514 Updates: 21/4001, Avg Grad: 0.0205816980451345440\n",
            "Epoch: 43, Loss: 0.0014144767774268985 Updates: 26/5001, Avg Grad: 0.020061263814568520\n",
            "Epoch: 43, Loss: 0.007721704430878162 Updates: 31/6001, Avg Grad: 0.0225276965647935870\n",
            "Epoch: 43, Loss: 0.0009209542185999453 Updates: 36/7001, Avg Grad: 0.034007765352725980\n",
            "Epoch: 43, Loss: 0.005077940411865711 Updates: 41/8001, Avg Grad: 0.0246010310947895050\n",
            "Epoch: 43, Loss: 0.003837179858237505 Updates: 46/9001, Avg Grad: 0.053892016410827640\n",
            "Epoch 42 iteration 0 Loss: 0.651 | Acc: 100.000% (1/1)\n",
            "Epoch 42 iteration 100 Loss: 0.574 | Acc: 70.297% (71/101)\n",
            "Epoch 42 iteration 200 Loss: 0.569 | Acc: 71.144% (143/201)\n",
            "Epoch 42 iteration 300 Loss: 0.548 | Acc: 71.761% (216/301)\n",
            "Epoch 42 iteration 400 Loss: 0.549 | Acc: 72.070% (289/401)\n",
            "Epoch 42 iteration 500 Loss: 0.560 | Acc: 72.255% (362/501)\n",
            "Test accuracy: 0.7225548624992371\n",
            "Epoch: 43, Loss: 0.005583843681961298 Updates: 51/10001, Avg Grad: 0.058135308325290680\n",
            "Epoch: 43, Loss: 0.0034351632930338383 Updates: 56/11001, Avg Grad: 0.010095058940351010\n",
            "Epoch: 43, Loss: 0.0022511316929012537 Updates: 61/12001, Avg Grad: 0.057700119912624360\n",
            "Epoch: 43, Loss: 0.0002444660058245063 Updates: 66/13001, Avg Grad: 0.0344993583858013150\n",
            "Epoch: 43, Loss: 0.007940275594592094 Updates: 71/14001, Avg Grad: 0.027219744399189950\n",
            "Epoch: 43, Loss: 0.00032751739490777254 Updates: 76/15001, Avg Grad: 0.07054621726274490\n",
            "Epoch: 43, Loss: 0.0007894828449934721 Updates: 81/16001, Avg Grad: 0.042902935296297070\n",
            "Epoch: 43, Loss: 0.009431778453290462 Updates: 86/17001, Avg Grad: 0.0346792936325073240\n",
            "Epoch: 43, Loss: 0.00040188603452406824 Updates: 91/18001, Avg Grad: 0.024273436516523360\n",
            "Epoch: 43, Loss: 0.0011412196327000856 Updates: 96/19001, Avg Grad: 0.0552361719310283660\n",
            "Epoch 42 iteration 0 Loss: 0.450 | Acc: 100.000% (1/1)\n",
            "Epoch 42 iteration 100 Loss: 0.646 | Acc: 65.347% (66/101)\n",
            "Epoch 42 iteration 200 Loss: 0.593 | Acc: 70.647% (142/201)\n",
            "Epoch 42 iteration 300 Loss: 0.600 | Acc: 68.771% (207/301)\n",
            "Epoch 42 iteration 400 Loss: 0.571 | Acc: 71.072% (285/401)\n",
            "Epoch 42 iteration 500 Loss: 0.572 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 43, Loss: 0.0011548054171726108 Updates: 101/20001, Avg Grad: 0.041478350758552550\n",
            "Epoch: 43, Loss: 0.0012630484998226166 Updates: 106/21001, Avg Grad: 0.0256470795720815660\n",
            "Epoch: 43, Loss: 0.003751736832782626 Updates: 111/22001, Avg Grad: 0.029523311182856560\n",
            "Epoch: 43, Loss: 0.006168629974126816 Updates: 116/23001, Avg Grad: 0.0339895710349082950\n",
            "Epoch: 43, Loss: 0.001809466746635735 Updates: 121/24001, Avg Grad: 0.040511049330234530\n",
            "Epoch: 43, Loss: 0.0006335591315291822 Updates: 126/25001, Avg Grad: 0.0251057129353284840\n",
            "Epoch: 43, Loss: 0.012294339016079903 Updates: 131/26001, Avg Grad: 0.0386321917176246640\n",
            "Epoch: 43, Loss: 0.0007456486928276718 Updates: 136/27001, Avg Grad: 0.0245469529181718830\n",
            "Epoch: 43, Loss: 0.0024547064676880836 Updates: 141/28001, Avg Grad: 0.043615747243165970\n",
            "Epoch: 43, Loss: 0.0011753241997212172 Updates: 146/29001, Avg Grad: 0.0272929761558771130\n",
            "Epoch 42 iteration 0 Loss: 0.953 | Acc: 0.000% (0/1)\n",
            "Epoch 42 iteration 100 Loss: 0.578 | Acc: 69.307% (70/101)\n",
            "Epoch 42 iteration 200 Loss: 0.629 | Acc: 66.169% (133/201)\n",
            "Epoch 42 iteration 300 Loss: 0.614 | Acc: 67.110% (202/301)\n",
            "Epoch 42 iteration 400 Loss: 0.606 | Acc: 68.828% (276/401)\n",
            "Epoch 42 iteration 500 Loss: 0.594 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 43, Loss: 0.00030368013540282845 Updates: 151/30001, Avg Grad: 0.0101393172517418860\n",
            "Epoch: 43, Loss: 0.010239550843834877 Updates: 156/31001, Avg Grad: 0.0428844504058361050\n",
            "Epoch: 43, Loss: 0.0025000539608299732 Updates: 161/32001, Avg Grad: 0.0144602153450250630\n",
            "Epoch 43 iteration 0 Loss: 0.079 | Acc: 100.000% (1/1)\n",
            "Epoch 43 iteration 100 Loss: 0.647 | Acc: 64.356% (65/101)\n",
            "Epoch 43 iteration 200 Loss: 0.627 | Acc: 64.179% (129/201)\n",
            "Epoch 43 iteration 300 Loss: 0.649 | Acc: 65.781% (198/301)\n",
            "Epoch 43 iteration 400 Loss: 0.656 | Acc: 66.334% (266/401)\n",
            "Epoch 43 iteration 500 Loss: 0.650 | Acc: 66.068% (331/501)\n",
            "Test accuracy: 0.6606786251068115\n",
            "Epoch: 44, Loss: 0.0011716552544385195 Updates: 1/1, Avg Grad: 0.00050926191033795480\n",
            "Epoch: 44, Loss: 0.0013402076438069344 Updates: 6/1001, Avg Grad: 0.052451606839895250\n",
            "Epoch: 44, Loss: 0.0006059441948309541 Updates: 11/2001, Avg Grad: 0.028885468840599060\n",
            "Epoch: 44, Loss: 0.0012258633505553007 Updates: 16/3001, Avg Grad: 0.0215232092887163160\n",
            "Epoch: 44, Loss: 0.0039561172015964985 Updates: 21/4001, Avg Grad: 0.0387080460786819460\n",
            "Epoch: 44, Loss: 0.0005655592540279031 Updates: 26/5001, Avg Grad: 0.0200707428157329560\n",
            "Epoch: 44, Loss: 0.0025758808478713036 Updates: 31/6001, Avg Grad: 0.068657502532005310\n",
            "Epoch: 44, Loss: 0.0008757236646488309 Updates: 36/7001, Avg Grad: 0.0140553498640656470\n",
            "Epoch: 44, Loss: 0.000511328864376992 Updates: 41/8001, Avg Grad: 0.037029102444648740\n",
            "Epoch: 44, Loss: 0.0029400906059890985 Updates: 46/9001, Avg Grad: 0.024883914738893510\n",
            "Epoch 43 iteration 0 Loss: 0.101 | Acc: 100.000% (1/1)\n",
            "Epoch 43 iteration 100 Loss: 0.589 | Acc: 72.277% (73/101)\n",
            "Epoch 43 iteration 200 Loss: 0.586 | Acc: 70.647% (142/201)\n",
            "Epoch 43 iteration 300 Loss: 0.579 | Acc: 69.767% (210/301)\n",
            "Epoch 43 iteration 400 Loss: 0.614 | Acc: 67.082% (269/401)\n",
            "Epoch 43 iteration 500 Loss: 0.599 | Acc: 68.862% (345/501)\n",
            "Test accuracy: 0.688622772693634\n",
            "Epoch: 44, Loss: 0.008691198192536831 Updates: 51/10001, Avg Grad: 0.0282614044845104220\n",
            "Epoch: 44, Loss: 0.008001960813999176 Updates: 56/11001, Avg Grad: 0.0162516906857490540\n",
            "Epoch: 44, Loss: 0.0019317615078762174 Updates: 61/12001, Avg Grad: 0.0201324634253978730\n",
            "Epoch: 44, Loss: 0.001759781502187252 Updates: 66/13001, Avg Grad: 0.0122492611408233640\n",
            "Epoch: 44, Loss: 0.006013324949890375 Updates: 71/14001, Avg Grad: 0.032942485064268110\n",
            "Epoch: 44, Loss: 0.0013917387695983052 Updates: 76/15001, Avg Grad: 0.038237288594245910\n",
            "Epoch: 44, Loss: 0.0016267979517579079 Updates: 81/16001, Avg Grad: 0.036768045276403430\n",
            "Epoch: 44, Loss: 0.0012650509597733617 Updates: 86/17001, Avg Grad: 0.03630332276225090\n",
            "Epoch: 44, Loss: 0.004726835992187262 Updates: 91/18001, Avg Grad: 0.0257050450891256330\n",
            "Epoch: 44, Loss: 0.000930445792619139 Updates: 96/19001, Avg Grad: 0.032864872366189960\n",
            "Epoch 43 iteration 0 Loss: 1.443 | Acc: 0.000% (0/1)\n",
            "Epoch 43 iteration 100 Loss: 0.657 | Acc: 66.337% (67/101)\n",
            "Epoch 43 iteration 200 Loss: 0.642 | Acc: 64.677% (130/201)\n",
            "Epoch 43 iteration 300 Loss: 0.610 | Acc: 66.777% (201/301)\n",
            "Epoch 43 iteration 400 Loss: 0.618 | Acc: 67.830% (272/401)\n",
            "Epoch 43 iteration 500 Loss: 0.598 | Acc: 68.463% (343/501)\n",
            "Test accuracy: 0.6846307516098022\n",
            "Epoch: 44, Loss: 0.0008309658151119947 Updates: 101/20001, Avg Grad: 0.027023850008845330\n",
            "Epoch: 44, Loss: 0.0012695376062765718 Updates: 106/21001, Avg Grad: 0.0219004601240158080\n",
            "Epoch: 44, Loss: 0.003307412611320615 Updates: 111/22001, Avg Grad: 0.016536092385649680\n",
            "Epoch: 44, Loss: 0.001654137042351067 Updates: 116/23001, Avg Grad: 0.036574911326169970\n",
            "Epoch: 44, Loss: 0.001256633666343987 Updates: 121/24001, Avg Grad: 0.0232133101671934130\n",
            "Epoch: 44, Loss: 0.006144491955637932 Updates: 126/25001, Avg Grad: 0.0234722718596458440\n",
            "Epoch: 44, Loss: 0.0012263394892215729 Updates: 131/26001, Avg Grad: 0.013648706488311290\n",
            "Epoch: 44, Loss: 0.0015000395942479372 Updates: 136/27001, Avg Grad: 0.045248031616210940\n",
            "Epoch: 44, Loss: 0.00531134195625782 Updates: 141/28001, Avg Grad: 0.0124117303639650340\n",
            "Epoch: 44, Loss: 0.001560232019983232 Updates: 146/29001, Avg Grad: 0.0142188500612974170\n",
            "Epoch 43 iteration 0 Loss: 0.280 | Acc: 100.000% (1/1)\n",
            "Epoch 43 iteration 100 Loss: 0.626 | Acc: 68.317% (69/101)\n",
            "Epoch 43 iteration 200 Loss: 0.615 | Acc: 69.652% (140/201)\n",
            "Epoch 43 iteration 300 Loss: 0.597 | Acc: 69.767% (210/301)\n",
            "Epoch 43 iteration 400 Loss: 0.603 | Acc: 69.576% (279/401)\n",
            "Epoch 43 iteration 500 Loss: 0.583 | Acc: 69.062% (346/501)\n",
            "Test accuracy: 0.6906187534332275\n",
            "Epoch: 44, Loss: 0.00036508432822301984 Updates: 151/30001, Avg Grad: 0.03385489434003830\n",
            "Epoch: 44, Loss: 0.005741830915212631 Updates: 156/31001, Avg Grad: 0.013416664674878120\n",
            "Epoch: 44, Loss: 0.006383663974702358 Updates: 161/32001, Avg Grad: 0.0099427206441760060\n",
            "Epoch 44 iteration 0 Loss: 1.306 | Acc: 0.000% (0/1)\n",
            "Epoch 44 iteration 100 Loss: 0.735 | Acc: 58.416% (59/101)\n",
            "Epoch 44 iteration 200 Loss: 0.696 | Acc: 61.194% (123/201)\n",
            "Epoch 44 iteration 300 Loss: 0.707 | Acc: 61.130% (184/301)\n",
            "Epoch 44 iteration 400 Loss: 0.693 | Acc: 62.095% (249/401)\n",
            "Epoch 44 iteration 500 Loss: 0.677 | Acc: 63.872% (320/501)\n",
            "Test accuracy: 0.6387225389480591\n",
            "Epoch: 45, Loss: 0.0005821777158416808 Updates: 1/1, Avg Grad: 0.00036573293618857860\n",
            "Epoch: 45, Loss: 0.00043671776074916124 Updates: 6/1001, Avg Grad: 0.0248733181506395340\n",
            "Epoch: 45, Loss: 0.005123300477862358 Updates: 11/2001, Avg Grad: 0.026691157370805740\n",
            "Epoch: 45, Loss: 0.0011980431154370308 Updates: 16/3001, Avg Grad: 0.028856212273240090\n",
            "Epoch: 45, Loss: 0.0072633433155715466 Updates: 21/4001, Avg Grad: 0.0180548373609781270\n",
            "Epoch: 45, Loss: 0.0012205807724967599 Updates: 26/5001, Avg Grad: 0.0315641276538372040\n",
            "Epoch: 45, Loss: 0.004397967830300331 Updates: 31/6001, Avg Grad: 0.034540250897407530\n",
            "Epoch: 45, Loss: 0.0010424574138596654 Updates: 36/7001, Avg Grad: 0.0189006086438894270\n",
            "Epoch: 45, Loss: 0.00844400841742754 Updates: 41/8001, Avg Grad: 0.022480092942714690\n",
            "Epoch: 45, Loss: 0.003660597139969468 Updates: 46/9001, Avg Grad: 0.027895076200366020\n",
            "Epoch 44 iteration 0 Loss: 1.010 | Acc: 0.000% (0/1)\n",
            "Epoch 44 iteration 100 Loss: 0.602 | Acc: 71.287% (72/101)\n",
            "Epoch 44 iteration 200 Loss: 0.547 | Acc: 73.134% (147/201)\n",
            "Epoch 44 iteration 300 Loss: 0.582 | Acc: 71.429% (215/301)\n",
            "Epoch 44 iteration 400 Loss: 0.586 | Acc: 71.322% (286/401)\n",
            "Epoch 44 iteration 500 Loss: 0.580 | Acc: 71.058% (356/501)\n",
            "Test accuracy: 0.7105788588523865\n",
            "Epoch: 45, Loss: 0.00210576388053596 Updates: 51/10001, Avg Grad: 0.032384544610977170\n",
            "Epoch: 45, Loss: 0.0005022971890866756 Updates: 56/11001, Avg Grad: 0.0299382824450731280\n",
            "Epoch: 45, Loss: 0.003680868074297905 Updates: 61/12001, Avg Grad: 0.0304292347282171250\n",
            "Epoch: 45, Loss: 0.009957501664757729 Updates: 66/13001, Avg Grad: 0.0409872494637966160\n",
            "Epoch: 45, Loss: 0.002245738171041012 Updates: 71/14001, Avg Grad: 0.0213904101401567460\n",
            "Epoch: 45, Loss: 0.005036871414631605 Updates: 76/15001, Avg Grad: 0.0148733286187052730\n",
            "Epoch: 45, Loss: 0.0014276630245149136 Updates: 81/16001, Avg Grad: 0.0148182036355137830\n",
            "Epoch: 45, Loss: 0.0030370017047971487 Updates: 86/17001, Avg Grad: 0.0191777404397726060\n",
            "Epoch: 45, Loss: 0.008115299977362156 Updates: 91/18001, Avg Grad: 0.0176005437970161440\n",
            "Epoch: 45, Loss: 0.0010762264719232917 Updates: 96/19001, Avg Grad: 0.0096427714452147480\n",
            "Epoch 44 iteration 0 Loss: 0.506 | Acc: 100.000% (1/1)\n",
            "Epoch 44 iteration 100 Loss: 0.639 | Acc: 64.356% (65/101)\n",
            "Epoch 44 iteration 200 Loss: 0.610 | Acc: 66.169% (133/201)\n",
            "Epoch 44 iteration 300 Loss: 0.597 | Acc: 69.435% (209/301)\n",
            "Epoch 44 iteration 400 Loss: 0.573 | Acc: 70.823% (284/401)\n",
            "Epoch 44 iteration 500 Loss: 0.582 | Acc: 70.659% (354/501)\n",
            "Test accuracy: 0.7065868377685547\n",
            "Epoch: 45, Loss: 0.0023893124889582396 Updates: 101/20001, Avg Grad: 0.046987511217594150\n",
            "Epoch: 45, Loss: 0.004770219791680574 Updates: 106/21001, Avg Grad: 0.052952233701944350\n",
            "Epoch: 45, Loss: 0.002961379010230303 Updates: 111/22001, Avg Grad: 0.019078304991126060\n",
            "Epoch: 45, Loss: 0.010938630439341068 Updates: 116/23001, Avg Grad: 0.0382336899638175960\n",
            "Epoch: 45, Loss: 0.0009744602721184492 Updates: 121/24001, Avg Grad: 0.0203084424138069150\n",
            "Epoch: 45, Loss: 0.0035183930303901434 Updates: 126/25001, Avg Grad: 0.0322565361857414250\n",
            "Epoch: 45, Loss: 0.0048556155525147915 Updates: 131/26001, Avg Grad: 0.0107885524630546570\n",
            "Epoch: 45, Loss: 0.007309411186724901 Updates: 136/27001, Avg Grad: 0.0265413168817758560\n",
            "Epoch: 45, Loss: 0.002001227345317602 Updates: 141/28001, Avg Grad: 0.0223217010498046880\n",
            "Epoch: 45, Loss: 0.0016444517532363534 Updates: 146/29001, Avg Grad: 0.0233233403414487840\n",
            "Epoch 44 iteration 0 Loss: 0.114 | Acc: 100.000% (1/1)\n",
            "Epoch 44 iteration 100 Loss: 0.485 | Acc: 78.218% (79/101)\n",
            "Epoch 44 iteration 200 Loss: 0.544 | Acc: 69.652% (140/201)\n",
            "Epoch 44 iteration 300 Loss: 0.532 | Acc: 71.096% (214/301)\n",
            "Epoch 44 iteration 400 Loss: 0.545 | Acc: 70.823% (284/401)\n",
            "Epoch 44 iteration 500 Loss: 0.573 | Acc: 68.663% (344/501)\n",
            "Test accuracy: 0.6866267323493958\n",
            "Epoch: 45, Loss: 0.00391705147922039 Updates: 151/30001, Avg Grad: 0.0126266125589609150\n",
            "Epoch: 45, Loss: 0.002226311946287751 Updates: 156/31001, Avg Grad: 0.0220280569046735760\n",
            "Epoch: 45, Loss: 0.0023156339302659035 Updates: 161/32001, Avg Grad: 0.0129562905058264730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-ideality considerations"
      ],
      "metadata": {
        "id": "lpVa-h80_Hz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b"
      ],
      "metadata": {
        "id": "fLC782vt1okJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_net_weight = net.fc1.W\n",
        "org_net_bias = net.fc1.b\n",
        "\n",
        "DAC_res = [4, 5, 6, 7]\n",
        "ADC_res = [6 + 0.2 * i for i in range(16)]\n",
        "device_resolution = [3,4,5,6,7,8]\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=True)\n",
        "\n",
        "err_org = network_tester(net, testloader, 400, 0, False).item()\n",
        "print(err_org)\n",
        "\n",
        "result = []\n",
        "\n",
        "for idx in range(6):\n",
        "    for res in device_resolution:\n",
        "      result_wire = []\n",
        "      for adc in range(1): #adc in range(20)\n",
        "          device_params = {\"Vdd\": 1.8,\n",
        "                  \"r_wl\": 20,\n",
        "                  \"r_bl\": 20,\n",
        "                  \"m\": 200,\n",
        "                  \"n\": 200,\n",
        "                  \"r_on_mean\": 1e4,\n",
        "                  \"r_on_stddev\": 1e3,\n",
        "                  \"r_off_mean\": 1e5,\n",
        "                  \"r_off_stddev\": 1e4,\n",
        "                  \"dac_resolution\": 4,\n",
        "                  \"adc_resolution\": 8.3,\n",
        "                  \"device_resolution\": res,\n",
        "                  \"bias_scheme\": 1/3,\n",
        "                  \"tile_rows\": 4,\n",
        "                  \"tile_cols\": 4,\n",
        "                  \"r_cmos_line\": 600,\n",
        "                  \"r_cmos_transistor\": 20,\n",
        "                  \"p_stuck_on\": 0.01,\n",
        "                  \"p_stuck_off\": 0.01}\n",
        "          crb_new = crossbar(device_params)\n",
        "\n",
        "          net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "          net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "          net.fc1.cb = crb_new\n",
        "          net.fc1.remap()\n",
        "\n",
        "          err = network_tester(net, testloader, 400, 0, False).item()\n",
        "          result_wire.append(err/err_org)\n",
        "\n",
        "      result.append(result_wire)\n",
        "\n",
        "    file_name = 'recordenob' + str(idx) + '.csv'\n",
        "    with open(file_name, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([])\n",
        "\n",
        "    for ele in result:\n",
        "      with open(file_name, 'a') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerow(ele)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbr71yPhJiiA",
        "outputId": "c577136a-f291-4b1d-9fe5-cc9ae0259de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6359102129936218\n",
            "[[0.9921568520854682], [0.9725490291647526], [0.9921568520854682], [0.8549019979092313], [1.0862745333286217], [0.909803939579726], [1.0196078229207157], [0.8039215833303887], [0.8156862583365725], [1.0196078229207157], [0.9333333833233213], [0.8549019979092313], [1.0980392083348056], [1.0627450895850266], [0.7843137604096729], [1.0117647687374114], [0.941176531237853], [1.0078431479145318], [1.031372591658127], [0.9725490291647526], [1.0117647687374114], [1.0078431479145318], [0.8000000562387365], [0.8705882937382947], [0.7882352875013251], [1.1294117999929327], [0.7882352875013251], [1.0666667104079062], [0.8509803770863517], [0.9843137041709366], [0.7921569083242047], [0.9647058812502208], [1.0039216208228796], [1.0392157395726587], [0.9372549104149734], [1.035294118749779]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crossbar update\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 200,\n",
        "                 \"n\": 200,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}\n",
        "crb_new = crossbar(device_params)\n",
        "\n",
        "net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "net.fc1.cb = crb_new\n",
        "net.fc1.remap()"
      ],
      "metadata": {
        "id": "gPPdW4rKbbK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DAC_res = [4, 5, 6, 7]\n",
        "ADC_res = [6 + 0.2 * i for i in range(16)]\n",
        "device_resolution = [3,4,5,6,7,8]\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=True)\n",
        "\n",
        "err_org = network_tester(net, testloader, 400, 0, False).item()\n",
        "print(err_org)\n",
        "\n",
        "result = []\n",
        "\n",
        "for idx in range(6):\n",
        "    for res in DAC_res:\n",
        "      result_wire = []\n",
        "      for adc in range(1): #adc in range(20)\n",
        "          device_params = {\"Vdd\": 1.8,\n",
        "                  \"r_wl\": 20,\n",
        "                  \"r_bl\": 20,\n",
        "                  \"m\": 200,\n",
        "                  \"n\": 200,\n",
        "                  \"r_on_mean\": 1e4,\n",
        "                  \"r_on_stddev\": 1e3,\n",
        "                  \"r_off_mean\": 1e5,\n",
        "                  \"r_off_stddev\": 1e4,\n",
        "                  \"dac_resolution\": res,\n",
        "                  \"adc_resolution\": 8.3,\n",
        "                  \"device_resolution\": 6,\n",
        "                  \"bias_scheme\": 1/3,\n",
        "                  \"tile_rows\": 4,\n",
        "                  \"tile_cols\": 4,\n",
        "                  \"r_cmos_line\": 600,\n",
        "                  \"r_cmos_transistor\": 20,\n",
        "                  \"p_stuck_on\": 0.01,\n",
        "                  \"p_stuck_off\": 0.01}\n",
        "          crb_new = crossbar(device_params)\n",
        "\n",
        "          net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "          net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "          net.fc1.cb = crb_new\n",
        "          net.fc1.remap()\n",
        "\n",
        "          err = network_tester(net, testloader, 400, 0, False).item()\n",
        "          result_wire.append(err/err_org)\n",
        "\n",
        "      result.append(result_wire)\n",
        "\n",
        "    file_name = 'recorddac' + str(idx) + '.csv'\n",
        "    with open(file_name, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([])\n",
        "\n",
        "    for ele in result:\n",
        "      with open(file_name, 'a') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerow(ele)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Db4t1bpG2d3",
        "outputId": "419876c4-1ce1-4cd8-bac7-bea38760034d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6857855319976807\n",
            "[[0.9636364015626515], [0.7963636227733832], [1.0836363459374292], [0.9709091386330032], [0.9090909169922191], [0.9599999895702708], [0.9418181903515966], [0.9709091386330032], [0.9054545919142482], [0.8181818339844381], [0.9599999895702708], [0.9527272524999192], [0.8618181694921383], [0.7818181486326798], [0.9490909274219482], [0.8981818548438965], [0.7381818131249798], [1.0363636853517582], [0.9563636644922999], [0.7636363494140057], [0.8945454428515157], [0.8036363598437348], [0.7200000139063055], [0.9163636540625707]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crossbar update\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 200,\n",
        "                 \"n\": 200,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}\n",
        "crb_new = crossbar(device_params)\n",
        "\n",
        "net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "net.fc1.cb = crb_new\n",
        "net.fc1.remap()"
      ],
      "metadata": {
        "id": "s1-Q6yxibzuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DAC_res = [4, 5, 6, 7]\n",
        "ADC_res = [6 + 0.2 * i for i in range(16)]\n",
        "device_resolution = [3,4,5,6,7,8]\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=True)\n",
        "\n",
        "err_org = network_tester(net, testloader, 400, 0, False).item()\n",
        "print(err_org)\n",
        "\n",
        "result = []\n",
        "\n",
        "for idx in range(6):\n",
        "    for res in ADC_res:\n",
        "      result_wire = []\n",
        "      for adc in range(1): #adc in range(20)\n",
        "          device_params = {\"Vdd\": 1.8,\n",
        "                  \"r_wl\": 20,\n",
        "                  \"r_bl\": 20,\n",
        "                  \"m\": 200,\n",
        "                  \"n\": 200,\n",
        "                  \"r_on_mean\": 1e4,\n",
        "                  \"r_on_stddev\": 1e3,\n",
        "                  \"r_off_mean\": 1e5,\n",
        "                  \"r_off_stddev\": 1e4,\n",
        "                  \"dac_resolution\": 5,\n",
        "                  \"adc_resolution\": res,\n",
        "                  \"device_resolution\": 6,\n",
        "                  \"bias_scheme\": 1/3,\n",
        "                  \"tile_rows\": 4,\n",
        "                  \"tile_cols\": 4,\n",
        "                  \"r_cmos_line\": 600,\n",
        "                  \"r_cmos_transistor\": 20,\n",
        "                  \"p_stuck_on\": 0.01,\n",
        "                  \"p_stuck_off\": 0.01}\n",
        "          crb_new = crossbar(device_params)\n",
        "\n",
        "          net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "          net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "          net.fc1.cb = crb_new\n",
        "          net.fc1.remap()\n",
        "\n",
        "          err = network_tester(net, testloader, 400, 0, False).item()\n",
        "          result_wire.append(err/err_org)\n",
        "\n",
        "      result.append(result_wire)\n",
        "\n",
        "    file_name = 'recordadc' + str(idx) + '.csv'\n",
        "    with open(file_name, 'w') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow([])\n",
        "\n",
        "    for ele in result:\n",
        "      with open(file_name, 'a') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerow(ele)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG3TL9RGHD7j",
        "outputId": "03d81889-987b-4cac-973a-af912f844376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6284289360046387\n",
            "[[0.9563491958106427], [0.9126983916212855], [0.9880952697109289], [0.9801587512358574], [1.0992063386677855], [1.0992063386677855], [1.1031745504817851], [0.7738095080016784], [1.027777767239214], [0.996031693338928], [1.015873036950143], [0.9484126773355712], [0.9682539260997138], [1.0198412487641426], [0.8650793756179287], [1.075396783242571], [1.0634920529535], [1.0674602647674996], [0.920634910096357], [1.0436508041893573], [0.9007936613322144], [0.8571428571428571], [0.9801587512358574], [0.8373016083787145], [1.1190475874319283], [0.9246031219103567], [0.9166666982823575], [0.9007936613322144], [0.9801587512358574], [0.9523809839966432], [1.138888836196071], [1.1349206243820713], [1.0436508041893573], [0.9801587512358574], [1.0595238411395003], [1.1428571428571428], [0.9365079470465001], [0.8928571428571429], [0.9523809839966432], [1.0714285714285714], [1.091269820192714], [1.1150793756179287], [0.9642857142857143], [0.8769841059069998], [1.0634920529535], [0.7579365184750715], [1.0], [0.9841269630498569], [0.996031693338928], [0.7658729895266069], [1.0555555344784284], [1.0476190160033567], [0.9920634815249285], [0.9007936613322144], [0.9166666982823575], [1.0515873226644288], [1.0436508041893573], [1.1428571428571428], [0.9126983916212855], [1.027777767239214], [0.825396783242571], [1.091269820192714], [0.8730158940930002], [1.015873036950143], [0.9484126773355712], [1.091269820192714], [0.9325396403854281], [1.0396824975282852], [1.027777767239214], [1.015873036950143], [0.9246031219103567], [1.091269820192714], [1.0992063386677855], [0.9603174076246423], [0.9761904445747854], [0.8968253546711424], [1.0436508041893573], [1.0515873226644288], [1.0357142857142858], [0.78174602647675], [0.9841269630498569], [0.825396783242571], [1.1150793756179287], [1.0714285714285714], [0.7539682592375357], [1.1190475874319283], [0.8492063386677856], [0.996031693338928], [1.0436508041893573], [0.8531745504817851], [0.9523809839966432], [0.9325396403854281], [1.0555555344784284], [0.9880952697109289], [1.0357142857142858], [1.0198412487641426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# crossbar update\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 100,\n",
        "                 \"n\": 100,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 6,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}\n",
        "crb_new = crossbar(device_params)\n",
        "\n",
        "net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "net.fc1.cb = crb_new\n",
        "net.fc1.remap()\n",
        "\n",
        "DAC_res = [4, 5, 6, 7]\n",
        "ADC_res = [6 + 0.2 * i for i in range(16)]\n",
        "device_resolution = [3,4,5,6,7,8]\n",
        "testloader = torch.utils.data.DataLoader(test, batch_size=1, shuffle=True)\n",
        "err_org = network_tester(net3, testloader, 400, 0, False).item()\n",
        "print(err_org)\n",
        "\n",
        "result = []\n",
        "\n",
        "for res in device_resolution:\n",
        "    result_wire = []\n",
        "    for adc in range(1): #adc in range(20)\n",
        "        device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 20,\n",
        "                 \"r_bl\": 20,\n",
        "                 \"m\": 200,\n",
        "                 \"n\": 200,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 4,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": res,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}\n",
        "        crb_new = crossbar(device_params)\n",
        "\n",
        "        net.fc1.W = torch.nn.parameter.Parameter(org_net_weight)\n",
        "        net.fc1.b = torch.nn.parameter.Parameter(org_net_bias)\n",
        "\n",
        "        net.fc1.cb = crb_new\n",
        "        net.fc1.remap()\n",
        "\n",
        "        err = network_tester(net, testloader, 400, 0, False).item()\n",
        "        result_wire.append(err/err_org)\n",
        "\n",
        "    result.append(result_wire)\n",
        "\n",
        "print(result)\n",
        "\n",
        "with open('recordenob5.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([])\n",
        "\n",
        "for ele in result:\n",
        "  with open('recordenob5.csv', 'a') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(ele)\n",
        "\n",
        "ax = sns.heatmap(result, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "FyVDcHL2_Jzu",
        "outputId": "c31cb745-3a15-4aa6-8993-ba264bba4ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6857855319976807\n",
            "[[0.7018181712304264], [0.7818181486326798], [0.5672727092577405], [0.6909091090821039], [0.5490909100390663], [0.5999999826171181]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rBfK5fDNBEV7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}