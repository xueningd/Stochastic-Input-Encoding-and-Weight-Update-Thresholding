{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01b0451e369e4678a164b3e6f963487b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_737bd8f21b224c19bf8c5ed40ba30467",
              "IPY_MODEL_3ac598bae6f14856b02c1e698714dfe4",
              "IPY_MODEL_220b0f21c221495b9bf339bc1ae6fb68"
            ],
            "layout": "IPY_MODEL_98f59cf40b174d07a6e893c23935c504"
          }
        },
        "737bd8f21b224c19bf8c5ed40ba30467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51d2f6b540304e3e99bf1c73a30489d4",
            "placeholder": "​",
            "style": "IPY_MODEL_1514f57a7cfd4375a46364e43442291f",
            "value": "100%"
          }
        },
        "3ac598bae6f14856b02c1e698714dfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e784d3777b4e58913cdc5f14285220",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73ac79faa0ee40b9a9e836b187408077",
            "value": 170498071
          }
        },
        "220b0f21c221495b9bf339bc1ae6fb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75328a6328ca4abe86a55257ddad182c",
            "placeholder": "​",
            "style": "IPY_MODEL_9d665705cff348a2ba4f2411b847916f",
            "value": " 170498071/170498071 [00:10&lt;00:00, 16141860.00it/s]"
          }
        },
        "98f59cf40b174d07a6e893c23935c504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51d2f6b540304e3e99bf1c73a30489d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1514f57a7cfd4375a46364e43442291f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e784d3777b4e58913cdc5f14285220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ac79faa0ee40b9a9e836b187408077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75328a6328ca4abe86a55257ddad182c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d665705cff348a2ba4f2411b847916f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-10 dataset and in-situ training"
      ],
      "metadata": {
        "id": "aKrfpfFF5Trw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crossbar"
      ],
      "metadata": {
        "id": "4_SY8oAbsv3p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEfdvG1IsqIK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "crossbar.py\n",
        "Louis Primeau\n",
        "University of Toronto Department of Electrical and Computer Engineering\n",
        "louis.primeau@mail.utoronto.ca\n",
        "July 29th 2020\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import savemat\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from scipy.io import savemat\n",
        "\n",
        "# Implements scipy's minmax scaler except just between 0 and 1 for torch Tensors.\n",
        "# Taken from a ptrblck post on the PyTorch forums. Love that dude.\n",
        "class MinMaxScaler(object):\n",
        "    def __call__(self, tensor):\n",
        "        self.scale = 1.0 / (tensor.max(dim=1, keepdim=True)[0] - tensor.min(dim=1, keepdim=True)[0])\n",
        "        self.min = tensor.min(dim=1, keepdim=True)[0]\n",
        "        tensor.sub_(self.min).mul_(self.scale)\n",
        "        return tensor\n",
        "    def inverse_transform(self, tensor):\n",
        "        tensor.div_(self.scale).add_(self.min)\n",
        "        return tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ticket:\n",
        "    def __init__(self, row, col, m_rows, m_cols, matrix, mat_scale_factor, crossbar, uvect, decode, inputres, outputres):\n",
        "        self.row, self.col = row, col\n",
        "        self.m_rows, self.m_cols = m_rows, m_cols\n",
        "        self.crossbar = crossbar\n",
        "        self.mat_scale_factor = mat_scale_factor\n",
        "        self.matrix = matrix\n",
        "        self.uvect = uvect\n",
        "        self.inputres = inputres\n",
        "        self.adcres = outputres\n",
        "        self.decode = torch.matmul(self.uvect.t(),self.matrix)\n",
        "\n",
        "    def prep_vector(self, vector, v_bits):\n",
        "\n",
        "        # Scale vector to [0, 2^v_bits]\n",
        "        vect_min = torch.min(vector)\n",
        "        vector = vector - vect_min\n",
        "        vect_scale_factor = torch.max(vector) / (2**v_bits - 1)\n",
        "        vector = vector / vect_scale_factor if vect_scale_factor != 0.0 else vector\n",
        "\n",
        "        # decompose vector by bit\n",
        "        bit_vector = torch.zeros(vector.size(0),v_bits)\n",
        "        bin2s = lambda x : ''.join(reversed( [str((int(x) >> i) & 1) for i in range(v_bits)] ) )\n",
        "        for j in range(vector.size(0)):\n",
        "            bit_vector[j,:] = torch.Tensor([float(i) for i in list(bin2s(vector[j]))])\n",
        "        bit_vector *= self.crossbar.V\n",
        "\n",
        "        # Pad bit vector with unselected voltages\n",
        "        pad_vector = torch.zeros(self.crossbar.size[0], v_bits)\n",
        "\n",
        "        pad_vector[self.row:self.row + self.m_rows,:] = bit_vector\n",
        "\n",
        "        return pad_vector, vect_scale_factor, vect_min\n",
        "\n",
        "    def vmm(self, vector):\n",
        "        # Baseline VMM operation without CODEX\n",
        "        v_bits = self.inputres\n",
        "        assert vector.size(1) == 1, \"vector wrong shape\"\n",
        "\n",
        "        crossbar = self.crossbar\n",
        "        # Rescale vector and convert to bits.\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        # Perform crossbar VMM\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        # Add binary outputs\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "\n",
        "        # Rescale binary outputs\n",
        "        rout = (rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5131 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "        return rout.view(-1,1)\n",
        "\n",
        "    def CODEXvmm(self, xvector):\n",
        "        # CODEX VMM operation\n",
        "        assert xvector.size(1) == 1, \"vector wrong shape\"\n",
        "        v_bits=self.inputres\n",
        "        crossbar = self.crossbar\n",
        "\n",
        "        #Add encoding vector u to x\n",
        "        vector = xvector + self.uvect\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits+1)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        # The rout on the line below this comment contains\n",
        "        # the raw output currents that the ADC will receive.\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "        rout = 2*(rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5231 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "        rout = rout - self.decode\n",
        "        return rout.view(-1,1)\n",
        "\n",
        "    def modified_CODEXvmm(self, xvector):\n",
        "        # CODEX VMM operation\n",
        "        assert xvector.size(1) == 1, \"vector wrong shape\"\n",
        "        v_bits=self.inputres\n",
        "        crossbar = self.crossbar\n",
        "\n",
        "        #Add encoding vector u to x\n",
        "        vector = xvector + self.uvect\n",
        "        pad_vector, vect_scale_factor, vect_min = self.prep_vector(vector, v_bits+1)\n",
        "\n",
        "        rW = self.crossbar.W[0:(self.matrix.shape[0]),0:(2*self.matrix.shape[1])]\n",
        "        rW = rW[:,1::2] - rW[:,0::2]\n",
        "\n",
        "        rV = torch.transpose(pad_vector[0:vector.size(0)],0,1)\n",
        "        # The rout on the line below this comment contains\n",
        "        # the raw output currents that the ADC will receive.\n",
        "        rout = torch.matmul(rV, rW)\n",
        "\n",
        "        # Round rout to input ADC resolution\n",
        "        rout_scale_factor = torch.max(rout) / (2**self.adcres - 1)\n",
        "        rout = rout / rout_scale_factor\n",
        "        rout = torch.round(rout)\n",
        "        rout = rout * rout_scale_factor\n",
        "\n",
        "        for i in range(rout.size(0)):\n",
        "            rout[i] *= 2**(v_bits - i - 1)\n",
        "        rout = torch.sum(rout, axis=0)\n",
        "        rout = 2*(rout / crossbar.V * vect_scale_factor*self.mat_scale_factor) / 1.5231 + torch.sum(vect_min*self.matrix,axis=0)\n",
        "\n",
        "        # do not decode except during inference\n",
        "        # rout = rout - self.decode\n",
        "        return rout.view(-1,1)\n",
        ""
      ],
      "metadata": {
        "id": "89rdDLevs1Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear"
      ],
      "metadata": {
        "id": "EeUj_nFps5BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import random\n",
        "import copy\n",
        "\n",
        "class linear(torch.autograd.Function):\n",
        "    #From Louis: Custom pytorch autograd function for crossbar VMM operation\n",
        "    @staticmethod\n",
        "    def forward(ctx, ticket, x, W, b):\n",
        "        ctx.save_for_backward(x, W, b)\n",
        "        #return ticket.CODEXvmm(x) + b\n",
        "        return ticket.CODEXvmm(x) + b\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dx):\n",
        "        x, W, b = ctx.saved_tensors\n",
        "        grad_input = W.t().mm(dx)\n",
        "        grad_weight = dx.mm(x.t())\n",
        "        grad_bias = dx\n",
        "        return (None, grad_input, grad_weight, grad_bias)\n",
        "\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, cb,uvect):\n",
        "        super(Linear, self).__init__()\n",
        "        self.W = torch.nn.parameter.Parameter(torch.rand(output_size, input_size))\n",
        "        self.b = torch.nn.parameter.Parameter(torch.rand(output_size, 1))\n",
        "        self.cb = cb\n",
        "\n",
        "        #Instantiate Linear layer with pool of random encoding vectors to sample from\n",
        "        self.uvectlist = uvect\n",
        "        self.uvectidx = 0\n",
        "        # Decoding vector is calculated ideally here off-chip, but calculating decoding vector on-chip is also possible\n",
        "        self.decode = torch.matmul(self.uvectlist[self.uvectidx].t(),torch.transpose(self.W,0,1)).detach().clone()\n",
        "        self.ticket = cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "        self.f = linear()\n",
        "        self.cbon = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.f.apply(self.ticket, x, self.W, self.b) if self.cbon else self.W.matmul(x) + self.b\n",
        "\n",
        "    def remap(self):\n",
        "        #Should call the remap crossbar function after 1 or a couple update steps\n",
        "        self.cb.clear()\n",
        "        self.ticket = self.cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "\n",
        "    def update_decode(self):\n",
        "        #Update decoding vector by updating U*G.\n",
        "        self.decode = torch.matmul(self.uvectlist[self.uvectidx].t(),torch.transpose(self.W,0,1)).detach().clone()\n",
        "\n",
        "    def resample(self):\n",
        "        #Sample random new uvector from provided uvectlist\n",
        "        self.cb.clear()\n",
        "        self.uvectidx = random.randint(0, len(uvectlist)-1)\n",
        "        self.ticket = self.cb.register_linear(torch.transpose(self.W,0,1),self.uvectlist[self.uvectidx],self.decode)\n",
        "\n",
        "    def use_cb(self, state):\n",
        "        self.cbon = state\n"
      ],
      "metadata": {
        "id": "B-9qvNpps6gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class crossbar:\n",
        "    def __init__(self, device_params):\n",
        "\n",
        "        # Power Supply Voltage\n",
        "        self.V = device_params[\"Vdd\"]\n",
        "\n",
        "        # DAC resolution\n",
        "        self.input_resolution = device_params[\"dac_resolution\"]\n",
        "        self.output_resolution = device_params[\"adc_resolution\"]\n",
        "\n",
        "        # Wordline Resistance\n",
        "        self.r_wl = torch.Tensor((device_params[\"r_wl\"],))\n",
        "        # Bitline Resistance\n",
        "        self.r_bl = torch.Tensor((device_params[\"r_bl\"],))\n",
        "\n",
        "        # Number of rows, columns\n",
        "        self.size = device_params[\"m\"], device_params[\"n\"]\n",
        "\n",
        "        # High resistance state\n",
        "        self.g_on = 1 / torch.normal(device_params[\"r_on_mean\"], device_params[\"r_on_stddev\"], size=self.size)\n",
        "        #self.g_on = (1 / device_params[\"r_on_mean\"]) * torch.ones(self.size)\n",
        "\n",
        "        # Low Resistance state\n",
        "        self.g_off = 1 / torch.normal(device_params[\"r_off_mean\"], device_params[\"r_off_stddev\"], size=self.size)\n",
        "        #self.g_off = (1 / device_params[\"r_off_mean\"]) * torch.ones(self.size)\n",
        "\n",
        "        self.g_wl = torch.Tensor((1 / device_params[\"r_wl\"],))\n",
        "        self.g_bl = torch.Tensor((1 / device_params[\"r_bl\"],))\n",
        "\n",
        "        # Resolution\n",
        "        self.resolution = device_params[\"device_resolution\"]\n",
        "        # Conductance tensor, m x n x 2**resolution\n",
        "\n",
        "        # 2**self.resolution - 1 so that there's a conductance state in the middle.\n",
        "        self.conductance_states = torch.cat([torch.cat([torch.linspace(self.g_off[i,j], self.g_on[i,j],2**self.resolution - 1).unsqueeze(0)\n",
        "                                                        for j in range(self.size[1])],dim=0).unsqueeze(0)\n",
        "                                             for i in range(self.size[0])],dim=0)\n",
        "\n",
        "        # Bias Scheme\n",
        "        self.bias_voltage = self.V * device_params[\"bias_scheme\"]\n",
        "\n",
        "        # Tile size (1x1 = 1T1R, nxm = passive, etc.)\n",
        "        self.tile_rows = device_params[\"tile_rows\"]\n",
        "        self.tile_cols = device_params[\"tile_cols\"]\n",
        "        assert self.size[0] % self.tile_rows == 0, \"tile size does not divide crossbar size in row direction\"\n",
        "        assert self.size[1] % self.tile_cols == 0, \"tile size does not divide crossbar size in col direction\"\n",
        "\n",
        "        # Resistance of CMOS lines\n",
        "        self.r_cmos_line = device_params[\"r_cmos_line\"]\n",
        "\n",
        "        # Conductance Matrix; initialize each memristor at the on resstance\n",
        "        self.W = torch.ones(self.size) * self.g_on\n",
        "\n",
        "        # Stuck-on & stuck-on device nonideality\n",
        "        self.p_stuck_on = device_params[\"p_stuck_on\"]\n",
        "        self.p_stuck_off = device_params[\"p_stuck_off\"]\n",
        "        self.devicefaults = False\n",
        "\n",
        "        self.mapped = []\n",
        "\n",
        "        self.saved_tiles = {}\n",
        "\n",
        "    def apply_stuck(self, p_stuck_on, p_stuck_off):\n",
        "\n",
        "        state_dist = torch.distributions.categorical.Categorical(probs=torch.Tensor([p_stuck_on, p_stuck_off, 1 - p_stuck_on - p_stuck_off]))\n",
        "        state_mask = state_dist.sample(self.size)\n",
        "\n",
        "        self.W[state_mask == 0] = self.g_off[state_mask==0]\n",
        "        self.W[state_mask == 1] = self.g_on[state_mask==1]\n",
        "\n",
        "        return None\n",
        "\n",
        "    def map(self, matrix):\n",
        "        assert not(matrix.size(0) > self.size[0] or matrix.size(1)*2 > self.size[1]), \"input too large\"\n",
        "        midpoint = self.conductance_states.size(2) // 2\n",
        "\n",
        "        for i in range(matrix.size(0)):\n",
        "            for j in range(matrix.size(1)):\n",
        "\n",
        "                shifted = self.conductance_states[i,j] - self.conductance_states[i,j,midpoint]\n",
        "                idx = torch.min(torch.abs(shifted - matrix[i,j]), dim=0)[1]\n",
        "\n",
        "                self.W[i,2*j+1] = self.conductance_states[i,j,idx]\n",
        "                self.W[i,2*j] = self.conductance_states[i,j,midpoint-(idx-midpoint)]\n",
        "\n",
        "    def solve(self, voltage):\n",
        "        output = torch.zeros((voltage.size(1), self.size[1]))\n",
        "        for i in range(self.size[0] // self.tile_rows):\n",
        "            for j in range(self.size[1] // self.tile_cols):\n",
        "                for k in range(voltage.size(1)):\n",
        "                    coords = (i*self.tile_rows, (i+1)*self.tile_rows, j*self.tile_cols, (j+1)*self.tile_rows)\n",
        "                    vect = voltage[i*self.tile_rows:(i+1)*self.tile_rows,k]\n",
        "                    solution = self.circuit_solve(coords, vect, torch.zeros(self.size[1]), torch.ones(self.size[1]), torch.zeros(self.size[0]))\n",
        "                    output[k] += torch.cat((torch.zeros(j*self.tile_cols), solution, torch.zeros((self.size[1] // self.tile_cols - j - 1) * self.tile_cols)))\n",
        "        return output\n",
        "\n",
        "    \"\"\"\n",
        "    A Comprehensive Crossbar Array Model With Solutions for Line Resistance and Nonlinear Device Characteristics\n",
        "    An Chen\n",
        "    IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL. 60, NO. 4, APRIL 2013\n",
        "    \"\"\"\n",
        "\n",
        "    def hash_M(self, a, b, c, d):\n",
        "        return str(a) + \"_\" + str(b) + \"_\" + str(c) + \"_\" + str(d)\n",
        "\n",
        "    def make_M(self, a, b, c, d):\n",
        "\n",
        "        conductances = self.W[a:b,c:d]\n",
        "        g_wl, g_bl = self.g_wl, self.g_bl\n",
        "        g_s_wl_in, g_s_wl_out = torch.ones(self.tile_rows) * 1, torch.ones(self.tile_rows) * 1e-9\n",
        "        g_s_bl_in, g_s_bl_out = torch.ones(self.tile_rows) * 1e-9, torch.ones(self.tile_rows) * 1\n",
        "        m, n = self.tile_rows, self.tile_cols\n",
        "\n",
        "        A = torch.block_diag(*tuple(torch.diag(conductances[i,:])\n",
        "                          + torch.diag(torch.cat((g_wl, g_wl * 2 * torch.ones(n-2), g_wl)))\n",
        "                          + torch.diag(g_wl * -1 *torch.ones(n-1), diagonal = 1)\n",
        "                          + torch.diag(g_wl * -1 *torch.ones(n-1), diagonal = -1)\n",
        "                          + torch.diag(torch.cat((g_s_wl_in[i].view(1), torch.zeros(n - 2), g_s_wl_out[i].view(1))))\n",
        "                                   for i in range(m)))\n",
        "\n",
        "        B = torch.block_diag(*tuple(-torch.diag(conductances[i,:]) for i in range(m)))\n",
        "\n",
        "        def makec(j):\n",
        "            c = torch.zeros(m, m*n)\n",
        "            for i in range(m):\n",
        "                c[i,n*(i) + j] = conductances[i,j]\n",
        "            return c\n",
        "\n",
        "        C = torch.cat([makec(j) for j in range(n)],dim=0)\n",
        "\n",
        "        def maked(j):\n",
        "            d = torch.zeros(m, m*n)\n",
        "\n",
        "            def c(k):\n",
        "                return(k - 1)\n",
        "\n",
        "            i = 1\n",
        "            d[c(i),c(j)] = -g_s_bl_in[c(j)] - g_bl - conductances[c(i),c(j)]\n",
        "            d[c(i), n*i + c(j)] = g_bl\n",
        "\n",
        "            i = m\n",
        "            d[c(i), n*(i-2) + c(j)] = g_bl\n",
        "            d[c(i), n*(i-1) + c(j)] = -g_s_bl_out[c(j)] - conductances[c(i),c(j)] - g_bl\n",
        "\n",
        "            for i in range(2, m):\n",
        "                d[c(i), n*(i-2) + c(j)] = g_bl\n",
        "                d[c(i), n*(i-1) + c(j)] = -g_bl - conductances[c(i),c(j)] - g_bl\n",
        "                d[c(i), n*(i+1) + c(j)] = g_bl\n",
        "\n",
        "            return d\n",
        "\n",
        "        D = torch.cat([maked(j) for j in range(1,n+1)], dim=0)\n",
        "\n",
        "        M = torch.cat((torch.cat((A,B),dim=1), torch.cat((C,D),dim=1)), dim=0)\n",
        "\n",
        "        self.saved_tiles[self.hash_M(a,b,c,d)] = M\n",
        "\n",
        "        return torch.inverse(M)\n",
        "\n",
        "    def circuit_solve(self, coords,  v_wl_in, v_bl_in, v_bl_out, v_wl_out):\n",
        "\n",
        "        g_wl, g_bl = self.g_wl, self.g_bl\n",
        "        g_s_wl_in, g_s_wl_out = torch.ones(self.tile_rows) * 1, torch.ones(self.tile_rows) * 1e-9\n",
        "        g_s_bl_in, g_s_bl_out = torch.ones(self.tile_rows) * 1e-9, torch.ones(self.tile_rows) * 1\n",
        "        m, n = self.tile_rows, self.tile_cols\n",
        "\n",
        "\n",
        "        if self.hash_M(*coords) not in self.saved_tiles.keys():\n",
        "            #print(coords)\n",
        "            M = self.make_M(*coords)\n",
        "        else:\n",
        "            M = self.saved_tiles[self.hash_M(*coords)]\n",
        "\n",
        "        E = torch.cat([torch.cat(((v_wl_in[i]*g_s_wl_in[i]).view(1), #EW\n",
        "                                  torch.zeros(n-2),\n",
        "                                  (v_wl_out[i]*g_s_wl_out[i]).view(1)))\n",
        "                                 for i in range(m)] +\n",
        "                      [torch.cat(((-v_bl_in[i]*g_s_bl_in[i]).view(1), #EB\n",
        "                                  torch.zeros(m-2),\n",
        "                                  (-v_bl_in[i]*g_s_bl_out[i]).view(1)))\n",
        "                                 for i in range(n)]\n",
        "        ).view(-1, 1)\n",
        "\n",
        "        V = torch.matmul(M, E)\n",
        "\n",
        "        V = torch.chunk(torch.solve(E, M)[0], 2)\n",
        "\n",
        "        return torch.sum((V[1] - V[0]).view(m,n)*self.W[coords[0]:coords[1],coords[2]:coords[3]],dim=0)\n",
        "\n",
        "    def register_linear(self, matrix, uvectlist, decode, bias=None):\n",
        "\n",
        "        row, col = self.find_space(matrix.size(0), matrix.size(1))\n",
        "        # Need to add checks for bias size and col size\n",
        "\n",
        "        # Scale matrix\n",
        "        mat_scale_factor = torch.max(torch.abs(matrix)) / torch.max(self.g_on) * 2\n",
        "        scaled_matrix = matrix / mat_scale_factor\n",
        "\n",
        "        midpoint = self.conductance_states.size(2) // 2\n",
        "        for i in range(row, row + scaled_matrix.size(0)):\n",
        "            for j in range(col, col + scaled_matrix.size(1)):\n",
        "\n",
        "                shifted = self.conductance_states[i,j] - self.conductance_states[i,j,midpoint]\n",
        "                idx = torch.min(torch.abs(shifted - scaled_matrix[i-row,j-col]), dim=0)[1]\n",
        "                self.W[i,2*j+1] = self.conductance_states[i,j,idx]\n",
        "                self.W[i,2*j] = self.conductance_states[i,j,midpoint-(idx-midpoint)]\n",
        "\n",
        "        return ticket(row, col, matrix.size(0), matrix.size(1), matrix, mat_scale_factor, self, uvectlist, decode, self.input_resolution, self.output_resolution)\n",
        "\n",
        "    def which_tiles(self, row, col, m_row, m_col):\n",
        "        return itertools.product(range(row // self.tile_rows, (row + m_row) // self.tile_rows + 1),\n",
        "                                 range(col // self.tile_cols,(col + m_col) // self.tile_cols + 1),\n",
        "        )\n",
        "\n",
        "    def find_space(self, m_row, m_col):\n",
        "        if not self.mapped:\n",
        "            self.mapped.append((0,0,m_row,m_col))\n",
        "        else:\n",
        "            self.mapped.append((self.mapped[-1][0] + self.mapped[-1][2], self.mapped[-1][1] + self.mapped[-1][3], m_row, m_col))\n",
        "        return self.mapped[-1][0], self.mapped[-1][1]\n",
        "\n",
        "    def clear(self):\n",
        "        self.mapped = []\n",
        "        self.W = torch.ones(self.size) * self.g_on\n",
        "\n",
        "    def conductance_update(self):\n",
        "        self.conductance_states = torch.cat([torch.cat([torch.linspace(self.g_off[i,j], self.g_on[i,j],2**self.resolution - 1).unsqueeze(0)\n",
        "                                                        for j in range(self.size[1])],dim=0).unsqueeze(0)\n",
        "                                             for i in range(self.size[0])],dim=0)\n"
      ],
      "metadata": {
        "id": "qfKKuw5ZtHTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device parameters"
      ],
      "metadata": {
        "id": "z1XmSctrs-NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Idea is that CODEX allows us to use higher ADC inpt resolution by\n",
        "# Reducing the ADC sensing range.\n",
        "device_params = {\"Vdd\": 1.8,\n",
        "                 \"r_wl\": 10,\n",
        "                 \"r_bl\": 10,\n",
        "                 \"m\": 600,\n",
        "                 \"n\": 600,\n",
        "                 \"r_on_mean\": 1e4,\n",
        "                 \"r_on_stddev\": 1e3,\n",
        "                 \"r_off_mean\": 1e5,\n",
        "                 \"r_off_stddev\": 1e4,\n",
        "                 \"dac_resolution\": 5,\n",
        "                 \"adc_resolution\": 8.3,\n",
        "                 \"device_resolution\": 8,\n",
        "                 \"bias_scheme\": 1/3,\n",
        "                 \"tile_rows\": 4,\n",
        "                 \"tile_cols\": 4,\n",
        "                 \"r_cmos_line\": 600,\n",
        "                 \"r_cmos_transistor\": 20,\n",
        "                 \"p_stuck_on\": 0.01,\n",
        "                 \"p_stuck_off\": 0.01}"
      ],
      "metadata": {
        "id": "drBva6eBtAJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training + Testing"
      ],
      "metadata": {
        "id": "P1RSSC3AtNcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def network_tester(model, test_loader, test_size, epoch, log = True):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            if batch_idx * len(data) > test_size:\n",
        "                break\n",
        "            output = model(data)\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            total+=target.size(0)\n",
        "            loss = F.nll_loss(output, target)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0 and log:\n",
        "              with open('log_baseline_test.csv', 'a') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow([batch_idx + test_size * 100, test_loss/(batch_idx+1), correct.item()/total])\n",
        "              print(\"Epoch\", epoch, 'iteration',batch_idx, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                          % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    return torch.div(correct, float(total))"
      ],
      "metadata": {
        "id": "sKmRXWQ9tWwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline"
      ],
      "metadata": {
        "id": "nh05UOzPtesG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net_trainer(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        # output = model(data)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        # init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "        #                        cur_avg_abs_grad, j+1)\n",
        "        # thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 100, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}\")\n",
        "                with open('log_baseline_train.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward()\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "            # if avg_loss == -1.0:\n",
        "            #     avg_loss = loss\n",
        "            # else:\n",
        "            #     avg_loss += loss\n",
        "            # if avg_loss > config['naive_loss_thr']:\n",
        "            #     thr = init_thr\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                cur_epoch_updates += 1\n",
        "                # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                # thr = local_sched(config['learning_rate'],\n",
        "                #                                 config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                optimizer.step()\n",
        "                model.fc1.remap()\n",
        "                #netowrk.fc2.remap()\n",
        "                optimizer.zero_grad()\n",
        "            #     avg_loss = -1.0\n",
        "            # thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss"
      ],
      "metadata": {
        "id": "zHiTtAsitgzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input modulation + thresholding"
      ],
      "metadata": {
        "id": "DlNbMigOtbu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net_trainer_thresh(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "    threshold_update = False\n",
        "\n",
        "    def local_sched(learning_rate, gamma, cur_avg_abs_grad, epoch_n):\n",
        "        new_thr = cur_avg_abs_grad*(1+(math.exp(-learning_rate*gamma*epoch_n)))\n",
        "        upper_bound = cur_avg_abs_grad * config['max_thresh_multiplier']\n",
        "\n",
        "        return min(new_thr, upper_bound)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "                               cur_avg_abs_grad, j+1)\n",
        "        thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 300, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}, Threshold: {thr}\")\n",
        "                with open('log_baseline_train.csv', 'a') as f:\n",
        "                  writer = csv.writer(f)\n",
        "                  writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 100, j, False).item()])\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward()\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "            if avg_loss == -1.0:\n",
        "                avg_loss = loss\n",
        "            else:\n",
        "                avg_loss += loss\n",
        "            # if avg_loss > config['naive_loss_thr']:\n",
        "            #     thr = init_thr\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                if threshold_update:\n",
        "                    thr = local_sched(config['learning_rate'],\n",
        "                                                    config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                    threshold_update = False\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                if cur_avg_abs_grad > thr:\n",
        "                    cur_epoch_updates += 1\n",
        "                    # cur_lr = get_current_lr(optimizer, 0, 0)\n",
        "                    # thr = local_sched(config['learning_rate'],\n",
        "                    #                                 config['gamma'], cur_avg_abs_grad, batch_idx+1)\n",
        "                    optimizer.step()\n",
        "                    model.fc1.remap()\n",
        "                    #netowrk.fc2.remap()\n",
        "                    optimizer.zero_grad()\n",
        "                    threshold_update = True\n",
        "                avg_loss = -1.0\n",
        "                thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss, thr_per_batch"
      ],
      "metadata": {
        "id": "iqBObph-tOsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manhattan\n"
      ],
      "metadata": {
        "id": "IMprFAoKZH92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "from typing import List, Optional\n",
        "import numpy as np\n",
        "\n",
        "count_list = []\n",
        "last_layer = []\n",
        "\n",
        "class ManhattanSGD(Optimizer):\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, *, maximize=False, foreach: Optional[bool] = None):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        maximize=maximize, foreach=foreach)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(ManhattanSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "            group.setdefault('maximize', False)\n",
        "            group.setdefault('foreach', None)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs the Manhattan Learning rule such that\n",
        "        \\Delta W(i,j) = sgn(\\Delta w(i,j))\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            d_p_list = []\n",
        "            momentum_buffer_list = []\n",
        "            has_sparse_grad = False\n",
        "\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        # buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                        #buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()#.mul_(group['lr'])\n",
        "                        buf = param_state['momentum_buffer'] =  torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device)\n",
        "                        #count_list.append(buf.cpu().detach().numpy().flatten().tolist())\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer'].float()\n",
        "                        #buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                        #buf.add_(1 - dampening, torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)).to(device))\n",
        "                        #buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()#.mul_(group['lr'])\n",
        "                        buf = param_state['momentum_buffer'] =  torch.from_numpy(np.where(torch.clone(d_p).cpu().detach()>0, 1, -1)*momentum*(1-dampening)).to(device)\n",
        "                        #count_list.append(buf.cpu().detach().numpy().flatten().tolist())\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "                else:\n",
        "                    p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "\n",
        "        return loss\n",
        ""
      ],
      "metadata": {
        "id": "AstuNCiZZJGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net_trainer_manh(model, train_dataloader, test_dataloader, config):\n",
        "    model.train()\n",
        "    avg_loss = -1\n",
        "    optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], dampening=config['dampening'])\n",
        "    #optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    update_per_epoch = []\n",
        "    loss_per_epoch = []\n",
        "    every_loss = []\n",
        "    thr_per_batch = []\n",
        "\n",
        "    for j in range(config['num_epochs']):\n",
        "        data, target = next(iter(train_dataloader))\n",
        "        optimizer.zero_grad()\n",
        "        # output = model(data)\n",
        "        # loss = F.nll_loss(output, target)\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        # cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "        # init_thr = local_sched(config['learning_rate'], config['gamma'],\n",
        "        #                        cur_avg_abs_grad, j+1)\n",
        "        # thr = init_thr\n",
        "        cur_epoch_updates = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "\n",
        "            if batch_idx % config['test_interval'] == 1:\n",
        "                print(f\"Test accuracy: {network_tester(model, test_dataloader, 400, j)}\")\n",
        "\n",
        "            if batch_idx % config['log_interval'] == 1:\n",
        "                print(f\"Epoch: {j + 1}, Loss: {loss.data} Updates: {cur_epoch_updates}/{batch_idx}, Avg Grad: {cur_avg_abs_grad}\")\n",
        "                with open('log_baseline_train.csv', 'a') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([batch_idx + j * config['log_interval'], loss.data.item(), network_tester(model, train_dataloader, 400, j, False).item()])\n",
        "\n",
        "            output = model(data)\n",
        "            loss = F.nll_loss(output, target) / config['batch_size']\n",
        "            loss.backward()\n",
        "            every_loss.append(loss.data)\n",
        "\n",
        "\n",
        "            if batch_idx % config['batch_size'] == 0:\n",
        "                cur_avg_abs_grad = model.fc1.W.grad.abs().mean()\n",
        "                # this is kind of hardcoded... Are we doing num_layers in this search?\n",
        "                # we want to perform searches on things that don't really affect the\n",
        "                # effectivity of thresholding, so maybe not\n",
        "                # perhaps let's do ADC resolution + learning rate?\n",
        "                cur_epoch_updates += 1\n",
        "\n",
        "                optimizer.step()\n",
        "                model.fc1.remap()\n",
        "                #netowrk.fc2.remap()\n",
        "                optimizer.zero_grad()\n",
        "            #     avg_loss = -1.0\n",
        "            # thr_per_batch.append(thr)\n",
        "\n",
        "\n",
        "        update_per_epoch.append(cur_epoch_updates)\n",
        "\n",
        "\n",
        "        loss_per_epoch.append(loss.data)\n",
        "    return loss_per_epoch, update_per_epoch, every_loss"
      ],
      "metadata": {
        "id": "w8Pc_0Q4ZgnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18"
      ],
      "metadata": {
        "id": "8Qao2IiOwgYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        # self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.uvect = [2*(torch.rand(512,1) - 0.5) for i in range(0,10)] #\n",
        "        crb1 = crossbar(device_params)\n",
        "        # Can test using more than 1 crossbar linear layers.\n",
        "        # Easiest implementation is to create a crossbar for each linear layer\n",
        "        self.fc1 = Linear(512, 10,crb1,self.uvect)\n",
        "        self.fc1.use_cb(True)\n",
        "        #self.fc2 = nn.Linear(64*2*2, 10)\n",
        "        self.traincount = 0\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(512, 1)\n",
        "        #print(out.shape)\n",
        "        out = self.fc1(out)\n",
        "        out = out.t()\n",
        "        out = F.log_softmax(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "Lk5aPnSZwh8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Image"
      ],
      "metadata": {
        "id": "jYHhWL_7xIzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=1, shuffle=True, num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=1, shuffle=False, num_workers=1)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "01b0451e369e4678a164b3e6f963487b",
            "737bd8f21b224c19bf8c5ed40ba30467",
            "3ac598bae6f14856b02c1e698714dfe4",
            "220b0f21c221495b9bf339bc1ae6fb68",
            "98f59cf40b174d07a6e893c23935c504",
            "51d2f6b540304e3e99bf1c73a30489d4",
            "1514f57a7cfd4375a46364e43442291f",
            "95e784d3777b4e58913cdc5f14285220",
            "73ac79faa0ee40b9a9e836b187408077",
            "75328a6328ca4abe86a55257ddad182c",
            "9d665705cff348a2ba4f2411b847916f"
          ]
        },
        "id": "uFTdMC2PxKph",
        "outputId": "aebd4bc1-b79f-41bf-cf8d-afd494b3b6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01b0451e369e4678a164b3e6f963487b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process"
      ],
      "metadata": {
        "id": "sEGuHEbtw-DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet18()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 35,\n",
        "    \"batch_size\" : 150,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.5,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss, c3f1_thr_per_batch = net_trainer(net, trainloader, testloader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_v5YZ0yCxHVp",
        "outputId": "6df2e316-807d-49d7-abb5-2c5e5dab9a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 7.162 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 5.991 | Acc: 7.921% (8/101)\n",
            "Test accuracy: 0.07920791953802109\n",
            "Epoch: 1, Loss: 0.02601216360926628 Updates: 1/1, Avg Grad: 0.0010897691827267408\n",
            "Epoch: 1, Loss: 0.010968198999762535 Updates: 7/1001, Avg Grad: 0.004348933696746826\n",
            "Epoch: 1, Loss: 0.042897649109363556 Updates: 14/2001, Avg Grad: 0.0069520012475550175\n",
            "Epoch: 1, Loss: 0.03277191147208214 Updates: 21/3001, Avg Grad: 0.004150037653744221\n",
            "Epoch: 1, Loss: 0.01962302252650261 Updates: 27/4001, Avg Grad: 0.0025889561511576176\n",
            "Epoch: 1, Loss: 0.014845818281173706 Updates: 34/5001, Avg Grad: 0.003305623773485422\n",
            "Epoch: 1, Loss: 0.014006637036800385 Updates: 41/6001, Avg Grad: 0.0018860778072848916\n",
            "Epoch: 1, Loss: 0.012566613964736462 Updates: 47/7001, Avg Grad: 0.0025752042420208454\n",
            "Epoch: 1, Loss: 0.01774468831717968 Updates: 54/8001, Avg Grad: 0.0026689129881560802\n",
            "Epoch: 1, Loss: 0.016343511641025543 Updates: 61/9001, Avg Grad: 0.0016055116429924965\n",
            "Epoch 0 iteration 0 Loss: 2.197 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 2.091 | Acc: 25.743% (26/101)\n",
            "Test accuracy: 0.2574257552623749\n",
            "Epoch: 1, Loss: 0.013156979344785213 Updates: 67/10001, Avg Grad: 0.0019413806730881333\n",
            "Epoch: 1, Loss: 0.011041567660868168 Updates: 74/11001, Avg Grad: 0.002942774910479784\n",
            "Epoch: 1, Loss: 0.009248768910765648 Updates: 81/12001, Avg Grad: 0.0018940601730719209\n",
            "Epoch: 1, Loss: 0.01534477062523365 Updates: 87/13001, Avg Grad: 0.0016209085006266832\n",
            "Epoch: 1, Loss: 0.011861218139529228 Updates: 94/14001, Avg Grad: 0.002513005631044507\n",
            "Epoch: 1, Loss: 0.02096792683005333 Updates: 101/15001, Avg Grad: 0.0022120620124042034\n",
            "Epoch: 1, Loss: 0.010805074125528336 Updates: 107/16001, Avg Grad: 0.0032205923926085234\n",
            "Epoch: 1, Loss: 0.01078650914132595 Updates: 114/17001, Avg Grad: 0.0028572489973157644\n",
            "Epoch: 1, Loss: 0.018351446837186813 Updates: 121/18001, Avg Grad: 0.002678194083273411\n",
            "Epoch: 1, Loss: 0.011652440764009953 Updates: 127/19001, Avg Grad: 0.0022522537037730217\n",
            "Epoch 0 iteration 0 Loss: 2.091 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 1.922 | Acc: 36.634% (37/101)\n",
            "Test accuracy: 0.3663366436958313\n",
            "Epoch: 1, Loss: 0.02465376816689968 Updates: 134/20001, Avg Grad: 0.0025672144256532192\n",
            "Epoch: 1, Loss: 0.012823157012462616 Updates: 141/21001, Avg Grad: 0.002120136981830001\n",
            "Epoch: 1, Loss: 0.018262911587953568 Updates: 147/22001, Avg Grad: 0.002075748285278678\n",
            "Epoch: 1, Loss: 0.01685297302901745 Updates: 154/23001, Avg Grad: 0.0018855519592761993\n",
            "Epoch: 1, Loss: 0.010841500014066696 Updates: 161/24001, Avg Grad: 0.001947044744156301\n",
            "Epoch: 1, Loss: 0.013399171642959118 Updates: 167/25001, Avg Grad: 0.0034445568453520536\n",
            "Epoch: 1, Loss: 0.006800045259296894 Updates: 174/26001, Avg Grad: 0.002707522828131914\n",
            "Epoch: 1, Loss: 0.011627407744526863 Updates: 181/27001, Avg Grad: 0.0026834974996745586\n",
            "Epoch: 1, Loss: 0.009710384532809258 Updates: 187/28001, Avg Grad: 0.003277157200500369\n",
            "Epoch: 1, Loss: 0.011711285449564457 Updates: 194/29001, Avg Grad: 0.0027290894649922848\n",
            "Epoch 0 iteration 0 Loss: 2.029 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 1.905 | Acc: 32.673% (33/101)\n",
            "Test accuracy: 0.32673266530036926\n",
            "Epoch: 1, Loss: 0.011028907261788845 Updates: 201/30001, Avg Grad: 0.001878966810181737\n",
            "Epoch: 1, Loss: 0.012602229602634907 Updates: 207/31001, Avg Grad: 0.0033091255463659763\n",
            "Epoch: 1, Loss: 0.011115701869130135 Updates: 214/32001, Avg Grad: 0.0019967067055404186\n",
            "Epoch: 1, Loss: 0.006700867787003517 Updates: 221/33001, Avg Grad: 0.0030658862087875605\n",
            "Epoch: 1, Loss: 0.010502934455871582 Updates: 227/34001, Avg Grad: 0.0036316202022135258\n",
            "Epoch: 1, Loss: 0.021635688841342926 Updates: 234/35001, Avg Grad: 0.0038402981590479612\n",
            "Epoch: 1, Loss: 0.01825283095240593 Updates: 241/36001, Avg Grad: 0.0022860660683363676\n",
            "Epoch: 1, Loss: 0.01586838625371456 Updates: 247/37001, Avg Grad: 0.0031483296770602465\n",
            "Epoch: 1, Loss: 0.010832537896931171 Updates: 254/38001, Avg Grad: 0.0029221707955002785\n",
            "Epoch: 1, Loss: 0.01258842833340168 Updates: 261/39001, Avg Grad: 0.003268210217356682\n",
            "Epoch 0 iteration 0 Loss: 1.934 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 1.720 | Acc: 30.693% (31/101)\n",
            "Test accuracy: 0.30693069100379944\n",
            "Epoch: 1, Loss: 0.01836559548974037 Updates: 267/40001, Avg Grad: 0.002789268735796213\n",
            "Epoch: 1, Loss: 0.009562414139509201 Updates: 274/41001, Avg Grad: 0.0025701194535940886\n",
            "Epoch: 1, Loss: 0.017514247447252274 Updates: 281/42001, Avg Grad: 0.0026569694746285677\n",
            "Epoch: 1, Loss: 0.009254039265215397 Updates: 287/43001, Avg Grad: 0.0040810019709169865\n",
            "Epoch: 1, Loss: 0.009654974564909935 Updates: 294/44001, Avg Grad: 0.0019354366231709719\n",
            "Epoch: 1, Loss: 0.014046980999410152 Updates: 301/45001, Avg Grad: 0.0021412274800240993\n",
            "Epoch: 1, Loss: 0.008144108578562737 Updates: 307/46001, Avg Grad: 0.002961532911285758\n",
            "Epoch: 1, Loss: 0.004475673194974661 Updates: 314/47001, Avg Grad: 0.0027632019482553005\n",
            "Epoch: 1, Loss: 0.006315669044852257 Updates: 321/48001, Avg Grad: 0.0028579491190612316\n",
            "Epoch: 1, Loss: 0.01002500019967556 Updates: 327/49001, Avg Grad: 0.0026509370654821396\n",
            "Epoch 1 iteration 0 Loss: 1.845 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 1.686 | Acc: 36.634% (37/101)\n",
            "Test accuracy: 0.3663366436958313\n",
            "Epoch: 2, Loss: 0.026003971695899963 Updates: 1/1, Avg Grad: 0.000157317757839337\n",
            "Epoch: 2, Loss: 0.018953606486320496 Updates: 7/1001, Avg Grad: 0.002808926161378622\n",
            "Epoch: 2, Loss: 0.0059332954697310925 Updates: 14/2001, Avg Grad: 0.004357464611530304\n",
            "Epoch: 2, Loss: 0.007822235114872456 Updates: 21/3001, Avg Grad: 0.0027122083120048046\n",
            "Epoch: 2, Loss: 0.007777887862175703 Updates: 27/4001, Avg Grad: 0.0026093360502272844\n",
            "Epoch: 2, Loss: 0.0051700458861887455 Updates: 34/5001, Avg Grad: 0.003089276375249028\n",
            "Epoch: 2, Loss: 0.014208784326910973 Updates: 41/6001, Avg Grad: 0.003963497467339039\n",
            "Epoch: 2, Loss: 0.009824272245168686 Updates: 47/7001, Avg Grad: 0.002737372647970915\n",
            "Epoch: 2, Loss: 0.0054186685010790825 Updates: 54/8001, Avg Grad: 0.0021612062118947506\n",
            "Epoch: 2, Loss: 0.010986478999257088 Updates: 61/9001, Avg Grad: 0.004286501556634903\n",
            "Epoch 1 iteration 0 Loss: 1.699 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 1.661 | Acc: 32.673% (33/101)\n",
            "Test accuracy: 0.32673266530036926\n",
            "Epoch: 2, Loss: 0.012821042910218239 Updates: 67/10001, Avg Grad: 0.004026033915579319\n",
            "Epoch: 2, Loss: 0.014328975230455399 Updates: 74/11001, Avg Grad: 0.0029449237044900656\n",
            "Epoch: 2, Loss: 0.008271932601928711 Updates: 81/12001, Avg Grad: 0.00337698170915246\n",
            "Epoch: 2, Loss: 0.007157955784350634 Updates: 87/13001, Avg Grad: 0.004672008566558361\n",
            "Epoch: 2, Loss: 0.0048965103924274445 Updates: 94/14001, Avg Grad: 0.003590180305764079\n",
            "Epoch: 2, Loss: 0.006665344350039959 Updates: 101/15001, Avg Grad: 0.003115519182756543\n",
            "Epoch: 2, Loss: 0.01763678900897503 Updates: 107/16001, Avg Grad: 0.00278422306291759\n",
            "Epoch: 2, Loss: 0.016846098005771637 Updates: 114/17001, Avg Grad: 0.0037636607885360718\n",
            "Epoch: 2, Loss: 0.010531220585107803 Updates: 121/18001, Avg Grad: 0.002882774919271469\n",
            "Epoch: 2, Loss: 0.010879650712013245 Updates: 127/19001, Avg Grad: 0.0030346272978931665\n",
            "Epoch 1 iteration 0 Loss: 1.679 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 1.627 | Acc: 40.594% (41/101)\n",
            "Test accuracy: 0.40594059228897095\n",
            "Epoch: 2, Loss: 0.016467435285449028 Updates: 134/20001, Avg Grad: 0.004256063140928745\n",
            "Epoch: 2, Loss: 0.01242599356919527 Updates: 141/21001, Avg Grad: 0.00331434840336442\n",
            "Epoch: 2, Loss: 0.012840289622545242 Updates: 147/22001, Avg Grad: 0.0031523946672677994\n",
            "Epoch: 2, Loss: 0.013528630137443542 Updates: 154/23001, Avg Grad: 0.0027770244050771\n",
            "Epoch: 2, Loss: 0.01176103763282299 Updates: 161/24001, Avg Grad: 0.0050832414999604225\n",
            "Epoch: 2, Loss: 0.01970677636563778 Updates: 167/25001, Avg Grad: 0.003868401749059558\n",
            "Epoch: 2, Loss: 0.010875612497329712 Updates: 174/26001, Avg Grad: 0.003925545606762171\n",
            "Epoch: 2, Loss: 0.004512034356594086 Updates: 181/27001, Avg Grad: 0.0027052194345742464\n",
            "Epoch: 2, Loss: 0.011890994384884834 Updates: 187/28001, Avg Grad: 0.003190563293173909\n",
            "Epoch: 2, Loss: 0.009921889752149582 Updates: 194/29001, Avg Grad: 0.0028233083430677652\n",
            "Epoch 1 iteration 0 Loss: 1.446 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 1.632 | Acc: 48.515% (49/101)\n",
            "Test accuracy: 0.48514851927757263\n",
            "Epoch: 2, Loss: 0.0027112422976642847 Updates: 201/30001, Avg Grad: 0.003749946830794215\n",
            "Epoch: 2, Loss: 0.010708427056670189 Updates: 207/31001, Avg Grad: 0.0037476825527846813\n",
            "Epoch: 2, Loss: 0.01900656335055828 Updates: 214/32001, Avg Grad: 0.0038673742674291134\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fdea5b294095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mc3f1_loss_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_update_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_every_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_thr_per_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-2d2b36a2d16c>\u001b[0m in \u001b[0;36mnet_trainer\u001b[0;34m(model, train_dataloader, test_dataloader, config)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mevery_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thresh_outputs, thresh_loss = [], []\n",
        "for i in range(500):\n",
        "    item = next(iter(testloader))\n",
        "    res = net(item[0])\n",
        "    thresh_outputs.append(res[0])\n",
        "    loss_curr =  F.nll_loss(res, item[1])\n",
        "    thresh_loss.append(loss_curr.item())\n",
        "\n",
        "with open(\"err_codex.txt\", 'w') as writefile:\n",
        "    writefile.write(str(thresh_loss))\n",
        "\n",
        "err2 = np.cumsum(thresh_loss)\n",
        "with open(\"err_codex_cumsum.txt\", 'w') as writefile:\n",
        "    writefile.write(str(err2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHuHDLx9-tIL",
        "outputId": "4a31cbd2-4eaa-4c4d-ed00-e470d800da4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet18()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 35,\n",
        "    \"batch_size\" : 200,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 1.1,\n",
        "    \"test_interval\": 10000,\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], train_config['num_epochs'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss, c3f1_thr_per_batch = net_trainer_thresh(net, trainloader, testloader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aewHtR88XFmO",
        "outputId": "ca662735-cc84-496c-8d33-6ece48d63f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 3.974 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 3.955 | Acc: 12.871% (13/101)\n",
            "Epoch 0 iteration 200 Loss: 4.027 | Acc: 13.930% (28/201)\n",
            "Epoch 0 iteration 300 Loss: 4.252 | Acc: 12.292% (37/301)\n",
            "Test accuracy: 0.1229235902428627\n",
            "Epoch: 1, Loss: 0.06031562760472298 Updates: 0/1, Avg Grad: 0.15655477344989777, Threshold: 0.17143213748931885\n",
            "Epoch: 1, Loss: 0.011132776737213135 Updates: 1/1001, Avg Grad: 0.1719636619091034, Threshold: 0.17143213748931885\n",
            "Epoch: 1, Loss: 0.07827382534742355 Updates: 3/2001, Avg Grad: 0.051185380667448044, Threshold: 0.051185380667448044\n",
            "Epoch: 1, Loss: 0.1328832507133484 Updates: 6/3001, Avg Grad: 0.019957710057497025, Threshold: 0.011061353608965874\n",
            "Epoch: 1, Loss: 0.011396464891731739 Updates: 8/4001, Avg Grad: 0.006320871412754059, Threshold: 0.006320871412754059\n",
            "Epoch: 1, Loss: 0.00982842780649662 Updates: 11/5001, Avg Grad: 0.008499400690197945, Threshold: 0.005156924016773701\n",
            "Epoch: 1, Loss: 0.012680484913289547 Updates: 13/6001, Avg Grad: 0.0036118044517934322, Threshold: 0.0036118044517934322\n",
            "Epoch: 1, Loss: 0.009307097643613815 Updates: 16/7001, Avg Grad: 0.006393451243638992, Threshold: 0.003421630710363388\n",
            "Epoch: 1, Loss: 0.009943532757461071 Updates: 18/8001, Avg Grad: 0.003201941726729274, Threshold: 0.003201941726729274\n",
            "Epoch: 1, Loss: 0.014989934861660004 Updates: 21/9001, Avg Grad: 0.007956096902489662, Threshold: 0.004483440890908241\n",
            "Epoch 0 iteration 0 Loss: 2.140 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 2.301 | Acc: 13.861% (14/101)\n",
            "Epoch 0 iteration 200 Loss: 2.299 | Acc: 16.418% (33/201)\n",
            "Epoch 0 iteration 300 Loss: 2.332 | Acc: 13.953% (42/301)\n",
            "Test accuracy: 0.13953489065170288\n",
            "Epoch: 1, Loss: 0.012458107434213161 Updates: 23/10001, Avg Grad: 0.002644822932779789, Threshold: 0.002644822932779789\n",
            "Epoch: 1, Loss: 0.00852229818701744 Updates: 26/11001, Avg Grad: 0.003857830073684454, Threshold: 0.002864154754206538\n",
            "Epoch: 1, Loss: 0.010748437605798244 Updates: 28/12001, Avg Grad: 0.002360524144023657, Threshold: 0.002360524144023657\n",
            "Epoch: 1, Loss: 0.01057183463126421 Updates: 31/13001, Avg Grad: 0.004195192363113165, Threshold: 0.0025115073658525944\n",
            "Epoch: 1, Loss: 0.010019983164966106 Updates: 33/14001, Avg Grad: 0.0016704803565517068, Threshold: 0.0016704803565517068\n",
            "Epoch: 1, Loss: 0.010320639237761497 Updates: 36/15001, Avg Grad: 0.00428044842556119, Threshold: 0.002402507234364748\n",
            "Epoch: 1, Loss: 0.009351377375423908 Updates: 38/16001, Avg Grad: 0.003828849643468857, Threshold: 0.003828849643468857\n",
            "Epoch: 1, Loss: 0.010981624945998192 Updates: 41/17001, Avg Grad: 0.0044748312793672085, Threshold: 0.0024037007242441177\n",
            "Epoch: 1, Loss: 0.01270777266472578 Updates: 43/18001, Avg Grad: 0.0021491844672709703, Threshold: 0.0021491844672709703\n",
            "Epoch: 1, Loss: 0.011091580614447594 Updates: 46/19001, Avg Grad: 0.0034047793596982956, Threshold: 0.0023593392688781023\n",
            "Epoch 0 iteration 0 Loss: 2.139 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 2.138 | Acc: 25.743% (26/101)\n",
            "Epoch 0 iteration 200 Loss: 2.150 | Acc: 21.891% (44/201)\n",
            "Epoch 0 iteration 300 Loss: 2.163 | Acc: 20.266% (61/301)\n",
            "Test accuracy: 0.2026578038930893\n",
            "Epoch: 1, Loss: 0.022195925936102867 Updates: 48/20001, Avg Grad: 0.0032209933269768953, Threshold: 0.0032209933269768953\n",
            "Epoch: 1, Loss: 0.011880427598953247 Updates: 51/21001, Avg Grad: 0.007264675106853247, Threshold: 0.004237140528857708\n",
            "Epoch: 1, Loss: 0.012515688315033913 Updates: 53/22001, Avg Grad: 0.0034839087165892124, Threshold: 0.0034839087165892124\n",
            "Epoch: 1, Loss: 0.009237309917807579 Updates: 56/23001, Avg Grad: 0.004060002509504557, Threshold: 0.0026026044506579638\n",
            "Epoch: 1, Loss: 0.01487079355865717 Updates: 58/24001, Avg Grad: 0.003442170098423958, Threshold: 0.003442170098423958\n",
            "Epoch: 1, Loss: 0.014459200203418732 Updates: 61/25001, Avg Grad: 0.0037992342840880156, Threshold: 0.0027556417044252157\n",
            "Epoch: 1, Loss: 0.012158856727182865 Updates: 63/26001, Avg Grad: 0.0021828729659318924, Threshold: 0.0021828729659318924\n",
            "Epoch: 1, Loss: 0.013019413687288761 Updates: 66/27001, Avg Grad: 0.005275479517877102, Threshold: 0.0026557515375316143\n",
            "Epoch: 1, Loss: 0.009739082306623459 Updates: 68/28001, Avg Grad: 0.0037673129700124264, Threshold: 0.0037673129700124264\n",
            "Epoch: 1, Loss: 0.012238191440701485 Updates: 71/29001, Avg Grad: 0.008508063852787018, Threshold: 0.003509483067318797\n",
            "Epoch 0 iteration 0 Loss: 1.777 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 2.185 | Acc: 14.851% (15/101)\n",
            "Epoch 0 iteration 200 Loss: 2.224 | Acc: 15.920% (32/201)\n",
            "Epoch 0 iteration 300 Loss: 2.258 | Acc: 15.615% (47/301)\n",
            "Test accuracy: 0.15614618360996246\n",
            "Epoch: 1, Loss: 0.012898488901555538 Updates: 73/30001, Avg Grad: 0.0029169318731874228, Threshold: 0.0029169318731874228\n",
            "Epoch: 1, Loss: 0.018696563318371773 Updates: 76/31001, Avg Grad: 0.0052688270807266235, Threshold: 0.0029753027483820915\n",
            "Epoch: 1, Loss: 0.010374908335506916 Updates: 78/32001, Avg Grad: 0.002241675276309252, Threshold: 0.002241675276309252\n",
            "Epoch: 1, Loss: 0.011103285476565361 Updates: 81/33001, Avg Grad: 0.003704853355884552, Threshold: 0.0028914310969412327\n",
            "Epoch: 1, Loss: 0.01028286200016737 Updates: 83/34001, Avg Grad: 0.0019092496950179338, Threshold: 0.0019092496950179338\n",
            "Epoch: 1, Loss: 0.011869792826473713 Updates: 86/35001, Avg Grad: 0.0040974486619234085, Threshold: 0.0031372499652206898\n",
            "Epoch: 1, Loss: 0.01090193074196577 Updates: 88/36001, Avg Grad: 0.002605136251077056, Threshold: 0.002605136251077056\n",
            "Epoch: 1, Loss: 0.009598183445632458 Updates: 91/37001, Avg Grad: 0.005589199252426624, Threshold: 0.0025232278276234865\n",
            "Epoch: 1, Loss: 0.009298164397478104 Updates: 93/38001, Avg Grad: 0.002700693439692259, Threshold: 0.002700693439692259\n",
            "Epoch: 1, Loss: 0.01178684365004301 Updates: 96/39001, Avg Grad: 0.0022013455163687468, Threshold: 0.0014285959769040346\n",
            "Epoch 0 iteration 0 Loss: 1.858 | Acc: 100.000% (1/1)\n",
            "Epoch 0 iteration 100 Loss: 2.212 | Acc: 12.871% (13/101)\n",
            "Epoch 0 iteration 200 Loss: 2.216 | Acc: 13.930% (28/201)\n",
            "Epoch 0 iteration 300 Loss: 2.205 | Acc: 15.282% (46/301)\n",
            "Test accuracy: 0.15282392501831055\n",
            "Epoch: 1, Loss: 0.010350668802857399 Updates: 98/40001, Avg Grad: 0.003727478440850973, Threshold: 0.003727478440850973\n",
            "Epoch: 1, Loss: 0.014273402281105518 Updates: 101/41001, Avg Grad: 0.005814196076244116, Threshold: 0.003377408953383565\n",
            "Epoch: 1, Loss: 0.015719151124358177 Updates: 103/42001, Avg Grad: 0.0033156920690089464, Threshold: 0.0033156920690089464\n",
            "Epoch: 1, Loss: 0.012447867542505264 Updates: 106/43001, Avg Grad: 0.005552603863179684, Threshold: 0.003328552469611168\n",
            "Epoch: 1, Loss: 0.017020687460899353 Updates: 108/44001, Avg Grad: 0.003996853716671467, Threshold: 0.003996853716671467\n",
            "Epoch: 1, Loss: 0.009566047228872776 Updates: 111/45001, Avg Grad: 0.004454396665096283, Threshold: 0.0029226779006421566\n",
            "Epoch: 1, Loss: 0.007701801136136055 Updates: 113/46001, Avg Grad: 0.003233204362913966, Threshold: 0.003233204362913966\n",
            "Epoch: 1, Loss: 0.008431105874478817 Updates: 116/47001, Avg Grad: 0.004236111883074045, Threshold: 0.0030937721021473408\n",
            "Epoch: 1, Loss: 0.007182040251791477 Updates: 118/48001, Avg Grad: 0.0022155737970024347, Threshold: 0.0022155737970024347\n",
            "Epoch: 1, Loss: 0.012760627083480358 Updates: 121/49001, Avg Grad: 0.0031397640705108643, Threshold: 0.0022890439722687006\n",
            "Epoch 1 iteration 0 Loss: 2.013 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 2.247 | Acc: 23.762% (24/101)\n",
            "Epoch 1 iteration 200 Loss: 2.282 | Acc: 21.393% (43/201)\n",
            "Epoch 1 iteration 300 Loss: 2.277 | Acc: 19.934% (60/301)\n",
            "Test accuracy: 0.19933554530143738\n",
            "Epoch: 2, Loss: 0.014167542569339275 Updates: 0/1, Avg Grad: 0.016397375613451004, Threshold: 0.018037114292383194\n",
            "Epoch: 2, Loss: 0.01787380687892437 Updates: 1/1001, Avg Grad: 0.010858610272407532, Threshold: 0.010858610272407532\n",
            "Epoch: 2, Loss: 0.014589870348572731 Updates: 4/2001, Avg Grad: 0.006599905900657177, Threshold: 0.003650238271802664\n",
            "Epoch: 2, Loss: 0.010670797899365425 Updates: 6/3001, Avg Grad: 0.0034932184498757124, Threshold: 0.0034932184498757124\n",
            "Epoch: 2, Loss: 0.008536957204341888 Updates: 9/4001, Avg Grad: 0.0059417979791760445, Threshold: 0.002677900018170476\n",
            "Epoch: 2, Loss: 0.008952719159424305 Updates: 11/5001, Avg Grad: 0.0031547751277685165, Threshold: 0.0031547751277685165\n",
            "Epoch: 2, Loss: 0.009954500012099743 Updates: 14/6001, Avg Grad: 0.0048384955152869225, Threshold: 0.0032348420936614275\n",
            "Epoch: 2, Loss: 0.01212252490222454 Updates: 16/7001, Avg Grad: 0.0029768156819045544, Threshold: 0.0029768156819045544\n",
            "Epoch: 2, Loss: 0.007973859086632729 Updates: 19/8001, Avg Grad: 0.003568545449525118, Threshold: 0.0025826632045209408\n",
            "Epoch: 2, Loss: 0.015639059245586395 Updates: 21/9001, Avg Grad: 0.002106980187818408, Threshold: 0.002106980187818408\n",
            "Epoch 1 iteration 0 Loss: 2.073 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 2.038 | Acc: 18.812% (19/101)\n",
            "Epoch 1 iteration 200 Loss: 2.087 | Acc: 18.905% (38/201)\n",
            "Epoch 1 iteration 300 Loss: 2.081 | Acc: 19.934% (60/301)\n",
            "Test accuracy: 0.19933554530143738\n",
            "Epoch: 2, Loss: 0.01120067574083805 Updates: 24/10001, Avg Grad: 0.0030498290434479713, Threshold: 0.0020242331083863974\n",
            "Epoch: 2, Loss: 0.013439280912280083 Updates: 26/11001, Avg Grad: 0.0024299591314047575, Threshold: 0.0024299591314047575\n",
            "Epoch: 2, Loss: 0.007958500646054745 Updates: 29/12001, Avg Grad: 0.0045352051965892315, Threshold: 0.0028431122191250324\n",
            "Epoch: 2, Loss: 0.007183500099927187 Updates: 31/13001, Avg Grad: 0.0019796905107796192, Threshold: 0.0019796905107796192\n",
            "Epoch: 2, Loss: 0.004452490713447332 Updates: 34/14001, Avg Grad: 0.002551066456362605, Threshold: 0.0021417972166091204\n",
            "Epoch: 2, Loss: 0.008886742405593395 Updates: 36/15001, Avg Grad: 0.001630590297281742, Threshold: 0.001630590297281742\n",
            "Epoch: 2, Loss: 0.011232980526983738 Updates: 39/16001, Avg Grad: 0.0021409422624856234, Threshold: 0.0017358694458380342\n",
            "Epoch: 2, Loss: 0.012282371520996094 Updates: 41/17001, Avg Grad: 0.0023116515949368477, Threshold: 0.0023116515949368477\n",
            "Epoch: 2, Loss: 0.004558999557048082 Updates: 44/18001, Avg Grad: 0.004646971821784973, Threshold: 0.0023657968267798424\n",
            "Epoch: 2, Loss: 0.008210909552872181 Updates: 46/19001, Avg Grad: 0.002705817576497793, Threshold: 0.002705817576497793\n",
            "Epoch 1 iteration 0 Loss: 1.724 | Acc: 0.000% (0/1)\n",
            "Epoch 1 iteration 100 Loss: 1.954 | Acc: 28.713% (29/101)\n",
            "Epoch 1 iteration 200 Loss: 1.964 | Acc: 25.871% (52/201)\n",
            "Epoch 1 iteration 300 Loss: 1.971 | Acc: 27.243% (82/301)\n",
            "Test accuracy: 0.27242523431777954\n",
            "Epoch: 2, Loss: 0.014545648358762264 Updates: 49/20001, Avg Grad: 0.004578224383294582, Threshold: 0.002595792757347226\n",
            "Epoch: 2, Loss: 0.014682467095553875 Updates: 51/21001, Avg Grad: 0.0024691701401025057, Threshold: 0.0024691701401025057\n",
            "Epoch: 2, Loss: 0.01456794049590826 Updates: 54/22001, Avg Grad: 0.003376900451257825, Threshold: 0.00221146154217422\n",
            "Epoch: 2, Loss: 0.0196981243789196 Updates: 56/23001, Avg Grad: 0.002965734340250492, Threshold: 0.002965734340250492\n",
            "Epoch: 2, Loss: 0.008791036903858185 Updates: 59/24001, Avg Grad: 0.004889021627604961, Threshold: 0.002721741097047925\n",
            "Epoch: 2, Loss: 0.01516796089708805 Updates: 61/25001, Avg Grad: 0.0021964036859571934, Threshold: 0.0021964036859571934\n",
            "Epoch: 2, Loss: 0.007789790164679289 Updates: 64/26001, Avg Grad: 0.005838959943503141, Threshold: 0.00355197349563241\n",
            "Epoch: 2, Loss: 0.007271734997630119 Updates: 66/27001, Avg Grad: 0.002658720826730132, Threshold: 0.002658720826730132\n",
            "Epoch: 2, Loss: 0.0070561314933001995 Updates: 69/28001, Avg Grad: 0.004680140875279903, Threshold: 0.0022895443253219128\n",
            "Epoch: 2, Loss: 0.008776168338954449 Updates: 71/29001, Avg Grad: 0.002612286014482379, Threshold: 0.002612286014482379\n",
            "Epoch 1 iteration 0 Loss: 1.639 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 1.850 | Acc: 36.634% (37/101)\n",
            "Epoch 1 iteration 200 Loss: 1.886 | Acc: 32.338% (65/201)\n",
            "Epoch 1 iteration 300 Loss: 1.880 | Acc: 32.226% (97/301)\n",
            "Test accuracy: 0.3222591280937195\n",
            "Epoch: 2, Loss: 0.004301211331039667 Updates: 74/30001, Avg Grad: 0.004168573301285505, Threshold: 0.0030850120820105076\n",
            "Epoch: 2, Loss: 0.011667906306684017 Updates: 76/31001, Avg Grad: 0.0031918049789965153, Threshold: 0.0031918049789965153\n",
            "Epoch: 2, Loss: 0.008427710272371769 Updates: 79/32001, Avg Grad: 0.005661069415509701, Threshold: 0.0032570709008723497\n",
            "Epoch: 2, Loss: 0.0071992045268416405 Updates: 81/33001, Avg Grad: 0.001859196461737156, Threshold: 0.001859196461737156\n",
            "Epoch: 2, Loss: 0.009178799577057362 Updates: 84/34001, Avg Grad: 0.003258329350501299, Threshold: 0.0026375132147222757\n",
            "Epoch: 2, Loss: 0.013738708570599556 Updates: 86/35001, Avg Grad: 0.0016139497747644782, Threshold: 0.0016139497747644782\n",
            "Epoch: 2, Loss: 0.011913185007870197 Updates: 89/36001, Avg Grad: 0.0038247876800596714, Threshold: 0.001976675121113658\n",
            "Epoch: 2, Loss: 0.008023661561310291 Updates: 91/37001, Avg Grad: 0.0017710871761664748, Threshold: 0.0017710871761664748\n",
            "Epoch: 2, Loss: 0.009185665287077427 Updates: 94/38001, Avg Grad: 0.005107100587338209, Threshold: 0.0031016687862575054\n",
            "Epoch: 2, Loss: 0.00864278431981802 Updates: 96/39001, Avg Grad: 0.0023901802487671375, Threshold: 0.0023901802487671375\n",
            "Epoch 1 iteration 0 Loss: 1.511 | Acc: 100.000% (1/1)\n",
            "Epoch 1 iteration 100 Loss: 1.803 | Acc: 36.634% (37/101)\n",
            "Epoch 1 iteration 200 Loss: 1.786 | Acc: 34.328% (69/201)\n",
            "Epoch 1 iteration 300 Loss: 1.802 | Acc: 32.558% (98/301)\n",
            "Test accuracy: 0.3255814015865326\n",
            "Epoch: 2, Loss: 0.00528052169829607 Updates: 98/40001, Avg Grad: 0.0027656774036586285, Threshold: 0.0027656774036586285\n",
            "Epoch: 2, Loss: 0.00530963484197855 Updates: 101/41001, Avg Grad: 0.0029830983839929104, Threshold: 0.0020390476565808058\n",
            "Epoch: 2, Loss: 0.012882674112915993 Updates: 103/42001, Avg Grad: 0.0038868379779160023, Threshold: 0.0038868379779160023\n",
            "Epoch: 2, Loss: 0.009092758409678936 Updates: 106/43001, Avg Grad: 0.005502145737409592, Threshold: 0.003259106073528528\n",
            "Epoch: 2, Loss: 0.006567277479916811 Updates: 108/44001, Avg Grad: 0.0024099897127598524, Threshold: 0.0024099897127598524\n",
            "Epoch: 2, Loss: 0.009868999943137169 Updates: 111/45001, Avg Grad: 0.002576393773779273, Threshold: 0.0015809076139703393\n",
            "Epoch: 2, Loss: 0.011475842446088791 Updates: 113/46001, Avg Grad: 0.003258963581174612, Threshold: 0.003258963581174612\n",
            "Epoch: 2, Loss: 0.008403342217206955 Updates: 116/47001, Avg Grad: 0.005145999602973461, Threshold: 0.002407799242064357\n",
            "Epoch: 2, Loss: 0.011082923039793968 Updates: 118/48001, Avg Grad: 0.0020133531652390957, Threshold: 0.0020133531652390957\n",
            "Epoch: 2, Loss: 0.014187296852469444 Updates: 121/49001, Avg Grad: 0.004538843408226967, Threshold: 0.002995531540364027\n",
            "Epoch 2 iteration 0 Loss: 2.034 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 2.645 | Acc: 13.861% (14/101)\n",
            "Epoch 2 iteration 200 Loss: 2.613 | Acc: 13.433% (27/201)\n",
            "Epoch 2 iteration 300 Loss: 2.673 | Acc: 13.953% (42/301)\n",
            "Test accuracy: 0.13953489065170288\n",
            "Epoch: 3, Loss: 0.010067407973110676 Updates: 0/1, Avg Grad: 0.022325556725263596, Threshold: 0.024558112025260925\n",
            "Epoch: 3, Loss: 0.018578169867396355 Updates: 1/1001, Avg Grad: 0.006506429519504309, Threshold: 0.006506429519504309\n",
            "Epoch: 3, Loss: 0.011686425656080246 Updates: 4/2001, Avg Grad: 0.010281872004270554, Threshold: 0.005587770603597164\n",
            "Epoch: 3, Loss: 0.009179145097732544 Updates: 6/3001, Avg Grad: 0.0046669067814946175, Threshold: 0.0046669067814946175\n",
            "Epoch: 3, Loss: 0.012445840053260326 Updates: 9/4001, Avg Grad: 0.008720632642507553, Threshold: 0.0039143674075603485\n",
            "Epoch: 3, Loss: 0.009939050301909447 Updates: 11/5001, Avg Grad: 0.002120818244293332, Threshold: 0.002120818244293332\n",
            "Epoch: 3, Loss: 0.009105565957725048 Updates: 14/6001, Avg Grad: 0.004722507204860449, Threshold: 0.0027370110619813204\n",
            "Epoch: 3, Loss: 0.007588001899421215 Updates: 16/7001, Avg Grad: 0.002216535387560725, Threshold: 0.002216535387560725\n",
            "Epoch: 3, Loss: 0.012080471962690353 Updates: 19/8001, Avg Grad: 0.003358658868819475, Threshold: 0.0018141319742426276\n",
            "Epoch: 3, Loss: 0.01676063984632492 Updates: 21/9001, Avg Grad: 0.00278714788146317, Threshold: 0.00278714788146317\n",
            "Epoch 2 iteration 0 Loss: 1.660 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 100 Loss: 2.000 | Acc: 21.782% (22/101)\n",
            "Epoch 2 iteration 200 Loss: 2.028 | Acc: 19.403% (39/201)\n",
            "Epoch 2 iteration 300 Loss: 2.015 | Acc: 20.598% (62/301)\n",
            "Test accuracy: 0.2059800624847412\n",
            "Epoch: 3, Loss: 0.006065724417567253 Updates: 24/10001, Avg Grad: 0.002900246297940612, Threshold: 0.0017413601744920015\n",
            "Epoch: 3, Loss: 0.008168142288923264 Updates: 26/11001, Avg Grad: 0.0019536057952791452, Threshold: 0.0019536057952791452\n",
            "Epoch: 3, Loss: 0.010208678431808949 Updates: 29/12001, Avg Grad: 0.0038502011448144913, Threshold: 0.0025010891258716583\n",
            "Epoch: 3, Loss: 0.011268248781561852 Updates: 31/13001, Avg Grad: 0.002187056466937065, Threshold: 0.002187056466937065\n",
            "Epoch: 3, Loss: 0.010405473411083221 Updates: 34/14001, Avg Grad: 0.004361637867987156, Threshold: 0.0022120659705251455\n",
            "Epoch: 3, Loss: 0.007005531340837479 Updates: 36/15001, Avg Grad: 0.0021846203599125147, Threshold: 0.0021846203599125147\n",
            "Epoch: 3, Loss: 0.011591954156756401 Updates: 39/16001, Avg Grad: 0.004532097838819027, Threshold: 0.0021920562721788883\n",
            "Epoch: 3, Loss: 0.009219393134117126 Updates: 41/17001, Avg Grad: 0.00272101117298007, Threshold: 0.00272101117298007\n",
            "Epoch: 3, Loss: 0.007719246204942465 Updates: 43/18001, Avg Grad: 0.0024470207281410694, Threshold: 0.0024470207281410694\n",
            "Epoch: 3, Loss: 0.006599356420338154 Updates: 45/19001, Avg Grad: 0.002598242834210396, Threshold: 0.002637682016938925\n",
            "Epoch 2 iteration 0 Loss: 1.619 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 1.940 | Acc: 27.723% (28/101)\n",
            "Epoch 2 iteration 200 Loss: 1.943 | Acc: 26.368% (53/201)\n",
            "Epoch 2 iteration 300 Loss: 1.941 | Acc: 26.578% (80/301)\n",
            "Test accuracy: 0.2657807171344757\n",
            "Epoch: 3, Loss: 0.009816033765673637 Updates: 48/20001, Avg Grad: 0.002141043310984969, Threshold: 0.002069030422717333\n",
            "Epoch: 3, Loss: 0.01410799752920866 Updates: 50/21001, Avg Grad: 0.0029369196854531765, Threshold: 0.0029369196854531765\n",
            "Epoch: 3, Loss: 0.004633753094822168 Updates: 53/22001, Avg Grad: 0.002356570214033127, Threshold: 0.001736689591780305\n",
            "Epoch: 3, Loss: 0.007389013189822435 Updates: 55/23001, Avg Grad: 0.0027246996760368347, Threshold: 0.0027246996760368347\n",
            "Epoch: 3, Loss: 0.006404967978596687 Updates: 58/24001, Avg Grad: 0.0052034892141819, Threshold: 0.0030050468631088734\n",
            "Epoch: 3, Loss: 0.009922998026013374 Updates: 60/25001, Avg Grad: 0.0029856369365006685, Threshold: 0.0029856369365006685\n",
            "Epoch: 3, Loss: 0.008158095180988312 Updates: 63/26001, Avg Grad: 0.005074935965240002, Threshold: 0.0027151445392519236\n",
            "Epoch: 3, Loss: 0.006441068835556507 Updates: 65/27001, Avg Grad: 0.004946579225361347, Threshold: 0.004946579225361347\n",
            "Epoch: 3, Loss: 0.003997078165411949 Updates: 68/28001, Avg Grad: 0.004693766124546528, Threshold: 0.003240297082811594\n",
            "Epoch: 3, Loss: 0.010911541059613228 Updates: 70/29001, Avg Grad: 0.003032765118405223, Threshold: 0.003032765118405223\n",
            "Epoch 2 iteration 0 Loss: 1.540 | Acc: 0.000% (0/1)\n",
            "Epoch 2 iteration 100 Loss: 1.871 | Acc: 28.713% (29/101)\n",
            "Epoch 2 iteration 200 Loss: 1.866 | Acc: 29.353% (59/201)\n",
            "Epoch 2 iteration 300 Loss: 1.835 | Acc: 30.897% (93/301)\n",
            "Test accuracy: 0.3089700937271118\n",
            "Epoch: 3, Loss: 0.007988358847796917 Updates: 73/30001, Avg Grad: 0.007268019951879978, Threshold: 0.003982021007686853\n",
            "Epoch: 3, Loss: 0.01104529108852148 Updates: 75/31001, Avg Grad: 0.0021799220703542233, Threshold: 0.0021799220703542233\n",
            "Epoch: 3, Loss: 0.006181053351610899 Updates: 78/32001, Avg Grad: 0.0045146094635128975, Threshold: 0.0028610597364604473\n",
            "Epoch: 3, Loss: 0.01214911974966526 Updates: 80/33001, Avg Grad: 0.002943619154393673, Threshold: 0.002943619154393673\n",
            "Epoch: 3, Loss: 0.003733737161383033 Updates: 83/34001, Avg Grad: 0.003472559852525592, Threshold: 0.0018065631156787276\n",
            "Epoch: 3, Loss: 0.007974516600370407 Updates: 85/35001, Avg Grad: 0.002283349633216858, Threshold: 0.002283349633216858\n",
            "Epoch: 3, Loss: 0.007423108443617821 Updates: 88/36001, Avg Grad: 0.003925853408873081, Threshold: 0.0030596056021749973\n",
            "Epoch: 3, Loss: 0.008601469919085503 Updates: 90/37001, Avg Grad: 0.0030766124837100506, Threshold: 0.0030766124837100506\n",
            "Epoch: 3, Loss: 0.00912139005959034 Updates: 93/38001, Avg Grad: 0.004718582145869732, Threshold: 0.0025846988428384066\n",
            "Epoch: 3, Loss: 0.006518704351037741 Updates: 95/39001, Avg Grad: 0.0036653936840593815, Threshold: 0.0036653936840593815\n",
            "Epoch 2 iteration 0 Loss: 1.250 | Acc: 100.000% (1/1)\n",
            "Epoch 2 iteration 100 Loss: 1.765 | Acc: 35.644% (36/101)\n",
            "Epoch 2 iteration 200 Loss: 1.796 | Acc: 32.338% (65/201)\n",
            "Epoch 2 iteration 300 Loss: 1.811 | Acc: 31.561% (95/301)\n",
            "Test accuracy: 0.31561461091041565\n",
            "Epoch: 3, Loss: 0.004580477252602577 Updates: 98/40001, Avg Grad: 0.004971588961780071, Threshold: 0.003291941015049815\n",
            "Epoch: 3, Loss: 0.0013768365606665611 Updates: 100/41001, Avg Grad: 0.0026444150134921074, Threshold: 0.0026444150134921074\n",
            "Epoch: 3, Loss: 0.006963929161429405 Updates: 103/42001, Avg Grad: 0.00617177551612258, Threshold: 0.003529439214617014\n",
            "Epoch: 3, Loss: 0.004203650169074535 Updates: 105/43001, Avg Grad: 0.003056759014725685, Threshold: 0.003056759014725685\n",
            "Epoch: 3, Loss: 0.007153132930397987 Updates: 108/44001, Avg Grad: 0.009140805341303349, Threshold: 0.004645159933716059\n",
            "Epoch: 3, Loss: 0.008941409178078175 Updates: 110/45001, Avg Grad: 0.0035730190575122833, Threshold: 0.0035730190575122833\n",
            "Epoch: 3, Loss: 0.020916752517223358 Updates: 113/46001, Avg Grad: 0.004304902162402868, Threshold: 0.002420976525172591\n",
            "Epoch: 3, Loss: 0.0035625097807496786 Updates: 115/47001, Avg Grad: 0.003781850915402174, Threshold: 0.003781850915402174\n",
            "Epoch: 3, Loss: 0.0125845642760396 Updates: 118/48001, Avg Grad: 0.004763237200677395, Threshold: 0.0022083919029682875\n",
            "Epoch: 3, Loss: 0.0022675280924886465 Updates: 120/49001, Avg Grad: 0.00410761684179306, Threshold: 0.00410761684179306\n",
            "Epoch 3 iteration 0 Loss: 1.651 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 2.150 | Acc: 24.752% (25/101)\n",
            "Epoch 3 iteration 200 Loss: 2.249 | Acc: 23.383% (47/201)\n",
            "Epoch 3 iteration 300 Loss: 2.260 | Acc: 23.256% (70/301)\n",
            "Test accuracy: 0.23255814611911774\n",
            "Epoch: 4, Loss: 0.006474666763097048 Updates: 0/1, Avg Grad: 0.022333350032567978, Threshold: 0.024584108963608742\n",
            "Epoch: 4, Loss: 0.023919865489006042 Updates: 1/1001, Avg Grad: 0.009044820442795753, Threshold: 0.009044820442795753\n",
            "Epoch: 4, Loss: 0.010139496065676212 Updates: 4/2001, Avg Grad: 0.013506199233233929, Threshold: 0.006677734665572643\n",
            "Epoch: 4, Loss: 0.0090084383264184 Updates: 6/3001, Avg Grad: 0.0027050082571804523, Threshold: 0.0027050082571804523\n",
            "Epoch: 4, Loss: 0.007856504060328007 Updates: 9/4001, Avg Grad: 0.0033568409271538258, Threshold: 0.002060935366898775\n",
            "Epoch: 4, Loss: 0.016922615468502045 Updates: 11/5001, Avg Grad: 0.0019386361818760633, Threshold: 0.0019386361818760633\n",
            "Epoch: 4, Loss: 0.015445458702743053 Updates: 14/6001, Avg Grad: 0.004818211309611797, Threshold: 0.00298617547377944\n",
            "Epoch: 4, Loss: 0.005663274321705103 Updates: 16/7001, Avg Grad: 0.002086799591779709, Threshold: 0.002086799591779709\n",
            "Epoch: 4, Loss: 0.011661414057016373 Updates: 19/8001, Avg Grad: 0.006944431457668543, Threshold: 0.003474974539130926\n",
            "Epoch: 4, Loss: 0.014774542301893234 Updates: 21/9001, Avg Grad: 0.002572300611063838, Threshold: 0.002572300611063838\n",
            "Epoch 3 iteration 0 Loss: 2.098 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 2.129 | Acc: 21.782% (22/101)\n",
            "Epoch 3 iteration 200 Loss: 2.128 | Acc: 18.905% (38/201)\n",
            "Epoch 3 iteration 300 Loss: 2.121 | Acc: 19.601% (59/301)\n",
            "Test accuracy: 0.19601328670978546\n",
            "Epoch: 4, Loss: 0.00911304447799921 Updates: 24/10001, Avg Grad: 0.004506520926952362, Threshold: 0.0031419277656823397\n",
            "Epoch: 4, Loss: 0.009689071215689182 Updates: 26/11001, Avg Grad: 0.0030501619912683964, Threshold: 0.0030501619912683964\n",
            "Epoch: 4, Loss: 0.011340844444930553 Updates: 29/12001, Avg Grad: 0.005923641845583916, Threshold: 0.0032549232710152864\n",
            "Epoch: 4, Loss: 0.009458359330892563 Updates: 31/13001, Avg Grad: 0.002615011529996991, Threshold: 0.002615011529996991\n",
            "Epoch: 4, Loss: 0.014026403427124023 Updates: 34/14001, Avg Grad: 0.006127545144408941, Threshold: 0.0024720821529626846\n",
            "Epoch: 4, Loss: 0.004054859280586243 Updates: 36/15001, Avg Grad: 0.0018760899547487497, Threshold: 0.0018760899547487497\n",
            "Epoch: 4, Loss: 0.012766683474183083 Updates: 39/16001, Avg Grad: 0.004263587296009064, Threshold: 0.002879620064049959\n",
            "Epoch: 4, Loss: 0.011443584226071835 Updates: 41/17001, Avg Grad: 0.0018041925504803658, Threshold: 0.0018041925504803658\n",
            "Epoch: 4, Loss: 0.010923496447503567 Updates: 44/18001, Avg Grad: 0.0035200018901377916, Threshold: 0.002047220477834344\n",
            "Epoch: 4, Loss: 0.011443963274359703 Updates: 46/19001, Avg Grad: 0.003621393349021673, Threshold: 0.003621393349021673\n",
            "Epoch 3 iteration 0 Loss: 2.119 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 1.963 | Acc: 24.752% (25/101)\n",
            "Epoch 3 iteration 200 Loss: 1.970 | Acc: 23.383% (47/201)\n",
            "Epoch 3 iteration 300 Loss: 1.967 | Acc: 24.252% (73/301)\n",
            "Test accuracy: 0.2425249218940735\n",
            "Epoch: 4, Loss: 0.008324021473526955 Updates: 49/20001, Avg Grad: 0.006509506143629551, Threshold: 0.003355287481099367\n",
            "Epoch: 4, Loss: 0.016444189473986626 Updates: 51/21001, Avg Grad: 0.0021765800192952156, Threshold: 0.0021765800192952156\n",
            "Epoch: 4, Loss: 0.00690089538693428 Updates: 54/22001, Avg Grad: 0.0029972826596349478, Threshold: 0.002511520404368639\n",
            "Epoch: 4, Loss: 0.013319463469088078 Updates: 56/23001, Avg Grad: 0.0016523634549230337, Threshold: 0.0016523634549230337\n",
            "Epoch: 4, Loss: 0.008908206596970558 Updates: 59/24001, Avg Grad: 0.0031431280076503754, Threshold: 0.0015626860549673438\n",
            "Epoch: 4, Loss: 0.008129688911139965 Updates: 61/25001, Avg Grad: 0.004117551725357771, Threshold: 0.004117551725357771\n",
            "Epoch: 4, Loss: 0.01628684811294079 Updates: 64/26001, Avg Grad: 0.004025926813483238, Threshold: 0.002188712591305375\n",
            "Epoch: 4, Loss: 0.009211385622620583 Updates: 66/27001, Avg Grad: 0.0015239650383591652, Threshold: 0.0015239650383591652\n",
            "Epoch: 4, Loss: 0.005341913551092148 Updates: 69/28001, Avg Grad: 0.0033954184036701918, Threshold: 0.0021867267787456512\n",
            "Epoch: 4, Loss: 0.011477756313979626 Updates: 71/29001, Avg Grad: 0.0022105786483734846, Threshold: 0.0022105786483734846\n",
            "Epoch 3 iteration 0 Loss: 1.528 | Acc: 100.000% (1/1)\n",
            "Epoch 3 iteration 100 Loss: 1.844 | Acc: 35.644% (36/101)\n",
            "Epoch 3 iteration 200 Loss: 1.825 | Acc: 32.836% (66/201)\n",
            "Epoch 3 iteration 300 Loss: 1.795 | Acc: 32.890% (99/301)\n",
            "Test accuracy: 0.3289036452770233\n",
            "Epoch: 4, Loss: 0.006561217363923788 Updates: 74/30001, Avg Grad: 0.007251451723277569, Threshold: 0.003806133521720767\n",
            "Epoch: 4, Loss: 0.0066270167008042336 Updates: 76/31001, Avg Grad: 0.0023052776232361794, Threshold: 0.0023052776232361794\n",
            "Epoch: 4, Loss: 0.010824027471244335 Updates: 79/32001, Avg Grad: 0.003842094913125038, Threshold: 0.002580490196123719\n",
            "Epoch: 4, Loss: 0.008299704641103745 Updates: 81/33001, Avg Grad: 0.0029188941698521376, Threshold: 0.0029188941698521376\n",
            "Epoch: 4, Loss: 0.00820421613752842 Updates: 84/34001, Avg Grad: 0.0056960200890898705, Threshold: 0.004282559733837843\n",
            "Epoch: 4, Loss: 0.006682008504867554 Updates: 86/35001, Avg Grad: 0.0018689054995775223, Threshold: 0.0018689054995775223\n",
            "Epoch: 4, Loss: 0.006444098893553019 Updates: 89/36001, Avg Grad: 0.003919600043445826, Threshold: 0.002513345330953598\n",
            "Epoch: 4, Loss: 0.008132600225508213 Updates: 91/37001, Avg Grad: 0.001697483123280108, Threshold: 0.001697483123280108\n",
            "Epoch: 4, Loss: 0.007832334376871586 Updates: 94/38001, Avg Grad: 0.0032722181640565395, Threshold: 0.002096883486956358\n",
            "Epoch: 4, Loss: 0.011037008836865425 Updates: 96/39001, Avg Grad: 0.0018247779225930572, Threshold: 0.0018247779225930572\n",
            "Epoch 3 iteration 0 Loss: 1.816 | Acc: 0.000% (0/1)\n",
            "Epoch 3 iteration 100 Loss: 1.685 | Acc: 47.525% (48/101)\n",
            "Epoch 3 iteration 200 Loss: 1.681 | Acc: 41.294% (83/201)\n",
            "Epoch 3 iteration 300 Loss: 1.686 | Acc: 39.203% (118/301)\n",
            "Test accuracy: 0.3920265734195709\n",
            "Epoch: 4, Loss: 0.009864256717264652 Updates: 99/40001, Avg Grad: 0.004855230450630188, Threshold: 0.0030427121091634035\n",
            "Epoch: 4, Loss: 0.007345362100750208 Updates: 101/41001, Avg Grad: 0.0022346426267176867, Threshold: 0.0022346426267176867\n",
            "Epoch: 4, Loss: 0.007441439665853977 Updates: 104/42001, Avg Grad: 0.005529542453587055, Threshold: 0.003399141598492861\n",
            "Epoch: 4, Loss: 0.007288695778697729 Updates: 106/43001, Avg Grad: 0.0038072895258665085, Threshold: 0.0038072895258665085\n",
            "Epoch: 4, Loss: 0.008076298050582409 Updates: 109/44001, Avg Grad: 0.005312707275152206, Threshold: 0.0032910718582570553\n",
            "Epoch: 4, Loss: 0.014589259400963783 Updates: 111/45001, Avg Grad: 0.00198379997164011, Threshold: 0.00198379997164011\n",
            "Epoch: 4, Loss: 0.013590732589364052 Updates: 114/46001, Avg Grad: 0.004987119231373072, Threshold: 0.003419290529564023\n",
            "Epoch: 4, Loss: 0.013372974470257759 Updates: 116/47001, Avg Grad: 0.004220639355480671, Threshold: 0.004220639355480671\n",
            "Epoch: 4, Loss: 0.0061711398884654045 Updates: 119/48001, Avg Grad: 0.006076744757592678, Threshold: 0.003422514069825411\n",
            "Epoch: 4, Loss: 0.0061225187964737415 Updates: 121/49001, Avg Grad: 0.004098773468285799, Threshold: 0.004098773468285799\n",
            "Epoch 4 iteration 0 Loss: 6.868 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 5.033 | Acc: 11.881% (12/101)\n",
            "Epoch 4 iteration 200 Loss: 5.042 | Acc: 11.443% (23/201)\n",
            "Epoch 4 iteration 300 Loss: 4.847 | Acc: 13.621% (41/301)\n",
            "Test accuracy: 0.13621261715888977\n",
            "Epoch: 5, Loss: 0.038395095616579056 Updates: 0/1, Avg Grad: 0.019755691289901733, Threshold: 0.02184233069419861\n",
            "Epoch: 5, Loss: 0.006647367496043444 Updates: 2/1001, Avg Grad: 0.007322040386497974, Threshold: 0.007322040386497974\n",
            "Epoch: 5, Loss: 0.017544429749250412 Updates: 5/2001, Avg Grad: 0.014160407707095146, Threshold: 0.00844516046345234\n",
            "Epoch: 5, Loss: 0.008750918321311474 Updates: 7/3001, Avg Grad: 0.003039358649402857, Threshold: 0.003039358649402857\n",
            "Epoch: 5, Loss: 0.012203450314700603 Updates: 10/4001, Avg Grad: 0.005959733389317989, Threshold: 0.003467416390776634\n",
            "Epoch: 5, Loss: 0.011599557474255562 Updates: 12/5001, Avg Grad: 0.0035654783714562654, Threshold: 0.0035654783714562654\n",
            "Epoch: 5, Loss: 0.012335145846009254 Updates: 15/6001, Avg Grad: 0.006174672394990921, Threshold: 0.004083184525370598\n",
            "Epoch: 5, Loss: 0.006472630426287651 Updates: 17/7001, Avg Grad: 0.0025802310556173325, Threshold: 0.0025802310556173325\n",
            "Epoch: 5, Loss: 0.003082971554249525 Updates: 20/8001, Avg Grad: 0.005328228697180748, Threshold: 0.0035030320286750793\n",
            "Epoch: 5, Loss: 0.01145441085100174 Updates: 22/9001, Avg Grad: 0.0033904635347425938, Threshold: 0.0033904635347425938\n",
            "Epoch 4 iteration 0 Loss: 1.854 | Acc: 0.000% (0/1)\n",
            "Epoch 4 iteration 100 Loss: 1.971 | Acc: 31.683% (32/101)\n",
            "Epoch 4 iteration 200 Loss: 1.978 | Acc: 28.358% (57/201)\n",
            "Epoch 4 iteration 300 Loss: 1.973 | Acc: 23.920% (72/301)\n",
            "Test accuracy: 0.23920266330242157\n",
            "Epoch: 5, Loss: 0.010537936352193356 Updates: 25/10001, Avg Grad: 0.004621044732630253, Threshold: 0.002531616948544979\n",
            "Epoch: 5, Loss: 0.008639303036034107 Updates: 27/11001, Avg Grad: 0.00466338312253356, Threshold: 0.00466338312253356\n",
            "Epoch: 5, Loss: 0.010544948279857635 Updates: 30/12001, Avg Grad: 0.007874859496951103, Threshold: 0.0046802060678601265\n",
            "Epoch: 5, Loss: 0.012233683839440346 Updates: 32/13001, Avg Grad: 0.004825052805244923, Threshold: 0.004825052805244923\n",
            "Epoch: 5, Loss: 0.011558327823877335 Updates: 35/14001, Avg Grad: 0.007347166538238525, Threshold: 0.003831995651125908\n",
            "Epoch: 5, Loss: 0.01058987621217966 Updates: 37/15001, Avg Grad: 0.003746848553419113, Threshold: 0.003746848553419113\n",
            "Epoch: 5, Loss: 0.008355707861483097 Updates: 40/16001, Avg Grad: 0.0047641354613006115, Threshold: 0.002498247195035219\n",
            "Epoch: 5, Loss: 0.012972700409591198 Updates: 42/17001, Avg Grad: 0.003813020884990692, Threshold: 0.003813020884990692\n",
            "Epoch: 5, Loss: 0.008983248844742775 Updates: 45/18001, Avg Grad: 0.004334518685936928, Threshold: 0.0022552902810275555\n",
            "Epoch: 5, Loss: 0.01287421677261591 Updates: 47/19001, Avg Grad: 0.002623502165079117, Threshold: 0.002623502165079117\n",
            "Epoch 4 iteration 0 Loss: 1.439 | Acc: 100.000% (1/1)\n",
            "Epoch 4 iteration 100 Loss: 1.959 | Acc: 23.762% (24/101)\n",
            "Epoch 4 iteration 200 Loss: 1.926 | Acc: 26.368% (53/201)\n",
            "Epoch 4 iteration 300 Loss: 1.874 | Acc: 29.568% (89/301)\n",
            "Test accuracy: 0.29568105936050415\n",
            "Epoch: 5, Loss: 0.006305047310888767 Updates: 50/20001, Avg Grad: 0.006322626024484634, Threshold: 0.004128366243094206\n",
            "Epoch: 5, Loss: 0.0071782986633479595 Updates: 52/21001, Avg Grad: 0.0017959276447072625, Threshold: 0.0017959276447072625\n",
            "Epoch: 5, Loss: 0.0064607043750584126 Updates: 55/22001, Avg Grad: 0.003985161893069744, Threshold: 0.0022355325054377317\n",
            "Epoch: 5, Loss: 0.012718974612653255 Updates: 57/23001, Avg Grad: 0.0022564847022295, Threshold: 0.0022564847022295\n",
            "Epoch: 5, Loss: 0.012970302253961563 Updates: 60/24001, Avg Grad: 0.005364209413528442, Threshold: 0.0026809596456587315\n",
            "Epoch: 5, Loss: 0.005551991518586874 Updates: 62/25001, Avg Grad: 0.0025903680361807346, Threshold: 0.0025903680361807346\n",
            "Epoch: 5, Loss: 0.009764226153492928 Updates: 65/26001, Avg Grad: 0.004920448176562786, Threshold: 0.0027576391585171223\n",
            "Epoch: 5, Loss: 0.005116189830005169 Updates: 67/27001, Avg Grad: 0.003680788679048419, Threshold: 0.003680788679048419\n",
            "Epoch: 5, Loss: 0.006800506263971329 Updates: 70/28001, Avg Grad: 0.003497294383123517, Threshold: 0.0027691377326846123\n",
            "Epoch: 5, Loss: 0.005794666241854429 Updates: 72/29001, Avg Grad: 0.005401118658483028, Threshold: 0.005401118658483028\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4ba3a3bb2dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mc3f1_loss_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_update_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_every_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_thr_per_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_trainer_thresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-5b29447bd894>\u001b[0m in \u001b[0;36mnet_trainer_thresh\u001b[0;34m(model, train_dataloader, test_dataloader, config)\u001b[0m\n\u001b[1;32m     40\u001b[0m                   \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_interval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_tester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f6688c7a8859>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f6688c7a8859>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net.fc1.cb.W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P48pIFgFjVW3",
        "outputId": "9eb7f94a-a628-4236-9916-02ac35abed3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5.2074e-05, 5.2074e-05, 4.9672e-05,  ..., 9.7597e-05, 9.5624e-05,\n",
            "         9.8300e-05],\n",
            "        [5.2284e-05, 5.2284e-05, 5.2047e-05,  ..., 9.8921e-05, 8.7966e-05,\n",
            "         9.0470e-05],\n",
            "        [5.1894e-05, 5.4662e-05, 5.6103e-05,  ..., 1.1025e-04, 1.0725e-04,\n",
            "         9.4334e-05],\n",
            "        ...,\n",
            "        [1.1123e-04, 9.3991e-05, 9.8402e-05,  ..., 1.0641e-04, 8.2949e-05,\n",
            "         9.7641e-05],\n",
            "        [1.1752e-04, 1.0429e-04, 9.8852e-05,  ..., 8.2507e-05, 9.0317e-05,\n",
            "         8.9049e-05],\n",
            "        [1.1125e-04, 9.5968e-05, 1.1941e-04,  ..., 9.7233e-05, 1.0448e-04,\n",
            "         1.0215e-04]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = ResNet18()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\" : 20,\n",
        "    \"batch_size\" : 800,\n",
        "    \"gamma\" : 1,\n",
        "    \"naive_loss_thr\" : 2,\n",
        "    'learning_rate' : 0.0002, #0.002,\n",
        "    \"log_interval\" : 1000,\n",
        "    \"momentum\": 0.9,\n",
        "    \"max_thresh_multiplier\": 2,\n",
        "    \"test_interval\": 10000,\n",
        "    \"dampening\": 0.1\n",
        "}\n",
        "\n",
        "# we know for this dataset the max n_epoch = 50,000\n",
        "def calc_gamma(lr, m_epoch):\n",
        "    return np.log(lr)/(-lr*m_epoch)\n",
        "\n",
        "train_config['gamma'] = calc_gamma(train_config['learning_rate'], 50000)\n",
        "#optimizer = ManhattanSGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'])\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "with open('log_baseline_train.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"train_loss\", \"train_acc\"])\n",
        "\n",
        "with open('log_baseline_test.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"iteration\", \"test_loss\", \"test_acc\"])\n",
        "\n",
        "c3f1_loss_per_epoch, c3f1_update_per_epoch, c3f1_every_loss = net_trainer_manh(net, trainloader, testloader, train_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "rlwfNdrrZvn6",
        "outputId": "5d704727-0591-4e2e-8e77-f3a6a227b91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 iteration 0 Loss: 6.252 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 5.950 | Acc: 10.891% (11/101)\n",
            "Epoch 0 iteration 200 Loss: 5.982 | Acc: 8.955% (18/201)\n",
            "Epoch 0 iteration 300 Loss: 6.140 | Acc: 8.970% (27/301)\n",
            "Epoch 0 iteration 400 Loss: 6.115 | Acc: 8.978% (36/401)\n",
            "Test accuracy: 0.08977556228637695\n",
            "Epoch: 1, Loss: 0.002918248064815998 Updates: 1/1, Avg Grad: 0.0001863234501797706\n",
            "Epoch: 1, Loss: 0.0011225983034819365 Updates: 2/1001, Avg Grad: 0.0039987629279494286\n",
            "Epoch: 1, Loss: 0.0012948412913829088 Updates: 3/2001, Avg Grad: 0.0043893177062273026\n",
            "Epoch: 1, Loss: 0.005354406777769327 Updates: 4/3001, Avg Grad: 0.007110339589416981\n",
            "Epoch: 1, Loss: 0.01574348285794258 Updates: 6/4001, Avg Grad: 0.03171857073903084\n",
            "Epoch: 1, Loss: 0.0023952170740813017 Updates: 7/5001, Avg Grad: 0.004082971252501011\n",
            "Epoch: 1, Loss: 0.002471892163157463 Updates: 8/6001, Avg Grad: 0.023377755656838417\n",
            "Epoch: 1, Loss: 0.0008532585925422609 Updates: 9/7001, Avg Grad: 0.0041057695634663105\n",
            "Epoch: 1, Loss: 0.0038892687298357487 Updates: 11/8001, Avg Grad: 0.004134806804358959\n",
            "Epoch: 1, Loss: 0.0021097285207360983 Updates: 12/9001, Avg Grad: 0.02214488945901394\n",
            "Epoch 0 iteration 0 Loss: 1.875 | Acc: 0.000% (0/1)\n",
            "Epoch 0 iteration 100 Loss: 4.513 | Acc: 11.881% (12/101)\n",
            "Epoch 0 iteration 200 Loss: 4.492 | Acc: 13.433% (27/201)\n",
            "Epoch 0 iteration 300 Loss: 4.473 | Acc: 13.953% (42/301)\n",
            "Epoch 0 iteration 400 Loss: 4.338 | Acc: 13.965% (56/401)\n",
            "Test accuracy: 0.1396508663892746\n",
            "Epoch: 1, Loss: 0.007514900527894497 Updates: 13/10001, Avg Grad: 0.004201293922960758\n",
            "Epoch: 1, Loss: 0.0015411082422360778 Updates: 14/11001, Avg Grad: 0.022664891555905342\n",
            "Epoch: 1, Loss: 0.01275691855698824 Updates: 16/12001, Avg Grad: 0.029628178104758263\n",
            "Epoch: 1, Loss: 0.005112815648317337 Updates: 17/13001, Avg Grad: 0.004261714871972799\n",
            "Epoch: 1, Loss: 0.006022932007908821 Updates: 18/14001, Avg Grad: 0.02558842860162258\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bde4599433f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"iteration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mc3f1_loss_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_update_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc3f1_every_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_trainer_manh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-84e9e9606f55>\u001b[0m in \u001b[0;36mnet_trainer_manh\u001b[0;34m(model, train_dataloader, test_dataloader, config)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mevery_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}